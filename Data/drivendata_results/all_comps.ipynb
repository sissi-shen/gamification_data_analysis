{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc72d340-2bfb-41d5-8818-1c418f42a8da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75271355-af48-4a5a-8784-be52f29844b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_comps = []\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Welcome to the Video Similarity Challenge! In this competition, you will build models that detect whether a query video contains a possibly manipulated clip from one or more videos in a reference set. The ability to identify and track content on social media platforms, called content tracing, is crucial to the experience of users on these platforms. Previously, Meta AI and DrivenData hosted the Image Similarity Challenge in which participants developed state-of-the-art models capable of accurately detecting when an image was derived from a known image. The motivation for detecting copies and manipulations with videos is similar -- enforcing copyright protections, identifying misinformation, and removing violent or objectionable content. \\n\n",
    "For the Descriptor Track, your goal is to generate useful vector representations of videos for this video similarity task. You will submit descriptors for both query and reference set videos. A standardized similarity search using pair-wise inner-product similarity will be used to generate ranked video match predictions. For the Matching Track, your goal is to create a model that directly detects which specific clips of a query video correspond to which specific clips in one or more videos in a large corpus of reference videos. You will submit predictions indicating which portions of a query video are derived from a reference video.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\"For this competition, Meta AI has compiled a new dataset composed of approximately 100,000 videos derived from the YFCC100M dataset. This dataset has been divided for the purposes of this competition into a training set, which participants may use to train their models, and a test set on which training is prohibited. Both the train and test sets are further divided into a set of reference videos, and a set of query videos that may or may not contain content derived from one or more videos in the reference set. Query videos are, on average, about 40 seconds long, while reference videos are, on average, about 30 seconds long. In the case of the training set, a set of labels is included indicating which query videos contain derived content.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" Submissions will be evaluated by a segment-matching version of micro-average precision, also known as global average precision. This is related to the common classification metric, average precision, also known as the area under the precision-recall curve or the average precision over recall values. The micro-average precision metric will be computed based on the matches the participants submit, and the result will be published on the Phase 1 leaderboard (remember that this is not the final leaderboard).\"\"\"\n",
    "    ,\n",
    "    \"url\" : \"https://www.drivendata.org/competitions/group/meta-video-similarity/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Privacy-enhancing technologies (PETs) have the potential to unlock more trustworthy innovation in data analysis and machine learning. Federated learning is one such technology that enables organisations to analyse sensitive data while providing improved privacy protections. These technologies could advance innovation and collaboration in new fields and help harness the power of data to tackle some of our most pressing societal challenges. That’s why the U.S. and U.K. governments are partnering to deliver a set of prize challenges to unleash the potential of these democracy-affirming technologies to make a positive impact. In particular, this challenge will tackle two critical problems via separate data tracks: Data Track A will help with the identification of financial crime, while Data Track B will bolster pandemic responses. \\n\n",
    "The goal of this prize challenge is to mature federated learning approaches and build trust in their adoption. The challenge organisers hope to accelerate the development of efficient privacy-preserving federated learning solutions that leverage a combination of input and output privacy techniques to: (1) Drive innovation in the technological development and application of novel privacy-enhancing technologies. (2) Deliver strong privacy guarantees against a set of common threats and privacy attacks. (3) Generate effective models to accomplish a set of predictive or analytical tasks that support the use cases. \\n \n",
    "In Phase 3 of the challenge, you will develop and conduct privacy attacks against the solutions of Phase 2's blue team finalists. The results of your attacks will inform the final rankings of the blue teams. Additionally, you will be considered for red team prizes based on the quality of your attacks and reported results.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"n/a\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" Blue teams: Solutions should aim to: (1) Provide robust privacy protection for the collaborating parties. (2) Minimise loss of overall accuracy in the model. (3) Minimise additional computational resources (including compute, memory, communication), as compared to a non-federated learning approach. In addition to this, the evaluation process will reward competitors who: (1) Show a high degree of novelty or innovation. (2) Demonstrate how their solution (or parts of it) could be applied or generalised to other use cases. (3) Effectively prove or demonstrate the privacy guarantees offered by their solution, in a form that is comprehensible to data owners or regulators. (3) Consider how their solution could be applied in a production environment. \\n\n",
    "Red teams: Effectiveness (40%): How completely does the attack break or test the privacy claims made by the target solution? (e.g. what portion of user data is revealed, and how accurately is it reconstructed)?; Applicability/Threat Model (30%): How realistic is the attack? How hard would it be to apply in a practical deployment?; Generalizability (20%): Is the attack specific to the target solution, or does it generalise to other solutions? Innovation (10%): How significantly does the attack improve on the state-of-the-art?\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"url\" : \"https://www.drivendata.org/competitions/group/uk-federated-learning/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Forests are adding and removing carbon dioxide from the air all the time. How do we know how much? The answer lies in aboveground biomass (AGBM), a widespread measure in the study of carbon release and sequestration by forests. Current approaches to measuring biomass, range from destructive sampling, which involves cutting down a representative sample of trees and measuring attributes such as the height and width of their crowns and trunks, to remote sensing methods. Remote sensing methods offer a much faster, less destructive, and more geographically expansive biomass estimate. In turn, such timely and detailed information allows landowners and policy makers to make better decisions for the conservation of forests. The goal of the BioMassters challenge was to estimate the yearly biomass of 2,560 meter by 2,560 meter patches of land in Finland's forests using Sentinel-1 and Sentinel-2 imagery of the same areas on a monthly basis. The ground truth for this competition came from LiDAR (Light Detection And Ranging) and in-situ measurements of the same forests collected by the Finnish Forest Centre on an annual basis.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" The feature data for this challenge is imagery collected by the Sentinel-1 and Sentinel-2 satellite missions for nearly 13,000 patches of forest in Finland. Each patch (also called a \"chip\") represents a different 2,560-by-2,560-meter area of forest. The data were collected over a period of 5 years between 2016 and 2021. Satellite imagery for the challenge is made available in public AWS S3 buckets.\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" To measure your model’s performance, we’ll use a metric called Average Root Mean Square Error (RMSE). RMSE is the square root of the mean of squared differences between estimated and observed values. RMSE will be calculated on a per-pixel basis (i.e., each pixel in your submitted tif for a patch will be compared to the corresponding pixel in the ground-truth tif for the patch). RMSE will be calculated for each image, and then averaged over all images in the test set. This is an error metric, so a lower value is better.\"\"\"\n",
    "    ,\n",
    "    \"url\" : \"https://www.drivendata.org/competitions/99/biomass-estimation/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Did Mars ever have livable environmental conditions? NASA missions like the Curiosity and Perseverance rovers carry a rich array of instruments that can collect data — such as the chemical makeup of rock and soil samples — to help build evidence around this question. However, these instruments cannot currently analyze samples automatically. This capability could help missions to guide science operations, reduce reliance on \"ground-in-the-loop\" analysis, and prioritize transmission over increasingly long distances. \\n\n",
    "The goal of this research challenge was to to build a model to automatically analyze data collected using a method of chemical analysis—gas chromatography–mass spectrometry (GCMS)—that is performed by the Curiosity rover's SAM instrument suite on soil and rock samples. Competitors were tasked with detecting certain families of chemical compounds that are of scientific interest in analyzing conditions for past habitability. This challenge built on the previous DrivenData competition Mars Spectrometry: Detect Evidence for Past Habitability, where participants developed approaches for automated analysis of data collected using evolved gas analysis (EGA), another type of chemical analysis performed by the SAM instrument suite.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" The data for this challenge comes from laboratory instruments at NASA's Goddard Space Flight Center that are operated by the Sample Analysis at Mars (SAM) science team. SAM is an instrument suite aboard the Curiosity rover on Mars. The data was collected by commercially manufactured instruments that have been configured as SAM analogs at Goddard. For more about SAM and the SAM team, see the \"About\" page.\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" The performance metric evaluated on the validation set will be used for leaderboard ranking while the competition is open, but will not be used for final ranking and prize determination. Labels for the validation set will be released at the beginning of Phase 2: Final Training so that you will have more data available for training your final model. Final ranking and prizes will be based on performance on the test set. There is also a bonus prize that will be awarded to the best write-up among the top 5 performers.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"url\" : \"https://www.drivendata.org/competitions/97/nasa-mars-gcms/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" The objective of the challenge is to develop a privacy-preserving federated learning solution that is capable of training an effective model while providing a demonstrable level of privacy against a broad range of privacy threats. For the purposes of this challenge, a privacy-preserving solution is defined as one which is able to provably ensure that sensitive information in the datasets remain confidential to the respective data owners across the machine learning lifecycle. This requires the raw data is protected during training (input privacy), and that it also cannot be reverse-engineered during inference (output privacy). \\n\n",
    "In Phase 1, your goal is to write a concept paper describing a privacy-preserving federated learning solution that tackles one or both of two tasks: financial crime prevention or pandemic forecasting. The challenge organizers are interested in efficient and usable federated learning solutions that provide end-to-end privacy and security protections while harnessing the potential of AI for overcoming significant global challenges. In Phase 2 of the challenge, you will develop a working prototype of the privacy-preserving federated learning solution that you proposed in Phase 1. As part of this phase, you will package your solution for containerized execution in a common environment provided for the testing, evaluation and benchmarking of solutions. In Phase 3 of the challenge, you will develop and conduct privacy attacks against the solutions of Phase 2's blue team finalists. The results of your attacks will inform the final rankings of the blue teams. Additionally, you will be considered for red team prizes based on the quality of your attacks and reported results.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" Innovators will use synthetic datasets representing data held by the SWIFT global payments network and by its partner banks. You will be provided with two datasets: Dataset 1: A synthetic dataset representing transaction data from the SWIFT global payment network. Dataset 2: Synthetic customer / account metadata, including flags, from SWIFT's partner banks\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" The concept paper will be evaluated according to the following criteria, in order of importance: Privacy, Innovation, Efficiency and Scalability, Accuracy, Technical Understanding,  Feasibility, Usability and Explainability, Generalizability. \\n\n",
    "The evaluation metric will be Area Under the Precision–Recall Curve (AUPRC), also known as average precision (AP), PR-AUC, or AUCPR. This is a commonly used metric for binary classification that summarizes model performance across all operating thresholds. This metric rewards models which can consistently assign anomalous transactions with a higher confidence score than negative non-anomalous transactions. \\n\n",
    "Red teams: Effectiveness (40%): How completely does the attack break or test the privacy claims made by the target solution? (e.g. what portion of user data is revealed, and how accurately is it reconstructed)?; Applicability/Threat Model (30%): How realistic is the attack? How hard would it be to apply in a practical deployment?; Generalizability (20%): Is the attack specific to the target solution, or does it generalise to other solutions? Innovation (10%): How significantly does the attack improve on the state-of-the-art? \"\"\"\n",
    "    ,\n",
    "    \"url\" : \"https://www.drivendata.org/competitions/group/nist-federated-learning/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Cook Inlet belugas are an endangered population of beluga whales at risk for extinction after years of hunting, and which continue to face threats related to vessel traffic in the busy Cook Inlet waterway. In order to more closely monitor their health and track individual whales, the NOAA Alaska Fishery Science Center conducts an annual photo-identification survey of Cook Inlet belugas. But processing and analyzing new whale images is largely manual, consuming significant time and resources. New and improved methods are needed to help automate this process and accurately identify matches of the same individual whale across different survey images. The goal of this challenge is to help wildlife researchers accurately identify endangered Cook Inlet beluga whale individuals from photographic images. Specifically, the task is query ranking for images of individual whales given a query image against an image database, which is a key step in the full photo-identification process. The Bureau of Ocean Energy Management (BOEM) is looking for an automated solution to integrate into the existing Flukebook platform. Accelerated and scalable photo-identification of individuals is critical to effective population assessment, management, and protection for the Cook Inlet belugas. Machine learning has the potential to expedite the creation and analysis of large datasets of endangered beluga whale images.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" The images in this dataset come from a sub-population of critically endangered beluga whales that live in Cook Inlet, Alaska. To help protect these animals from the risk of extinction, the Marine Mammal Laboratory at the NOAA Alaska Fishery Science Center conducts an annual photographic survey of Cook Inlet belugas to more closely monitor and track individual whales. The photographs are then passed through an auto-detection algorithm developed by Wild Me which draws bounding boxes around each whale and then rotates and crops the image. These cropped images are what you will be working with for this competition.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" This challenge uses a scenario-based evaluation approach to test how well your model performs under various conditions of the query image and database images. The test set consists of multiple scenarios, with each scenario defining a distinct set of query images and a distinct database. The scenarios have been constructed to test performance of the following aspects of the task: Query top-view images against a database of top-view images; Query top-view images against a database of top-view images from the same year (e.g., 2017 against 2017; 2018 against 2018); Query top-view images against a database of top-view images from a previous year (e.g., 2019 against 2018; 2019 against 2017); Query lateral-view images against a database of top-view images; Query top-view images against a database of lateral-view images.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"url\" : \"https://www.drivendata.org/competitions/96/beluga-whales/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Can you classify the wildlife species that appear in camera trap images collected by conservation researchers? Welcome to the African jungle! In recent years, automated surveillance systems called camera traps have helped conservationists study and monitor a wide range of ecologies while limiting human interference. Camera traps are triggered by motion or heat, and passively record the behavior of species in the area without significantly disturbing their natural tendencies. However, camera traps also generate a vast amount of data that quickly exceeds the capacity of humans to sift through. That's where machine learning can help! Advances in computer vision can help automate tasks like species detection and classification, localization, depth estimation, and individual identification so humans can more effectively learn from and protect these ecologies. Welcome to Taï National Park, data scientists! \\n\n",
    "In this challenge, your goal is to classify the species that appear in camera trap images collected by our research partners at the Wild Chimpanzee Foundation and the Max Planck Institute for Evolutionary Anthropology. As mentioned in the about page, camera traps are one of the best tools available to study and monitor wildlife populations, and the enormous amounts of data they provide can be used to track different species for conservation efforts—once they are processed.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" For this challenge, you are provided with images, along with a few attributes of each image that might be helpful in setting up your training and testing sets. The images for the training and testing sets are in train_features and test_features respectively, and additional information about each image is in train_features.csv and test_features.csv. Let's take a look at a few of our featured critters!\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" To measure your model's accuracy by looking at prediction error, we`ll use a metric called log loss. This is an error metric, so a lower value is better (as opposed to an accuracy metric, where a higher value is better).\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"url\" : \"https://www.drivendata.org/competitions/87/competition-image-classification-wildlife-conservation/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Did Mars ever have livable environmental conditions? NASA missions like the Curiosity and Perseverance rovers carry a rich array of instruments that can collect data — such as the chemical makeup of rock and soil samples — to help build evidence around this question. However, these instruments cannot currently analyze samples automatically. This capability could help missions to guide science operations, reduce reliance on \"ground-in-the-loop\" analysis, and prioritize transmission over increasingly long distances. The goal of this research challenge was to build a model to automatically analyze mass spectrometry data collected for Mars exploration. Models were tasked with detecting certain families of chemical compounds that are of scientific interest in analyzing conditions for past habitability.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" The data from this challenge comes from laboratory instruments at NASA's Goddard Space Flight Center and Johnson Space Center that are affiliated with the Sample Analysis at Mars (SAM) science team. SAM is an instrument suite aboard the Curiosity rover on Mars. For more about SAM and the SAM team, see the \"About\" page.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" The performance metric evaluated on the validation set will be used for leaderboard ranking while the competition is open, but will not be used for final ranking and prize determination. Labels for the validation set will be released at the beginning of Phase 2: Final Training on March 18, 2022 so that you will have more data available for training your final model. Final ranking and prizes will be based on performance on the test set. There is also a bonus prize awarded for best modeling methodology for the SAM testbed samples, with eligible finalists selected based on their performance on the SAM testbed samples within the test set.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"url\" : \"https://www.drivendata.org/competitions/93/nasa-mars-spectrometry/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Coordinating our nation’s airways is the role of the National Airspace System (NAS). The NAS is arguably the most complex transportation system in the world. Operational changes can save or cost airlines, taxpayers, consumers, and the economy at large thousands to millions of dollars on a regular basis. It is critical that decisions to change procedures are done with as much lead time and certainty as possible. The NAS is investing in new ways to bring vast amounts of data together with state-of-the-art machine learning to improve air travel for everyone. An important part of this equation is airport configuration, the combination of runways used for arrivals and departures and the flow direction on those runways. For example, one configuration may use a set of runways in a north-to-south flow (or just \"south flow\") while another uses south-to-north flow (\"north flow\"). Air traffic officials may change an airport configuration depending on weather, traffic, or other inputs. \\n\n",
    "The goal of this challenge is to automatically predict airport configuration changes from real-time data sources including air traffic and weather. Better algorithms for predicting future airport configurations can support critical decisions, reduce costs, conserve energy, and mitigate delays across the national airspace network. The challenge involved three phases. In the Open Arena, anyone could explore the data and submit predictions to see how they fared against others on the open leaderboard. The Prescreened Arena, available to U.S. university-affiliated participants, offered participants an opportunity to further develop their models and package their code to submit for the final evaluation. In the final evaluation phase, we ran the participants' code on a brand new dataset to see how well their models generalized to data that was not available during training.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" This challenge is possible because of the effort that NASA, the FAA, airlines, and other agencies undertake to collect, process, and distribute data to decision makers in near real-time. You will be working with around a year of historical data, but any solution you develop using this data could be translated directly into a pipeline with access to these same features in real-time.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\"To measure your model's accuracy by looking at prediction error, we'll use a metric called log loss. This is an error metric, so a lower value is better (as opposed to an accuracy metric, where a higher value is better). \"\"\"\n",
    "    ,\n",
    "    \"url\" : \"https://www.drivendata.org/competitions/group/competition-nasa-airport-configuration/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Air pollution is one of the greatest environmental threats to human health. Currently, no single satellite instrument provides ready-to-use, high resolution information on surface-level air pollutants, while existing high-quality ground monitors are expensive and have large gaps in coverage. This gap in information means that millions of people cannot take daily action to protect their health. Models that make use of widely available satellite data have the potential to provide local, daily air quality information. The goal of this challenge was to use remote sensing data and other geospatial data sources to estimate daily levels of air pollution with high spatial resolution (5km by 5km). This competition focused on two critical air quality measures: particulate matter less than 2.5 micrometers in size (PM2.5) and nitrogen dioxide (NO2).\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" There are two types of pre-approved data that can be used as input to your model: satellite and ancillary (meteorological and topological) data. We provide satellite data through a public s3 bucket hosted by DrivenData. These files have already been subsetted to the correct times and geographies. Ancillary data can be accessed through public portals. You may use any approved data sources you like, but you must use at least one dataset from the list of approved satellite data.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\"To measure your model’s performance, we’ll use a metric called the coefficient of determination R2 (R squared). R2 indicates the proportion of the variation in the dependent variable that is predictable from the independent variables. This is an accuracy metric, so a higher value is better.\"\"\"\n",
    "    ,\n",
    "    \"url\" : \"https://www.drivendata.org/competitions/group/competition-air-quality/\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87bb3de6-39b8-4fdc-918f-a3091b7f95d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02e7957e-9f8c-4546-b5ff-cf241239b7df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Seasonal mountain snowpack is a critical water resource throughout the Western U.S. Snow water equivalent (SWE) is the most commonly used measurement in water forecasts because it combines information on snow depth and density. While ground-based instruments can be used to monitor snowpacks, ground stations tend to be spatially limited and are not easily installed at high elevations. Given the diverse landscape in the Western U.S. and shifting climate, new and improved methods are needed to accurately measure SWE at a high spatiotemporal resolution to inform water management decisions. \\n\n",
    "The goal of this challenge was to estimate SWE in real-time each week for 1km x 1km grid cells across the Western U.S. Participants could use real-time satellite, ground station, and climate data, as well as static data sources on elevation, soil, water, and land cover. During the development phase, participants tested submissions using historical data. Then, for the first time, predictions generated by participants each week were scored against new ground truth measures as they came in! In total, competitors were evaluated against over 42,000 ground truth SWE measurements spanning 20 weeks across the 2022 peak snow and melt seasons.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" n/a\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\"To measure your model’s performance, we’ll use a metric called Root Mean Square Error (RMSE), which is a measure of accuracy and quantifies differences between estimated and observed values. RMSE is the square root of the mean of the squared differences between the predicted values and the actual values. This is an error metric, so a lower value is better.\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/group/competition-reclamation-snow-water/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Satellite imagery is critical for a wide variety of applications from disaster management and recovery, to agriculture, to military intelligence. Sentinel-2 multispectral imagery in particular has been used in applications like tracking erupting volcanos, mapping deforestation, and monitoring wildfires. A major obstacle for all of these use cases is the presence of clouds, which introduce noise and inaccuracy in image-based models. As a result, clouds usually have to be identified and removed to most effectively use these satellite data sources. The goal of this challenge was to most accurately detect cloud cover in multispectral satellite imagery from the Sentinel-2 mission. Algorithms submitted by participants were run on test imagery to produce cloud masks, which were compared against human cloud annotations. The availability of labeled data has been a major obstacle for cloud detection efforts, and this challenge featured a unique set of human-verified labels spanning imagery and cloud conditions across three continents.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\"The dataset consists of Sentinel-2 satellite imagery stored as GeoTiffs. There are almost 12,000 chips in the training data, collected between 2018 and 2020. Each chip is imagery of a specific area captured at a specific point in time. Sentinel-2 flies over the part of the Earth between 56° South (Cape Horn, South America) and 82.8° North (above Greenland), so our observations are all between these two latitudes. The chips are mostly from Africa, South America, and Australia. For more background about how data from Sentinel-2 is collected, see the About page.\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" To measure your model’s performance, we’ll use a metric called Jaccard index, also known as Generalized Intersection over Union (IoU). Jaccard index is a similarity measure between two label sets. In this case, it is defined as the size of the intersection divided by the size of the union of non-missing pixels. In this competition there should be no missing data. Because it is an accuracy metric, a higher value is better. \"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/83/cloud-cover/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" To protect the Earth's natural resources amid environmental and human pressures, conservationists need to be able to monitor species population sizes and population change. Camera traps are widely used in conservation research to capture images and videos of wildlife without human interference. Using statistical models for distance sampling, the frequency of animal sightings can be combined with the distance of each animal from the camera to estimate a species' full population size. However, getting distances from camera trap footage currently entails an extremely manual, time-intensive process. This creates a bottleneck for conservationists working to understand and protect wild animals and their ecosystems. The goal of this challenge was to use machine learning and advances in monocular (single-lens) depth estimation techniques to automatically estimate the distance between a camera trap and an animal contained in its video footage. The challenge drew on a unique labeled dataset from research teams from the Max Planck Institute for Evolutionary Anthropology (MPI-EVA) and the Wild Chimpanzee Foundation (WCF). Participants were evaluated based on how accurately their automated algorithms could predict the distance between the camera and wildlife at each point in time.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" The main features in this challenge are the videos themselves. Each video is up to a minute long and was captured automatically by a motion-triggered camera. Videos capture six different species: bushbucks, chimpanzees, duikers, elephants, leopards, and monkeys. Duikers are the most commonly seen, while leopards and elephants are the most rare. There are roughly 3,900 videos, split across the train and test sets. Note that height, width, and length of videos are not consistent across the data. Videos also vary in quality.\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\"Performance is evaluated according to Mean Absolute Error (MAE), which measures how much the estimated values differ from the observed values. MAE is the mean of the magnitude of the differences between the predicted values and the ground truth. MAE is always non-negative, with lower values indicating a better fit to the data. The competitor that minimizes this metric will top the leaderboard.\n",
    "\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/82/competition-wildlife-video-depth-estimation/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Flooding is the most frequent and costly natural disaster in the world. During a flood event, it is critical that governments and humanitarian organizations be able to accurately measure flood extent in near real-time to strengthen early warning systems, assess risk, and target relief. Yet ground measures only measure water height, are spatially limited, and can be expensive to maintain. High resolution synthetic-aperture radar (SAR) imaging has strengthened monitoring systems by providing data in otherwise inaccessible areas at frequent time intervals. By operating in the microwave band of the electromagnetic spectrum, SAR can capture images through clouds, precipitation, smoke, and vegetation, making it especially valuable for flood detection. The goal of this challenge was to build machine learning algorithms that can map floodwater using Sentinel-1 global SAR imagery along with supplementary data on elevation and permanent water. To support the development of models, a newly updated dataset of satellite images captured between 2016 and 2020 was prepared and labeled by Cloud to Street and made available through Microsoft’s Planetary Computer.\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" This dataset consists of Sentinel-1 radar images stored as GeoTIFFs. Additionally, you are given a set of metadata for the training set that contains country and date information for each chip.\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" To measure your model’s performance, we’ll use a metric called Jaccard index, also known as Generalized Intersection over Union (IoU). Jaccard index is a similarity measure between two label sets. In this case, it is defined as the size of the intersection divided by the size of the union of non-missing pixels. This computation excludes predictions on missing data. Because it is an accuracy metric, a higher value is better.\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/81/detect-flood-water/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Welcome to the Image Similarity Challenge! In this competition, you will be building models that help detect whether a given query image is derived from any of the images in a large reference set. Content tracing is a crucial component on all social media platforms today, used for such tasks as flagging misinformation and manipulative advertising, preventing uploads of graphic violence, and enforcing copyright protections. But when dealing with the billions of new images generated every day on sites like Facebook, manual content moderation just doesn't scale. They depend on algorithms to help automatically flag or remove bad content. This competition allows you to test your skills in building a key part of that content tracing system, and in so doing contribute to making social media more trustworthy and safe for the people who use it.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" For this competition Facebook has compiled a new dataset, the Dataset for ISC21 (DISC21), composed of 1 million reference images and an accompanying set of 50,000 query images. A subset of the query images have been derived in some way from the reference images, and the rest of the query images have not.\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" Submissions will be evaluated by their micro-average precision, also known as the area under the precision-recall curve or the average precision over recall values. This metric will be computed against a held-out test set that no participants have access to, and the result will be published on the Phase 1 leaderboard (remember that this is not the final leaderboard).\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/group/image-similarity-challenge/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Overhead satellite imagery provides critical time-sensitive information for use areas like disaster response, navigation, and security. Most current methods for using aerial imagery assume images are taken from directly overhead, or “near-nadir”. However, the first images available are often taken from an angle, or are “oblique”. Effects from these camera orientations complicate useful tasks like change detection, vision-aided navigation, and map alignment. In this challenge, your goal is to make satellite imagery taken from a significant angle more useful for time-sensitive applications like disaster and emergency response. To take on the challenge, you will transform RGB images taken from a satellite to more accurately determine each object’s real-world structure or “geocentric pose”. Geocentric pose is an object’s height above the ground and its orientation with respect to gravity. Calculating geocentric pose helps with detecting and classifying objects and determining object boundaries.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" The data set for this challenge includes satellite images of four cities: Jacksonville, Florida, USA; Omaha, Nebraska, USA; Atlanta, Georgia, USA; and San Fernando, Argentina. There are a total of 5,923 training images and 1,025 test images.\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" Submissions will be evaluated using the coefficient of determination R2, which is a form of squared error normalized by the value range. Test locations have rural, suburban, and urban scenes, each with different value ranges for object heights and their corresponding flow vectors. For leaderboard evaluation, R2 for heights and flow vectors will be assessed for each geographic location independently and then averaged to produce a final score.\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/78/overhead-geopose-challenge/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" The transfer of energy from solar wind to Earth's magnetic field can cause massive geomagnetic storms, wreaking havoc on key infrastructure systems like GPS, satellite communication, and electric power transmission. The severity of these geomagnetic storms is measured by the Disturbance Storm-time Index, or Dst. In the past three decades, empirical, physics-based, and machine learning models have made advances in forecasting Dst from real-time solar wind data. However, predicting extreme geomagnetic events remains especially hard, and robust solutions are needed that can work with raw, real-time data streams under realistic conditions like sensor malfunctions and noise. The goal of this challenge was to develop models for forecasting Dst that 1) push the boundary of predictive performance, 2) under operationally viable constraints, and 3) using specified real-time solar-wind data feeds. This is a hard problem where the best approaches are not evident at the outset. Competitors were tasked with improving forecasts both for the current Dst value (t0) and Dst one hour in the future (t1). Participants needed to submit code that could execute in a simulated real-time environment with operations constraints on runtime and programming inputs.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" The input data for this challenge is composed of solar wind measurements collected from two satellites: NASA's Advanced Composition Explorer (ACE) and NOAA's Deep Space Climate Observatory (DSCOVR). Your goal is to predict the Disturbance Storm-Time Index (Dst), a measure of magnetic activity, from the provided data up to the time of prediction.\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" Performance is evaluated according to Root Mean Squared Error (RMSE). RMSE will be calculated on t0 and t+1 simultaneously.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/73/noaa-magnetic-forecasting/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Hurricanes can cause upwards of 1,000 deaths and $50 billion in damages in a single event, and have been responsible for well over 160,000 deaths globally in recent history. During a tropical cyclone, humanitarian response efforts hinge on accurate risk approximation models that depend on wind speed measurements at different points in time throughout a storm’s life cycle. For several decades, forecasters have relied on visual pattern recognition of complex cloud features in visible and infrared imagery. While the longevity of this technique indicates the strong relationship between spatial patterns and cyclone intensity, visual inspection is manual, subjective, and often leads to inconsistent estimates between even well-trained analysts. \\n\n",
    "There is a vital need to develop automated, objective, and accurate tropical cyclone intensity estimation tools from satellite image data. In 2018, the NASA IMPACT team launched an experimental framework to investigate the applicability of deep learning-based models for estimating wind speeds in near-real time. The goal of the Wind-dependent Variables challenge was to improve on this model using satellite images captured throughout a storm’s life cycle. To build their solutions, participants drew on a dataset of single-band satellite images and wind speed annotations from over 600 storms prepared by the NASA IMPACT team and Radiant Earth Foundation.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" The features in this dataset are the images themselves. Each image is 366 x 366 pixels. The train set consists of 70,257 images and the test set consists of 44,377 images. The images were captured by Geostationary Operational Environmental Satellites (GOES) positioned in a geostationary orbit around the Earth to be able to capture images at high temporal frequency. The data included in this competition is from band #13 (10.3 microns), which is a long-wave infrared frequency. Clouds have high brightness at this frequency, which helps to better capture the spatial structure of a storm and is key to estimating wind speed.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" To measure your model’s performance, we’ll use a metric called Root Mean Square Error (RMSE), which is a measure of accuracy and quantifies differences between estimated and observed values. RMSE is the square root of the mean of the squared differences between the predicted values and the actual values. RMSE is always non-negative, with lower values indicating a better fit to the data.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/72/predict-wind-speeds/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" US presidential elections come but once every 4 years, and this one's a big one. The new president will help shape policies on the pandemic response, healthcare, the environment, the economy, and more. There are lots of people trying to predict what will happen. Can you top them? In this challenge, you are asked to predict the fraction of each state that will vote for each major candidate. You can use any data that is freely available to the public. Come election night (or election week... or election month), we'll see who's model had the most accurate vision for the country!\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" We don't provide any one set of feature data that you must use. Instead, you can use any publicly available data to make your predictions. For some suggestions, check out the resources page!\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" Submissions will be scored according to the root mean squared error metric, which will quantify how far from the true vote shares your regression predictions are. This metric is implemented in Scikit Learn, but you'll need to set the squared parameter to False. See the submissions page for more information.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/71/election-competition-2020/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Large data sets containing personally identifiable information (PII) are exceptionally valuable resources for research and policy analysis in a host of fields supporting America's First Responders such as emergency planning and epidemiology. Temporal map data is of particular interest to the public safety community in applications such as optimizing response time and personnel placement, natural disaster response, epidemic tracking, demographic data and civic planning. Yet, the ability to track a person's location over a period of time presents particularly serious privacy concerns. The Differential Privacy Temporal Map Challenge required participants to develop algorithms that preserve data utility as much as possible while guaranteeing individual privacy is protected. The challenge featured a series of coding sprints to apply differential privacy methods to temporal map data, where each record is tied to a location and each individual may contribute to a sequence of events.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" Spain 1: The provided data set for this sprint consists of a 1,455,609 row CSV file which records 911 calls that occurred in Baltimore in 2019. As a reminder, for purposes of differential privacy, the 2019 dataset provided during the development phase is considered to be previously released, public data. Lessons learned from working with the 2019 dataset (e.g. similarity of neighborhoods) can be included in your algorithm without loss of privacy. \\n\n",
    "Sprint 2: The provided data set for this sprint consists of a 1,033,968 row CSV file which records quantitative survey information about a sample of the American population from 2012 - 2018. The data is adapted from the American Community Survey (ACS) provided by IPUMS USA, with simulated individuals connecting records longitudinally across years. As a reminder, for purposes of differential privacy, the data set provided during the development phase is considered to be previously released, public data. Lessons learned from working with this dataset (e.g. similarity of features) can be included in your algorithm without loss of privacy. \\n\n",
    "Sprint 3: The provided data set for this sprint consists of a 16 million row CSV file which records information about taxi trips in Chicago, Illinois. The data is adapted from the Chicago Open Data Portal. As a reminder, only the data set provided to participants during the development phase is considered to be previously publicly released data for differential privacy. Lessons learned from working with this dataset (e.g. similarity of features) can be included in your algorithm without loss of privacy.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" Synthetic data submissions will be compared to ground truth values to evaluate similarity using a version of the k-marginal evaluation metric. The intent behind the metric is to quantify the ability of each differential privacy algorithm to conserve clustering characteristics of the dataset being privatized. The original version of the metric was developed to score solutions in the NIST DeID1 Synthetic Data Challenge.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/group/competition-differential-privacy-deid2/\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "690bb32d-e42a-4da0-b036-f7575a199265",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "410f3680-1b32-40cb-95bb-0e099faf60eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" A biopsy is a sample of tissue examined at a microscopic level to diagnose cancer or signs of pre-cancer. Digital pathology has developed considerably over the past decade as it has become possible to work with digitized \"whole slide images\" (WSIs). These heavy image files contain all the information required to diagnose lesions as malignant or benign, yet present huge challenges to use effectively. This challenge focused on epithelial lesions of the uterine cervix, and featured a unique collection of thousands expert-labeled WSIs collected from medical centers across France. This is a sizable dataset (700GB) of extremely high resolution images. Given the scale of the dataset, handling the data efficiently is a critical problem to solve in the process of developing an accurate approach to diagnosis. In this competition, participants were tasked with building machine learning models that could predict the most severe lesions in each digital biopsy slide. What's more, participants needed to submit code for executing their solution on test data in the cloud, ensuring that the model could run fast enough on this large scale data to be useful in practice. This setup rewards models that perform well on unseen images and brings these innovations one step closer to impact.\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" This dataset consists of high resolution images of microscopic slides created from cervical biopsies. Additionally, you are given slide metadata as well as annotations for the training set that outline some (but not necessarily all) of the lesions present on a slide.\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" Performance is evaluated according to a custom metric devised by a panel of expert pathologists. The score for each prediction equals 1 minus the error, where the error is defined by the following values set by an expert consensus within the scientific council. The total score will be the average across all predictions. Note that the metric is symmetric, e.g. predicting class 3 when it's actually class 0 produces the same error as predicting class 0 when it's actually class 3.\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/67/competition-cervical-biopsy/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Synthetic biology offers fantastic benefits for society, but its anonymity opens the door for reckless or malicious actors to cause serious harm. Currently, there is no easy way of tracing genetically engineered DNA back to its lab-of-origin. This task is known as attribution, and it's a pivotal part of ensuring that genetic engineering progresses responsibly. When manipulating DNA, a designer has many decisions she must make, such as promoter choice and cloning method. These choices leave clues in the genetic material, and together, compose a \"genetic fingerprint\" that can be traced back to the designer. The goal of the Genetic Engineering Attribution Challenge was to develop tools that help human decision makers identify the lab-of-origin from genetically engineered DNA. This competition was composed of two tracks. In the Prediction Track, participants competed to attribute DNA samples to its lab-of-origin with the highest possible accuracy. In the Innovation Track, competitors that beat the BLAST benchmark were invited to submit reports demonstrating how their lab-of-origin prediction models excel in domains beyond raw accuracy.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" This dataset consists of DNA sequences from genetically engineered plasmids. Plasmids are small, circular DNA molecules that replicate independently from chromosomes.\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" Performance will be evaluated using top ten accuracy. This method considers the solution’s ability to correctly place the true lab-of-origin in the top ten most likely labs predicted for each sequence. That is, how often does the set of ten labs with the highest predicted probabilities contain the true lab of origin? The submission with the highest top ten accuracy score will top the leaderboard.\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/63/genetic-engineering-attribution/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" 5.8 million Americans live with Alzheimer’s dementia, including 10% of all seniors 65 and older. Scientists at Cornell have discovered links between “stalls,” or clogged blood vessels in the brain, and Alzheimer’s. The ability to prevent or remove stalls may transform how Alzheimer’s disease is treated. However, finding these stalls is extremely time intensive, especially as only around 1% of image stacks contain stalls. In this challenge, participants were tasked with building machine learning models that could classify blood vessels in 3D image stacks as stalled or flowing. The exceptional challenge dataset came from Stall Catchers, a citizen science project that crowdsources the identification of stalls using research data provided by Cornell University’s Department of Biomedical Engineering. The researchers behind Stall Catchers believe that some portion of the data may be within reach of machine learning models. Model predictions would only be used in cases where models had been validated to meet the researchers’ data quality requirements.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" The main features in this challenge are the videos themselves! These are image stacks taken from live mouse brains showing blood vessels and blood flow. The full training dataset contains over 570,000 videos, which is around 1.4 terabytes! To help facilitate faster model prototyping, we've created two subsets of the dataset, referred to as \"nano\" and \"micro.\" See the table below for details about each version. Note that the nano and micro subsets have been designed to have much more balanced classes than the full dataset.\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" erformance is evaluated according to Matthew's correlation coefficient (MCC). This metric takes into account all four components of the confusion matrix: true positives, true negatives, false positives, and false negatives. MCC ranges between +1 and -1. A coefficient of +1 represents a perfect prediction, 0 represents no better than random, and -1 represents complete disagreement between predictions and true values. The competitor that maximizes this metric will top the leaderboard.\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/65/clog-loss-alzheimers-research/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" At the massive scale of the internet, the task of detecting multimodal hate is both extremely important and particularly difficult. Relying on just text or just images to determine whether a meme is hateful is insufficient. By using certain types of images, text, or combinations, a meme can become a multimodal type of hate speech. The goal of this challenge was to develop multimodal machine learning models—which combine text and image feature information—to automatically classify memes as hateful or not. The team at Facebook AI created the Hateful Memes dataset to engage a broader community in the development of better multimodal models for problems like this. Participants were given a limited number of submissions to achieve the highest AUC ROC score when classifying an unseen test set of memes.\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" The features in this data set are the meme images themselves and string representations of the text in the image—you do not need to apply your own OCR algorithm to extract the meme text! The meme images and text extractions live in separate files, we'll show you how they can can be matched to each other using the meme id.\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" Model performance and leaderboard rankings will be determined using the AUC ROC, or, the Area Under the Curve of the Receiver Operating Characteristic. The metric measures how well your binary classifier discriminates between the classes as its decision threshold is varied. This means you'll need to submit probabilities for each each prediction.\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/group/hateful-memes/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Can you predict whether people got H1N1 and seasonal flu vaccines using information they shared about their backgrounds, opinions, and health behaviors? In this challenge, we will take a look at vaccination, a key public health measure used to fight infectious diseases. Vaccines provide immunization for individuals, and enough immunization in a community can further reduce the spread of diseases through \"herd immunity.” Your goal is to predict how likely individuals are to receive their H1N1 and seasonal flu vaccines. Specifically, you'll be predicting two probabilities: one for h1n1_vaccine and one for seasonal_vaccine.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" Data is provided courtesy of the United States National Center for Health Statistics.\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" Performance will be evaluated according to the area under the receiver operating characteristic curve (ROC AUC) for each of the two target variables. The mean of these two scores will be the overall score. A higher value indicates stronger performance.\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/66/flu-shot-learning/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" As urban populations grow, more people are exposed to the benefits and hazards of city life. To manage the risk of natural disasters in this dynamic built environment, buildings need to be mapped frequently and in enough detail to help communities prepare and respond. ML algorithms are becoming critical to scaling these mapping efforts by learning to use aerial imagery to automatically create building footprints. To power these models, this competition featured high-resolution drone imagery from 12 African cities and regions covering more than 700,000 buildings. This imagery was paired with building footprints annotated with the help of local OpenStreetMap communities. Throughout the challenge, participants competed to build computer vision models that could most accurately map buildings on the ground. In parallel, the novel Responsible AI track gave participants an opportunity to engage with the ethical implications of applying ML in disaster risk contexts.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" The features in this dataset are the images themselves and the building footprints in the GeoJSONs, which can be used to train a building segmentation model. All training data (with the exception of the labels for images of Zanzibar) are pulled from OpenStreetMap. \"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" To measure your model's performance, we'll use a metric called Jaccard index. This is a similarity measure between two label sets, and is defined as the intersection divided by the union. It is an accuracy metric, so a higher value is better (as opposed to an error metric, where a lower value is better). \"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/60/building-segmentation-disaster-resilience/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Leverage millions of images of animals on the Serengeti to build a classifier that distinguishes between gazelles, lions, and more! In this competition, participants will predict the presence and species of wildlife in new camera trap data from the Snapshot Serengeti project, which boasts over 6 million images. Camera traps are motion-triggered systems for passively collecting animal behavior data with minimal disturbance to their natural tendencies. Camera traps are an invaluable tool in conservation research, but the sheer amount of data they generate presents a huge barrier to using them effectively. This is where AI can help! The competition is designed with a few objectives in mind: Innovation: Participants use state-of-the-art approaches in computer vision and AI and get live feedback on how well their solutions perform. Generalization: This competition is designed to reward the best generalizable solutions. The private test data used to determine the winners will come entirely from the latest, unreleased season of data from the Snapshot Serengeti project (season 11). For more information on the competition timeline and evaluation, see the problem description. Execution: Models are trained locally and submitted to execute inference in the cloud - read on! Openness: All prize-winning models are released under an open source license for anyone to use and learn from\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" The features provided in this challenge are the images themselves as well as the date and time of the capture. There is additional metadata available for download from LILA that contains information such as the total number of frames in the sequence and which frame number each image corresponds to. You are welcome to use this metadata during the training of your model but keep in mind that this metadata will not be provided for the test set.\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" Performance is evaluated according to a mean aggregated binary log loss. For each possible category in a sequence, the binary log loss will be computed and then the results will be summed (this accounts for potential presence of multiple species in a single sequence). The sum of the binary losses represent the total loss for the sequence. The competitor that minimizes the mean value of this loss over all test cases will top the leaderboard.\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/59/camera-trap-serengeti/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\"Natural hazards like earthquakes, hurricanes, and floods can have a devastating impact on the people and communities they affect. While buildings can be retrofit to better prepare them for disaster, the traditional method for identifying high-risk buildings involves going door to door by foot, taking many weeks if not months and costing millions of dollars. Roof material is one of the main risk factors for earthquakes and hurricanes, and a predictor of other factors like building material that are as not readily seen from the air. The World Bank Global Program for Resilient Housing and WeRobotics teamed up to prepare aerial drone imagery of buildings across the Caribbean annotated with characteristics that matter to building inspectors. Then in this competition, participants worked with this data to build their best rooftop classifiers. Computer vision models that most accurately map disaster risk from drone imagery will help drive faster, cheaper prioritization of building inspections and target resources for disaster preparation where they will have the most impact.\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" The only features in this dataset are the images themselves and the building footprints in the GeoJSONs. The images consist of seven large high-resolution Cloud Optimized GeoTiffs of the seven different areas. The spatial resolution of the images is roughly 4 cm. \"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" To measure your model's accuracy by looking at prediction error, we'll use a metric called log loss. This is an error metric, so a lower value is better (as opposed to an accuracy metric, where a higher value is better). \"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/58/disaster-response-roof-type/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Based on aspects of building location and construction, your goal is to predict the level of damage to buildings caused by the 2015 Gorkha earthquake in Nepal. We're trying to predict the ordinal variable damage_grade, which represents a level of damage to the building that was hit by the earthquake. There are 3 grades of the damage:\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" The data was collected through surveys by Kathmandu Living Labs and the Central Bureau of Statistics, which works under the National Planning Commission Secretariat of Nepal. This survey is one of the largest post-disaster datasets ever collected, containing valuable information on earthquake impacts, household conditions, and socio-economic-demographic statistics. The dataset mainly consists of information on the buildings' structure and their legal ownership. Each row in the dataset represents a specific building in the region that was hit by Gorkha earthquake. There are 39 columns in this dataset, where the building_id column is a unique and random identifier. The remaining 38 features are described in the section below. Categorical variables have been obfuscated random lowercase ascii characters. The appearance of the same character in distinct columns does not imply the same original value.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" We are predicting the level of damage from 1 to 3. The level of damage is an ordinal variable meaning that ordering is important. This can be viewed as a classification or an ordinal regression problem. (Ordinal regression is sometimes described as an problem somewhere in between classification and regression.) To measure the performance of our algorithms, we'll use the F1 score which balances the precision and recall of a classifier. Traditionally, the F1 score is used to evaluate performance on a binary classifier, but since we have three possible labels we will use a variant called the micro averaged F1 score.\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/57/nepal-earthquake/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Efficient cleaning of production equipment is vital in the Food & Beverage industry. Strict industry cleaning standards apply to the presence of particles, bacteria, allergens, and other potentially dangerous materials. At the same time, the execution of cleaning processes requires substantial resources in the form of time and cleaning supplies. Better foresight into cleanliness levels at the end of these pipelines can help minimize the use of water, energy and time, all while ensuring high cleaning standards. Schneider Electric ran a machine learning competition to predict levels of turbidity – a standard industry measure of cleanliness – detected in the final stage of cleaning processes. In Stage 1, more than 1,200 data scientists competed over 50 days to build the most reliable predictions. In Stage 2, the 15 best-performing teams were invited to submit brief reports to illuminate business implications of quantitative patterns in the data.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" train_values.csv and test_values.csv contain metadata on the cleaning process, phase, and object as well as time series measurements, sampled every 2 seconds. The time series data pertain to the monitoring and control of different cleaning process variables in both supply and return Clean-In-Place lines as well as in cleaning material tanks during the cleaning operations.\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" The performance metric is a variant of mean absolute percentage error, called mean adjusted absolute percent error.\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/56/predict-cleaning-time-series/\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9918861f-7788-410b-ac5f-75e6a41a368b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "400101bb-3e6e-40d9-aec1-c49283930226",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Increasing the efficiency of energy consumption has benefits for consumers, providers, and the environment. Good forecasts of building energy use play a critical role in planning efficient policies, optimizing storage for renewable sources, and detecting wasteful anomalies. But what if a building is just becoming operational, and we don't have much past data for making forecasts? This “cold start problem” was the focus of a data science challenge that drew on data from Schneider Electric. More than 1,200 data scientists competed to build the most reliable predictions given only a few days of historical data for each building. What’s more, competitors had to work with different time windows to provide accurate hourly, daily, and weekly forecasts.\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" In the test set, varying amounts of historical consumption and temperature data are given for each series, ranging from 1 day to 2 weeks. The temperature data contains a portion of wrong / missing values. In the training set, 4 week series of hourly consumption and temperature data are provided. These series can be used to create different cold start regimes (varying amounts of provided data and prediction resolutions) for local training and testing. Basic building information such as suface area, base temperature, and on/off days are given for each series in the training and test sets.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" The performance metric is a normalized version of mean absolute error.\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/55/schneider-cold-start/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" We've all got to start somewhere. This is one of the smallest datasets on DrivenData. That makes it a great place to dive into the world of data science competitions. Get your heart thumping and try your hand at predicting heart disease. Heart disease is the number one cause of death worldwide, so if you're looking to use data science for good you've come to the right place. To learn how to prevent heart disease we must first learn to reliably detect it. Your goal is to predict the binary class heart_disease_present, which represents whether or not a patient has heart disease:\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" Our dataset is from a study of heart disease that has been open to the public for many years. The study collects various measurements on patient health and cardiovascular statistics, and of course makes patient identities anonymous. Data is provided courtesy of the Cleveland Heart Disease Database via the UCI Machine Learning repository.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" Performance is evaluated according to binary log loss.\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/54/machine-learning-with-a-heart/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Flexibility in energy management is essential to avoid costly reinforcements of the power system and to maintain secure supply while increasing the penetration of renewable sources. Energy storage can increase smart building flexibility, while time of use tariffs can incite use of energy when it is the most available. This is a delicate balance, where algorithms can help battery charging systems to be as efficient as possible (for instance, buy more energy when its price is lowest, and buy less or sell energy when its price is highest). Using data from Schneider Electric, competitors built models to control a battery charging over a simulation period. At the end of the challenge, the simulation was scored across all submitted algorithms, and those that spent the least amount of money over that period rose to the top of the leaderboard.\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" For this challenge, you will be provided the input data where each row represents a time at which you have to make a decision whether to consume power from the battery or to charge the battery. Your algorithm must be engineered to accept data as defined by the BatteryController.propose_state function and use that data to propose a new state for the charge of the battery.\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" At the end of the competition, submitted battery_controller.py files and assets folders will be used in a simulation under the above constraints. We will test each submission against multiple 10 day periods (same for all competitors) with different properties and record the results as a ratio of the money spent using the battery over the money spent without the battery. These results will be averaged over all of the simulations for a final score where a lower score is better. Winning competitors will be notified by email.\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/53/optimize-photovoltaic-battery/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\":\"\"\" As energy consumption of buildings has steadily increased, more and more building do not perform as intended by their designers. Typical buildings consume around 20% more energy than necessary due to faulty construction, malfunctioning equipment, incorrectly configured control systems, and inappropriate operating procedures. Automatic, quick-responding, accurate, and reliable fault detection can ensure better operations and save energy. In this competition, data scientists all over the world built algorithms to identify anomalous energy consumption. The overall objective is to provide methods for automatically detecting abnormal energy consumption data in buildings, as well as potential savings opportunities with all relevant details. Beyond detecting overconsumption, it is important to provide correct interpretation of overconsumption and, when possible, provide actionable recommendations.\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" A few data sets, corresponding to different types of building sites from different geographies, will be provided. Some building sites will correspond to a unique main activity (office) while some will be mixed (office, research, cafeteria). The goal is to cover various situations and test solutions in differing conditions. Data sets will include global energy consumption in Watt-hours (Wh), at intervals of 10, 15, or 30 minutes (depending on local utility standards). Some will also optionally include sub-metering information along with some usage data (daily building occupancy, schedule, weather). Data sets may include missing and incorrect values.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" There are two \"tracks\" for this competition. The first is an algorithm for anomaly detection that approaches the behavior of expert-system-detected and hand-labeled anomalies from Schneider Electric. A fraction of the labels will be used for a public leaderboard, and other labels for a private leaderboard. The second is a submitted report. These reports will suggest anomaly detection methodology and outline classes of anomalies identified by the algorithms. The reports will be reviewed by an expert judging panel. The algorithm submissions will be scored using a weighted combination of precision and recall.\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/52/anomaly-detection-electricity/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" The ability to forecast a building’s energy consumption plays a critical role in energy efficiency. Good forecasts can help implement energy-saving policies and optimize operations of chillers, boilers and energy storage systems. They also provide a baseline for flagging potentially wasteful discrepancies between expected and actual energy use. Often these forecasts need to be built using limited data and still be as accurate as possible. Using selected “time windows” of data from Schneider Electric, competitors built models to predict future energy consumption across buildings. These predictions were compared with the actual recorded consumption, information that was withheld from the developers. The models that found the most useful signals in the limited past data won prizes for the most accurate estimates.\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" More than 200 building sites are considered. Three time horizons and time steps are distinguished. Historical data are given at the granularity that is required for the consumption forecast. So, when historical data are given by steps of 15 minutes, forecasts are required by steps of 15 minutes. When historical data are given by steps of 1 hour, forecasts are required by steps of 1 hour. When historical data are given by steps of 1 day, forecasts are required by steps of 1 day. These data may contain a small portion of wrong / missing values.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" For each building and test period, the quality of the forecast will be evaluated using the Weighted Root Mean Squared Error (WRMSE) measure. \"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/51/electricity-prediction-machine-learning/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" The World Bank aims to end extreme poverty by 2030. To achieve this goal, they need efficient pipelines for measuring, tracking, and predicting poverty. But measuring poverty is currently hard, time consuming, and expensive. Estimates are typically collected through complex household consumption surveys with data on hundreds of different variables, each of which may be useful when assessing poverty levels. Machine learning offers new approaches for determining which variables are most predictive and how they can be most effectively combined. In this competition, data scientists from more than 130 countries around the world built algorithms to predict household-level poverty status using surveys data from three developing countries, each with a different distribution of wealth. \"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" The dataset has been structured so that the id columns match across the individual and house hold datasets. For both datasets, an assessment of whether or not the household is above or below the poverty line is in the poor column. This binary variable is the target variable for the competition. Each column in the dataset corresponds with a survey question. Each question is either multiple choice, in which case each choice has been encoded as random string, or it is a numeric value. Many of the multiple choice questions are about consumable goods--for example does your household have items such as Bar soap, Cooking oil, Matches, and Salt. Numeric questions often ask things like How many working cell phones in total does your household own? or How many separate rooms do the members of your household occupy?\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" Performance is evaluated according to a mean log loss. That is, log loss scores are generated for each country, and the mean of those scores will be your overall score. The competitor that minimizes the value of this loss over all test cases will top the leaderboard.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/50/worldbank-poverty-prediction/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" While camera traps have become powerful non-invasive tools in research and conservation efforts, they can't yet autonomously label the species they observe. It takes a lot of valuable time to determine whether there are any animals present (or just passing winds), and if there are, which ones. Through the Chimp&See Zooniverse project, a global community crowd-labeled research videos showing wildlife or blank frames. Then, the DrivenData community used cutting edge computer vision techniques to turn those labels into algorithms for automated species detection. The top 3 submissions best able to predict the presence and type of wildlife across new videos won this challenge. Your model should identify the animals (or lack thereof) in a given Chimp&See video. There are 24 categories in total: 23 animal categories plus 1 category corresponding to no animal.\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" The only features in this challenge are the videos themselves, named as subject_id.mp4. Each video is 15 seconds long, but it's unlikely that you'll need all 15 seconds of frames to make a good prediction. Whether or not you downsample the videos is up to you! We have used the crowd-sourced annotations from Chimp&See to generate ground truth labels for each video in the dataset. Some videos have no animals in them, in which case the blank category of the video's labels will be 1 and all other columns will be 0. Otherwise, if a species is present its entry will be a 1. Multiple species may be present!\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" Performance is evaluated according to a mean aggregated binary log loss. For each possible category in a video the binary log loss will be computed then the results will be summed (this accounts for potential presence of multiple species in a single video). The sum of the binary losses represents the total loss for the video. The competitor that minimizes the mean value of this loss over all test cases will top the leaderboard.\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/49/deep-learning-camera-trap-animals/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Keeping fish populations at healthy levels is critical to sustainable fishing practices, but also puts added requirements on boats to demonstrate they’re meeting the standards. Electronic monitoring systems, like video cameras, could offer an affordable way for fishermen to show their work and keep consumers and fisheries managers confident in the sustainability of their seafood. Data scientists from around the world competed to bring advances in computer vision to count, measure, and identify the species of fish from on-board footage.  This challenge was the first to use actual fishing video to develop machine learning tools that can classify multiple parameters, such as size and species.\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" You are provided with video segments with one or many fish in each video segment. These videos are gathered on different boats in that are fishing for ground fish in the Gulf of Maine. The videos are collected from fixed-position cameras that are placed to look down on a ruler. A fish is placed on the ruler, the fisherman removes their hands from the ruler, and then either discards or keeps the fish based on the species and the size. The camera captures 5 frames per second.\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" There are three tasks that are important to this project. Our ultimate goal is to create an algorithm that generates automatically generates annotations for video files, where the annotations are comprised of: (1) The sequence of fish that appear; (2) The species of each fish that appears in the video; (3) The length of each fish that appears in the video. Given that this is a competition, we've created an aggregate metric that will give a general sense of performance on all of these tasks. The metric is a simple weighted combination of an individual metric for each of the tasks. While there are certain weights, we recommend you focus on a well-rounded algorithm that can contribute to each of these tasks!\n",
    "\"\"\" \n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/48/identify-fish-challenge/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Penguins - in addition to being the best dressed animals in the Antarctic - are important indicators of the health of their fragile ecosystem. Monitoring and modeling their population dynamics over time supports long-term management of the species and provides important information about broader environmental changes. Using data from MAPPPD, the most comprehensive counts of Antarctic penguin populations, competitors built models to predict changes in four species of penguins based on data about their environment. The competition allowed a massive number of alternative prediction approaches to be explored, unbiased by researchers’ prior expectations for how the system should work.\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" All of the data for this competition and the MAPPPD project comes from the hard work of scientists around the globe who are dedicated to collecting data on penguins. The data comes from the published works of these contributors, and full citations to this work can be found on the MAPPPD page.\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" Performance is evaluated according to an adjusted MAPE calculation. Since some penguin counts have differing accuracies, the percent error is weighted differently based on this expected accuracy.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/47/penguins/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Budgets for schools and school districts are huge, complex, and unwieldy. It's no easy task to digest where and how schools are using their resources. Education Resource Strategies is a non-profit that tackles just this task with the goal of letting districts be smarter, more strategic, and more effective in their spending. Your task is a multi-class-multi-label classification problem with the goal of attaching canonical labels to the freeform text in budget line items. These labels let ERS understand how schools are spending money and tailor their strategy recommendations to improve outcomes for students, teachers, and administrators.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" In order to compare budget or expenditure data across districts, ERS assigns every line item to certain categories in a comprehensive financial spending framework. For instance, Object_Type describes what the spending \"is\"—Base Salary/Compensation, Benefits, Stipends & Other Compensation, Equipment & Equipment Lease, Property Rental, and so on. Other categories describe what the spending \"does,\" which groups of students benefit, and where the funds come from. Your goal is to predict the probability that a certain label is attached to a budget line item. Each row in the budget has mostly free-form text features, except for the two below that are noted as float. Any of the fields may or may not be empty\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" Your goal is to predict a probability for each possible label in the dataset given a row of new data. Each of these probabilities goes in a separate column in the submission file. The submission must be 50064x104 where 50064 is the number of rows in the test dataset (excluding the header) and 104 is the number of columns (excluding a first column of row ids). \"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/46/box-plots-for-education-reboot/\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7c7b673-982e-4f32-8590-d896ac38b4e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "149444e2-f008-4a0e-8b66-720550acf07a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Can you predict local epidemics of dengue fever? Dengue fever is a mosquito-borne disease that occurs in tropical and sub-tropical parts of the world. In mild cases, symptoms are similar to the flu: fever, rash, and muscle and joint pain. In severe cases, dengue fever can cause severe bleeding, low blood pressure, and even death. Using environmental data collected by various U.S. Federal Government agencies—from the Centers for Disease Control and Prevention to the National Oceanic and Atmospheric Administration in the U.S. Department of Commerce—can you predict the number of dengue fever cases reported each week in San Juan, Puerto Rico and Iquitos, Peru? Your goal is to predict the total_cases label for each (city, year, weekofyear) in the test set. There are two cities, San Juan and Iquitos, with test data for each city spanning 5 and 3 years respectively.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" The data for this competition comes from multiple sources aimed at supporting the Predict the Next Pandemic Initiative. Dengue surveillance data is provided by the U.S. Centers for Disease Control and prevention, as well as the Department of Defense's Naval Medical Research Unit 6 and the Armed Forces Health Surveillance Center, in collaboration with the Peruvian government and U.S. universities. Environmental and climate data is provided by the National Oceanic and Atmospheric Administration (NOAA), an agency of the U.S. Department of Commerce.\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" Performance is evaluated according to the mean absolute error.\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/44/dengai-predicting-disease-spread/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" US presidential elections come but once every 4 years, and this one's a big one. The new president will help shape policies on education, healthcare, energy, the environment, international relations, aid, and more. There are lots of people trying to predict what will happen. Can you top them? In this challenge, you'll predict the percent of each state that will vote for each candidate. You can use any data you can get your hands on. Come election night, we'll see who's model had the best vision for the country!\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" We don't provide any one set of feature data that you must use. Instead, you can use any available data to make your predictions. For some suggestions, check out the resources page!\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" n/a\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/43/americas-next-top-statistical-model/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Just in the US, more than 10 million adults are 65+ years of age and live alone. One of the biggest impediments to independent aging-in-place is the ability to detect and respond when something goes wrong. We now have the ability to use passive sensors (like motion detectors and accelerometers) to monitor some aspects of an older person’s activity privately and unobtrusively. But we don’t yet have reliable ways to turn this information into an actionable view of what is happening to them in real life. The SPHERE Inter-disciplinary Research Collaboration assembled a unique dataset in their research home in Bristol, UK. In addition to recording the outputs from smart technologies like wearables and environmental sensors, the center also has human-labeled descriptions of what seniors were doing—their activities, like meal preparation or watching TV; and their positions, like walking or sitting—while those technologies were in use. Competitors were charged with building computer algorithms that could translate the easierto- collect sensor data into harder-to-collect information about real activities.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" To gather data, researchers in the SPHERE Inter-disciplinary Research Collaboration (IRC) equipped volunteers with accelerometers similar to those found in cell phones or fitness wearables, and then had the subjects go about normal activities of daily living in a home-like environment that was also equipped with motion detectors. After gathering a robust set of sensor data, they had multiple annotators use camera footage to establish the ground truth, labeling chunks of sensor data as one of twenty specifically chosen activities (e.g. walk, sit, stand-to-bend, ascend stairs, descend stairs, etc).\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" Of the nearly 1000 submission generated during the competition, the winning models were able to make that translation correctly (predicting what seniors were doing as described by human researchers) with ~90% accuracy, a hugely impressive result. This performance improved ~35% over the previous benchmark and surpassed even internal state-of-the-art models that the researchers had developed.\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/42/senior-data-science-safe-aging-with-sphere/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Water in southwest Morocco is scarce and transporting water is difficult. Women and girls spend up to four hours a day to collect poor quality water and carry barrels back to their communities. Dar Si Hmad (DSH) manages fog collection nets that capture and disseminate the fog that rolls over nearby mountains. Managing water dispersal and communicating the system with the community is a difficult task, and essential to providing a dependable solution. DSH had assembled years of data about weather patterns and water yield. In partnership with the Tifawin Institute and Tableau Foundation, DrivenData ran an online tournament to put this data to use. Participants were given two tasks: predict how much water DSH can expect in the future, and create clear and insightful ways to visualize the collections system. Your challenge is to develop a model that will predict the yield of DSH’s fog nets for every day during an evaluation period, using historical data about meteorological conditions and the fog net installations. Accurate predictions will enable DSH to operate more effectively and the communities it serves to be have greater access to fresh water throughout the year.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" For this competition, we have both weather data from a sensor co-located with the fog nets (microclimate data) and data from weather stations in three nearby cities in Morocco (macroclimate data). Various metorological measures are recorded at these different weather stations. Your goal is to use these data sources to predict the yield of a collection of fog nets. The yield of these nets is measured every two hours. For the test set, windows of 4 days have been removed from the training data at regular intervals, and competitors will attempt to predict most accurately the yield during these intervals. For some of these intervals, concurrent microclimate data is provided. For others, only macroclimate data is available.\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" Top visualizations were judged by a panel of experts and used to foster new conversations with members of the communities that DSH serves. The visualizations that resonated most with each of these audiences were selected for prizes. In addition, the top algorithm improved 25% over the previous benchmark in accurately predicting freshwater output from the fog nets. Measures of leaf wetness and humidity were found to be most useful in making these predictions.\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/9/from-fog-nets-to-neural-nets/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Gaining insight into changing bee populations is critical to understand the effects of colony collapses. Unfortunately, it takes a lot of time and effort for researchers to gather data on wild bees. Although BeeSpotter is making this process easier by using images submitted by citizen scientists, they still require experts to go through each image to examine and identify each bee. This obviously is very time consuming. Build an algorithm that identifies the type of bee in each photograph. METIS has asked the Drivendata community to build an algorithm that is able to succesfully identify honey bees and bumble bees in a photograph, even when those photographs have great variety in their backgrounds, positions, and image resolutions. Your goal is to predict for each image whether it is a bumble bee (Bombus) or a honey bee (Apis).\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" All of the images have been scaled and cropped so they are 200px x 200px. These images are submitted by citizen scientists and vary in terms of image quality, distance from subject, background and position of the bee.\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"n/a\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/8/naive-bees-classifier/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" The City of Boston inspects every restaurant to monitor and improve food safety and public health. Health inspections are usually random, which can increase time spent at clean restaurants that have been following the rules carefully — and missed opportunities to improve health and hygiene at places with food safety issues. The winning algorithm uses data from social media to narrow the search for health code violations in Boston. Competitors had access to historical hygiene violation records from the City of Boston and Yelp’s consumer reviews. Algorithms detect words, phrases, ratings, and patterns that predict violations, to help public health inspectors do their job better.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" The features in this dataset are the data provided by Yelp. This data includes business descriptions, restaurant reviews, restaurant tips, user review history, and check-ins. It's up to you to process these reviews into features that are predictive of hygeine inspection violations. All of the Yelp data is structured as one JSON object per line in the file. You can find some examples of working with Yelp data on their Academic Dataset GitHub repository.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" There are two phases for this competition. PHASE I runs like every other competition with a public test set and private training set. During PHASE I submit your predictions for the restaurant inspections in SubmissionFormat.csv and your score will appear on the PHASE I leaderboard. However, the final leaderboard is not determined during this phase. In PHASE II, we will release the latest Yelp reviews that occurred between the start of the competition and the beginning of the second phase. Competitors will then have one week to submit predictions for the coming 6 weeks. At this point, submissions for PHASE II will close. During those subsequent 6 weeks, we will score these predictions against new inspections that are reported by health inspectors in the City of Boston.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/5/keeping-it-fresh-predict-restaurant-inspections/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Planned Parenthood is the nation’s leading provider and advocate of high-quality, affordable healthcare for women, men, and young people, as well as the nation’s largest provider of sex education. With approximately 700 health centers across the country, Planned Parenthood organizations serve all patients with care and compassion, with respect and without judgment. Understanding the trends in women’s health care is critical to delivering the expert, quality care that is the hallmark of Planned Parenthood. Planned Parenthood is an innovator in health care delivery, continually looking to find the best ways to expand access to quality, affordable care to everyone who needs it. We want your help to better understand the complex dynamics of health care in order to better serve the needs of those who depend on us. The goal of this competition is to drive innovation and analysis in the field of population health by predicting which reproductive health care services are accessed by women. The end product of the competition will improve public health with novel predictive analytics as part of our effort to give back to the research and healthcare community.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" This data is based on the National Survey of Family Growth conducted by the United States Center for Disease Control and Prevention. People across the United States were asked a series of over 1700 questions about their demographics, pregnancies, family planning, use of healthcare services, and medical insurance. We're focusing on the respondents to these questions that are women, and each row in the provided data represents an individual.\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"n/a\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/6/countable-care-modeling-womens-health-care-decisions/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Can you predict which water pumps are faulty? Using data from Taarifa and the Tanzanian Ministry of Water, can you predict which pumps are functional, which need some repairs, and which don't work at all? This is an intermediate-level practice competition. Predict one of these three classes based on a number of variables about what kind of pump is operating, when it was installed, and how it is managed. A smart understanding of which waterpoints will fail can improve maintenance operations and ensure that clean, potable water is available to communities across Tanzania.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"n/a\"\n",
    "    ,\n",
    "    \"eval_overview\": \"n/a\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Budgets for schools and school districts are huge, complex, and unwieldy. It’s no easy task to digest where and how schools are using their resources. Education Resource Strategies (ERS) tackles just this task with the goal of letting districts be smarter, more strategic, and more effective in their spending. The right algorithm, paired with some human checks, allows ERS to code financial files more accurately, more quickly, and more cheaply. As a result, they’re able to offer these valuable insights to many more districts at a much lower cost, greatly extending their impact.\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" Your goal is to predict the probability that a certain label is attached to a budget line item. Each row in the budget has mostly free-form text features, except for the two below that are noted as float. Any of the fields may or may not be empty\n",
    "\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" ERS was thrilled with the results, and notes that it was an excellent return on their investment. The winning algorithm came from a competitor who had submitted 50+ times; this algorithm will tag files with over 90% accuracy and will save ERS 75% of the time usually taken to code financial files. After the competition was complete, DrivenData worked with ERS to develop a user-friendly Excel tool to incorporate the algorithm into their everyday workflow.\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/4/box-plots-for-education/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" We've all got to start somewhere. This is the smallest, least complex dataset on DrivenData. That makes it a great place to dive into the world of data science competitions. Get your blood pumping and try your hand at predicting donations. Blood donation has been around for a long time. The first successful recorded transfusion was between two dogs in 1665, and the first medical use of human blood in a transfusion occurred in 1818. Even today, donated blood remains a critical resource during emergencies. Our dataset is from a mobile blood donation vehicle in Taiwan. The Blood Transfusion Service Center drives to different universities and collects blood as part of a blood drive. We want to predict whether or not a donor will give blood the next time the vehicle comes to campus.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" Our dataset is from a mobile blood donation vehicle in Taiwan. The Blood Transfusion Service Center drives to different universities and collects blood as part of a blood drive. The UCI Machine Learning Repository is a great resource for practicing your data science skills. They provide a wide range of datasets for testing machine learning algorithms. Finding a subject matter you're interested in can be a great way to test yourself on real-world data problems. Given our mission, we're interested in predicting if a blood donor will donate within a given time window.\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" This competitions uses log loss as its evaluation metric, so the predictions you submit are the probability that a donor made a donation in March 2007.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/2/warm-up-predict-blood-donations/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" In the year 2000, the member states of the United Nations agreed to a set of goals to measure the progress of global development. The aim of these goals was to increase standards of living around the world by emphasizing human capital, infrastructure, and human rights. The UN measures progress towards these goals using indicators such as percent of the population making over one dollar per day. Your task is to predict the change in these indicators one year and five years into the future. Predicting future progress will help us to understand how we achieve these goals by uncovering complex relations between these goals and other economic indicators. The UN set 2015 as the target for measurable progress. Given the data from 1972 - 2007, you need to predict a specific indicator for each of these goals in 2008 and 2012.\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" Since its founding in 1944, the World Bank has been gathering data to help it alleviate poverty by focusing on foreign investment, international trade, and capital investment. The World Bank provides these data to the public through their data portal. We've aggregated their data from 1972-2007 on over 1200 macroeconomic indicators in 214 countries around the world. A random snapshot of the data looks like the below. Each row represents a timeseries for a specific indicator and country. The row has an id, a country name, a series code, a series name, and data for the years 1972 - 2007.\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"n/a\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/1/united-nations-millennium-development-goals/\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7d90867-1250-4dd8-9238-5113c3176017",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "255c8234-1f5d-42c0-afec-57b983a73d7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_comps.append({\n",
    "    \"comp_overview\": \"In this challenge, you will build models that classify instructional activities using multimodal classroom data. Classroom observation videos provide valuable insights into a teacher's instruction, student interactions, and classroom dynamics. Over the past 15 years, their use in teacher preparation and the study of teacher quality has increased significantly. Classroom videos are also a common source of data for educational researchers studying classroom interactions as well as a resource for professional development. Despite this growth, using video at scale remains challenging due to the time and resources required for processing and analysis. In this challenge, you will build models to help automate classroom observation so that it can be offered at scale and inform future teaching.\",\n",
    "    \"data_overview\": \"For this challenge, your goal is to classify each second of a set of videos and accompanying audio transcripts according to taxonomies of instructional activities and discourse content. After successfully submitting a data access agreement, you will be granted access to a training set containing approximately 117 hours of annotated videos of English language arts (ELA) and math lessons from elementary classrooms along with labeled audio transcripts for approximately 39 of the 117 hours. You will also be granted access to a Phase 1 testing set of approximately 26 hours of unlabeled videos (with accompanying unlabeled audio transcripts for approximately 11 of those 26 hours) of ELA and math lessons from elementary classrooms. Competitors who successfully submit at least one Phase 1 solution will be granted access to Phase 2. In Phase 2, competitors will make one submission against a new test set. The Phase 2 test dataset contains approximately 23 hours of unlabeled classroom videos and 10.5 hours of unlabeled audio transcripts. Prior to the end of Phase 1, competitors must select one submission whose corresponding model they will use to generate predictions on the Phase 2 test set. The final prize rankings will be determined by a weighted average of 25% of the Phase 1 score and 75% of the Phase 2 score.\",\n",
    "    \"eval_overview\": \"Performance is evaluated according to macro-weighted F1-score across all 24 instructional activity labels and 19 discourse labels. Note that this weights video classes slightly more than audio classes because there are more classes for videos. This is intentional and meant to account for a slight discrepancy in support between audio and video classes.\",\n",
    "    \"url\": \"https://www.drivendata.org/competitions/group/competition-uva-aiai-challenge/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\"Can you write abstracts for academic social science papers using a large language model that runs on your computer? OK, but can you do it well? Scientific papers describing research methodology and findings are the gold standard for sharing scientific work. These papers typically start with an abstract that summarizes the entire paper in a single paragraph. Your goal in this practice competition is to generate these abstracts from SocArXiv papers using large language models, or LLMs. In this practice competition, you have three goals: Assemble one-paragraph summaries of a corpus of social science academic papers. Gain experience working with large language models (LLMs). Be a good friend and neighbor (accomplish this one in whatever way suits you). Document summarization is one of the classic LLM applications, so this should provide a good introduction to getting useful, reliable, work from LLMs (which isn't always their strong suit).\"\"\",\n",
    "    \n",
    "    \"data_overview\": \"\"\"The data for this competition come from SocArXiv, a moderated repository for scientific documents in the social sciences hosted on the OSF preprints. (It's named after arXiv). Repositories like SocArXiv enable scientists to share their research outside of traditional, publisher-managed outlets. SocArXiv is referred to as a preprint repository, but it includes a variety of document types: working papers, manuscript drafts uploaded for early circulation and feedback; preprints, completed manuscript drafts that have not undergone formal peer-review; post-prints, author-formatted final or near-final versions of published, peer-reviewed articles. All the bodies of the papers were modified in an effort to remove the abstract text, references, and other text that would be either irrelevant or make the task too easy.\"\"\",\n",
    "    \n",
    "    \"eval_overview\": \"\"\"For scoring how well your summaries align with the author-written abstracts, we'll be using the average F1 of the ROUGE-2 score per document. ROUGE-2 measures the overlap of bigrams (word pairs) between a given piece of text and a reference text. We take the arithmetic mean of the F1 scores for each document.\"\"\",\n",
    "    \n",
    "    \"url\": \"https://www.drivendata.org/competitions/297/whats-up-docs/\"\n",
    "})\n",
    "\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\"Literacy—the ability to read, write, and comprehend language—is a fundamental skill that underlies personal development, academic success, career opportunities, and active participation in society. Many children in the United States need more support with their language skills. A national test of literacy in 2022 estimated that 37% of U.S. fourth graders lack basic reading skills. Addressing shortfalls as early as preschool is a promising approach given the strong relationship between early and later childhood literacy. \\n\n",
    "In order to provide effective early literacy intervention, teachers must be able to reliably identify the students who need support. Currently, teachers across the U.S. are tasked with administering and scoring literacy screeners, which are written or verbal tests that are manually scored following detailed rubrics. Manual scoring methods not only take time, but they may be unreliable, producing different results depending on who is scoring the test and how thoroughly they were trained. \\n\n",
    "Machine learning approaches to score literacy assessments can help teachers quickly and reliably identify children in need of early literacy intervention. By advancing state-of-the-art machine learning approaches, there is an opportunity to transform the landscape of literacy screening and intervention in classrooms across the United States. \\n\n",
    "Your goal in this challenge is to develop a model to score audio recordings from literacy screener exercises completed by students in kindergarten through 3rd grade.\"\"\"\n",
    ",\n",
    "    \"data_overview\": \"\"\"The data for this challenge include thousands of audio clips from literacy skill assessments of students in kindergarten through 3rd grade. You will develop an automatic scoring model for the literacy exercises. Your task is to estimate the likelihood that an audio clip of a speech task is correct.\"\"\"\n",
    ",\n",
    "    \"eval_overview\": \"\"\"Performance is evaluated according to log loss. Log loss (a.k.a. logistic loss or cross-entropy loss) penalizes confident but incorrect predictions. It also rewards confidence scores that are well-calibrated probabilities, meaning that they accurately reflect the long-run probability of being correct. This is an error metric, so a lower value is better.\"\"\",\n",
    "    \"url\": \"https://www.drivendata.org/competitions/298/literacy-screening/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\"Suicide is one of the leading causes of death in the United States for 5-24 year-olds. Researchers and policymakers study the circumstances of youth suicides to better understand them and reduce their occurrence. One key source of information is the National Violent Death Reporting System (NVDRS). The NVDRS captures information about violent deaths across the United States that has been abstracted from sources including law enforcement reports, coroner/medical examiner reports, toxicology reports, and death certificates. \\n\n",
    "The NVDRS contains narrative summaries drawing from those sources as well as standard variables that are useful to researchers. The process of recording standardized variables is time consuming and prone to human error. \\n\n",
    "The goal of this challenge was to improve both the quality and coverage of standard variables in the NVDRS. Higher-quality data can enable researchers across the country to better understand and prevent youth suicides on a national scale. In the Automated Abstraction track, participants created machine learning models to generate standard variables from NVDRS narratives. Participants trained models on 4,000 narratives from the NVDRS, and submitted executable code to generate predictions on 1,000 test set narratives. In the Novel Variables track, participants explored the narratives to suggest new standard variables that could advance youth mental health research. Submissions consisted of a qualitative writeup describing the suggested new variables, any motivation based on existing research, and the methodologies used to study the data.\n",
    "\"\"\"\n",
    ",\n",
    "    \"data_overview\": \"\"\"The NVDRS contains narrative summaries drawing from those sources as well as standard variables that are useful to researchers. The process of recording standardized variables is time consuming and prone to human error. In the Automated Abstraction track, participants created machine learning models to generate standard variables from NVDRS narratives. Participants trained models on 4,000 narratives from the NVDRS, and submitted executable code to generate predictions on 1,000 test set narratives.\n",
    "\"\"\"\n",
    ",\n",
    "    \"eval_overview\": \"\"\"In the Automated Abstraction track, DrivenData kicked off the competition by posting a simple LLM prompting benchmark that received an F1 score of 56%. More than 50 participants beat the benchmark, with top solutions achieving an F1 score of over 86%. In the Novel Variables track, participants studied topics like social media use, video games, gender, sexuality, and sleep.\n",
    "\"\"\"\n",
    ",\n",
    "    \"url\": \"https://www.drivendata.org/competitions/group/cdc-narratives/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\"This competition is part of a broader effort to provide safe, low-cost methods of assessing and potentially repairing damaged ships in space using an inspector spacecraft. An ideal version of an inspector spacecraft would be like a fire extinguisher: a small, cheap, standard safety device. In the Pose Bowl Challenge, solvers helped advance space inspection technology by competing to develop solutions for in-space inspection that would work on any type of target spacecraft. Solutions were developed against severe constraints on compute resources to reflect the limitations of the compute hardware available on a spacecraft. The data for this challenge were simulated images of spacecraft taken from a nearby location in space, as if from the perspective of a chaser. In the Object Detection track, solvers developed solutions to detect target spacecraft depicted in an image. In the Pose Estimation track, solvers worked to determine the relative pose of the chaser camera across a images of the target spacecraft taken as the chaser moves around it.\n",
    "\"\"\"\n",
    ",\n",
    "    \"data_overview\": \"\"\"The data for this challenge consists of simulated images of spacecraft taken from a nearby location in space, as if from the perspective of a chaser spacecraft. Images were created using the open-source 3D software Blender using models of representative host spacecraft against simulated backgrounds. A limited number of models and backgrounds were used to generate images; models and backgrounds appear in multiple images. Distortions were applied to some images in post-processing to realistically simulate image imperfections that result from camera defects or field conditions. The distortions applied to the images include blur (e.g., as chaser vehicle camera would be moving), hot pixels (a common defect in which some pixels have an intensity value of 0 or 255), and random noise (a typical distortion method to support generalizability and robustness). \"\"\"\n",
    ",\n",
    "    \"eval_overview\": \"\"\"To measure your solution’s performance, we’ll use a metric called Jaccard index, also known as Generalized Intersection over Union (IoU). Jaccard index is a similarity measure between two label sets. In this case, it is defined as the size of the intersection divided by the size of the union of pixels. Because it is an accuracy metric, a higher value is better. \\n\n",
    "The performance metric for this challenge is a pose error score that is the sum of the normalized translational and rotational components of the absolute pose error of your pose estimates. \\n\n",
    "The translational error for one image is the magnitude of the difference between your estimated translation and the ground truth translation, normalized by the magnitude of the ground truth translation.\n",
    "\"\"\"\n",
    ",\n",
    "    \"url\": \"https://www.drivendata.org/competitions/group/competition-nasa-spacecraft/#competition_list\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\"Much of the world's healthcare data is stored in free-text documents, usually clinical notes taken by doctors. This unstructured data can be challenging to analyze and extract meaningful insights from. However, by applying a standardized terminology like SNOMED CT, healthcare organizations can convert this free-text data into a structured format that can be readily analyzed by computers, in turn stimulating the development of new medicines, treatment pathways, and better patient outcomes. One way to analyze clinical notes is to identify and label the portions of each note that correspond to specific medical concepts. This process is called entity linking because it involves identifying candidate spans in the unstructured text (the entities) and linking them to a particular concept in a knowledge base of medical terminology. However, clinical entity linking is hard!  Medical notes are often rife with abbreviations (some of them context-dependent) and assumed knowledge. Furthermore, the target knowledge bases can easily include hundreds of thousands of concepts, many of which occur infrequently leading to a “long tail” effect in the distribution of concepts.  \\n\n",
    "The objective of this competition is to link spans of text in clinical notes with specific topics in the SNOMED CT clinical terminology. Participants will train models based on real-world doctor's notes which have been de-identified and annotated with SNOMED CT concepts by medically trained professionals. This is the largest publicly available dataset of labelled clinical notes, and you can be one of the first to use it!\n",
    "\"\"\"\n",
    ",\n",
    "    \"data_overview\": \"\"\"MIMIC-IV is a large repository of multi-modal, clinical datasets hosted on the PhysioNet platform by the MIT Laboratory for Computational Physiology. The dataset we are using for this challenge comes from MIMIC-IV-Note, which contains 331,794 de-identified hospital discharge notes provided by the Beth Israel Deaconess Medical Center in Boston, MA. The terms of the provision of the MIMIC data to PhysioNet preclude third parties from re-publishing it. This means that you will need to access the notes for this challenge via PhysioNet. Furthermore, the MIMIC license requires that users are registered for access and have undertaken a short training course on handling data from human subjects. Details of the access procedures that you’ll need to follow are given here.\n",
    "\"\"\"\n",
    ",\n",
    "    \"eval_overview\": \"\"\"For this challenge, participants may use a variety of tokenizers that lead to small variations in token start and end indices. To account for this variation, we use a character-level metric instead of a token-based one. Performance for this challenge is evaluated according to a class macro-averaged character intersection-over-union (IoU). This is an accuracy metric, so a higher value is better.\n",
    "\"\"\"\n",
    ",\n",
    "    \"url\": \"https://www.drivendata.org/competitions/258/competition-snomed-ct/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\"Kelp, we need somebody! In this challenge, participants are invited to help map and monitor kelp forests—underwater habitats that cover large swaths of ocean coastlines around the world and are essential to many ecosystems and species. Giant kelp serves as a foundation of many coastal marine ecosystems, providing habitat and food for thousands of species. Humans benefit too; it's estimated that kelp forests generate over US$500 billion annually through services like fisheries production. Yet, kelp forests face mounting threats from climate change, overfishing, and unsustainable harvesting practices. To preserve and protect these critical ecosystems, there is a pressing need to improve methods for mapping and monitoring them. However, comprehensive and robust monitoring is a tricky task. Kelp forests are dynamic environments that respond quickly to factors such as rising temperatures, large wave disturbances, and nutrient availability. To address this, researchers at Kelpwatch.org have introduced an innovative approach by leveraging machine learning to analyze coastal satellite imagery. This breakthrough enables scientists to estimate the presence of kelp forests across expansive areas over time. This cost-effective method is a first step to comprehensively monitoring kelp forest dynamics on a large scale. \\n\n",
    "In this challenge, we invite you to help advance this critical research. While Kelpwatch currently focuses on the west coast of North America, the next version aims to have a global application. Successful models will ideally be adaptable to new locations. Furthermore, models should exhibit robustness in the face of potential data issues, both in the satellite data and in the labels. Your goal is to detect the presence of kelp canopy using Landsat satellite imagery and labels generated by citizen scientists participating in the Floating Forests project. Successful algorithms will not only advance scientific knowledge, but also provide a critical tool for kelp forest managers and policy makers to preserve these at-risk ecosystems.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\"The feature data is a geospatial dataset composed primarily of remote sensing observations collected by Landsat mission satellites. It is provided in the form of unreferenced GeoTIFFs, meaning that the geospatial information has been intentionally removed. These GeoTIFFs consist of seven distinct bands, all of which have been coreferenced (spatially aligned) and have a spatial resolution of 30 meters. The satellite images have been cropped to square 350 x 350 pixel \"tiles\". Each tile has been assigned a unique tile_id and corresponds to patches of the coastal waters surrounding the Falkland Islands. These tiles span multiple decades, offering a comprehensive view of the region's kelp forests over time.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\"To evaluate the performance of your model in this binary semantic segmentation task, we will use the Dice Coefficient (also known as the Sørensen-Dice Index) as the performance metric. The Dice Coefficient quantifies the similarity between the predicted and ground-truth binary masks. A higher Dice Coefficient indicates better segmentation accuracy. The Dice Coefficient is calculated on a per-pixel basis, where each pixel in your submitted TIFF image for a patch is compared to the corresponding pixel in the ground-truth TIFF image for the same patch. This calculation is performed for each image in the test set, and the resulting Dice Coefficients are then averaged to provide an overall assessment of your model's performance.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/255/kelp-forest-segmentation/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\"Our world is facing many urgent challenges, such as climate change, water insecurity, and food insecurity. Maintaining and improving quality of life around the world requires bringing together innovators across disciplines and countries to find creative solutions. One critical tool for understanding and improving the urgent challenges facing our world is Earth observation data, meaning data that is gathered in outer space about life here on Earth! Earth observation data provides accurate and publicly accessible information on our atmosphere, oceans, ecosystems, land cover, and built environment. The United States and its partners have a long history of exploring outer space and making satellite, airborne, and in-situ sensor datasets openly available to all. \\n\n",
    "Your goal in this challenge is to create a visualization using Earth observation data that advances at least one of the following Sustainable Development Goals (SDGs): 2: Zero Hunger; 6: Clean Water and Sanitation; 13: Climate Action. By participating, you can be part of NASA's initiative to Transform to Open Science and to make Earth observation data available to all.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\"Your visualization must use at least one publicly available Earth observation dataset collected by a U.S. government agency. \"Earth observation data\" means observations about the Earth collected in space, such as satellite data, airborne, and in-situ sensors. The data resources blog post suggests some datasets that satisfy this requirement. However, it is not a comprehensive list and we encourage you to explore other datasets too. If you are unsure whether a specific dataset meets this requirements, just ask! Head over to the competition forum to post questions and to find teammates.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\"A representative panel of experts on Earth observation data and the Sustainable Development Goals will review submissions. Submissions will be judged based on the rubric below. Impact (20%): (1) How well does the submission further at least one of the key UN Sustainable Development Goals (zero hunger, clean water and sanitation, climate action)? (2) To what extent could the submission inform an action or decision? Integrity (20%): (1) Has the team thoughtfully engaged with members of the community primarily affected or with the broader context of the chosen issue (historical, social, political, etc.)? (2) Does the submission demonstrate ethical and equitable science? Rigor (20%): (1) Is the visualization built on sound quantitative analysis? (2) Does it demonstrate a deep understanding of the data used and tools available? Usability (20%): (1) Could others in the research or scientific community understand and build on the work? (2) Is the process of creating the visual open and transparent? Interpretability (20%): How easy is it to accurately interpret the visualization?\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/256/pale-blue-dot/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\"This challenge sought models for creating probabilistic forecasts of naturalized cumulative streamflow volume for 26 sites in the Western U.S. over specific seasonal periods. Participants explored over two dozen data sources, including antecedent streamflow, snowpack conditions, meteorological conditions, seasonal temperature and precipitation forecasts, climate teleconnection indices, and more. Models were evaluated over multiple challenge stages based on forecast accuracy and additional factors such as model rigor, innovation, generalizability, efficiency, scalability, and report clarity. The stages included near-real time forecasts during the 2024 season as well as a rigorous cross-validation over historical streamflow data. Panels of hydrologists, water resource experts, and machine learning practitioners from U.S. federal agencies judged the submissions. Additionally, an Explainability and Communication Bonus Track challenged participants to create clear and useful forecast summaries for key forecast consumers like water resource managers.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\"Seasonal water supply is influenced by a range of hydrological, weather, and climate factors. Relevant data sources for streamflow volume prediction include antecedent streamflow measurements, snowpack estimates, short and long-term meteorological forecasts, and climate teleconnection indices. You are encouraged to experiment with and use a variety of relevant data sources in your modeling. However, only approved data sources are permitted to be used as input into models for valid submissions. See the Approved Data Sources page for an up-to-date list of approved data sources. You will be responsible for downloading your own feature data from approved sources for model training. Example code for downloading and reading the approved data sources is available in the challenge data and runtime repository.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\"Forecast Skill (Hindcast cross-validation) (30%): Solutions will be evaluated based on cross-validation results over the 20-year hindcast period. Forecast Skill (Forecast) (10%): Solutions will be evaluated based on their predictions' quantile score from the Forecast Stage evaluation. Rigor (20%): To what extent is the solution methodology based on a sound physical and/or statistical foundation? Judges will consider how methodological decisions support or limit different aspects of rigor, such as avoiding overfitting, avoiding data leakage, assessing and mitigating biases, and potential for the model to produce valid predictions in an applied context. Judges will also consider whether any aspects of the methodology are physically implausible. Innovation (10%): To what extent does the solution use datasets or modeling techniques that advance the state-of-the-art in water supply forecasting? Judges will consider innovation in any aspect of the technical approach, including but not limited to the data sources used, feature engineering, algorithm and architecture selection, or the approach to training and evaluation. Generalizability (10%): How well does the solution generalize to the varied sites and conditions tested in the challenge? Judges will consider reported information and hypotheses about the model’s performance under different geographic, environmental, and temporal conditions. Efficiency & Scalability (10%): How computationally efficient is the solution, and how well could it scale to an increased number of sites? Judges will consider all aspects of efficiency such as the reported total test runtime (including data processing), training resource costs (e.g., hardware, memory usage), and any reported potential for efficiency improvements or optimizations. Clarity (10%): How clearly are model mechanics exposed, communicated, and visualized in the report? Judges will consider how well organized and presented the report is.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/group/reclamation-water-supply-forecast/\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1753f183-b08b-4d7e-b98f-7ce4c63bb779",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\"The National Institute on Aging (NIA), part of the National Institutes of Health (NIH), is running the PREPARE Challenge (Pioneering Research for Early Prediction of Alzheimer's and Related Dementias EUREKA Challenge) to advance solutions for accurate, innovative, and representative early prediction of AD/ADRD. The goal of the PREPARE Challenge is to inform novel approaches to early detection that might ultimately lead to more accurate tests, tools, and methodologies for clinical and research purposes. Advances in artificial intelligence (AI), machine learning (ML), and computing ecosystems increase possibilities of intelligent data collection and analysis, including better algorithms and methods that could be leveraged for the prediction of biological, psychological (cognitive), socio-behavioral, functional, and clinical changes related to AD/ADRD. To achieve this goal, the challenge will feature three phases that successively build on each other. Phase 1: Find, curate, or contribute data to create representative and open datasets that can be used for early prediction of AD/ADRD. Phase 2: Advance algorithms and analytic approaches for early prediction of AD/ADRD, with an emphasis on explainability of predictions. Phase 3: Top solvers from Phase 2 demonstrate algorithmic approaches on diverse datasets and share their results at an innovation event.\\n\n",
    "    The purpose of the Explainability Bonus Track is to demonstrate effective methods for sharing predictions and relevant insights with preclinical patients, care providers, and clinicians. This track provides an opportunity for participants to demonstrate how their models can support clinical decision-making through clear communication of individual predictions (local explainability). While the main model report focuses on explaining how the model makes predictions generally, this bonus track focuses on explaining why a specific prediction was made. The intended audience of explainability bonus submissions is a patient or other people involved in their care, like a physician, caretaker, or family member, rather than a researcher. Given that this challenge focuses on advancing early prediction of AD/ADRD, assume that the fictional patient in this case is preclinical (i.e., cognitively healthy).\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\"Data for the Acoustic track: The feature data in this competition is a series of audio recordings collected from individuals diagnosed with some form of cognitive decline as well as healthy controls. The data include 2,058 individuals from multiple different studies. Participants have access to ~30-second clips from the raw audio recordings, as well as pre-generated acoustic features. Participants can choose whether to use the audio recordings, the pre-generated features, or both. The focus of this challenge is on acoustic biomarkers, or voice-based features that may signal the presence of cognitive impairment. However, we encourage solvers to explore all possible features, including linguistic and semantic ones. \\n\n",
    "Data for the Social Determinants track: The data for this competition comes from a national longitudinal study of adults 50 years and older in Mexico, the Mexican Health and Aging Study (MHAS). The study includes information about demographics, economic circumstances, migration, physical limitations, self-reported health, and lifestyle behaviors.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\"Model Performance and Methodology (40%) How sound is the solution's methodology, including feature selection, validation techniques, and strategies to prevent data leakage and overfitting? How well does the model perform overall? Insights and Innovation (20%) What is the depth and relevance of insights gained from the modeling process (e.g., based on how predictions are made, feature importance)? To what extent does the approach demonstrate innovation or apply novel techniques? Bias Exploration and Mitigation (20%) How thoroughly does the solution explore and address potential biases in the data and model (e.g., systematic measurement error in important features)? How effective are the proposed strategies for mitigating identified biases? Generalizability (10%) How likely is the model to produce valid predictions on new data and in applied settings? Clarity & Communication (10%) How clearly and effectively are the findings communicated, both in text and visuals?\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/group/nih-nia-alzheimers-adrd-competition/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\"In this challenge, you will prototype and demo an AI-based research assistant solution for the NASA workforce. Like many of us who depend on technical and scientific literature in our own work, NASA researchers need to understand the state of research in a particular domain or multiple domains. They are also often faced with the daunting task of becoming familiar with a body of literature that they don't have prior experience with. Whether you are an academic doing your own research, a data scientist exploring new machine learning techniques, or anyone else delving into a field of study that's new to them, this problem may feel very familiar. With the AI Assistants challenge, NASA is seeking innovative approaches to help assess emerging capabilities in AI-based research assistants. Here's how it works.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"n/a\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\"Submissions will be judged using the following rubric of equally-weighted criteria. In assigning scores to each submission, judges will consider the questions outlined below for each criteria. 1. Relevance (25%) Does the solution address one of the research assistant tasks or another task that is relevant to NASA researcher workflows? How clearly is the relevance of the solution communicated in the video and writeup? 2. Effectiveness (25%) How effectively does the solution address a pain point or limitation in existing workflows? Where possible, does the submission provide quantitative measures of effectiveness (such as productivity increased, time saved, etc.)? Are there gaps or shortcomings in the solution that the participant may not have addressed? Are the solution outputs (recommendations, summarizations, etc.) trustworthy or verifiable? How clearly is the overall effectiveness of the solution communicated in the video and writeup? 3. Deployability (25%) How easy or difficult would the solution be to implement? To what extent is there a clear and feasible path for deploying the demonstrated solution so that it can be used on an ongoing basis by NASA researchers? Is the feasible path consistent with contemporary industry best practices for software development? Does it make use of or remain compatible with cloud-based services where appropriate? How high are the financial costs (API usage, cloud costs, subscriptions, etc.) to implement the solution? Do the video and writeup clearly show how the solution would be reproduced by a user or developer other than the participant? How clearly are these deployability concerns communicated in the video and writeup? 4. Novelty (25%) Does the solution employ a novel or unique approach relative to what is currently available to researchers? Is it unique compared with other challenge solutions? Does it make an original contribution to the public discourse about leveraging AI for this task? How clearly is the novelty of the solution communicated in the video and writeup?\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/252/ai-research-assistants/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Falls among adults 65 and older is the leading cause of injury-related deaths. Falls can also result in serious injuries to the head and/or broken bones. Some risk factors associated with falls can be reduced through appropriate interventions like treating vision problems, exercising for strength and balance, and removing tripping hazards in your home. Medical record narratives are a rich yet under-explored source of potential insights about how, when, and why people fall. However, narrative data sources can be difficult to work with, often requiring carefully designed, time-intensive manual coding procedures. Modern machine learning approaches to working with narrative data have the potential to effectively extract insights about older adult falls from narrative medical record data at scale. The goal in this challenge is to identify effective methods of using unsupervised machine learning to extract insights about older adult falls from emergency department narratives. Insights extracted from medical record narratives can potentially inform interventions for reducing falls.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\"The data for this challenge come from a large, public dataset: the National Electronic Injury Surveillance System (NEISS). NEISS data are managed and maintained by the Consumer Product Safety Commission and come from a representative sample of emergency departments in the United States. NEISS only includes injury records when a product is involved. The information in this data has been manually extracted from ED health records.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" Winners will be selected by a judging panel of domain experts and researchers. Submissions will be judged according to the following weighted criteria: 1. Novelty (35%)\n",
    " To what extent does this submission utilize creative, cutting-edge, or innovative techniques? This can be demonstrated in any or all parts of the submission (e.g., preprocessing, embeddings, models, visualizations). 2. Communication (25%)\n",
    " To what extent are findings clearly and effectively communicated? This includes both text and visuals. 3. Rigor (20%) To what extent is this submission based on appropriate and correctly implemented methods and approaches (e.g., preprocessing, embeddings, models) with adequate sample sizes? 4. Insight (20%) To what extent does this submission contain useful insights about the effectiveness of unsupervised machine learning methods at uncovering patterns in this data and/or informative findings that can advance the research on circumstances related to older adult falls?\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/217/cdc-fall-narratives/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Melanoma is a cancer of the skin which develops from cells responsible for skin pigmentation. In 2020, over 325,000 people were diagnosed with skin melanoma, with 57,000 deaths in the same year.1 Melanomas represent 10% of all skin cancers and are the most dangerous due to high likelihood of metastasizing (spreading).2 Patients are initially diagnosed with melanoma after a pathologist examines a portion of the cancerous tissue under a microscope. At this stage, the pathologist assesses the risk of relapse—a return of cancerous cells after the melanoma has been treated—based on information such as the thickness of the tumor and the presence of an ulceration. Combined with factors such as age, sex, and medical history of the patient, these microscopic observations can help a dermatologist assess the severity of the disease and determine appropriate surgical and medical treatment. Preventative treatments can be administered to patients with high likelihood for relapse. However, these are costly and expose patients to significant drug toxicity. Assessing the risk of relapse therefore a vital but difficult task. It requires specialized training and careful examination of microscopic tissue. Currently, machine learning approaches can help analyze whole slide images (WSIs) for basic tasks like measuring area. However, computer vision has also shown some potential in classifying tumor subtypes, and in time may serve as a powerful tool to aid pathologists in making the most accurate diagnosis and prognosis. 3, 4, 5 Your goal in this challenge is to predict whether a relapse will occur in the 5 years following the initial diagnosis using digitized versions of microscopic slides.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" The data for this challenge includes thousands of microscopic slides of skin melanomas from medical centers across France. Your task is to estimate the likelihood that a relapse will occur within five years.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" Performance is evaluated according to log loss. Log loss (a.k.a. logistic loss or cross-entropy loss) penalizes confident but incorrect predictions. It also rewards confidence scores that are well-calibrated probabilities, meaning that they accurately reflect the long-run probability of being correct. This is an error metric, so a lower value is better.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/148/visiomel-melanoma/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Help the National Airspace System (NAS) keep flights running on time! In this challenge you will use air traffic and weather data to automatically predict the time an aircraft pushes back from its gate. Accurate estimates of pushback time can help air traffic management systems more efficiently use the limited capacity of airports, runways and the National Airspace System. This challenge features two competition arenas: Phase 1: Open Arena is available for all participants to work with the data and test their solutions to see how they fare against others on the open leaderboard. Phase 1: Prescreened Arena is available to prize eligibile participants to build out and submit code for their solutions. Phase 2 is available to prize eligibile finalists from Phase 1. To qualify for a prize, finalists will develop their winning models into a federated learning models.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" This challenge is possible because of the effort that NASA, the FAA, airlines, and other agencies undertake to collect, process, and distribute data to decision makers in near real-time. You will be working with around two years of historical data, but any solution you develop could be translated directly into a pipeline with access to data collected in real time.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\" Performance is evaluated according to Mean Absolute Error (MAE), which measures how much the estimated values differ from the observed values. MAE is the mean of the magnitude of the differences between the predicted values and the ground truth. MAE is always non-negative, with lower values indicating a better fit to the data. The competitor that minimizes this metric will top the leaderboard. During prediction, each airline will use the final trained model to make pushback predictions for all of the flights it operates. To directly compare your Phase 1 centralized model with your Phase 2 federated model, both will be evaluated on the exact same dataset: the held-out test set from Phase 1—the only difference is that the private variables can only be accessed by the airline that produced them.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/group/competition-nasa-airport-pushback/\"\n",
    "})\n",
    "\n",
    "all_comps.append({\n",
    "    \"comp_overview\": \"\"\" Inland water bodies provide a variety of critical services for both human and aquatic life, including drinking water, recreational and economic opportunities, and marine habitats. A significant challenge water quality managers face is the formation of harmful algal blooms (HABs). One of the major types of HABs is cyanobacteria. HABs produce toxins that are poisonous to humans and their pets, and threaten marine ecosystems by blocking sunlight and oxygen. Manual water sampling, or “in situ” sampling, is generally used to monitor cyanobacteria in inland water bodies. In situ sampling is accurate, but time intensive and difficult to perform continuously. Your goal in this challenge is to use satellite imagery to detect and classify the severity of cyanobacteria blooms in small, inland water bodies. The resulting algorithm will help water quality managers better allocate resources for in situ sampling, and make more informed decisions around public health warnings for critical resources like drinking water reservoirs. Ultimately, more accurate and more timely detection of algal blooms helps keep both the human and marine life that rely on these water bodies safe and healthy.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"data_overview\": \"\"\" The feature data for this challenge is a combination of satellite imagery, climate data, and elevation data. Sentinel-2 is the only required data source — using any of the additional sources below is up to you! The four approved data sources are: Sentinel-2 satellite imagery; Landsat satellite imagery; NOAA's High-Resolution Rapid Refresh (HRRR) climate data; Copernicus DEM elevation data. \n",
    "\"\"\"\n",
    "    ,\n",
    "    \"eval_overview\": \"\"\"To measure your model’s performance, we’ll use Region-Averaged Root Mean Squared Error (RMSE). RMSE is the square root of the mean of squared differences between estimated and observed values. This is an error metric, so a lower value is better. RMSE is implemented in scikit-learn, with the squared parameter set to False.\n",
    "\"\"\"\n",
    "    ,\n",
    "    \"url\": \"https://www.drivendata.org/competitions/143/tick-tick-bloom/\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97fe6008-5153-4cbc-9f86-e50b7bbb66f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dfd5a77-9cad-4f98-baac-0e3d63ec2a89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"Data/drivendata_results/all_comps.json\", \"w\") as f:\n",
    "    json.dump(all_comps, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffd0041-e93b-43b9-b599-a5fb492f3c02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
