[
    {
        "competition_overview": "PETs Prize Challenge: Advancing Privacy-Preserving Federated Learning\nOverview\nPrivacy-enhancing technologies (PETs) have the potential to unlock more trustworthy innovation in data analysis and machine learning. Federated learning is one such technology that enables organizations to analyze sensitive data while providing improved privacy protections. These technologies could advance innovation and collaboration in new fields and help harness the power of data to tackle some of our most pressing societal challenges.\nThat\u2019s why the U.S. and U.K. governments are partnering to deliver a set of prize challenges to unleash the potential of these democracy-affirming technologies to make a positive impact. In particular, this challenge will tackle two critical problems via separate data tracks: Data Track A will help with the identification of financial crime, while Data Track B will bolster pandemic responses.\nBy entering the prize challenges, innovators will have the opportunity to compete for cash prizes and engage with regulators and government agencies. Announced at the Summit for Democracy in December 2021, the prize challenges are a product of a collaboration between multiple government departments and agencies on both sides of the Atlantic. Winning solutions will have the opportunity to be profiled at the second Summit for Democracy, to be convened by President Joe Biden, in early 2023.\nObjectives\nThe goal of this prize challenge is to mature federated learning approaches and build trust in their adoption. The challenge organizers hope to accelerate the development of efficient privacy-preserving federated learning solutions that leverage a combination of input and output privacy techniques to:\nDrive innovation in the technological development and application of novel privacy-enhancing technologies\nDeliver strong privacy guarantees against a set of common threats and privacy attacks\nGenerate effective models to accomplish a set of predictive or analytical tasks that support the use cases\nChallenge Structure\nYou will develop solutions that enhance privacy protections across the lifecycle of federated learning. This challenge offers two different data use cases\u2014financial crime prevention (Track A) and pandemic response and forecasting (Track B). You may develop solutions directed at either one or both tracks. If you choose to develop and submit a generalizable solution that applies to both Track A and B, you can also qualify for additional prizes dedicated to Generalizable Solutions.\nData Track A \u2013 Financial crime prevention \u2013 The United Nations estimates that up to $2 trillion of cross-border money laundering takes place each year, financing organized crime and undermining economic prosperity. Financial institutions such as banks and credit agencies, along with organizations that process transactions between institutions, such as the SWIFT global financial messaging provider, must protect personal and financial data, while also trying to report and deter illicit financial activities. Using synthetic datasets provided by SWIFT, you will design and later develop innovative privacy-preserving federated learning solutions that facilitate cross-institution and cross-border anomaly detection to combat financial crime. This use case features both vertical and horizontal data partitioning. Check out the Financial Crime overview page for more information.\nData Track B \u2013 Pandemic response and forecasting \u2013 As we continue to deal with COVID-19, it has become apparent that better ways to harness the power of data through analytics are critical for preparing for and responding to public health crises. Federated learning approaches could allow for responsible use of sensitive data to develop cross-organization and cross-border data analysis that would result in more robust forecasting and pandemic response capabilities. Using synthetic population datasets, you will design and later develop privacy-preserving federated learning solutions that can predict an individual\u2019s risk for infection. This use case features horizontal data partitioning. Check out the Pandemic Forecasting overview page for more information.\nGeneralizable Solutions \u2013 Cross-organization, cross-border use cases are certainly not limited to the financial or public health domains. Developing out-of-the-box generalized models that can be adapted for use with specific data or problem sets has great potential to advance the adoption and widespread use of privacy-preserving federated learning for public- and private-sector organizations in multiple sectors. To demonstrate generalizability, you may develop a solution using both the Track A and Track B datasets to be eligible for additional awards dedicated to generalizable solutions.\nMore information on how to get started can be found in the Problem Description.\nEligibility\nAt the time of entry, the Official Representative (individual or team lead, in the case of a group project) must be age 18 or older and a U.S. citizen or permanent resident of the United States or its territories. In the case of a private entity, the business shall be incorporated in and maintain a place of business in the United States or its territories.\nParticipants in this Challenge, whether they are individuals, entities, or team members, are prohibited from participating in the U.K. prize challenge, and participants in the U.K. prize challenge are likewise prohibited from participating in this Challenge. Individuals, entities, or team members who are found to have entered both the U.S. and the U.K. challenges will be disqualified from participating in either. If you wish to instead participate in the U.K. competition, please register here.\nIf you're looking to find teammates for this challenge, the community forum or the cross-U.S.\u2013U.K. public Slack channel are great places to start. You can request access to the Slack channel here.\nFor more details, please refer to the official rules.\nPhase 1 Timeline and Prizes\nThis is the first in a series of challenge phases with a total prize pool of $800,000! Each phase in the PETs Prize Challenge invites participants to further test and apply their innovations in privacy-preserving federated learning. Phase 1 is for Blue Teams to begin development on their solutions\u2014more details are provided on blue teams and red teams below in the Full Challenge Timeline and Prizes section.\nNote that your team must register and meet all requirements for Phase 1 in order to continue to be eligible to participate in Phase 2.\nPhase 1 Key Dates\nLaunch & Blue Team Registration Opened July 20, 2022\nAbstracts Due & Blue Team Registration Closed September 4, 2022 at 11:59:59 PM UTC\nConcept Papers Due September 19, 2022 at 11:59:59 PM UTC\nWinners Announced November 10, 2022\nPhase 1 Prizes\nConcept Paper Deadline:\nSept. 19, 2022, 11:59 p.m. UTC\nPlace Prize Amount\n1st $30,000\n2nd $15,000\n3rd $10,000\nFull Challenge Timeline and Prizes\nTimeline Overview\nThere are three main phases in the challenge with two types of participants based on a red team/blue team approach. Blue Teams develop privacy-preserving solutions, while Red Teams act as adversaries to test those solutions.\nPhase 1: Concept Development (Jul\u2013Sept 2022): Blue Teams propose privacy-preserving federated learning solution concepts.\nPhase 2: Solution Development (Oct 2022\u2013Jan 2023): Blue Teams develop working prototypes of their solutions.\nPhase 3: Red Teaming (Nov 2022\u2013Feb 2023): Red Teams prepare and test privacy attacks on top blue team solutions from Phase 2.\nPrize Overview\nTrack Prize Pool\nPhase 1: Concept Paper (CURRENT) $55,000\nPhase 2: Solution Development $575,000\nPhase 3: Red Teaming $120,000\nOpen Source $140,000*\nTotal $800,000\n* UPDATE March 22, 2023: Open Source award amounts have been increased to $20,000 per award and the number of awards have been increased to 7, for a total of up to $140,000. The increase is reallocated prize moneys that were not awarded in earlier phases, and the total pool of prize awards for the Challenge remains $800,000.\nPhase Details and Prizes\nPlace Prize Amount\n1st $30,000\n2nd $15,000\n3rd $10,000\nPhase 1: Concept Paper\nJuly 20\u2013September 19, 2022 (YOU ARE HERE)\nParticipants will produce an abstract and technical concept paper laying out their proposed solution. Concept papers will be evaluated by a panel of judges across a set of weighted criteria. Participants will be eligible to win prizes awarded to the top technical papers, ranked by points awarded. Participants must complete a paper in Phase 1 in order to be eligible to compete in Phase 2.\nOpen to Blue Team participants.\nData Track A: Financial Crime Prevention\nPrize Amount\n1st $100,000\n2nd $50,000\n3rd $25,000\nData Track B: Pandemic Response and Forecasting\nPrize Amount\n1st $100,000\n2nd $50,000\n3rd $25,000\nGeneralized Solutions\nPrize Amount\n1st $100,000\n2nd $50,000\n3rd $25,000\nSpecial Recognition\nPrize Amount\nPool $50,000\nPhase 2: Solution Development\nOctober 5, 2022\u2013January 26, 2023\nPut your papers into action! Registered teams from Phase 1 will develop working prototypes and submit them to a remote execution environment for federated training and evaluation. These solutions are expected to be functional, i.e., capable of training a model and predicting against the evaluation data set with measurement of relevant performance and accuracy metrics. Solutions will be evaluated by a panel of judges across a set of weighted criteria. The top solutions, ranked by points awarded, will have their final rankings determined by incorporating red team evaluation from the red teams from Phase 3.\nTeams can qualify for one or multiple of Data Track A: Financial Crime Prevention, Data Track B: Pandemic Response and Forecasting, and Generalized Solutions prize categories depending on how their solutions address the two privacy-preserving federated learning tasks in the challenge.\nA separate Special Recognition prize pool is set aside to award up to five solutions that do not win prizes from the three main prize categories but demonstrate excellence in specific areas of privacy innovation: novelty, advancement in a specific privacy technology, usability, and efficiency.\nOpen to Blue Team participants.\nPrize Amount\n1st $60,000\n2nd $40,000\n3rd $20,000\nPhase 3: Red Teaming\nNovember 10, 2022\u2013February 28, 2023\nPrivacy researchers are invited to form red teams to put the privacy claims of Phase 2's blue team finalists to the test. The red teams will prepare and test privacy attacks on the Phase 2 finalist solutions, which will be incorporated into the final Phase 2 rankings. Top red teams will be evaluated for success and rigor and will be awarded prizes for their performance during this phase.\nOpen to Red Team participants.\nPrize Amount\nPool $140,000\nOpen Source\nSubmissions due April 20, 2023\nUp to 7 of the final blue team winners from Phase 2 will be invited to release their solutions as open-source software. Each verified participating blue team will be awarded an Open Source prize of $20,000.\nOpen to top Blue Team winners from Phase 2.\n\nHow to compete (Phase 1)\nClick the \"Compete!\" button in the sidebar to enroll in the competition.\nClick on \"Team\" if you need to create or join a team with other participants. To find other participants to form a team with, check out our community forum or the cross-U.S.\u2013U.K. public Slack channel. You can request access to the Slack channel here.\nGet familiar with the problem through the data overview and problem description pages.\nSummarize your modeling and privacy preserving techniques in an abstract, making sure to follow the instructions on the problem description page. Submit it by clicking on \"Abstract Submission\" in the sidebar and filling out the form. Partway there!\nDive into the details of your modeling and privacy preserving techniques in a concept paper, making sure to follow the instructions on the problem description page. Submit it by clicking on \"Concept Paper Submission\" in the sidebar and filling out the form. You're in!\nNote that registration closes on September 4, 2022 at the same time that abstracts are due. Your team must register by this deadline to participate in Phase 1 or Phase 2 of the challenge.\nThis challenge is sponsored by the National Institute for Standards and Technology (NIST) and the National Science Foundation (NSF)\n\n        \nWith additional collaboration from the Office of Science and Technology Policy (OSTP), NASA, SWIFT, and the University of Virginia's Biocomplexity Institute.\nThis prize challenge and its U.K. counterpart have been developed as part of a joint collaboration between the United Kingdom and the United States.",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/98/nist-federated-learning-1/"
    },
    {
        "competition_overview": "PETs Prize Challenge: Advancing Privacy-Preserving Federated Learning\nThis is the second in a series of challenge phases with a total prize pool of $800,000! Each phase in the PETs Prize Challenge invites participants to test and apply their innovations in privacy-preserving federated learning. Phase 2 is for Blue Teams to implement and submit working code for their solutions.\nYou are on the Phase 2 home page for the Financial Crime Data Track. You can find the Pandemic Forecasting Data Track here.\nBackground\nPrivacy-enhancing technologies (PETs) have the potential to unlock more trustworthy innovation in data analysis and machine learning. Federated learning is one such technology that enables organizations to analyze sensitive data while providing improved privacy protections. These technologies could advance innovation and collaboration in new fields and help harness the power of data to tackle some of our most pressing societal challenges.\nThat\u2019s why the U.S. and U.K. governments are partnering to deliver a set of prize challenges to unleash the potential of these democracy-affirming technologies to make a positive impact. In particular, this challenge will tackle two critical problems via separate data tracks: Data Track A will help with the identification of financial crime, while Data Track B will bolster pandemic responses.\nBy entering the prize challenges, innovators will have the opportunity to compete for cash prizes and engage with regulators and government agencies. Announced at the Summit for Democracy in December 2021, the prize challenges are a product of a collaboration between multiple government departments and agencies on both sides of the Atlantic. Winning solutions will have the opportunity to be profiled at the second Summit for Democracy, to be convened by President Joe Biden, in early 2023.\nObjectives\nThe goal of this prize challenge is to mature federated learning approaches and build trust in their adoption. The challenge organizers hope to accelerate the development of efficient privacy-preserving federated learning solutions that leverage a combination of input and output privacy techniques to:\nDrive innovation in the technological development and application of novel privacy-enhancing technologies\nDeliver strong privacy guarantees against a set of common threats and privacy attacks\nGenerate effective models to accomplish a set of predictive or analytical tasks that support the use cases\nChallenge Structure\nThe Solution Development Phase will have two data use case tracks matching those previously from the Concept Paper Phase\u2014Track A: Financial Crime Prevention and Track B: Pandemic Response and Forecasting. Teams can elect to participate in either or both tracks, with solutions that apply to one track individually or with a generalized solution. Each solution must correspond to a concept paper that met the requirements from the Concept Paper Phase.\nEach data use case track has a prize category for top solutions. All solutions that apply to Track A or Track B are eligible for a prize within that respective track. Teams who submit solutions for Track A and Track B are eligible to win a prize from each track.\nAdditionally, participants can enter a third Generalized Solutions category of prizes for the best solutions that are shown to be generalized and applied to both use cases with minor adaptations. Teams submitting generalized solutions are eligible for Generalized Solution prizes in addition to being eligible for prizes from both Track A and Track B.\nData Track A \u2013 Financial crime prevention \u2013 The United Nations estimates that up to $2 trillion of cross-border money laundering takes place each year, financing organized crime and undermining economic prosperity. Financial institutions such as banks and credit agencies, along with organizations that process transactions between institutions, such as the SWIFT global financial messaging provider, must protect personal and financial data, while also trying to report and deter illicit financial activities. Using synthetic datasets provided by SWIFT, you will design and later develop innovative privacy-preserving federated learning solutions that facilitate cross-institution and cross-border anomaly detection to combat financial crime. This use case features both vertical and horizontal data partitioning. Check out the Financial Crime overview page for more information.\nData Track B \u2013 Pandemic response and forecasting \u2013 As we continue to deal with COVID-19, it has become apparent that better ways to harness the power of data through analytics are critical for preparing for and responding to public health crises. Federated learning approaches could allow for responsible use of sensitive data to develop cross-organization and cross-border data analysis that would result in more robust forecasting and pandemic response capabilities. Using synthetic population datasets, you will design and later develop privacy-preserving federated learning solutions that can predict an individual\u2019s risk for infection. This use case features horizontal data partitioning. Check out the Pandemic Forecasting overview page for more information.\nGeneralizable Solutions \u2013 Cross-organization, cross-border use cases are certainly not limited to the financial or public health domains. Developing out-of-the-box generalized models that can be adapted for use with specific data or problem sets has great potential to advance the adoption and widespread use of privacy-preserving federated learning for public- and private-sector organizations in multiple sectors. To demonstrate generalizability, you may develop a solution using both the Track A and Track B datasets to be eligible for additional awards dedicated to generalizable solutions.\nMore information on how to get started can be found in the Problem Description.\nEligibility\nPhase 2 is open to Blue Team Participants who won invitations because their Concept Papers met the minimum criteria described in the challenge rules. No further registration will be required for the invitees to advance to Phase 2; however, invited Blue Team Participants should make necessary updates to their registration via their DrivenData account to reflect any changes in team composition or contact information. For more details, please refer to the official rules.\nPhase 2 Timeline and Prizes\nPhase 2 Key Dates\nLaunch October 5, 2022\nDeadline to Open Pull Requests for Runtime Environment January 11, 2023 at 11:59:59 PM UTC\nSubmissions Due January 26, 2023 at 11:59:59 PM UTC\nAnnouncement of Finalists to be Tested in Phase 3 Red Teaming February 13, 2023\nWinners Announced March 30, 2023\nPhase 2 Prizes\nOpen to Blue Team participants.\nData Track A: Financial Crime Prevention\nPrize Amount\n1st $100,000\n2nd $50,000\n3rd $25,000\nData Track B: Pandemic Response and Forecasting\nPrize Amount\n1st $100,000\n2nd $50,000\n3rd $25,000\nGeneralized Solutions\nPrize Amount\n1st $100,000\n2nd $50,000\n3rd $25,000\nSpecial Recognition\nPrize Amount\nPool $50,000\nPut your papers into action! Registered teams from Phase 1 will develop working prototypes and submit them to a remote execution environment for federated training and evaluation. These solutions are expected to be functional, i.e., capable of training a model and predicting against the evaluation data set with measurement of relevant performance and accuracy metrics. Solutions will be evaluated by a panel of judges across a set of weighted criteria. The top solutions, ranked by points awarded, will have their final rankings determined by incorporating red team evaluation from the red teams from Phase 3.\nTeams can qualify for one or multiple of Data Track A: Financial Crime Prevention, Data Track B: Pandemic Response and Forecasting, and Generalized Solutions prize categories depending on how their solutions address the two privacy-preserving federated learning tasks in the challenge.\nA separate Special Recognition prize pool is set aside to award up to five solutions that do not win prizes from the three main prize categories but demonstrate excellence in specific areas of privacy innovation: novelty, advancement in a specific privacy technology, usability, and efficiency.\nFull Challenge Timeline and Prizes\nTimeline Overview\nThere are three main phases in the challenge with two types of participants based on a red team/blue team approach. Blue Teams develop privacy-preserving solutions, while Red Teams act as adversaries to test those solutions.\nPhase 1: Concept Development (Jul\u2013Sept 2022): Blue Teams propose privacy-preserving federated learning solution concepts.\nPhase 2: Solution Development (Oct 2022\u2013Jan 2023): Blue Teams develop working prototypes of their solutions.\nPhase 3: Red Teaming (Nov 2022\u2013Feb 2023): Red Teams prepare and test privacy attacks on top blue team solutions from Phase 2.\nPrize Overview\nTrack Prize Pool\nPhase 1: Concept Paper $55,000\nPhase 2: Solution Development $575,000\nPhase 3: Red Teaming $120,000\nOpen Source $140,000*\nTotal $800,000\n* UPDATE March 22, 2023: Open Source award amounts have been increased to $20,000 per award and the number of awards have been increased to 7, for a total of up to $140,000. The increase is reallocated prize moneys that were not awarded in earlier phases, and the total pool of prize awards for the Challenge remains $800,000.\nAdditional Phase Details and Prizes\nPlace Prize Amount\n1st $30,000\n2nd $15,000\n3rd $10,000\nPhase 1: Concept Paper\nJuly 20\u2013September 19, 2022\nParticipants will produce an abstract and technical concept paper laying out their proposed solution. Concept papers will be evaluated by a panel of judges across a set of weighted criteria. Participants will be eligible to win prizes awarded to the top technical papers, ranked by points awarded. Participants must complete a paper in Phase 1 in order to be eligible to compete in Phase 2.\nOpen to Blue Team participants.\nPrize Amount\n1st $60,000\n2nd $40,000\n3rd $20,000\nPhase 3: Red Teaming\nNovember 10, 2022\u2013February 28, 2023\nPrivacy researchers are invited to form red teams to put the privacy claims of Phase 2's blue team finalists to the test. The red teams will prepare and test privacy attacks on the Phase 2 finalist solutions, which will be incorporated into the final Phase 2 rankings. Top red teams will be evaluated for success and rigor and will be awarded prizes for their performance during this phase.\nOpen to Red Team participants.\nPrize Amount\nPool $140,000\nOpen Source\nSubmissions due April 20, 2023\nUp to 7 of the final blue team winners from Phase 2 will be invited to release their solutions as open-source software. Each verified participating blue team will be awarded an Open Source prize of $20,000.\nOpen to top Blue Team winners from Phase 2.\n\nHow to compete (Phase 2)\nOnly blue teams who participated in Phase 1 and met minimum requirements are eligible to participate in Phase 2. Eligible teams are automatically registered. If you need to make changes to your team, please contact info@drivendata.org.\nGet familiar with the problem through the problem description and code submission format pages.\nDevelop and submit your centralized solution. Check out the centralized code submission format page for more details.\nDevelop and submit your federated solution. Check out the federated code submission format page for more details.\nGood luck!\nThis challenge is sponsored by the National Institute for Standards and Technology (NIST) and the National Science Foundation (NSF)\n\n        \nWith additional collaboration from the Office of Science and Technology Policy (OSTP), NASA, SWIFT, and the University of Virginia's Biocomplexity Institute.\nThis prize challenge and its U.K. counterpart have been developed as part of a joint collaboration between the United Kingdom and the United States.",
        "dataset_overview": "Data Track A: Financial Crime\nTransforming Financial Crime Prevention\nThere are two data use case tracks for the PETs prize challenge. This is the Phase 2 data overview page for the financial crime prevention track. In this track, innovators will develop end-to-end privacy-preserving federated learning solutions to detect potentially anomalous payments, leveraging a combination of input- and output-privacy techniques.\nBackground\nBackground\nData\nOverview\nThreat Profile\nModeling\nAnomaly Detection\nPartitioning Data\nExample Centralized Baseline\nBackground\nThe financial crime track is focused on enhancing cross-organization, cross-border data access to support efforts to combat fraud, money laundering and other financial crime. You are asked to develop innovative, privacy-preserving solutions to enable detection of potentially anomalous payments, utilizing synthetic datasets representing data held by the SWIFT payments network and datasets held by partner banks.\n\u201cAnomalous transactions\u201d covers a range of payments that vary significantly from the norms seen in the dataset, and thus may be indicative of fraud, money laundering, or other financial crime. Examples include a transaction that is of an unexpected amount or currency, uses unusual corridors (senders/receivers), has unusual timestamps, or contains other unusual fields. In the scope of this challenge, the problem is framed as a classification task. The training datasets are labeled with anomalies, and therefore you do not need a detailed understanding of financial crime issues.\nThis is a high-impact and exciting use case for novel privacy-enhancing technologies. There are currently challenging trade-offs between enabling sufficient access to data to build tools to effectively detect illegal financial activity, and limiting the identifiability of innocent individuals or inference of their sensitive information within those data sets. The scale of the problem is vast: the UN estimates that US$800-2000bn is laundered each year, representing 2-5% of global GDP.\nThough novel innovation for this use case alone could achieve significant real-world impact, the challenge is designed to incentivize development of privacy technologies that can be applied to other use cases where data is distributed across multiple organisations or jurisdictions, both in financial services and elsewhere. The best solutions will deliver meaningful innovation towards deployable solutions in this space, with consideration of how to evidence the privacy guarantees offered to data owners and regulators, but also have the potential to generalize to other situations.\nData Overview\nInnovators will use synthetic datasets representing data held by the SWIFT global payments network and by its partner banks. In Phase 1, you were provided with two development datasets:\nDataset 1: A synthetic dataset representing transaction data from the SWIFT global payment network\nDataset 2: Synthetic customer / account metadata, including flags, from SWIFT's partner banks\nThere are approximately 4 million rows across the two development datasets.\nAdditional development data may be released for Phase 2.\nNote: The challenges are based on synthetic data to minimize the security burden placed on competitors during the development phase; of course the intent of the challenge is that privacy solutions are developed that would be appropriate for use on real data sets with demonstrable privacy guarantees. However, competitors must adhere to a data use agreement (see competition rules for more details).\nDataset 1: Transaction data held by SWIFT\nIn Phase 1, you were provided a synthetic dataset derived from data from the SWIFT global payment network. Each row in this dataset is an individual transaction, representing a payment from one sending bank to one receiving bank. The dataset will:\nContain data elements as defined in the ISO20022 pacs.008 / MT103 message format\nComprise transactions between fictitious originators and beneficiaries, sender and receiving banks, payment corridor, amount and timestamps\nExpertise in financial crime or ISO20022 messaging is not an expected prerequisite for entering the challenge, and the assessment process will not focus on detailed understanding of the use case itself. The details in the sections below should be sufficient for understanding the data within the scope of the challenge. However, participants unfamiliar with this space may find it informative to consult a general introduction to ISO20022. You may also find the ISO20022 message definitions informative.\nThe dataset reflects a snapshot of transactions sent by an ordering customer or institution to credit a beneficiary customer or institution. The dataset covers roughly a month\u2019s worth of transactions involving 50 institutions.\nThe synthetic data is not generated based on any real traffic and will not contain any statistical properties of the real SWIFT transaction data (SWIFT has applied normal and uniform distributions).\nDataset 1 details\nDataset 1 contains the following fields:\nMessageId - Globally unique identifier within this dataset for individual transactions\nUETR - The Unique End-to-end Transaction Reference\u2014a 36-character string enabling traceability of all individual transactions associated with a single end-to-end transaction\nTransactionReference - Unique identifier for an individual transaction\nTimestamp - Time at which the individual transaction was initiated\nSender - Institution (bank) initiating/sending the individual transaction\nReceiver - Institution (bank) receiving the individual transaction\nOrderingAccount - Account identifier for the originating ordering entity (individual or organization) for end-to-end transaction,\nOrderingName - Name for the originating ordering entity\nOrderingStreet - Street address for the originating ordering entity\nOrderingCountryCityZip - Remaining address details for the originating ordering entity\nBeneficiaryAccount - Account identifier for the final beneficiary entity (individual or organization) for end-to-end transaction\nBeneficiaryName - Name for the final beneficiary entity\nBeneficiaryStreet - Street address for the final beneficiary entity\nBeneficiaryCountryCityZip - Remaining address details for the final beneficiary entity\nSettlementDate - Date the individual transaction was settled\nSettlementCurrency - Currency used for transaction\nSettlementAmount - Value of the transaction net of fees/transfer charges/forex\nInstructedCurrency - Currency of the individual transaction as instructed to be paid by the Sender\nInstructedAmount - Value of the individual transaction as instructed to be paid by the Sender\nLabel - Boolean indicator of whether the transaction is anomalous or not. This is the target variable for the prediction task.\nEnd-to-end transactions\nEach row in this dataset is an individual transaction, representing a payment from a sender bank to a receiver bank. An end-to-end transaction is a transaction from an originating ordering entity (a.k.a. ultimate debtor) to a final beneficiary entity (a.k.a. ultimate creditor) and may involve one or more individual transactions. The end-to-end transaction is one individual transaction in the case where the originating orderer's bank sends payment directly to the final beneficiary's bank. However, it may be the case where the payment is not directly sent, but is instead routed through one or more intermediary banks. In such a case, there are multiple individual transactions belonging to the single end-to-end transaction, with each individual transaction representing a bank-to-bank payment. Each end-to-end transaction is uniquely identified by the UETR field. In the case of a sequence of multiple individual transactions for one end-to-end transaction, all individual transactions share a value for UETR, and the Sender and Receiver banks form a chain from the originating ordering bank through one or more intermediary banks to the final beneficiary bank.\nBecause each end-to-end transaction is defined by one originating orderer and one final beneficiary, this means the Ordering* columns for the orderer and Beneficiary* columns for the beneficiary have been included in this dataset in a denormalized fashion\u2014the values are duplicated across all the individual transactions (rows) belonging to the same end-to-end transaction. Additionally, this means that the OrderingAccount and BeneficiaryAccount in a given row may not necessarily belong to the bank in that row's Sender and the bank in that row's Receiver, respectively. The correct way to associate an OrderingAccount to the correct bank is to identify the Sender bank in the originating (first) individual transaction in that end-to-end transaction, and the correct way to associate a BeneficiaryAccount to the correct bank is to identify the Receiver bank in the final (last) individual transaction in that end-to-end transaction.\nMessageId UETR Sender Receiver OrderingAccount BeneficiaryAccount ...\n... ... ... ... ... ... ...\n10 00012345-... A B 111 222 ...\n11 00012345-... B C 111 222\n12 00012345-... C D 111 222\n... ... ... ... ... ... ...\nIllustrative example showing the how to associate the originating orderer and final beneficiary information with the correct banks for one end-to-end transaction made up of three individual transactions. The orderer and beneficiary account information is duplicated across all rows in this group, and the sender and receiver banks form a chain. The bank and account information of the originating orderer is highlighted in blue, and the bank and account information for the final beneficiary is highlighted in yellow.\nDataset 2: Account data held by banks\nParticipants were provided access to account-related data representative of that held by banks. This dataset contains account-level information, including flags signaling whether the account is valid, suspended, etc.\nData can be linked using the Account field in the bank data and the OrderingAccount or BeneficiaryAccount in the SWIFT transaction data. Please see the previous section on end-to-end transactions for details on how to identify which bank an OrderingAccount or BeneficiaryAccount should be linked to.\nNote that bank nodes will not have access to data on the SWIFT node and vice-versa\u2014a case of vertical data partitioning. It is up to you to determine how to exchange this information in a secure and private way.\nDataset 2 details\nDataset 2 will contain the following fields:\nBank - Identifier for the bank\nAccount - Identifier for the account\nName - Name of the account\nStreet - Street address associated with the account\nCountryCityZip - Remaining address details associated with the account\nFlags - Enumerated data type indicating potential issues or special features that have been associated with an account. Flag definitions are provided below:\n00 - No flags\n01 - Account closed\n03 - Account recently opened\n04 - Name mismatch\n05 - Account under monitoring\n06 - Account suspended\n07 - Account frozen\n08 - Non-transaction amount\n09 - Beneficiary deceased\n10 - Invalid company ID\n11 - Invalid individual ID\nNote that this dataset is provided unpartitioned, with all banks' data in one table.\nNote that the flags may not be representative of real-world practices. For example, in the real world, banks may use different flags and may interpret or weight them differently based on appetite for risk.\nDevelopment and Evaluation Data\nThe datasets being provided are intended for local development use in both Phase 1 and Phase 2. The transaction dataset has been split in time\u2013the bulk of the dataset is the training set, and the final week of the dataset is a test set. The prediction task, as detailed in a later section, is to predict a confidence score for each individual transaction in the test set as to whether it is an anomalous transaction. The ground truth is provided for both the training and set sets in the development dataset.\nIn Phase 2, a separate and held-out dataset will be used for solution evaluation. Some aspects of the Phase 2 evaluation data may be changed that should be learnable by your model. You will submit code for your solution to a code execution environment. The code execution runtime will run cold-start federated training on the new dataset's training split and then run inference to generate predictions for the new dataset's test split. Your solution's performance will be measured by evaluating its predictions against the ground truth for the new dataset's test split.\nNew Development Dataset\nNew development dataset published December 3, 2022.\nA new development dataset has been provided by our data partners at SWIFT and is available on the data download page, indicated by the [NEW] tag. This new development dataset is a synthetic dataset that is largely similar to the previously-provided synthetic development dataset (also still available on the data download page), but has some differences that better represent what is observed in the real world. You should expect that the evaluation dataset (held out and will never be made available) will have similar distributions as this new dataset. Local development and self-reported results in the technical paper should primarily use the new development dataset.\nThreat Profile\nYou will design and develop end-to-end solutions that preserve privacy across a range of possible threats and attack scenarios, through all stages of the machine learning model lifecycle. You should therefore carefully consider the overall privacy of your solution, focusing on the protection of sensitive information held by all parties involved in the federated learning scenario. The solutions you design and develop should include comprehensive measures to address the threat profiles described below. These measures will provide an appropriate degree of resilience to a wide range of potential attacks defined within the threat profile. For more information on threat profiles, please visit the privacy threat profile section of the problem description.\nScope of sensitive data\nYour solution must prevent the unintended disclosure of\na) sensitive information in the SWIFT transaction dataset\nb) sensitive information in the bank dataset, to any other party, including other insider stakeholders (for example, SWIFT and other financial institutions) and outsiders.\nThe sensitive information for the SWIFT dataset is all personally-identifiable information about the originating orderer (a.k.a. ultimate debtor) and final beneficiary (a.k.a. ultimate creditor) parties, including personal details like names and addresses, and group membership information. This includes but is not limited to the raw private data about the orderer/beneficiary stored directly in the account number, name, and address fields, and the transaction identifiers and timestamps.\nThe sensitive information for the bank datasets include all personally identifiable information about parties involved in the transactions, including names and addresses and group membership information. This includes but is not limited to the raw private / business data reflected in account numbers/names/addresses and flags.\nAnomaly detection models\nThe analytical objective is to train a model that enables SWIFT to identify anomalous transactions. In the context of this challenge, this is a classification model to be trained on provided training data with ground truth labels. In real-world deployments, such transactions might be subject to additional verification actions or flagged for further investigation, dependent on context.\nPrediction Target and Evaluation Metric\nThe target variable for the modeling task is a confidence score (between 0.0 and 1.0) for whether each individual transaction is anomalous. As discussed previously, anomalous is not precisely defined and should be learned by your model via supervised learning on provided training data.\nThe evaluation metric will be Area Under the Precision\u2013Recall Curve (AUPRC), also known as average precision (AP), PR-AUC, or AUCPR. This is a commonly used metric for binary classification that summarizes model performance across all operating thresholds. This metric rewards models which can consistently assign anomalous transactions with a higher confidence score than negative non-anomalous transactions. AUPRC is computed as follows:\nAUPRC=\n\u2211\nn\n(\nR\nn\n\u2212\nR\nn\u22121\n)\nP\nn\nwhere\nP\nn\nand\nR\nn\nare the precision and recall, respectively, when thresholding at the nth individual transaction sorted in order of increasing recall.\nPartitioned Datasets for Federated Learning\nPartitioning Overview\nA number of banks are working with SWIFT to collaboratively train a model. The parties are working jointly to do this, and can take a common approach to technical design, infrastructure, etc., but are not able to enable access to each other's data. In the real world there are a number of barriers that might prevent this; banks are subject to a variety of privacy, competition and financial industry regulations, may be operating in different jurisdictions, and have legitimate commercial and ethical reasons for not sharing customer data with competitors.\nThe key task of this challenge is to design a privacy solution so that SWIFT can safely train and deploy such a model without compromising the privacy requirements (more details on the requirements, and an associated threat model, are described in the problem description).\nAbove: Diagram comparing the centralized model (MC) with a privacy-preserving federated learning model (MPF).\nPartitioning Details\nThis use case features both vertical and horizontal data partitioning. Data is vertically partitioned between Dataset 1 (SWIFT) and Dataset 2 (partner banks), and it is horizontally partitioned within Dataset 2 (between each partner bank).\nFor local development, you were provided a full, unpartitioned dataset. In Phase 2 evaluation, evaluation will occur with predetermined partitioning along institutional boundaries. The SWIFT data will always belong a single federation unit who represents the SWIFT Data Store and only has access to the SWIFT data. Banks will be split up among federation units such that one bank's account data entirely belongs within one partition. In cases where there are fewer bank partitions than the number of banks, each bank partition may contain data from more than one bank.\nAny partitioning of the data that you might perform in your local development experiments should take this into account. Your solution should be able to handle any number of bank partitions, and in Phase 2, we may evaluate your solution with a number of bank partitions between 1 and 10.\nKey Task\nFor the purposes of the challenge, you should demonstrate your solution by training two models:\nM\nC\n= a centralized model trained on datasets 1 and 2 in a non-privacy preserving way\nM\nPF\n= a privacy-preserving federated model trained using your privacy solution\nExample Centralized Baseline\nIn Phase 1, SWIFT provided sample Python code for training a centralized anomaly detection model (\nM\nC\n) on the ISO20022 training data. This code snippet used the dataset provided as input and trained a simple anomaly detection model using the XGBoost Classifier.\nThe core of the evaluation will be assessing the comparison between a centralized model\nM\nC\n, and an alternative model\nM\nPF\nthat combines a federated learning approach with innovative privacy-preserving techniques.\nIn the real world, SWIFT may wish to train a model collaboratively with a number of banks, in order to increase the volume and variety of data being used to train the model. You should therefore aim to develop scalable solutions that enable additional nodes to be integrated into the federated network whilst incurring an acceptable additional performance overhead.\nThe federated learning scenario thus consists of one node hosting the SWIFT dataset, and N nodes hosting bank data. We may evaluate solutions for values of N between 1 and 10, in order to assess how well solutions scale as more banks are added to the network. During solution development, you may have full autonomy over how you partition the bank dataset in order to understand the scalability of your solution.\nFull details on evaluation criteria can be found in the problem description.\nFor additional reference, here is a technical brief for this use case track provided through the U.K. challenge. This brief has been assembled in collaboration by the U.K. and U.S. challenge organizers. This may help to give a sense for the use case and capabilities expected, though note that details in the brief may not match exactly how the U.S. challenge will operate.\nGood luck\nGood luck and enjoy this problem! For more details on submission and evaluation, visit the problem description page. If you have any questions, you can always ask the community by visiting the DrivenData user forum or the cross-U.S.\u2013U.K. public Slack channel. You can request access to the Slack channel here.",
        "evaluation_overview": "Data Track A: Financial Crime\nTransforming Financial Crime Prevention\nThere are two data use case tracks for the PETs prize challenge. This is the Phase 2 data overview page for the financial crime prevention track. In this track, innovators will develop end-to-end privacy-preserving federated learning solutions to detect potentially anomalous payments, leveraging a combination of input- and output-privacy techniques.\nBackground\nBackground\nData\nOverview\nThreat Profile\nModeling\nAnomaly Detection\nPartitioning Data\nExample Centralized Baseline\nBackground\nThe financial crime track is focused on enhancing cross-organization, cross-border data access to support efforts to combat fraud, money laundering and other financial crime. You are asked to develop innovative, privacy-preserving solutions to enable detection of potentially anomalous payments, utilizing synthetic datasets representing data held by the SWIFT payments network and datasets held by partner banks.\n\u201cAnomalous transactions\u201d covers a range of payments that vary significantly from the norms seen in the dataset, and thus may be indicative of fraud, money laundering, or other financial crime. Examples include a transaction that is of an unexpected amount or currency, uses unusual corridors (senders/receivers), has unusual timestamps, or contains other unusual fields. In the scope of this challenge, the problem is framed as a classification task. The training datasets are labeled with anomalies, and therefore you do not need a detailed understanding of financial crime issues.\nThis is a high-impact and exciting use case for novel privacy-enhancing technologies. There are currently challenging trade-offs between enabling sufficient access to data to build tools to effectively detect illegal financial activity, and limiting the identifiability of innocent individuals or inference of their sensitive information within those data sets. The scale of the problem is vast: the UN estimates that US$800-2000bn is laundered each year, representing 2-5% of global GDP.\nThough novel innovation for this use case alone could achieve significant real-world impact, the challenge is designed to incentivize development of privacy technologies that can be applied to other use cases where data is distributed across multiple organisations or jurisdictions, both in financial services and elsewhere. The best solutions will deliver meaningful innovation towards deployable solutions in this space, with consideration of how to evidence the privacy guarantees offered to data owners and regulators, but also have the potential to generalize to other situations.\nData Overview\nInnovators will use synthetic datasets representing data held by the SWIFT global payments network and by its partner banks. In Phase 1, you were provided with two development datasets:\nDataset 1: A synthetic dataset representing transaction data from the SWIFT global payment network\nDataset 2: Synthetic customer / account metadata, including flags, from SWIFT's partner banks\nThere are approximately 4 million rows across the two development datasets.\nAdditional development data may be released for Phase 2.\nNote: The challenges are based on synthetic data to minimize the security burden placed on competitors during the development phase; of course the intent of the challenge is that privacy solutions are developed that would be appropriate for use on real data sets with demonstrable privacy guarantees. However, competitors must adhere to a data use agreement (see competition rules for more details).\nDataset 1: Transaction data held by SWIFT\nIn Phase 1, you were provided a synthetic dataset derived from data from the SWIFT global payment network. Each row in this dataset is an individual transaction, representing a payment from one sending bank to one receiving bank. The dataset will:\nContain data elements as defined in the ISO20022 pacs.008 / MT103 message format\nComprise transactions between fictitious originators and beneficiaries, sender and receiving banks, payment corridor, amount and timestamps\nExpertise in financial crime or ISO20022 messaging is not an expected prerequisite for entering the challenge, and the assessment process will not focus on detailed understanding of the use case itself. The details in the sections below should be sufficient for understanding the data within the scope of the challenge. However, participants unfamiliar with this space may find it informative to consult a general introduction to ISO20022. You may also find the ISO20022 message definitions informative.\nThe dataset reflects a snapshot of transactions sent by an ordering customer or institution to credit a beneficiary customer or institution. The dataset covers roughly a month\u2019s worth of transactions involving 50 institutions.\nThe synthetic data is not generated based on any real traffic and will not contain any statistical properties of the real SWIFT transaction data (SWIFT has applied normal and uniform distributions).\nDataset 1 details\nDataset 1 contains the following fields:\nMessageId - Globally unique identifier within this dataset for individual transactions\nUETR - The Unique End-to-end Transaction Reference\u2014a 36-character string enabling traceability of all individual transactions associated with a single end-to-end transaction\nTransactionReference - Unique identifier for an individual transaction\nTimestamp - Time at which the individual transaction was initiated\nSender - Institution (bank) initiating/sending the individual transaction\nReceiver - Institution (bank) receiving the individual transaction\nOrderingAccount - Account identifier for the originating ordering entity (individual or organization) for end-to-end transaction,\nOrderingName - Name for the originating ordering entity\nOrderingStreet - Street address for the originating ordering entity\nOrderingCountryCityZip - Remaining address details for the originating ordering entity\nBeneficiaryAccount - Account identifier for the final beneficiary entity (individual or organization) for end-to-end transaction\nBeneficiaryName - Name for the final beneficiary entity\nBeneficiaryStreet - Street address for the final beneficiary entity\nBeneficiaryCountryCityZip - Remaining address details for the final beneficiary entity\nSettlementDate - Date the individual transaction was settled\nSettlementCurrency - Currency used for transaction\nSettlementAmount - Value of the transaction net of fees/transfer charges/forex\nInstructedCurrency - Currency of the individual transaction as instructed to be paid by the Sender\nInstructedAmount - Value of the individual transaction as instructed to be paid by the Sender\nLabel - Boolean indicator of whether the transaction is anomalous or not. This is the target variable for the prediction task.\nEnd-to-end transactions\nEach row in this dataset is an individual transaction, representing a payment from a sender bank to a receiver bank. An end-to-end transaction is a transaction from an originating ordering entity (a.k.a. ultimate debtor) to a final beneficiary entity (a.k.a. ultimate creditor) and may involve one or more individual transactions. The end-to-end transaction is one individual transaction in the case where the originating orderer's bank sends payment directly to the final beneficiary's bank. However, it may be the case where the payment is not directly sent, but is instead routed through one or more intermediary banks. In such a case, there are multiple individual transactions belonging to the single end-to-end transaction, with each individual transaction representing a bank-to-bank payment. Each end-to-end transaction is uniquely identified by the UETR field. In the case of a sequence of multiple individual transactions for one end-to-end transaction, all individual transactions share a value for UETR, and the Sender and Receiver banks form a chain from the originating ordering bank through one or more intermediary banks to the final beneficiary bank.\nBecause each end-to-end transaction is defined by one originating orderer and one final beneficiary, this means the Ordering* columns for the orderer and Beneficiary* columns for the beneficiary have been included in this dataset in a denormalized fashion\u2014the values are duplicated across all the individual transactions (rows) belonging to the same end-to-end transaction. Additionally, this means that the OrderingAccount and BeneficiaryAccount in a given row may not necessarily belong to the bank in that row's Sender and the bank in that row's Receiver, respectively. The correct way to associate an OrderingAccount to the correct bank is to identify the Sender bank in the originating (first) individual transaction in that end-to-end transaction, and the correct way to associate a BeneficiaryAccount to the correct bank is to identify the Receiver bank in the final (last) individual transaction in that end-to-end transaction.\nMessageId UETR Sender Receiver OrderingAccount BeneficiaryAccount ...\n... ... ... ... ... ... ...\n10 00012345-... A B 111 222 ...\n11 00012345-... B C 111 222\n12 00012345-... C D 111 222\n... ... ... ... ... ... ...\nIllustrative example showing the how to associate the originating orderer and final beneficiary information with the correct banks for one end-to-end transaction made up of three individual transactions. The orderer and beneficiary account information is duplicated across all rows in this group, and the sender and receiver banks form a chain. The bank and account information of the originating orderer is highlighted in blue, and the bank and account information for the final beneficiary is highlighted in yellow.\nDataset 2: Account data held by banks\nParticipants were provided access to account-related data representative of that held by banks. This dataset contains account-level information, including flags signaling whether the account is valid, suspended, etc.\nData can be linked using the Account field in the bank data and the OrderingAccount or BeneficiaryAccount in the SWIFT transaction data. Please see the previous section on end-to-end transactions for details on how to identify which bank an OrderingAccount or BeneficiaryAccount should be linked to.\nNote that bank nodes will not have access to data on the SWIFT node and vice-versa\u2014a case of vertical data partitioning. It is up to you to determine how to exchange this information in a secure and private way.\nDataset 2 details\nDataset 2 will contain the following fields:\nBank - Identifier for the bank\nAccount - Identifier for the account\nName - Name of the account\nStreet - Street address associated with the account\nCountryCityZip - Remaining address details associated with the account\nFlags - Enumerated data type indicating potential issues or special features that have been associated with an account. Flag definitions are provided below:\n00 - No flags\n01 - Account closed\n03 - Account recently opened\n04 - Name mismatch\n05 - Account under monitoring\n06 - Account suspended\n07 - Account frozen\n08 - Non-transaction amount\n09 - Beneficiary deceased\n10 - Invalid company ID\n11 - Invalid individual ID\nNote that this dataset is provided unpartitioned, with all banks' data in one table.\nNote that the flags may not be representative of real-world practices. For example, in the real world, banks may use different flags and may interpret or weight them differently based on appetite for risk.\nDevelopment and Evaluation Data\nThe datasets being provided are intended for local development use in both Phase 1 and Phase 2. The transaction dataset has been split in time\u2013the bulk of the dataset is the training set, and the final week of the dataset is a test set. The prediction task, as detailed in a later section, is to predict a confidence score for each individual transaction in the test set as to whether it is an anomalous transaction. The ground truth is provided for both the training and set sets in the development dataset.\nIn Phase 2, a separate and held-out dataset will be used for solution evaluation. Some aspects of the Phase 2 evaluation data may be changed that should be learnable by your model. You will submit code for your solution to a code execution environment. The code execution runtime will run cold-start federated training on the new dataset's training split and then run inference to generate predictions for the new dataset's test split. Your solution's performance will be measured by evaluating its predictions against the ground truth for the new dataset's test split.\nNew Development Dataset\nNew development dataset published December 3, 2022.\nA new development dataset has been provided by our data partners at SWIFT and is available on the data download page, indicated by the [NEW] tag. This new development dataset is a synthetic dataset that is largely similar to the previously-provided synthetic development dataset (also still available on the data download page), but has some differences that better represent what is observed in the real world. You should expect that the evaluation dataset (held out and will never be made available) will have similar distributions as this new dataset. Local development and self-reported results in the technical paper should primarily use the new development dataset.\nThreat Profile\nYou will design and develop end-to-end solutions that preserve privacy across a range of possible threats and attack scenarios, through all stages of the machine learning model lifecycle. You should therefore carefully consider the overall privacy of your solution, focusing on the protection of sensitive information held by all parties involved in the federated learning scenario. The solutions you design and develop should include comprehensive measures to address the threat profiles described below. These measures will provide an appropriate degree of resilience to a wide range of potential attacks defined within the threat profile. For more information on threat profiles, please visit the privacy threat profile section of the problem description.\nScope of sensitive data\nYour solution must prevent the unintended disclosure of\na) sensitive information in the SWIFT transaction dataset\nb) sensitive information in the bank dataset, to any other party, including other insider stakeholders (for example, SWIFT and other financial institutions) and outsiders.\nThe sensitive information for the SWIFT dataset is all personally-identifiable information about the originating orderer (a.k.a. ultimate debtor) and final beneficiary (a.k.a. ultimate creditor) parties, including personal details like names and addresses, and group membership information. This includes but is not limited to the raw private data about the orderer/beneficiary stored directly in the account number, name, and address fields, and the transaction identifiers and timestamps.\nThe sensitive information for the bank datasets include all personally identifiable information about parties involved in the transactions, including names and addresses and group membership information. This includes but is not limited to the raw private / business data reflected in account numbers/names/addresses and flags.\nAnomaly detection models\nThe analytical objective is to train a model that enables SWIFT to identify anomalous transactions. In the context of this challenge, this is a classification model to be trained on provided training data with ground truth labels. In real-world deployments, such transactions might be subject to additional verification actions or flagged for further investigation, dependent on context.\nPrediction Target and Evaluation Metric\nThe target variable for the modeling task is a confidence score (between 0.0 and 1.0) for whether each individual transaction is anomalous. As discussed previously, anomalous is not precisely defined and should be learned by your model via supervised learning on provided training data.\nThe evaluation metric will be Area Under the Precision\u2013Recall Curve (AUPRC), also known as average precision (AP), PR-AUC, or AUCPR. This is a commonly used metric for binary classification that summarizes model performance across all operating thresholds. This metric rewards models which can consistently assign anomalous transactions with a higher confidence score than negative non-anomalous transactions. AUPRC is computed as follows:\nAUPRC=\n\u2211\nn\n(\nR\nn\n\u2212\nR\nn\u22121\n)\nP\nn\nwhere\nP\nn\nand\nR\nn\nare the precision and recall, respectively, when thresholding at the nth individual transaction sorted in order of increasing recall.\nPartitioned Datasets for Federated Learning\nPartitioning Overview\nA number of banks are working with SWIFT to collaboratively train a model. The parties are working jointly to do this, and can take a common approach to technical design, infrastructure, etc., but are not able to enable access to each other's data. In the real world there are a number of barriers that might prevent this; banks are subject to a variety of privacy, competition and financial industry regulations, may be operating in different jurisdictions, and have legitimate commercial and ethical reasons for not sharing customer data with competitors.\nThe key task of this challenge is to design a privacy solution so that SWIFT can safely train and deploy such a model without compromising the privacy requirements (more details on the requirements, and an associated threat model, are described in the problem description).\nAbove: Diagram comparing the centralized model (MC) with a privacy-preserving federated learning model (MPF).\nPartitioning Details\nThis use case features both vertical and horizontal data partitioning. Data is vertically partitioned between Dataset 1 (SWIFT) and Dataset 2 (partner banks), and it is horizontally partitioned within Dataset 2 (between each partner bank).\nFor local development, you were provided a full, unpartitioned dataset. In Phase 2 evaluation, evaluation will occur with predetermined partitioning along institutional boundaries. The SWIFT data will always belong a single federation unit who represents the SWIFT Data Store and only has access to the SWIFT data. Banks will be split up among federation units such that one bank's account data entirely belongs within one partition. In cases where there are fewer bank partitions than the number of banks, each bank partition may contain data from more than one bank.\nAny partitioning of the data that you might perform in your local development experiments should take this into account. Your solution should be able to handle any number of bank partitions, and in Phase 2, we may evaluate your solution with a number of bank partitions between 1 and 10.\nKey Task\nFor the purposes of the challenge, you should demonstrate your solution by training two models:\nM\nC\n= a centralized model trained on datasets 1 and 2 in a non-privacy preserving way\nM\nPF\n= a privacy-preserving federated model trained using your privacy solution\nExample Centralized Baseline\nIn Phase 1, SWIFT provided sample Python code for training a centralized anomaly detection model (\nM\nC\n) on the ISO20022 training data. This code snippet used the dataset provided as input and trained a simple anomaly detection model using the XGBoost Classifier.\nThe core of the evaluation will be assessing the comparison between a centralized model\nM\nC\n, and an alternative model\nM\nPF\nthat combines a federated learning approach with innovative privacy-preserving techniques.\nIn the real world, SWIFT may wish to train a model collaboratively with a number of banks, in order to increase the volume and variety of data being used to train the model. You should therefore aim to develop scalable solutions that enable additional nodes to be integrated into the federated network whilst incurring an acceptable additional performance overhead.\nThe federated learning scenario thus consists of one node hosting the SWIFT dataset, and N nodes hosting bank data. We may evaluate solutions for values of N between 1 and 10, in order to assess how well solutions scale as more banks are added to the network. During solution development, you may have full autonomy over how you partition the bank dataset in order to understand the scalability of your solution.\nFull details on evaluation criteria can be found in the problem description.\nFor additional reference, here is a technical brief for this use case track provided through the U.K. challenge. This brief has been assembled in collaboration by the U.K. and U.S. challenge organizers. This may help to give a sense for the use case and capabilities expected, though note that details in the brief may not match exactly how the U.S. challenge will operate.\nGood luck\nGood luck and enjoy this problem! For more details on submission and evaluation, visit the problem description page. If you have any questions, you can always ask the community by visiting the DrivenData user forum or the cross-U.S.\u2013U.K. public Slack channel. You can request access to the Slack channel here.",
        "url": "https://www.drivendata.org/competitions/105/nist-federated-learning-2-financial-crime-federated/"
    },
    {
        "competition_overview": "PETs Prize Challenge: Advancing Privacy-Preserving Federated Learning\nThis is the second in a series of challenge phases with a total prize pool of $800,000! Each phase in the PETs Prize Challenge invites participants to test and apply their innovations in privacy-preserving federated learning. Phase 2 is for Blue Teams to implement and submit working code for their solutions.\nYou are on the Phase 2 home page for the Financial Crime Data Track. You can find the Pandemic Forecasting Data Track here.\nBackground\nPrivacy-enhancing technologies (PETs) have the potential to unlock more trustworthy innovation in data analysis and machine learning. Federated learning is one such technology that enables organizations to analyze sensitive data while providing improved privacy protections. These technologies could advance innovation and collaboration in new fields and help harness the power of data to tackle some of our most pressing societal challenges.\nThat\u2019s why the U.S. and U.K. governments are partnering to deliver a set of prize challenges to unleash the potential of these democracy-affirming technologies to make a positive impact. In particular, this challenge will tackle two critical problems via separate data tracks: Data Track A will help with the identification of financial crime, while Data Track B will bolster pandemic responses.\nBy entering the prize challenges, innovators will have the opportunity to compete for cash prizes and engage with regulators and government agencies. Announced at the Summit for Democracy in December 2021, the prize challenges are a product of a collaboration between multiple government departments and agencies on both sides of the Atlantic. Winning solutions will have the opportunity to be profiled at the second Summit for Democracy, to be convened by President Joe Biden, in early 2023.\nObjectives\nThe goal of this prize challenge is to mature federated learning approaches and build trust in their adoption. The challenge organizers hope to accelerate the development of efficient privacy-preserving federated learning solutions that leverage a combination of input and output privacy techniques to:\nDrive innovation in the technological development and application of novel privacy-enhancing technologies\nDeliver strong privacy guarantees against a set of common threats and privacy attacks\nGenerate effective models to accomplish a set of predictive or analytical tasks that support the use cases\nChallenge Structure\nThe Solution Development Phase will have two data use case tracks matching those previously from the Concept Paper Phase\u2014Track A: Financial Crime Prevention and Track B: Pandemic Response and Forecasting. Teams can elect to participate in either or both tracks, with solutions that apply to one track individually or with a generalized solution. Each solution must correspond to a concept paper that met the requirements from the Concept Paper Phase.\nEach data use case track has a prize category for top solutions. All solutions that apply to Track A or Track B are eligible for a prize within that respective track. Teams who submit solutions for Track A and Track B are eligible to win a prize from each track.\nAdditionally, participants can enter a third Generalized Solutions category of prizes for the best solutions that are shown to be generalized and applied to both use cases with minor adaptations. Teams submitting generalized solutions are eligible for Generalized Solution prizes in addition to being eligible for prizes from both Track A and Track B.\nData Track A \u2013 Financial crime prevention \u2013 The United Nations estimates that up to $2 trillion of cross-border money laundering takes place each year, financing organized crime and undermining economic prosperity. Financial institutions such as banks and credit agencies, along with organizations that process transactions between institutions, such as the SWIFT global financial messaging provider, must protect personal and financial data, while also trying to report and deter illicit financial activities. Using synthetic datasets provided by SWIFT, you will design and later develop innovative privacy-preserving federated learning solutions that facilitate cross-institution and cross-border anomaly detection to combat financial crime. This use case features both vertical and horizontal data partitioning. Check out the Financial Crime overview page for more information.\nData Track B \u2013 Pandemic response and forecasting \u2013 As we continue to deal with COVID-19, it has become apparent that better ways to harness the power of data through analytics are critical for preparing for and responding to public health crises. Federated learning approaches could allow for responsible use of sensitive data to develop cross-organization and cross-border data analysis that would result in more robust forecasting and pandemic response capabilities. Using synthetic population datasets, you will design and later develop privacy-preserving federated learning solutions that can predict an individual\u2019s risk for infection. This use case features horizontal data partitioning. Check out the Pandemic Forecasting overview page for more information.\nGeneralizable Solutions \u2013 Cross-organization, cross-border use cases are certainly not limited to the financial or public health domains. Developing out-of-the-box generalized models that can be adapted for use with specific data or problem sets has great potential to advance the adoption and widespread use of privacy-preserving federated learning for public- and private-sector organizations in multiple sectors. To demonstrate generalizability, you may develop a solution using both the Track A and Track B datasets to be eligible for additional awards dedicated to generalizable solutions.\nMore information on how to get started can be found in the Problem Description.\nEligibility\nPhase 2 is open to Blue Team Participants who won invitations because their Concept Papers met the minimum criteria described in the challenge rules. No further registration will be required for the invitees to advance to Phase 2; however, invited Blue Team Participants should make necessary updates to their registration via their DrivenData account to reflect any changes in team composition or contact information. For more details, please refer to the official rules.\nPhase 2 Timeline and Prizes\nPhase 2 Key Dates\nLaunch October 5, 2022\nDeadline to Open Pull Requests for Runtime Environment January 11, 2023 at 11:59:59 PM UTC\nSubmissions Due January 26, 2023 at 11:59:59 PM UTC\nAnnouncement of Finalists to be Tested in Phase 3 Red Teaming February 13, 2023\nWinners Announced March 30, 2023\nPhase 2 Prizes\nOpen to Blue Team participants.\nData Track A: Financial Crime Prevention\nPrize Amount\n1st $100,000\n2nd $50,000\n3rd $25,000\nData Track B: Pandemic Response and Forecasting\nPrize Amount\n1st $100,000\n2nd $50,000\n3rd $25,000\nGeneralized Solutions\nPrize Amount\n1st $100,000\n2nd $50,000\n3rd $25,000\nSpecial Recognition\nPrize Amount\nPool $50,000\nPut your papers into action! Registered teams from Phase 1 will develop working prototypes and submit them to a remote execution environment for federated training and evaluation. These solutions are expected to be functional, i.e., capable of training a model and predicting against the evaluation data set with measurement of relevant performance and accuracy metrics. Solutions will be evaluated by a panel of judges across a set of weighted criteria. The top solutions, ranked by points awarded, will have their final rankings determined by incorporating red team evaluation from the red teams from Phase 3.\nTeams can qualify for one or multiple of Data Track A: Financial Crime Prevention, Data Track B: Pandemic Response and Forecasting, and Generalized Solutions prize categories depending on how their solutions address the two privacy-preserving federated learning tasks in the challenge.\nA separate Special Recognition prize pool is set aside to award up to five solutions that do not win prizes from the three main prize categories but demonstrate excellence in specific areas of privacy innovation: novelty, advancement in a specific privacy technology, usability, and efficiency.\nFull Challenge Timeline and Prizes\nTimeline Overview\nThere are three main phases in the challenge with two types of participants based on a red team/blue team approach. Blue Teams develop privacy-preserving solutions, while Red Teams act as adversaries to test those solutions.\nPhase 1: Concept Development (Jul\u2013Sept 2022): Blue Teams propose privacy-preserving federated learning solution concepts.\nPhase 2: Solution Development (Oct 2022\u2013Jan 2023): Blue Teams develop working prototypes of their solutions.\nPhase 3: Red Teaming (Nov 2022\u2013Feb 2023): Red Teams prepare and test privacy attacks on top blue team solutions from Phase 2.\nPrize Overview\nTrack Prize Pool\nPhase 1: Concept Paper $55,000\nPhase 2: Solution Development $575,000\nPhase 3: Red Teaming $120,000\nOpen Source $140,000*\nTotal $800,000\n* UPDATE March 22, 2023: Open Source award amounts have been increased to $20,000 per award and the number of awards have been increased to 7, for a total of up to $140,000. The increase is reallocated prize moneys that were not awarded in earlier phases, and the total pool of prize awards for the Challenge remains $800,000.\nAdditional Phase Details and Prizes\nPlace Prize Amount\n1st $30,000\n2nd $15,000\n3rd $10,000\nPhase 1: Concept Paper\nJuly 20\u2013September 19, 2022\nParticipants will produce an abstract and technical concept paper laying out their proposed solution. Concept papers will be evaluated by a panel of judges across a set of weighted criteria. Participants will be eligible to win prizes awarded to the top technical papers, ranked by points awarded. Participants must complete a paper in Phase 1 in order to be eligible to compete in Phase 2.\nOpen to Blue Team participants.\nPrize Amount\n1st $60,000\n2nd $40,000\n3rd $20,000\nPhase 3: Red Teaming\nNovember 10, 2022\u2013February 28, 2023\nPrivacy researchers are invited to form red teams to put the privacy claims of Phase 2's blue team finalists to the test. The red teams will prepare and test privacy attacks on the Phase 2 finalist solutions, which will be incorporated into the final Phase 2 rankings. Top red teams will be evaluated for success and rigor and will be awarded prizes for their performance during this phase.\nOpen to Red Team participants.\nPrize Amount\nPool $140,000\nOpen Source\nSubmissions due April 20, 2023\nUp to 7 of the final blue team winners from Phase 2 will be invited to release their solutions as open-source software. Each verified participating blue team will be awarded an Open Source prize of $20,000.\nOpen to top Blue Team winners from Phase 2.\n\nHow to compete (Phase 2)\nOnly blue teams who participated in Phase 1 and met minimum requirements are eligible to participate in Phase 2. Eligible teams are automatically registered. If you need to make changes to your team, please contact info@drivendata.org.\nGet familiar with the problem through the problem description and code submission format pages.\nDevelop and submit your centralized solution. Check out the centralized code submission format page for more details.\nDevelop and submit your federated solution. Check out the federated code submission format page for more details.\nGood luck!\nThis challenge is sponsored by the National Institute for Standards and Technology (NIST) and the National Science Foundation (NSF)\n\n        \nWith additional collaboration from the Office of Science and Technology Policy (OSTP), NASA, SWIFT, and the University of Virginia's Biocomplexity Institute.\nThis prize challenge and its U.K. counterpart have been developed as part of a joint collaboration between the United Kingdom and the United States.",
        "dataset_overview": "Data Track A: Financial Crime\nTransforming Financial Crime Prevention\nThere are two data use case tracks for the PETs prize challenge. This is the Phase 2 data overview page for the financial crime prevention track. In this track, innovators will develop end-to-end privacy-preserving federated learning solutions to detect potentially anomalous payments, leveraging a combination of input- and output-privacy techniques.\nBackground\nBackground\nData\nOverview\nThreat Profile\nModeling\nAnomaly Detection\nPartitioning Data\nExample Centralized Baseline\nBackground\nThe financial crime track is focused on enhancing cross-organization, cross-border data access to support efforts to combat fraud, money laundering and other financial crime. You are asked to develop innovative, privacy-preserving solutions to enable detection of potentially anomalous payments, utilizing synthetic datasets representing data held by the SWIFT payments network and datasets held by partner banks.\n\u201cAnomalous transactions\u201d covers a range of payments that vary significantly from the norms seen in the dataset, and thus may be indicative of fraud, money laundering, or other financial crime. Examples include a transaction that is of an unexpected amount or currency, uses unusual corridors (senders/receivers), has unusual timestamps, or contains other unusual fields. In the scope of this challenge, the problem is framed as a classification task. The training datasets are labeled with anomalies, and therefore you do not need a detailed understanding of financial crime issues.\nThis is a high-impact and exciting use case for novel privacy-enhancing technologies. There are currently challenging trade-offs between enabling sufficient access to data to build tools to effectively detect illegal financial activity, and limiting the identifiability of innocent individuals or inference of their sensitive information within those data sets. The scale of the problem is vast: the UN estimates that US$800-2000bn is laundered each year, representing 2-5% of global GDP.\nThough novel innovation for this use case alone could achieve significant real-world impact, the challenge is designed to incentivize development of privacy technologies that can be applied to other use cases where data is distributed across multiple organisations or jurisdictions, both in financial services and elsewhere. The best solutions will deliver meaningful innovation towards deployable solutions in this space, with consideration of how to evidence the privacy guarantees offered to data owners and regulators, but also have the potential to generalize to other situations.\nData Overview\nInnovators will use synthetic datasets representing data held by the SWIFT global payments network and by its partner banks. In Phase 1, you were provided with two development datasets:\nDataset 1: A synthetic dataset representing transaction data from the SWIFT global payment network\nDataset 2: Synthetic customer / account metadata, including flags, from SWIFT's partner banks\nThere are approximately 4 million rows across the two development datasets.\nAdditional development data may be released for Phase 2.\nNote: The challenges are based on synthetic data to minimize the security burden placed on competitors during the development phase; of course the intent of the challenge is that privacy solutions are developed that would be appropriate for use on real data sets with demonstrable privacy guarantees. However, competitors must adhere to a data use agreement (see competition rules for more details).\nDataset 1: Transaction data held by SWIFT\nIn Phase 1, you were provided a synthetic dataset derived from data from the SWIFT global payment network. Each row in this dataset is an individual transaction, representing a payment from one sending bank to one receiving bank. The dataset will:\nContain data elements as defined in the ISO20022 pacs.008 / MT103 message format\nComprise transactions between fictitious originators and beneficiaries, sender and receiving banks, payment corridor, amount and timestamps\nExpertise in financial crime or ISO20022 messaging is not an expected prerequisite for entering the challenge, and the assessment process will not focus on detailed understanding of the use case itself. The details in the sections below should be sufficient for understanding the data within the scope of the challenge. However, participants unfamiliar with this space may find it informative to consult a general introduction to ISO20022. You may also find the ISO20022 message definitions informative.\nThe dataset reflects a snapshot of transactions sent by an ordering customer or institution to credit a beneficiary customer or institution. The dataset covers roughly a month\u2019s worth of transactions involving 50 institutions.\nThe synthetic data is not generated based on any real traffic and will not contain any statistical properties of the real SWIFT transaction data (SWIFT has applied normal and uniform distributions).\nDataset 1 details\nDataset 1 contains the following fields:\nMessageId - Globally unique identifier within this dataset for individual transactions\nUETR - The Unique End-to-end Transaction Reference\u2014a 36-character string enabling traceability of all individual transactions associated with a single end-to-end transaction\nTransactionReference - Unique identifier for an individual transaction\nTimestamp - Time at which the individual transaction was initiated\nSender - Institution (bank) initiating/sending the individual transaction\nReceiver - Institution (bank) receiving the individual transaction\nOrderingAccount - Account identifier for the originating ordering entity (individual or organization) for end-to-end transaction,\nOrderingName - Name for the originating ordering entity\nOrderingStreet - Street address for the originating ordering entity\nOrderingCountryCityZip - Remaining address details for the originating ordering entity\nBeneficiaryAccount - Account identifier for the final beneficiary entity (individual or organization) for end-to-end transaction\nBeneficiaryName - Name for the final beneficiary entity\nBeneficiaryStreet - Street address for the final beneficiary entity\nBeneficiaryCountryCityZip - Remaining address details for the final beneficiary entity\nSettlementDate - Date the individual transaction was settled\nSettlementCurrency - Currency used for transaction\nSettlementAmount - Value of the transaction net of fees/transfer charges/forex\nInstructedCurrency - Currency of the individual transaction as instructed to be paid by the Sender\nInstructedAmount - Value of the individual transaction as instructed to be paid by the Sender\nLabel - Boolean indicator of whether the transaction is anomalous or not. This is the target variable for the prediction task.\nEnd-to-end transactions\nEach row in this dataset is an individual transaction, representing a payment from a sender bank to a receiver bank. An end-to-end transaction is a transaction from an originating ordering entity (a.k.a. ultimate debtor) to a final beneficiary entity (a.k.a. ultimate creditor) and may involve one or more individual transactions. The end-to-end transaction is one individual transaction in the case where the originating orderer's bank sends payment directly to the final beneficiary's bank. However, it may be the case where the payment is not directly sent, but is instead routed through one or more intermediary banks. In such a case, there are multiple individual transactions belonging to the single end-to-end transaction, with each individual transaction representing a bank-to-bank payment. Each end-to-end transaction is uniquely identified by the UETR field. In the case of a sequence of multiple individual transactions for one end-to-end transaction, all individual transactions share a value for UETR, and the Sender and Receiver banks form a chain from the originating ordering bank through one or more intermediary banks to the final beneficiary bank.\nBecause each end-to-end transaction is defined by one originating orderer and one final beneficiary, this means the Ordering* columns for the orderer and Beneficiary* columns for the beneficiary have been included in this dataset in a denormalized fashion\u2014the values are duplicated across all the individual transactions (rows) belonging to the same end-to-end transaction. Additionally, this means that the OrderingAccount and BeneficiaryAccount in a given row may not necessarily belong to the bank in that row's Sender and the bank in that row's Receiver, respectively. The correct way to associate an OrderingAccount to the correct bank is to identify the Sender bank in the originating (first) individual transaction in that end-to-end transaction, and the correct way to associate a BeneficiaryAccount to the correct bank is to identify the Receiver bank in the final (last) individual transaction in that end-to-end transaction.\nMessageId UETR Sender Receiver OrderingAccount BeneficiaryAccount ...\n... ... ... ... ... ... ...\n10 00012345-... A B 111 222 ...\n11 00012345-... B C 111 222\n12 00012345-... C D 111 222\n... ... ... ... ... ... ...\nIllustrative example showing the how to associate the originating orderer and final beneficiary information with the correct banks for one end-to-end transaction made up of three individual transactions. The orderer and beneficiary account information is duplicated across all rows in this group, and the sender and receiver banks form a chain. The bank and account information of the originating orderer is highlighted in blue, and the bank and account information for the final beneficiary is highlighted in yellow.\nDataset 2: Account data held by banks\nParticipants were provided access to account-related data representative of that held by banks. This dataset contains account-level information, including flags signaling whether the account is valid, suspended, etc.\nData can be linked using the Account field in the bank data and the OrderingAccount or BeneficiaryAccount in the SWIFT transaction data. Please see the previous section on end-to-end transactions for details on how to identify which bank an OrderingAccount or BeneficiaryAccount should be linked to.\nNote that bank nodes will not have access to data on the SWIFT node and vice-versa\u2014a case of vertical data partitioning. It is up to you to determine how to exchange this information in a secure and private way.\nDataset 2 details\nDataset 2 will contain the following fields:\nBank - Identifier for the bank\nAccount - Identifier for the account\nName - Name of the account\nStreet - Street address associated with the account\nCountryCityZip - Remaining address details associated with the account\nFlags - Enumerated data type indicating potential issues or special features that have been associated with an account. Flag definitions are provided below:\n00 - No flags\n01 - Account closed\n03 - Account recently opened\n04 - Name mismatch\n05 - Account under monitoring\n06 - Account suspended\n07 - Account frozen\n08 - Non-transaction amount\n09 - Beneficiary deceased\n10 - Invalid company ID\n11 - Invalid individual ID\nNote that this dataset is provided unpartitioned, with all banks' data in one table.\nNote that the flags may not be representative of real-world practices. For example, in the real world, banks may use different flags and may interpret or weight them differently based on appetite for risk.\nDevelopment and Evaluation Data\nThe datasets being provided are intended for local development use in both Phase 1 and Phase 2. The transaction dataset has been split in time\u2013the bulk of the dataset is the training set, and the final week of the dataset is a test set. The prediction task, as detailed in a later section, is to predict a confidence score for each individual transaction in the test set as to whether it is an anomalous transaction. The ground truth is provided for both the training and set sets in the development dataset.\nIn Phase 2, a separate and held-out dataset will be used for solution evaluation. Some aspects of the Phase 2 evaluation data may be changed that should be learnable by your model. You will submit code for your solution to a code execution environment. The code execution runtime will run cold-start federated training on the new dataset's training split and then run inference to generate predictions for the new dataset's test split. Your solution's performance will be measured by evaluating its predictions against the ground truth for the new dataset's test split.\nNew Development Dataset\nNew development dataset published December 3, 2022.\nA new development dataset has been provided by our data partners at SWIFT and is available on the data download page, indicated by the [NEW] tag. This new development dataset is a synthetic dataset that is largely similar to the previously-provided synthetic development dataset (also still available on the data download page), but has some differences that better represent what is observed in the real world. You should expect that the evaluation dataset (held out and will never be made available) will have similar distributions as this new dataset. Local development and self-reported results in the technical paper should primarily use the new development dataset.\nThreat Profile\nYou will design and develop end-to-end solutions that preserve privacy across a range of possible threats and attack scenarios, through all stages of the machine learning model lifecycle. You should therefore carefully consider the overall privacy of your solution, focusing on the protection of sensitive information held by all parties involved in the federated learning scenario. The solutions you design and develop should include comprehensive measures to address the threat profiles described below. These measures will provide an appropriate degree of resilience to a wide range of potential attacks defined within the threat profile. For more information on threat profiles, please visit the privacy threat profile section of the problem description.\nScope of sensitive data\nYour solution must prevent the unintended disclosure of\na) sensitive information in the SWIFT transaction dataset\nb) sensitive information in the bank dataset, to any other party, including other insider stakeholders (for example, SWIFT and other financial institutions) and outsiders.\nThe sensitive information for the SWIFT dataset is all personally-identifiable information about the originating orderer (a.k.a. ultimate debtor) and final beneficiary (a.k.a. ultimate creditor) parties, including personal details like names and addresses, and group membership information. This includes but is not limited to the raw private data about the orderer/beneficiary stored directly in the account number, name, and address fields, and the transaction identifiers and timestamps.\nThe sensitive information for the bank datasets include all personally identifiable information about parties involved in the transactions, including names and addresses and group membership information. This includes but is not limited to the raw private / business data reflected in account numbers/names/addresses and flags.\nAnomaly detection models\nThe analytical objective is to train a model that enables SWIFT to identify anomalous transactions. In the context of this challenge, this is a classification model to be trained on provided training data with ground truth labels. In real-world deployments, such transactions might be subject to additional verification actions or flagged for further investigation, dependent on context.\nPrediction Target and Evaluation Metric\nThe target variable for the modeling task is a confidence score (between 0.0 and 1.0) for whether each individual transaction is anomalous. As discussed previously, anomalous is not precisely defined and should be learned by your model via supervised learning on provided training data.\nThe evaluation metric will be Area Under the Precision\u2013Recall Curve (AUPRC), also known as average precision (AP), PR-AUC, or AUCPR. This is a commonly used metric for binary classification that summarizes model performance across all operating thresholds. This metric rewards models which can consistently assign anomalous transactions with a higher confidence score than negative non-anomalous transactions. AUPRC is computed as follows:\nAUPRC=\n\u2211\nn\n(\nR\nn\n\u2212\nR\nn\u22121\n)\nP\nn\nwhere\nP\nn\nand\nR\nn\nare the precision and recall, respectively, when thresholding at the nth individual transaction sorted in order of increasing recall.\nPartitioned Datasets for Federated Learning\nPartitioning Overview\nA number of banks are working with SWIFT to collaboratively train a model. The parties are working jointly to do this, and can take a common approach to technical design, infrastructure, etc., but are not able to enable access to each other's data. In the real world there are a number of barriers that might prevent this; banks are subject to a variety of privacy, competition and financial industry regulations, may be operating in different jurisdictions, and have legitimate commercial and ethical reasons for not sharing customer data with competitors.\nThe key task of this challenge is to design a privacy solution so that SWIFT can safely train and deploy such a model without compromising the privacy requirements (more details on the requirements, and an associated threat model, are described in the problem description).\nAbove: Diagram comparing the centralized model (MC) with a privacy-preserving federated learning model (MPF).\nPartitioning Details\nThis use case features both vertical and horizontal data partitioning. Data is vertically partitioned between Dataset 1 (SWIFT) and Dataset 2 (partner banks), and it is horizontally partitioned within Dataset 2 (between each partner bank).\nFor local development, you were provided a full, unpartitioned dataset. In Phase 2 evaluation, evaluation will occur with predetermined partitioning along institutional boundaries. The SWIFT data will always belong a single federation unit who represents the SWIFT Data Store and only has access to the SWIFT data. Banks will be split up among federation units such that one bank's account data entirely belongs within one partition. In cases where there are fewer bank partitions than the number of banks, each bank partition may contain data from more than one bank.\nAny partitioning of the data that you might perform in your local development experiments should take this into account. Your solution should be able to handle any number of bank partitions, and in Phase 2, we may evaluate your solution with a number of bank partitions between 1 and 10.\nKey Task\nFor the purposes of the challenge, you should demonstrate your solution by training two models:\nM\nC\n= a centralized model trained on datasets 1 and 2 in a non-privacy preserving way\nM\nPF\n= a privacy-preserving federated model trained using your privacy solution\nExample Centralized Baseline\nIn Phase 1, SWIFT provided sample Python code for training a centralized anomaly detection model (\nM\nC\n) on the ISO20022 training data. This code snippet used the dataset provided as input and trained a simple anomaly detection model using the XGBoost Classifier.\nThe core of the evaluation will be assessing the comparison between a centralized model\nM\nC\n, and an alternative model\nM\nPF\nthat combines a federated learning approach with innovative privacy-preserving techniques.\nIn the real world, SWIFT may wish to train a model collaboratively with a number of banks, in order to increase the volume and variety of data being used to train the model. You should therefore aim to develop scalable solutions that enable additional nodes to be integrated into the federated network whilst incurring an acceptable additional performance overhead.\nThe federated learning scenario thus consists of one node hosting the SWIFT dataset, and N nodes hosting bank data. We may evaluate solutions for values of N between 1 and 10, in order to assess how well solutions scale as more banks are added to the network. During solution development, you may have full autonomy over how you partition the bank dataset in order to understand the scalability of your solution.\nFull details on evaluation criteria can be found in the problem description.\nFor additional reference, here is a technical brief for this use case track provided through the U.K. challenge. This brief has been assembled in collaboration by the U.K. and U.S. challenge organizers. This may help to give a sense for the use case and capabilities expected, though note that details in the brief may not match exactly how the U.S. challenge will operate.\nGood luck\nGood luck and enjoy this problem! For more details on submission and evaluation, visit the problem description page. If you have any questions, you can always ask the community by visiting the DrivenData user forum or the cross-U.S.\u2013U.K. public Slack channel. You can request access to the Slack channel here.",
        "evaluation_overview": "Data Track A: Financial Crime\nTransforming Financial Crime Prevention\nThere are two data use case tracks for the PETs prize challenge. This is the Phase 2 data overview page for the financial crime prevention track. In this track, innovators will develop end-to-end privacy-preserving federated learning solutions to detect potentially anomalous payments, leveraging a combination of input- and output-privacy techniques.\nBackground\nBackground\nData\nOverview\nThreat Profile\nModeling\nAnomaly Detection\nPartitioning Data\nExample Centralized Baseline\nBackground\nThe financial crime track is focused on enhancing cross-organization, cross-border data access to support efforts to combat fraud, money laundering and other financial crime. You are asked to develop innovative, privacy-preserving solutions to enable detection of potentially anomalous payments, utilizing synthetic datasets representing data held by the SWIFT payments network and datasets held by partner banks.\n\u201cAnomalous transactions\u201d covers a range of payments that vary significantly from the norms seen in the dataset, and thus may be indicative of fraud, money laundering, or other financial crime. Examples include a transaction that is of an unexpected amount or currency, uses unusual corridors (senders/receivers), has unusual timestamps, or contains other unusual fields. In the scope of this challenge, the problem is framed as a classification task. The training datasets are labeled with anomalies, and therefore you do not need a detailed understanding of financial crime issues.\nThis is a high-impact and exciting use case for novel privacy-enhancing technologies. There are currently challenging trade-offs between enabling sufficient access to data to build tools to effectively detect illegal financial activity, and limiting the identifiability of innocent individuals or inference of their sensitive information within those data sets. The scale of the problem is vast: the UN estimates that US$800-2000bn is laundered each year, representing 2-5% of global GDP.\nThough novel innovation for this use case alone could achieve significant real-world impact, the challenge is designed to incentivize development of privacy technologies that can be applied to other use cases where data is distributed across multiple organisations or jurisdictions, both in financial services and elsewhere. The best solutions will deliver meaningful innovation towards deployable solutions in this space, with consideration of how to evidence the privacy guarantees offered to data owners and regulators, but also have the potential to generalize to other situations.\nData Overview\nInnovators will use synthetic datasets representing data held by the SWIFT global payments network and by its partner banks. In Phase 1, you were provided with two development datasets:\nDataset 1: A synthetic dataset representing transaction data from the SWIFT global payment network\nDataset 2: Synthetic customer / account metadata, including flags, from SWIFT's partner banks\nThere are approximately 4 million rows across the two development datasets.\nAdditional development data may be released for Phase 2.\nNote: The challenges are based on synthetic data to minimize the security burden placed on competitors during the development phase; of course the intent of the challenge is that privacy solutions are developed that would be appropriate for use on real data sets with demonstrable privacy guarantees. However, competitors must adhere to a data use agreement (see competition rules for more details).\nDataset 1: Transaction data held by SWIFT\nIn Phase 1, you were provided a synthetic dataset derived from data from the SWIFT global payment network. Each row in this dataset is an individual transaction, representing a payment from one sending bank to one receiving bank. The dataset will:\nContain data elements as defined in the ISO20022 pacs.008 / MT103 message format\nComprise transactions between fictitious originators and beneficiaries, sender and receiving banks, payment corridor, amount and timestamps\nExpertise in financial crime or ISO20022 messaging is not an expected prerequisite for entering the challenge, and the assessment process will not focus on detailed understanding of the use case itself. The details in the sections below should be sufficient for understanding the data within the scope of the challenge. However, participants unfamiliar with this space may find it informative to consult a general introduction to ISO20022. You may also find the ISO20022 message definitions informative.\nThe dataset reflects a snapshot of transactions sent by an ordering customer or institution to credit a beneficiary customer or institution. The dataset covers roughly a month\u2019s worth of transactions involving 50 institutions.\nThe synthetic data is not generated based on any real traffic and will not contain any statistical properties of the real SWIFT transaction data (SWIFT has applied normal and uniform distributions).\nDataset 1 details\nDataset 1 contains the following fields:\nMessageId - Globally unique identifier within this dataset for individual transactions\nUETR - The Unique End-to-end Transaction Reference\u2014a 36-character string enabling traceability of all individual transactions associated with a single end-to-end transaction\nTransactionReference - Unique identifier for an individual transaction\nTimestamp - Time at which the individual transaction was initiated\nSender - Institution (bank) initiating/sending the individual transaction\nReceiver - Institution (bank) receiving the individual transaction\nOrderingAccount - Account identifier for the originating ordering entity (individual or organization) for end-to-end transaction,\nOrderingName - Name for the originating ordering entity\nOrderingStreet - Street address for the originating ordering entity\nOrderingCountryCityZip - Remaining address details for the originating ordering entity\nBeneficiaryAccount - Account identifier for the final beneficiary entity (individual or organization) for end-to-end transaction\nBeneficiaryName - Name for the final beneficiary entity\nBeneficiaryStreet - Street address for the final beneficiary entity\nBeneficiaryCountryCityZip - Remaining address details for the final beneficiary entity\nSettlementDate - Date the individual transaction was settled\nSettlementCurrency - Currency used for transaction\nSettlementAmount - Value of the transaction net of fees/transfer charges/forex\nInstructedCurrency - Currency of the individual transaction as instructed to be paid by the Sender\nInstructedAmount - Value of the individual transaction as instructed to be paid by the Sender\nLabel - Boolean indicator of whether the transaction is anomalous or not. This is the target variable for the prediction task.\nEnd-to-end transactions\nEach row in this dataset is an individual transaction, representing a payment from a sender bank to a receiver bank. An end-to-end transaction is a transaction from an originating ordering entity (a.k.a. ultimate debtor) to a final beneficiary entity (a.k.a. ultimate creditor) and may involve one or more individual transactions. The end-to-end transaction is one individual transaction in the case where the originating orderer's bank sends payment directly to the final beneficiary's bank. However, it may be the case where the payment is not directly sent, but is instead routed through one or more intermediary banks. In such a case, there are multiple individual transactions belonging to the single end-to-end transaction, with each individual transaction representing a bank-to-bank payment. Each end-to-end transaction is uniquely identified by the UETR field. In the case of a sequence of multiple individual transactions for one end-to-end transaction, all individual transactions share a value for UETR, and the Sender and Receiver banks form a chain from the originating ordering bank through one or more intermediary banks to the final beneficiary bank.\nBecause each end-to-end transaction is defined by one originating orderer and one final beneficiary, this means the Ordering* columns for the orderer and Beneficiary* columns for the beneficiary have been included in this dataset in a denormalized fashion\u2014the values are duplicated across all the individual transactions (rows) belonging to the same end-to-end transaction. Additionally, this means that the OrderingAccount and BeneficiaryAccount in a given row may not necessarily belong to the bank in that row's Sender and the bank in that row's Receiver, respectively. The correct way to associate an OrderingAccount to the correct bank is to identify the Sender bank in the originating (first) individual transaction in that end-to-end transaction, and the correct way to associate a BeneficiaryAccount to the correct bank is to identify the Receiver bank in the final (last) individual transaction in that end-to-end transaction.\nMessageId UETR Sender Receiver OrderingAccount BeneficiaryAccount ...\n... ... ... ... ... ... ...\n10 00012345-... A B 111 222 ...\n11 00012345-... B C 111 222\n12 00012345-... C D 111 222\n... ... ... ... ... ... ...\nIllustrative example showing the how to associate the originating orderer and final beneficiary information with the correct banks for one end-to-end transaction made up of three individual transactions. The orderer and beneficiary account information is duplicated across all rows in this group, and the sender and receiver banks form a chain. The bank and account information of the originating orderer is highlighted in blue, and the bank and account information for the final beneficiary is highlighted in yellow.\nDataset 2: Account data held by banks\nParticipants were provided access to account-related data representative of that held by banks. This dataset contains account-level information, including flags signaling whether the account is valid, suspended, etc.\nData can be linked using the Account field in the bank data and the OrderingAccount or BeneficiaryAccount in the SWIFT transaction data. Please see the previous section on end-to-end transactions for details on how to identify which bank an OrderingAccount or BeneficiaryAccount should be linked to.\nNote that bank nodes will not have access to data on the SWIFT node and vice-versa\u2014a case of vertical data partitioning. It is up to you to determine how to exchange this information in a secure and private way.\nDataset 2 details\nDataset 2 will contain the following fields:\nBank - Identifier for the bank\nAccount - Identifier for the account\nName - Name of the account\nStreet - Street address associated with the account\nCountryCityZip - Remaining address details associated with the account\nFlags - Enumerated data type indicating potential issues or special features that have been associated with an account. Flag definitions are provided below:\n00 - No flags\n01 - Account closed\n03 - Account recently opened\n04 - Name mismatch\n05 - Account under monitoring\n06 - Account suspended\n07 - Account frozen\n08 - Non-transaction amount\n09 - Beneficiary deceased\n10 - Invalid company ID\n11 - Invalid individual ID\nNote that this dataset is provided unpartitioned, with all banks' data in one table.\nNote that the flags may not be representative of real-world practices. For example, in the real world, banks may use different flags and may interpret or weight them differently based on appetite for risk.\nDevelopment and Evaluation Data\nThe datasets being provided are intended for local development use in both Phase 1 and Phase 2. The transaction dataset has been split in time\u2013the bulk of the dataset is the training set, and the final week of the dataset is a test set. The prediction task, as detailed in a later section, is to predict a confidence score for each individual transaction in the test set as to whether it is an anomalous transaction. The ground truth is provided for both the training and set sets in the development dataset.\nIn Phase 2, a separate and held-out dataset will be used for solution evaluation. Some aspects of the Phase 2 evaluation data may be changed that should be learnable by your model. You will submit code for your solution to a code execution environment. The code execution runtime will run cold-start federated training on the new dataset's training split and then run inference to generate predictions for the new dataset's test split. Your solution's performance will be measured by evaluating its predictions against the ground truth for the new dataset's test split.\nNew Development Dataset\nNew development dataset published December 3, 2022.\nA new development dataset has been provided by our data partners at SWIFT and is available on the data download page, indicated by the [NEW] tag. This new development dataset is a synthetic dataset that is largely similar to the previously-provided synthetic development dataset (also still available on the data download page), but has some differences that better represent what is observed in the real world. You should expect that the evaluation dataset (held out and will never be made available) will have similar distributions as this new dataset. Local development and self-reported results in the technical paper should primarily use the new development dataset.\nThreat Profile\nYou will design and develop end-to-end solutions that preserve privacy across a range of possible threats and attack scenarios, through all stages of the machine learning model lifecycle. You should therefore carefully consider the overall privacy of your solution, focusing on the protection of sensitive information held by all parties involved in the federated learning scenario. The solutions you design and develop should include comprehensive measures to address the threat profiles described below. These measures will provide an appropriate degree of resilience to a wide range of potential attacks defined within the threat profile. For more information on threat profiles, please visit the privacy threat profile section of the problem description.\nScope of sensitive data\nYour solution must prevent the unintended disclosure of\na) sensitive information in the SWIFT transaction dataset\nb) sensitive information in the bank dataset, to any other party, including other insider stakeholders (for example, SWIFT and other financial institutions) and outsiders.\nThe sensitive information for the SWIFT dataset is all personally-identifiable information about the originating orderer (a.k.a. ultimate debtor) and final beneficiary (a.k.a. ultimate creditor) parties, including personal details like names and addresses, and group membership information. This includes but is not limited to the raw private data about the orderer/beneficiary stored directly in the account number, name, and address fields, and the transaction identifiers and timestamps.\nThe sensitive information for the bank datasets include all personally identifiable information about parties involved in the transactions, including names and addresses and group membership information. This includes but is not limited to the raw private / business data reflected in account numbers/names/addresses and flags.\nAnomaly detection models\nThe analytical objective is to train a model that enables SWIFT to identify anomalous transactions. In the context of this challenge, this is a classification model to be trained on provided training data with ground truth labels. In real-world deployments, such transactions might be subject to additional verification actions or flagged for further investigation, dependent on context.\nPrediction Target and Evaluation Metric\nThe target variable for the modeling task is a confidence score (between 0.0 and 1.0) for whether each individual transaction is anomalous. As discussed previously, anomalous is not precisely defined and should be learned by your model via supervised learning on provided training data.\nThe evaluation metric will be Area Under the Precision\u2013Recall Curve (AUPRC), also known as average precision (AP), PR-AUC, or AUCPR. This is a commonly used metric for binary classification that summarizes model performance across all operating thresholds. This metric rewards models which can consistently assign anomalous transactions with a higher confidence score than negative non-anomalous transactions. AUPRC is computed as follows:\nAUPRC=\n\u2211\nn\n(\nR\nn\n\u2212\nR\nn\u22121\n)\nP\nn\nwhere\nP\nn\nand\nR\nn\nare the precision and recall, respectively, when thresholding at the nth individual transaction sorted in order of increasing recall.\nPartitioned Datasets for Federated Learning\nPartitioning Overview\nA number of banks are working with SWIFT to collaboratively train a model. The parties are working jointly to do this, and can take a common approach to technical design, infrastructure, etc., but are not able to enable access to each other's data. In the real world there are a number of barriers that might prevent this; banks are subject to a variety of privacy, competition and financial industry regulations, may be operating in different jurisdictions, and have legitimate commercial and ethical reasons for not sharing customer data with competitors.\nThe key task of this challenge is to design a privacy solution so that SWIFT can safely train and deploy such a model without compromising the privacy requirements (more details on the requirements, and an associated threat model, are described in the problem description).\nAbove: Diagram comparing the centralized model (MC) with a privacy-preserving federated learning model (MPF).\nPartitioning Details\nThis use case features both vertical and horizontal data partitioning. Data is vertically partitioned between Dataset 1 (SWIFT) and Dataset 2 (partner banks), and it is horizontally partitioned within Dataset 2 (between each partner bank).\nFor local development, you were provided a full, unpartitioned dataset. In Phase 2 evaluation, evaluation will occur with predetermined partitioning along institutional boundaries. The SWIFT data will always belong a single federation unit who represents the SWIFT Data Store and only has access to the SWIFT data. Banks will be split up among federation units such that one bank's account data entirely belongs within one partition. In cases where there are fewer bank partitions than the number of banks, each bank partition may contain data from more than one bank.\nAny partitioning of the data that you might perform in your local development experiments should take this into account. Your solution should be able to handle any number of bank partitions, and in Phase 2, we may evaluate your solution with a number of bank partitions between 1 and 10.\nKey Task\nFor the purposes of the challenge, you should demonstrate your solution by training two models:\nM\nC\n= a centralized model trained on datasets 1 and 2 in a non-privacy preserving way\nM\nPF\n= a privacy-preserving federated model trained using your privacy solution\nExample Centralized Baseline\nIn Phase 1, SWIFT provided sample Python code for training a centralized anomaly detection model (\nM\nC\n) on the ISO20022 training data. This code snippet used the dataset provided as input and trained a simple anomaly detection model using the XGBoost Classifier.\nThe core of the evaluation will be assessing the comparison between a centralized model\nM\nC\n, and an alternative model\nM\nPF\nthat combines a federated learning approach with innovative privacy-preserving techniques.\nIn the real world, SWIFT may wish to train a model collaboratively with a number of banks, in order to increase the volume and variety of data being used to train the model. You should therefore aim to develop scalable solutions that enable additional nodes to be integrated into the federated network whilst incurring an acceptable additional performance overhead.\nThe federated learning scenario thus consists of one node hosting the SWIFT dataset, and N nodes hosting bank data. We may evaluate solutions for values of N between 1 and 10, in order to assess how well solutions scale as more banks are added to the network. During solution development, you may have full autonomy over how you partition the bank dataset in order to understand the scalability of your solution.\nFull details on evaluation criteria can be found in the problem description.\nFor additional reference, here is a technical brief for this use case track provided through the U.K. challenge. This brief has been assembled in collaboration by the U.K. and U.S. challenge organizers. This may help to give a sense for the use case and capabilities expected, though note that details in the brief may not match exactly how the U.S. challenge will operate.\nGood luck\nGood luck and enjoy this problem! For more details on submission and evaluation, visit the problem description page. If you have any questions, you can always ask the community by visiting the DrivenData user forum or the cross-U.S.\u2013U.K. public Slack channel. You can request access to the Slack channel here.",
        "url": "https://www.drivendata.org/competitions/144/nist-federated-learning-2-financial-crime-centralized/"
    },
    {
        "competition_overview": "PETs Prize Challenge: Advancing Privacy-Preserving Federated Learning\nThis is the second in a series of challenge phases with a total prize pool of $800,000! Each phase in the PETs Prize Challenge invites participants to test and apply their innovations in privacy-preserving federated learning. Phase 2 is for Blue Teams to implement and submit working code for their solutions.\nYou are on the Phase 2 home page for the Pandemic Forecasting Data Track. You can find the Financial Crime Data Track here.\nBackground\nPrivacy-enhancing technologies (PETs) have the potential to unlock more trustworthy innovation in data analysis and machine learning. Federated learning is one such technology that enables organizations to analyze sensitive data while providing improved privacy protections. These technologies could advance innovation and collaboration in new fields and help harness the power of data to tackle some of our most pressing societal challenges.\nThat\u2019s why the U.S. and U.K. governments are partnering to deliver a set of prize challenges to unleash the potential of these democracy-affirming technologies to make a positive impact. In particular, this challenge will tackle two critical problems via separate data tracks: Data Track A will help with the identification of financial crime, while Data Track B will bolster pandemic responses.\nBy entering the prize challenges, innovators will have the opportunity to compete for cash prizes and engage with regulators and government agencies. Announced at the Summit for Democracy in December 2021, the prize challenges are a product of a collaboration between multiple government departments and agencies on both sides of the Atlantic. Winning solutions will have the opportunity to be profiled at the second Summit for Democracy, to be convened by President Joe Biden, in early 2023.\nObjectives\nThe goal of this prize challenge is to mature federated learning approaches and build trust in their adoption. The challenge organizers hope to accelerate the development of efficient privacy-preserving federated learning solutions that leverage a combination of input and output privacy techniques to:\nDrive innovation in the technological development and application of novel privacy-enhancing technologies\nDeliver strong privacy guarantees against a set of common threats and privacy attacks\nGenerate effective models to accomplish a set of predictive or analytical tasks that support the use cases\nChallenge Structure\nThe Solution Development Phase will have two data use case tracks matching those previously from the Concept Paper Phase\u2014Track A: Financial Crime Prevention and Track B: Pandemic Response and Forecasting. Teams can elect to participate in either or both tracks, with solutions that apply to one track individually or with a generalized solution. Each solution must correspond to a concept paper that met the requirements from the Concept Paper Phase.\nEach data use case track has a prize category for top solutions. All solutions that apply to Track A or Track B are eligible for a prize within that respective track. Teams who submit solutions for Track A and Track B are eligible to win a prize from each track.\nAdditionally, participants can enter a third Generalized Solutions category of prizes for the best solutions that are shown to be generalized and applied to both use cases with minor adaptations. Teams submitting generalized solutions are eligible for Generalized Solution prizes in addition to being eligible for prizes from both Track A and Track B.\nData Track A \u2013 Financial crime prevention \u2013 The United Nations estimates that up to $2 trillion of cross-border money laundering takes place each year, financing organized crime and undermining economic prosperity. Financial institutions such as banks and credit agencies, along with organizations that process transactions between institutions, such as the SWIFT global financial messaging provider, must protect personal and financial data, while also trying to report and deter illicit financial activities. Using synthetic datasets provided by SWIFT, you will design and later develop innovative privacy-preserving federated learning solutions that facilitate cross-institution and cross-border anomaly detection to combat financial crime. This use case features both vertical and horizontal data partitioning. Check out the Financial Crime overview page for more information.\nData Track B \u2013 Pandemic response and forecasting \u2013 As we continue to deal with COVID-19, it has become apparent that better ways to harness the power of data through analytics are critical for preparing for and responding to public health crises. Federated learning approaches could allow for responsible use of sensitive data to develop cross-organization and cross-border data analysis that would result in more robust forecasting and pandemic response capabilities. Using synthetic population datasets, you will design and later develop privacy-preserving federated learning solutions that can predict an individual\u2019s risk for infection. This use case features horizontal data partitioning. Check out the Pandemic Forecasting overview page for more information.\nGeneralizable Solutions \u2013 Cross-organization, cross-border use cases are certainly not limited to the financial or public health domains. Developing out-of-the-box generalized models that can be adapted for use with specific data or problem sets has great potential to advance the adoption and widespread use of privacy-preserving federated learning for public- and private-sector organizations in multiple sectors. To demonstrate generalizability, you may develop a solution using both the Track A and Track B datasets to be eligible for additional awards dedicated to generalizable solutions.\nMore information on how to get started can be found in the Problem Description.\nEligibility\nPhase 2 is open to Blue Team Participants who won invitations because their Concept Papers met the minimum criteria described in the challenge rules. No further registration will be required for the invitees to advance to Phase 2; however, invited Blue Team Participants should make necessary updates to their registration via their DrivenData account to reflect any changes in team composition or contact information. For more details, please refer to the official rules.\nPhase 2 Timeline and Prizes\nPhase 2 Key Dates\nLaunch October 5, 2022\nDeadline to Open Pull Requests for Runtime Environment January 11, 2023 at 11:59:59 PM UTC\nSubmissions Due January 26, 2023 at 11:59:59 PM UTC\nAnnouncement of Finalists to be Tested in Phase 3 Red Teaming February 13, 2023\nWinners Announced March 30, 2023\nPhase 2 Prizes\nOpen to Blue Team participants.\nData Track A: Financial Crime Prevention\nPrize Amount\n1st $100,000\n2nd $50,000\n3rd $25,000\nData Track B: Pandemic Response and Forecasting\nPrize Amount\n1st $100,000\n2nd $50,000\n3rd $25,000\nGeneralized Solutions\nPrize Amount\n1st $100,000\n2nd $50,000\n3rd $25,000\nSpecial Recognition\nPrize Amount\nPool $50,000\nPut your papers into action! Registered teams from Phase 1 will develop working prototypes and submit them to a remote execution environment for federated training and evaluation. These solutions are expected to be functional, i.e., capable of training a model and predicting against the evaluation data set with measurement of relevant performance and accuracy metrics. Solutions will be evaluated by a panel of judges across a set of weighted criteria. The top solutions, ranked by points awarded, will have their final rankings determined by incorporating red team evaluation from the red teams from Phase 3.\nTeams can qualify for one or multiple of Data Track A: Financial Crime Prevention, Data Track B: Pandemic Response and Forecasting, and Generalized Solutions prize categories depending on how their solutions address the two privacy-preserving federated learning tasks in the challenge.\nA separate Special Recognition prize pool is set aside to award up to five solutions that do not win prizes from the three main prize categories but demonstrate excellence in specific areas of privacy innovation: novelty, advancement in a specific privacy technology, usability, and efficiency.\nFull Challenge Timeline and Prizes\nTimeline Overview\nThere are three main phases in the challenge with two types of participants based on a red team/blue team approach. Blue Teams develop privacy-preserving solutions, while Red Teams act as adversaries to test those solutions.\nPhase 1: Concept Development (Jul\u2013Sept 2022): Blue Teams propose privacy-preserving federated learning solution concepts.\nPhase 2: Solution Development (Oct 2022\u2013Jan 2023): Blue Teams develop working prototypes of their solutions.\nPhase 3: Red Teaming (Nov 2022\u2013Feb 2023): Red Teams prepare and test privacy attacks on top blue team solutions from Phase 2.\nPrize Overview\nTrack Prize Pool\nPhase 1: Concept Paper $55,000\nPhase 2: Solution Development $575,000\nPhase 3: Red Teaming $120,000\nOpen Source $140,000*\nTotal $800,000\n* UPDATE March 22, 2023: Open Source award amounts have been increased to $20,000 per award and the number of awards have been increased to 7, for a total of up to $140,000. The increase is reallocated prize moneys that were not awarded in earlier phases, and the total pool of prize awards for the Challenge remains $800,000.\nAdditional Phase Details and Prizes\nPlace Prize Amount\n1st $30,000\n2nd $15,000\n3rd $10,000\nPhase 1: Concept Paper\nJuly 20\u2013September 19, 2022\nParticipants will produce an abstract and technical concept paper laying out their proposed solution. Concept papers will be evaluated by a panel of judges across a set of weighted criteria. Participants will be eligible to win prizes awarded to the top technical papers, ranked by points awarded. Participants must complete a paper in Phase 1 in order to be eligible to compete in Phase 2.\nOpen to Blue Team participants.\nPrize Amount\n1st $60,000\n2nd $40,000\n3rd $20,000\nPhase 3: Red Teaming\nNovember 10, 2022\u2013February 28, 2023\nPrivacy researchers are invited to form red teams to put the privacy claims of Phase 2's blue team finalists to the test. The red teams will prepare and test privacy attacks on the Phase 2 finalist solutions, which will be incorporated into the final Phase 2 rankings. Top red teams will be evaluated for success and rigor and will be awarded prizes for their performance during this phase.\nOpen to Red Team participants.\nPrize Amount\nPool $140,000\nOpen Source\nSubmissions due April 20, 2023\nUp to 7 of the final blue team winners from Phase 2 will be invited to release their solutions as open-source software. Each verified participating blue team will be awarded an Open Source prize of $20,000.\nOpen to top Blue Team winners from Phase 2.\n\nHow to compete (Phase 2)\nOnly blue teams who participated in Phase 1 and met minimum requirements are eligible to participate in Phase 2. Eligible teams are automatically registered. If you need to make changes to your team, please contact info@drivendata.org.\nGet familiar with the problem through the problem description and code submission format pages.\nDevelop and submit your centralized solution. Check out the centralized code submission format page for more details.\nDevelop and submit your federated solution. Check out the federated code submission format page for more details.\nGood luck!\nThis challenge is sponsored by the National Institute for Standards and Technology (NIST) and the National Science Foundation (NSF)\n\n        \nWith additional collaboration from the Office of Science and Technology Policy (OSTP), NASA, SWIFT, and the University of Virginia's Biocomplexity Institute.\nThis prize challenge and its U.K. counterpart have been developed as part of a joint collaboration between the United Kingdom and the United States.",
        "dataset_overview": "Data Track B: Pandemic Forecasting\nTransforming Pandemic Response and Forecasting\nThere are two data use case tracks for the PETs prize challenge. This is the Phase 2 data overview page for the pandemic forecasting track. In this track, innovators will develop end-to-end privacy-preserving federated learning solutions to accurately forecast individual risk of infection.\nBackground\nBackground\nData\nOverview\nThreat Profile\nModeling\nForecasting Risk\nPartitioning Data\nBackground\nFederated learning (FL), or more generally collaborative learning, shows huge promise for machine learning applications derived from sensitive data by enabling training on distributed data sets without sharing the data among the participating parties.\nThe PETs Prize Challenge is focused on enhancing cross-organization, cross-border data sharing to support efforts to enable better public health related forecasting to bolster pandemic response capabilities. The COVID-19 pandemic has taken an immense toll on human lives and had unprecedented level of socio-economic impact on individuals and societies around the globe. As we continue to deal with COVID-19, it has become apparent that better ways to harness the power of data by employing privacy-preserving data sharing and analytics are critical to preparing for such pandemics or public health crises.\nIn this Data Track, you are asked to develop innovative, privacy-preserving FL solutions utilizing dataset of a synthetic population (a.k.a. digital twin) that simulates a population with statistical and dynamical properties similar to a real population. You will design and later develop privacy-preserving federated learning solutions that can predict an individual\u2019s risk for infection. If an individual knows that they are likely to be infected in the next week, they can take more prophylactic measures (masking, changing plans, etc.) than they usually might. Furthermore, this knowledge could help public health officials better predict interventions and staffing needs for specific regions.\nBy focusing on this public health use case, the Challenge aims to drive innovation in privacy-enhancing technologies by exploring their role in a set of high-impact use cases where there are currently challenging trade-offs between enabling sufficient access to data to and limiting the identifiability of individuals or inference of their privacy sensitive information within those data sets. Though novel innovation for this use case alone could achieve significant real world impact, the challenge is designed to incentivize development of privacy technologies that can be applied to other use cases where data is distributed across multiple organizations or jurisdictions, both in public health and elsewhere. The best solutions will deliver meaningful innovation towards deployable solutions in this space, with consideration of how to evidence the privacy guarantees offered to data owners and regulators, but also have the potential to generalize to other situations.\nThe Data\nIn Phase 1, you were provided with datasets representing synthetic population data that includes:\na social contact network, capturing when and where any two people come into contact, and the duration of the contact\ndemographic attributes of individuals\nobservations of individuals\u2019 health state (i.e., whether they are infected or not).\nThese datasets have been generated by the University of Virginia\u2019s Biocomplexity Institute (UVA-BII). Two synthetic population datasets are provided: the first simulates the population of the state of Virginia, USA, and the second the population of the U.K. The two datasets (Virginia and U.K.) have identical structures and schemas (detailed below), but differ in size and scale. The code execution evaluation primarily focuses on the Virginia population. Teams are welcome to use the U.K. dataset for development and local experimentation and to report any such results in their technical paper.\nUsing these datasets and an agent-based outbreak simulation, the UVA-BII team created 63 days of simulated disease outbreak data, subsequently split into 56 days of training data and 7 days of target data. You will be provided with the synthetic populations and the first 56 days of the simulated outbreak datasets for model training. Your task is to predict a risk score for the binary disease state (infected or not infected) of each individual in the final week of the simulation. That is, you should predict the risk of whether each person in the population will be in an infected state on any day in the last 7 days.\nDetails about the size of the synthetic datasets:\nVirginia dataset U.K. dataset\nPopulation size 7.7 million 62 million\nNumber of social contacts 181 million 722 million\nNumber of disease state records (upper bound based on 1 reading per individual per day) 430 million 3.5 billion\nNote: The challenges are based on synthetic data to minimize the security burden placed on participants during the development phase; of course the intent of the challenge is to develop privacy solutions appropriate for use on real datasets with demonstrable privacy guarantees. However, participants must adhere to a data use agreement (see competition rules for more details).\nDisease States and Asymptomatic Infection\nEach individual in the population will have a disease state for each day of the dataset. The disease state acts like a finite-state machine with the following states:\nS: indicates \"susceptible\". This individual is susceptible to infection.\nI: indicates \"infected\". This individual has been infected and is infectious. They can infect other individuals (who are susceptible) through contact.\nR: indicates \"recovered\". This individual has recovered and is no longer infectious or infectable.\n\nState transition diagram for disease state in challenge dataset.\nAn individual disease state can only progress from S to I to R. Once an individual is in the recovered R state, they will not be able to become infected again within this simulation.\nThis dataset also features an additional layer of complexity around asymptomatic infection. Some individuals in the simulation will be asymptomatically infected and will transition to I states, but the I state is hidden from the data. Such an individual is still infectious while in the I state but will appear in the data as being in the S state before transitiong to the R state. This is in constrast to individuals with the normal symptomatic infection whose disease state is transparently visible in the data. Individuals in both normal symptomatic infection and asymptomatic infection states will transition to a visible R state upon recovery.\nState transition diagram for disease state in challenge dataset showing both symptomatic and asymptomatic infections. Asymptomatically infected individuals will have a hidden infected state in the simulation, but appear as susceptible in the dataset. For this reason, asymptomatic infections may appear to transition directly from susceptible to recovered.\nDevelopment and Evaluation Data\nThe datasets provided in Phase 1 are intended for local development use in both Phase 1 and Phase 2. Each dataset has been split in time\u2014the first 56 days of the dataset is the training set, and the final 7 days of the dataset is the forecast target data. The prediction task, as detailed in a later section, is to make predictions for the final 7 days of the dataset. The ground truth is provided for the forecast target period for the development datasets.\nIn Phase 2, a separate and held-out dataset will be used for solution evaluation. You can expect the Phase 2 evaluation dataset be close in size and statistical distributions to the development Virginia dataset, but have a different contact network and an independent disease outbreak simulation. In Phase 2, you will submit code for your solution to a code execution environment. The code execution runtime will run cold-start federated training on the new dataset's training split and then run inference to generate predictions for the forecast target period. Your solution's performance will be measured by evaluating its predictions against the ground truth for the new dataset's target period.\nData Details\nEach synthetic population dataset can be considered as a relational database consisting of eight tables. Each of the eight tables will be provided as a separate CSV file. Each table has data for a particular data schema. The fields and their description for each table are shown below. The eight tables are:\nPerson data - Information about individuals in the dataset\nHousehold data - Information about individual households\nResidence location data - Location information about household residences\nActivity location data - Information about locations where non-home activities take place\nActivity location assignment data - Information about which activities each person was involved in, where it took place, and timing information. Activities are repeated every day, as if it were like the film Groundhog Day\nPopulation contact network data - Information about the contact network between people, including location, time and duration. This table is generated from the Activity location assignment data table, and also repeats every day\nDisease outcome training data - Information about each individual's health status on a particular day\nDisease outcome target data - Binary ground truth infected label for each individual for the forecast period\nTable 1: Person data has the following fields:\nHousehold ID (hid) - An integer identifying a household\nPerson ID (pid) - A unique integer identifying a person\nPerson number (person_number) - Sequence ID of a person within the household. A household of size 3 will have people with person_numbers 1, 2, and 3.\nAge - Age of person\nSex - Indicating the gender of person\nTable 2: Household data has the following fields:\nHousehold ID (hid) - A unique integer identifying the household\nResidence ID (rlid) - An integer identifying the residence\nAdmin 1 - ADCW ID for the admin1 region for U.K. or state FIPS code for Virginia\nAdmin 2 - ADCW ID for the admin1 region for U.K.; Equals to admin1 if for U.K. (ADCW does not assign admin2 region ID for U.K.); 3-digit county FIPS code for Virginia\nHousehold Size (hh_size) - Number of persons in the household\nTable 3: Residence location data has the following fields:\nResidence ID (rlid) - A unique integer identifying the residence\nLongitude - the longitude of the location\nLatitude - the latitude of the location\nAdmin 1 - See household file description\nAdmin 2 - See household file description\nTable 4: Activity location data has the following fields:\nActivity location ID (alid) - Unique integer identifying the location where activity took place\nLongitude - Longitude of the location\nLatitude - Latitude of the location\nAdmin 1 - See household file description\nAdmin 2 - See household file description\nWork - Does location support work activity? (Value is 0 or 1)\nShopping - Does location support shopping activity? (Value is 0 or 1)\nSchool - Does location support school activity? (Value is 0 or 1)\nOther - Does location support other activity? (Value is 0 or 1)\nCollege - Does location support college activity? (Value is 0 or 1)\nReligion - Does location support religion activity? (Value is 0 or 1)\nTable 5: Activity location assignment data has the following fields:\nHousehold ID - Household ID of the person\nPerson ID - Person ID of the person\nActivity number - Number of the activity in the activity sequence to which it belongs\nActivity type - Activity type (1: Home, 2: Work, 3: Shopping, 4: Other, 5: School, 6: College)\nStart Time - Start time of activity in seconds since midnight\nDuration - Duration of the activity in seconds\nLocation ID - Location ID of the location where the activity takes place (rlid or alid)\nTable 6: Population contact network data has the following fields:\nPerson ID 1 (pid1) - Person ID number 1 of this edge\nPerson ID 2 (pid2) - Person ID number 2 of this edge\nLocation ID (lid) - Location ID where contact (edge) arises\nStart time - Start time of the contact between Person ID 1 and Person ID 2 measured in seconds since midnight\nDuration - Duration of the contact measured in seconds\nActivity ID 1 (activity1) - Activity type of Person ID 1 at time of contact (see above)\nActivity ID 2 (activity2) - Activity type of Person ID 2 at time of contact (see above)\nTable 7: Disease outcome training data has the following fields:\nDay - Simulation day\nPerson ID - ID of the person in the Person data or the Contact Network data\nDisease state - Disease-related state of the person on that day with possible values S for \"susceptible\", I for \"infected\", and R for \"recovered\". See previous section for details.\nTable 8: Disease outcome target data has the following fields:\nPerson ID - ID of the person in the Person data or the Contact Network data\nInfected - Binary variable with 1 if the person was infected in the final week of the simulation and 0 if otherwise\nAbove: Overall data model of the synthetic population data. The arrows indicate dependencies between files; for example, the hid element in the person file should be constrained to the values available in the household file.\nThreat Profile\nYour end-to-end solution should preserve privacy across a range of possible threats and attack scenarios, through all stages of the machine learning model lifecycle. You should therefore carefully consider the overall privacy of your solution, focusing on the protection of sensitive information held by all parties involved in the federated learning scenario. The solutions you design and develop should include comprehensive measures to address the threat profiles described below. These measures will provide an appropriate degree of resilience to a wide range of potential attacks defined within the threat profile.\nScope of sensitive data\nYour solution must prevent the unintended disclosure of sensitive information in the population dataset to any other party, including other insider stakeholders (i.e., the other federation units) and outsiders.\nThe following information in the dataset should be treated as sensitive:\nAll personally identifiable information, including the identity of a person, their housing information, their demographics such as age, etc.\nLocation activities of an individual\nThe health state of an individual\nSocial contact information, including the location of any social contact, and the identities of who was involved in the social contact.\nForecasting Individual Risk of Infection\nThe key analytical objective of the PPFL task is to effectively train a model that can predict the risk of infection for individuals in a population over a period of time in a privacy-preserving manner. Such a predictive capability is important from the perspective of an individual; if they know they are likely to be infected in the next week, they may choose to take additional preventative measures (such as wearing a face covering, reducing social contacts, or taking antivirals prophylactically). Additionally, this capability can help inform the measures public health authorities implement to respond to such outbreaks.\nPrediction Target and Evaluation Metric\nThe target variable for the modeling task is a risk score (between 0.0 and 1.0) for each individual in the population. That risk score corresponds to a confidence that that individual enters into an symptomatic infected I disease state at any time during the final week of the simulation (between days 56 and 63).\nNote that, as discussed in the previous section, some individuals become infected asymptomatically. Those individuals count as negative cases (not infected) for the purposes of the ground truth.\nThe evaluation metric will be Area Under the Precision\u2013Recall Curve (AUPRC), also known as average precision (AP), PR-AUC, or AUCPR. This is a commonly used metric for binary classification that summarizes model performance across all operating thresholds. This metric rewards models which can consistently assign positive cases with a higher risk score than negative cases. AUPRC is computed as follows:\nAUPRC=\n\u2211\nn\n(\nR\nn\n\u2212\nR\nn\u22121\n)\nP\nn\nwhere\nP\nn\nand\nR\nn\nare the precision and recall, respectively, when thresholding at the nth individual sorted in order of increasing recall.\nPartitioned Datasets for Federated learning\nOverview\nThe dataset will be horizontally partitioned into local datasets belonging to different federation units. This is intended to mirror how data is distributed across different health districts, hospitals, etc. These federation units have access to the full data tables described above. The dataset assumes that people are in precisely one federation unit, so a health district can see everywhere that one of its individuals goes, but does not have any access to information about other people who reside outside of the health district. As a result, the contacts between individuals from two different federation units are not represented in the population contact network in either of the local datasets.\nThe federated units work jointly to train a global FL model, and can take a common approach to technical design, infrastructure, etc., but are not able to access each other\u2019s raw data. In the real world there are a number of barriers that might prevent this; the parties and the datasets they contribute for PPFL may be subject to a variety of privacy, competition and industry specific regulations (e.g., HIPPA), may be operating in different jurisdictions, and have legitimate commercial and ethical reasons for not sharing customer data with other parties.\nAbove: Diagram comparing the centralized model (MC) with a privacy-preserving federated learning model (MPF).\nPartitioning Details\nFor local development, you were provided a full, unpartitioned dataset in Phase 1. In Phase 2, the evaluation data will be partitioned along administrative boundaries, e.g., by grouped FIPS codes/counties. Any cross-partition edges will be dropped, where an edge is created between two people (nodes) that come into contact as specified by the population contact network table. That is, each partition will only have visibility into edges between people that reside within the same partition.\nMoreover, a partition will have access to all information for all residents within the partition's counties, including residences, persons, and activity assignments. This is true even if an activity occurs in a county outside of that partition. Said another way, a partition knows about all locations within its counties, and all locations outside of its counties where an activity or contact occured.\nCross-partition edges will continue to affect the spread of infection, but will not be visible to any of the partitions. Any partitioning of the data that you might perform in your local development experiments should take this into account. Your solution should be able to handle any number of partitions, and in Phase 2, we may evaluate your solution with a number of partitions between 1 and 10.\nKey Task\nThe key task of this challenge is to design a privacy solution so that the collaborating parties can jointly train and deploy such a model without compromising the privacy requirements (more details on the requirements, and an associated threat model, are described in the problem description).\nFor the purposes of the challenge, you should demonstrate your solution by training two models:\nM\nC\n= a centralized model trained on the datasets in a non-privacy preserving way\nM\nPF\n= a privacy-preserving federated model trained using your privacy solution\nGood luck\nGood luck and enjoy this problem! For more details on submission and evaluation, visit the problem description page. If you have any questions, you can always ask the community by visiting the DrivenData user forum or the cross-U.S.\u2013U.K. public Slack channel. You can request access to the Slack channel here.",
        "evaluation_overview": "Data Track B: Pandemic Forecasting\nTransforming Pandemic Response and Forecasting\nThere are two data use case tracks for the PETs prize challenge. This is the Phase 2 data overview page for the pandemic forecasting track. In this track, innovators will develop end-to-end privacy-preserving federated learning solutions to accurately forecast individual risk of infection.\nBackground\nBackground\nData\nOverview\nThreat Profile\nModeling\nForecasting Risk\nPartitioning Data\nBackground\nFederated learning (FL), or more generally collaborative learning, shows huge promise for machine learning applications derived from sensitive data by enabling training on distributed data sets without sharing the data among the participating parties.\nThe PETs Prize Challenge is focused on enhancing cross-organization, cross-border data sharing to support efforts to enable better public health related forecasting to bolster pandemic response capabilities. The COVID-19 pandemic has taken an immense toll on human lives and had unprecedented level of socio-economic impact on individuals and societies around the globe. As we continue to deal with COVID-19, it has become apparent that better ways to harness the power of data by employing privacy-preserving data sharing and analytics are critical to preparing for such pandemics or public health crises.\nIn this Data Track, you are asked to develop innovative, privacy-preserving FL solutions utilizing dataset of a synthetic population (a.k.a. digital twin) that simulates a population with statistical and dynamical properties similar to a real population. You will design and later develop privacy-preserving federated learning solutions that can predict an individual\u2019s risk for infection. If an individual knows that they are likely to be infected in the next week, they can take more prophylactic measures (masking, changing plans, etc.) than they usually might. Furthermore, this knowledge could help public health officials better predict interventions and staffing needs for specific regions.\nBy focusing on this public health use case, the Challenge aims to drive innovation in privacy-enhancing technologies by exploring their role in a set of high-impact use cases where there are currently challenging trade-offs between enabling sufficient access to data to and limiting the identifiability of individuals or inference of their privacy sensitive information within those data sets. Though novel innovation for this use case alone could achieve significant real world impact, the challenge is designed to incentivize development of privacy technologies that can be applied to other use cases where data is distributed across multiple organizations or jurisdictions, both in public health and elsewhere. The best solutions will deliver meaningful innovation towards deployable solutions in this space, with consideration of how to evidence the privacy guarantees offered to data owners and regulators, but also have the potential to generalize to other situations.\nThe Data\nIn Phase 1, you were provided with datasets representing synthetic population data that includes:\na social contact network, capturing when and where any two people come into contact, and the duration of the contact\ndemographic attributes of individuals\nobservations of individuals\u2019 health state (i.e., whether they are infected or not).\nThese datasets have been generated by the University of Virginia\u2019s Biocomplexity Institute (UVA-BII). Two synthetic population datasets are provided: the first simulates the population of the state of Virginia, USA, and the second the population of the U.K. The two datasets (Virginia and U.K.) have identical structures and schemas (detailed below), but differ in size and scale. The code execution evaluation primarily focuses on the Virginia population. Teams are welcome to use the U.K. dataset for development and local experimentation and to report any such results in their technical paper.\nUsing these datasets and an agent-based outbreak simulation, the UVA-BII team created 63 days of simulated disease outbreak data, subsequently split into 56 days of training data and 7 days of target data. You will be provided with the synthetic populations and the first 56 days of the simulated outbreak datasets for model training. Your task is to predict a risk score for the binary disease state (infected or not infected) of each individual in the final week of the simulation. That is, you should predict the risk of whether each person in the population will be in an infected state on any day in the last 7 days.\nDetails about the size of the synthetic datasets:\nVirginia dataset U.K. dataset\nPopulation size 7.7 million 62 million\nNumber of social contacts 181 million 722 million\nNumber of disease state records (upper bound based on 1 reading per individual per day) 430 million 3.5 billion\nNote: The challenges are based on synthetic data to minimize the security burden placed on participants during the development phase; of course the intent of the challenge is to develop privacy solutions appropriate for use on real datasets with demonstrable privacy guarantees. However, participants must adhere to a data use agreement (see competition rules for more details).\nDisease States and Asymptomatic Infection\nEach individual in the population will have a disease state for each day of the dataset. The disease state acts like a finite-state machine with the following states:\nS: indicates \"susceptible\". This individual is susceptible to infection.\nI: indicates \"infected\". This individual has been infected and is infectious. They can infect other individuals (who are susceptible) through contact.\nR: indicates \"recovered\". This individual has recovered and is no longer infectious or infectable.\n\nState transition diagram for disease state in challenge dataset.\nAn individual disease state can only progress from S to I to R. Once an individual is in the recovered R state, they will not be able to become infected again within this simulation.\nThis dataset also features an additional layer of complexity around asymptomatic infection. Some individuals in the simulation will be asymptomatically infected and will transition to I states, but the I state is hidden from the data. Such an individual is still infectious while in the I state but will appear in the data as being in the S state before transitiong to the R state. This is in constrast to individuals with the normal symptomatic infection whose disease state is transparently visible in the data. Individuals in both normal symptomatic infection and asymptomatic infection states will transition to a visible R state upon recovery.\nState transition diagram for disease state in challenge dataset showing both symptomatic and asymptomatic infections. Asymptomatically infected individuals will have a hidden infected state in the simulation, but appear as susceptible in the dataset. For this reason, asymptomatic infections may appear to transition directly from susceptible to recovered.\nDevelopment and Evaluation Data\nThe datasets provided in Phase 1 are intended for local development use in both Phase 1 and Phase 2. Each dataset has been split in time\u2014the first 56 days of the dataset is the training set, and the final 7 days of the dataset is the forecast target data. The prediction task, as detailed in a later section, is to make predictions for the final 7 days of the dataset. The ground truth is provided for the forecast target period for the development datasets.\nIn Phase 2, a separate and held-out dataset will be used for solution evaluation. You can expect the Phase 2 evaluation dataset be close in size and statistical distributions to the development Virginia dataset, but have a different contact network and an independent disease outbreak simulation. In Phase 2, you will submit code for your solution to a code execution environment. The code execution runtime will run cold-start federated training on the new dataset's training split and then run inference to generate predictions for the forecast target period. Your solution's performance will be measured by evaluating its predictions against the ground truth for the new dataset's target period.\nData Details\nEach synthetic population dataset can be considered as a relational database consisting of eight tables. Each of the eight tables will be provided as a separate CSV file. Each table has data for a particular data schema. The fields and their description for each table are shown below. The eight tables are:\nPerson data - Information about individuals in the dataset\nHousehold data - Information about individual households\nResidence location data - Location information about household residences\nActivity location data - Information about locations where non-home activities take place\nActivity location assignment data - Information about which activities each person was involved in, where it took place, and timing information. Activities are repeated every day, as if it were like the film Groundhog Day\nPopulation contact network data - Information about the contact network between people, including location, time and duration. This table is generated from the Activity location assignment data table, and also repeats every day\nDisease outcome training data - Information about each individual's health status on a particular day\nDisease outcome target data - Binary ground truth infected label for each individual for the forecast period\nTable 1: Person data has the following fields:\nHousehold ID (hid) - An integer identifying a household\nPerson ID (pid) - A unique integer identifying a person\nPerson number (person_number) - Sequence ID of a person within the household. A household of size 3 will have people with person_numbers 1, 2, and 3.\nAge - Age of person\nSex - Indicating the gender of person\nTable 2: Household data has the following fields:\nHousehold ID (hid) - A unique integer identifying the household\nResidence ID (rlid) - An integer identifying the residence\nAdmin 1 - ADCW ID for the admin1 region for U.K. or state FIPS code for Virginia\nAdmin 2 - ADCW ID for the admin1 region for U.K.; Equals to admin1 if for U.K. (ADCW does not assign admin2 region ID for U.K.); 3-digit county FIPS code for Virginia\nHousehold Size (hh_size) - Number of persons in the household\nTable 3: Residence location data has the following fields:\nResidence ID (rlid) - A unique integer identifying the residence\nLongitude - the longitude of the location\nLatitude - the latitude of the location\nAdmin 1 - See household file description\nAdmin 2 - See household file description\nTable 4: Activity location data has the following fields:\nActivity location ID (alid) - Unique integer identifying the location where activity took place\nLongitude - Longitude of the location\nLatitude - Latitude of the location\nAdmin 1 - See household file description\nAdmin 2 - See household file description\nWork - Does location support work activity? (Value is 0 or 1)\nShopping - Does location support shopping activity? (Value is 0 or 1)\nSchool - Does location support school activity? (Value is 0 or 1)\nOther - Does location support other activity? (Value is 0 or 1)\nCollege - Does location support college activity? (Value is 0 or 1)\nReligion - Does location support religion activity? (Value is 0 or 1)\nTable 5: Activity location assignment data has the following fields:\nHousehold ID - Household ID of the person\nPerson ID - Person ID of the person\nActivity number - Number of the activity in the activity sequence to which it belongs\nActivity type - Activity type (1: Home, 2: Work, 3: Shopping, 4: Other, 5: School, 6: College)\nStart Time - Start time of activity in seconds since midnight\nDuration - Duration of the activity in seconds\nLocation ID - Location ID of the location where the activity takes place (rlid or alid)\nTable 6: Population contact network data has the following fields:\nPerson ID 1 (pid1) - Person ID number 1 of this edge\nPerson ID 2 (pid2) - Person ID number 2 of this edge\nLocation ID (lid) - Location ID where contact (edge) arises\nStart time - Start time of the contact between Person ID 1 and Person ID 2 measured in seconds since midnight\nDuration - Duration of the contact measured in seconds\nActivity ID 1 (activity1) - Activity type of Person ID 1 at time of contact (see above)\nActivity ID 2 (activity2) - Activity type of Person ID 2 at time of contact (see above)\nTable 7: Disease outcome training data has the following fields:\nDay - Simulation day\nPerson ID - ID of the person in the Person data or the Contact Network data\nDisease state - Disease-related state of the person on that day with possible values S for \"susceptible\", I for \"infected\", and R for \"recovered\". See previous section for details.\nTable 8: Disease outcome target data has the following fields:\nPerson ID - ID of the person in the Person data or the Contact Network data\nInfected - Binary variable with 1 if the person was infected in the final week of the simulation and 0 if otherwise\nAbove: Overall data model of the synthetic population data. The arrows indicate dependencies between files; for example, the hid element in the person file should be constrained to the values available in the household file.\nThreat Profile\nYour end-to-end solution should preserve privacy across a range of possible threats and attack scenarios, through all stages of the machine learning model lifecycle. You should therefore carefully consider the overall privacy of your solution, focusing on the protection of sensitive information held by all parties involved in the federated learning scenario. The solutions you design and develop should include comprehensive measures to address the threat profiles described below. These measures will provide an appropriate degree of resilience to a wide range of potential attacks defined within the threat profile.\nScope of sensitive data\nYour solution must prevent the unintended disclosure of sensitive information in the population dataset to any other party, including other insider stakeholders (i.e., the other federation units) and outsiders.\nThe following information in the dataset should be treated as sensitive:\nAll personally identifiable information, including the identity of a person, their housing information, their demographics such as age, etc.\nLocation activities of an individual\nThe health state of an individual\nSocial contact information, including the location of any social contact, and the identities of who was involved in the social contact.\nForecasting Individual Risk of Infection\nThe key analytical objective of the PPFL task is to effectively train a model that can predict the risk of infection for individuals in a population over a period of time in a privacy-preserving manner. Such a predictive capability is important from the perspective of an individual; if they know they are likely to be infected in the next week, they may choose to take additional preventative measures (such as wearing a face covering, reducing social contacts, or taking antivirals prophylactically). Additionally, this capability can help inform the measures public health authorities implement to respond to such outbreaks.\nPrediction Target and Evaluation Metric\nThe target variable for the modeling task is a risk score (between 0.0 and 1.0) for each individual in the population. That risk score corresponds to a confidence that that individual enters into an symptomatic infected I disease state at any time during the final week of the simulation (between days 56 and 63).\nNote that, as discussed in the previous section, some individuals become infected asymptomatically. Those individuals count as negative cases (not infected) for the purposes of the ground truth.\nThe evaluation metric will be Area Under the Precision\u2013Recall Curve (AUPRC), also known as average precision (AP), PR-AUC, or AUCPR. This is a commonly used metric for binary classification that summarizes model performance across all operating thresholds. This metric rewards models which can consistently assign positive cases with a higher risk score than negative cases. AUPRC is computed as follows:\nAUPRC=\n\u2211\nn\n(\nR\nn\n\u2212\nR\nn\u22121\n)\nP\nn\nwhere\nP\nn\nand\nR\nn\nare the precision and recall, respectively, when thresholding at the nth individual sorted in order of increasing recall.\nPartitioned Datasets for Federated learning\nOverview\nThe dataset will be horizontally partitioned into local datasets belonging to different federation units. This is intended to mirror how data is distributed across different health districts, hospitals, etc. These federation units have access to the full data tables described above. The dataset assumes that people are in precisely one federation unit, so a health district can see everywhere that one of its individuals goes, but does not have any access to information about other people who reside outside of the health district. As a result, the contacts between individuals from two different federation units are not represented in the population contact network in either of the local datasets.\nThe federated units work jointly to train a global FL model, and can take a common approach to technical design, infrastructure, etc., but are not able to access each other\u2019s raw data. In the real world there are a number of barriers that might prevent this; the parties and the datasets they contribute for PPFL may be subject to a variety of privacy, competition and industry specific regulations (e.g., HIPPA), may be operating in different jurisdictions, and have legitimate commercial and ethical reasons for not sharing customer data with other parties.\nAbove: Diagram comparing the centralized model (MC) with a privacy-preserving federated learning model (MPF).\nPartitioning Details\nFor local development, you were provided a full, unpartitioned dataset in Phase 1. In Phase 2, the evaluation data will be partitioned along administrative boundaries, e.g., by grouped FIPS codes/counties. Any cross-partition edges will be dropped, where an edge is created between two people (nodes) that come into contact as specified by the population contact network table. That is, each partition will only have visibility into edges between people that reside within the same partition.\nMoreover, a partition will have access to all information for all residents within the partition's counties, including residences, persons, and activity assignments. This is true even if an activity occurs in a county outside of that partition. Said another way, a partition knows about all locations within its counties, and all locations outside of its counties where an activity or contact occured.\nCross-partition edges will continue to affect the spread of infection, but will not be visible to any of the partitions. Any partitioning of the data that you might perform in your local development experiments should take this into account. Your solution should be able to handle any number of partitions, and in Phase 2, we may evaluate your solution with a number of partitions between 1 and 10.\nKey Task\nThe key task of this challenge is to design a privacy solution so that the collaborating parties can jointly train and deploy such a model without compromising the privacy requirements (more details on the requirements, and an associated threat model, are described in the problem description).\nFor the purposes of the challenge, you should demonstrate your solution by training two models:\nM\nC\n= a centralized model trained on the datasets in a non-privacy preserving way\nM\nPF\n= a privacy-preserving federated model trained using your privacy solution\nGood luck\nGood luck and enjoy this problem! For more details on submission and evaluation, visit the problem description page. If you have any questions, you can always ask the community by visiting the DrivenData user forum or the cross-U.S.\u2013U.K. public Slack channel. You can request access to the Slack channel here.",
        "url": "https://www.drivendata.org/competitions/103/nist-federated-learning-2-pandemic-forecasting-federated/"
    },
    {
        "competition_overview": "PETs Prize Challenge: Advancing Privacy-Preserving Federated Learning\nThis is the second in a series of challenge phases with a total prize pool of $800,000! Each phase in the PETs Prize Challenge invites participants to test and apply their innovations in privacy-preserving federated learning. Phase 2 is for Blue Teams to implement and submit working code for their solutions.\nYou are on the Phase 2 home page for the Pandemic Forecasting Data Track. You can find the Financial Crime Data Track here.\nBackground\nPrivacy-enhancing technologies (PETs) have the potential to unlock more trustworthy innovation in data analysis and machine learning. Federated learning is one such technology that enables organizations to analyze sensitive data while providing improved privacy protections. These technologies could advance innovation and collaboration in new fields and help harness the power of data to tackle some of our most pressing societal challenges.\nThat\u2019s why the U.S. and U.K. governments are partnering to deliver a set of prize challenges to unleash the potential of these democracy-affirming technologies to make a positive impact. In particular, this challenge will tackle two critical problems via separate data tracks: Data Track A will help with the identification of financial crime, while Data Track B will bolster pandemic responses.\nBy entering the prize challenges, innovators will have the opportunity to compete for cash prizes and engage with regulators and government agencies. Announced at the Summit for Democracy in December 2021, the prize challenges are a product of a collaboration between multiple government departments and agencies on both sides of the Atlantic. Winning solutions will have the opportunity to be profiled at the second Summit for Democracy, to be convened by President Joe Biden, in early 2023.\nObjectives\nThe goal of this prize challenge is to mature federated learning approaches and build trust in their adoption. The challenge organizers hope to accelerate the development of efficient privacy-preserving federated learning solutions that leverage a combination of input and output privacy techniques to:\nDrive innovation in the technological development and application of novel privacy-enhancing technologies\nDeliver strong privacy guarantees against a set of common threats and privacy attacks\nGenerate effective models to accomplish a set of predictive or analytical tasks that support the use cases\nChallenge Structure\nThe Solution Development Phase will have two data use case tracks matching those previously from the Concept Paper Phase\u2014Track A: Financial Crime Prevention and Track B: Pandemic Response and Forecasting. Teams can elect to participate in either or both tracks, with solutions that apply to one track individually or with a generalized solution. Each solution must correspond to a concept paper that met the requirements from the Concept Paper Phase.\nEach data use case track has a prize category for top solutions. All solutions that apply to Track A or Track B are eligible for a prize within that respective track. Teams who submit solutions for Track A and Track B are eligible to win a prize from each track.\nAdditionally, participants can enter a third Generalized Solutions category of prizes for the best solutions that are shown to be generalized and applied to both use cases with minor adaptations. Teams submitting generalized solutions are eligible for Generalized Solution prizes in addition to being eligible for prizes from both Track A and Track B.\nData Track A \u2013 Financial crime prevention \u2013 The United Nations estimates that up to $2 trillion of cross-border money laundering takes place each year, financing organized crime and undermining economic prosperity. Financial institutions such as banks and credit agencies, along with organizations that process transactions between institutions, such as the SWIFT global financial messaging provider, must protect personal and financial data, while also trying to report and deter illicit financial activities. Using synthetic datasets provided by SWIFT, you will design and later develop innovative privacy-preserving federated learning solutions that facilitate cross-institution and cross-border anomaly detection to combat financial crime. This use case features both vertical and horizontal data partitioning. Check out the Financial Crime overview page for more information.\nData Track B \u2013 Pandemic response and forecasting \u2013 As we continue to deal with COVID-19, it has become apparent that better ways to harness the power of data through analytics are critical for preparing for and responding to public health crises. Federated learning approaches could allow for responsible use of sensitive data to develop cross-organization and cross-border data analysis that would result in more robust forecasting and pandemic response capabilities. Using synthetic population datasets, you will design and later develop privacy-preserving federated learning solutions that can predict an individual\u2019s risk for infection. This use case features horizontal data partitioning. Check out the Pandemic Forecasting overview page for more information.\nGeneralizable Solutions \u2013 Cross-organization, cross-border use cases are certainly not limited to the financial or public health domains. Developing out-of-the-box generalized models that can be adapted for use with specific data or problem sets has great potential to advance the adoption and widespread use of privacy-preserving federated learning for public- and private-sector organizations in multiple sectors. To demonstrate generalizability, you may develop a solution using both the Track A and Track B datasets to be eligible for additional awards dedicated to generalizable solutions.\nMore information on how to get started can be found in the Problem Description.\nEligibility\nPhase 2 is open to Blue Team Participants who won invitations because their Concept Papers met the minimum criteria described in the challenge rules. No further registration will be required for the invitees to advance to Phase 2; however, invited Blue Team Participants should make necessary updates to their registration via their DrivenData account to reflect any changes in team composition or contact information. For more details, please refer to the official rules.\nPhase 2 Timeline and Prizes\nPhase 2 Key Dates\nLaunch October 5, 2022\nDeadline to Open Pull Requests for Runtime Environment January 11, 2023 at 11:59:59 PM UTC\nSubmissions Due January 26, 2023 at 11:59:59 PM UTC\nAnnouncement of Finalists to be Tested in Phase 3 Red Teaming February 13, 2023\nWinners Announced March 30, 2023\nPhase 2 Prizes\nOpen to Blue Team participants.\nData Track A: Financial Crime Prevention\nPrize Amount\n1st $100,000\n2nd $50,000\n3rd $25,000\nData Track B: Pandemic Response and Forecasting\nPrize Amount\n1st $100,000\n2nd $50,000\n3rd $25,000\nGeneralized Solutions\nPrize Amount\n1st $100,000\n2nd $50,000\n3rd $25,000\nSpecial Recognition\nPrize Amount\nPool $50,000\nPut your papers into action! Registered teams from Phase 1 will develop working prototypes and submit them to a remote execution environment for federated training and evaluation. These solutions are expected to be functional, i.e., capable of training a model and predicting against the evaluation data set with measurement of relevant performance and accuracy metrics. Solutions will be evaluated by a panel of judges across a set of weighted criteria. The top solutions, ranked by points awarded, will have their final rankings determined by incorporating red team evaluation from the red teams from Phase 3.\nTeams can qualify for one or multiple of Data Track A: Financial Crime Prevention, Data Track B: Pandemic Response and Forecasting, and Generalized Solutions prize categories depending on how their solutions address the two privacy-preserving federated learning tasks in the challenge.\nA separate Special Recognition prize pool is set aside to award up to five solutions that do not win prizes from the three main prize categories but demonstrate excellence in specific areas of privacy innovation: novelty, advancement in a specific privacy technology, usability, and efficiency.\nFull Challenge Timeline and Prizes\nTimeline Overview\nThere are three main phases in the challenge with two types of participants based on a red team/blue team approach. Blue Teams develop privacy-preserving solutions, while Red Teams act as adversaries to test those solutions.\nPhase 1: Concept Development (Jul\u2013Sept 2022): Blue Teams propose privacy-preserving federated learning solution concepts.\nPhase 2: Solution Development (Oct 2022\u2013Jan 2023): Blue Teams develop working prototypes of their solutions.\nPhase 3: Red Teaming (Nov 2022\u2013Feb 2023): Red Teams prepare and test privacy attacks on top blue team solutions from Phase 2.\nPrize Overview\nTrack Prize Pool\nPhase 1: Concept Paper $55,000\nPhase 2: Solution Development $575,000\nPhase 3: Red Teaming $120,000\nOpen Source $140,000*\nTotal $800,000\n* UPDATE March 22, 2023: Open Source award amounts have been increased to $20,000 per award and the number of awards have been increased to 7, for a total of up to $140,000. The increase is reallocated prize moneys that were not awarded in earlier phases, and the total pool of prize awards for the Challenge remains $800,000.\nAdditional Phase Details and Prizes\nPlace Prize Amount\n1st $30,000\n2nd $15,000\n3rd $10,000\nPhase 1: Concept Paper\nJuly 20\u2013September 19, 2022\nParticipants will produce an abstract and technical concept paper laying out their proposed solution. Concept papers will be evaluated by a panel of judges across a set of weighted criteria. Participants will be eligible to win prizes awarded to the top technical papers, ranked by points awarded. Participants must complete a paper in Phase 1 in order to be eligible to compete in Phase 2.\nOpen to Blue Team participants.\nPrize Amount\n1st $60,000\n2nd $40,000\n3rd $20,000\nPhase 3: Red Teaming\nNovember 10, 2022\u2013February 28, 2023\nPrivacy researchers are invited to form red teams to put the privacy claims of Phase 2's blue team finalists to the test. The red teams will prepare and test privacy attacks on the Phase 2 finalist solutions, which will be incorporated into the final Phase 2 rankings. Top red teams will be evaluated for success and rigor and will be awarded prizes for their performance during this phase.\nOpen to Red Team participants.\nPrize Amount\nPool $140,000\nOpen Source\nSubmissions due April 20, 2023\nUp to 7 of the final blue team winners from Phase 2 will be invited to release their solutions as open-source software. Each verified participating blue team will be awarded an Open Source prize of $20,000.\nOpen to top Blue Team winners from Phase 2.\n\nHow to compete (Phase 2)\nOnly blue teams who participated in Phase 1 and met minimum requirements are eligible to participate in Phase 2. Eligible teams are automatically registered. If you need to make changes to your team, please contact info@drivendata.org.\nGet familiar with the problem through the problem description and code submission format pages.\nDevelop and submit your centralized solution. Check out the centralized code submission format page for more details.\nDevelop and submit your federated solution. Check out the federated code submission format page for more details.\nGood luck!\nThis challenge is sponsored by the National Institute for Standards and Technology (NIST) and the National Science Foundation (NSF)\n\n        \nWith additional collaboration from the Office of Science and Technology Policy (OSTP), NASA, SWIFT, and the University of Virginia's Biocomplexity Institute.\nThis prize challenge and its U.K. counterpart have been developed as part of a joint collaboration between the United Kingdom and the United States.",
        "dataset_overview": "Data Track B: Pandemic Forecasting\nTransforming Pandemic Response and Forecasting\nThere are two data use case tracks for the PETs prize challenge. This is the Phase 2 data overview page for the pandemic forecasting track. In this track, innovators will develop end-to-end privacy-preserving federated learning solutions to accurately forecast individual risk of infection.\nBackground\nBackground\nData\nOverview\nThreat Profile\nModeling\nForecasting Risk\nPartitioning Data\nBackground\nFederated learning (FL), or more generally collaborative learning, shows huge promise for machine learning applications derived from sensitive data by enabling training on distributed data sets without sharing the data among the participating parties.\nThe PETs Prize Challenge is focused on enhancing cross-organization, cross-border data sharing to support efforts to enable better public health related forecasting to bolster pandemic response capabilities. The COVID-19 pandemic has taken an immense toll on human lives and had unprecedented level of socio-economic impact on individuals and societies around the globe. As we continue to deal with COVID-19, it has become apparent that better ways to harness the power of data by employing privacy-preserving data sharing and analytics are critical to preparing for such pandemics or public health crises.\nIn this Data Track, you are asked to develop innovative, privacy-preserving FL solutions utilizing dataset of a synthetic population (a.k.a. digital twin) that simulates a population with statistical and dynamical properties similar to a real population. You will design and later develop privacy-preserving federated learning solutions that can predict an individual\u2019s risk for infection. If an individual knows that they are likely to be infected in the next week, they can take more prophylactic measures (masking, changing plans, etc.) than they usually might. Furthermore, this knowledge could help public health officials better predict interventions and staffing needs for specific regions.\nBy focusing on this public health use case, the Challenge aims to drive innovation in privacy-enhancing technologies by exploring their role in a set of high-impact use cases where there are currently challenging trade-offs between enabling sufficient access to data to and limiting the identifiability of individuals or inference of their privacy sensitive information within those data sets. Though novel innovation for this use case alone could achieve significant real world impact, the challenge is designed to incentivize development of privacy technologies that can be applied to other use cases where data is distributed across multiple organizations or jurisdictions, both in public health and elsewhere. The best solutions will deliver meaningful innovation towards deployable solutions in this space, with consideration of how to evidence the privacy guarantees offered to data owners and regulators, but also have the potential to generalize to other situations.\nThe Data\nIn Phase 1, you were provided with datasets representing synthetic population data that includes:\na social contact network, capturing when and where any two people come into contact, and the duration of the contact\ndemographic attributes of individuals\nobservations of individuals\u2019 health state (i.e., whether they are infected or not).\nThese datasets have been generated by the University of Virginia\u2019s Biocomplexity Institute (UVA-BII). Two synthetic population datasets are provided: the first simulates the population of the state of Virginia, USA, and the second the population of the U.K. The two datasets (Virginia and U.K.) have identical structures and schemas (detailed below), but differ in size and scale. The code execution evaluation primarily focuses on the Virginia population. Teams are welcome to use the U.K. dataset for development and local experimentation and to report any such results in their technical paper.\nUsing these datasets and an agent-based outbreak simulation, the UVA-BII team created 63 days of simulated disease outbreak data, subsequently split into 56 days of training data and 7 days of target data. You will be provided with the synthetic populations and the first 56 days of the simulated outbreak datasets for model training. Your task is to predict a risk score for the binary disease state (infected or not infected) of each individual in the final week of the simulation. That is, you should predict the risk of whether each person in the population will be in an infected state on any day in the last 7 days.\nDetails about the size of the synthetic datasets:\nVirginia dataset U.K. dataset\nPopulation size 7.7 million 62 million\nNumber of social contacts 181 million 722 million\nNumber of disease state records (upper bound based on 1 reading per individual per day) 430 million 3.5 billion\nNote: The challenges are based on synthetic data to minimize the security burden placed on participants during the development phase; of course the intent of the challenge is to develop privacy solutions appropriate for use on real datasets with demonstrable privacy guarantees. However, participants must adhere to a data use agreement (see competition rules for more details).\nDisease States and Asymptomatic Infection\nEach individual in the population will have a disease state for each day of the dataset. The disease state acts like a finite-state machine with the following states:\nS: indicates \"susceptible\". This individual is susceptible to infection.\nI: indicates \"infected\". This individual has been infected and is infectious. They can infect other individuals (who are susceptible) through contact.\nR: indicates \"recovered\". This individual has recovered and is no longer infectious or infectable.\n\nState transition diagram for disease state in challenge dataset.\nAn individual disease state can only progress from S to I to R. Once an individual is in the recovered R state, they will not be able to become infected again within this simulation.\nThis dataset also features an additional layer of complexity around asymptomatic infection. Some individuals in the simulation will be asymptomatically infected and will transition to I states, but the I state is hidden from the data. Such an individual is still infectious while in the I state but will appear in the data as being in the S state before transitiong to the R state. This is in constrast to individuals with the normal symptomatic infection whose disease state is transparently visible in the data. Individuals in both normal symptomatic infection and asymptomatic infection states will transition to a visible R state upon recovery.\nState transition diagram for disease state in challenge dataset showing both symptomatic and asymptomatic infections. Asymptomatically infected individuals will have a hidden infected state in the simulation, but appear as susceptible in the dataset. For this reason, asymptomatic infections may appear to transition directly from susceptible to recovered.\nDevelopment and Evaluation Data\nThe datasets provided in Phase 1 are intended for local development use in both Phase 1 and Phase 2. Each dataset has been split in time\u2014the first 56 days of the dataset is the training set, and the final 7 days of the dataset is the forecast target data. The prediction task, as detailed in a later section, is to make predictions for the final 7 days of the dataset. The ground truth is provided for the forecast target period for the development datasets.\nIn Phase 2, a separate and held-out dataset will be used for solution evaluation. You can expect the Phase 2 evaluation dataset be close in size and statistical distributions to the development Virginia dataset, but have a different contact network and an independent disease outbreak simulation. In Phase 2, you will submit code for your solution to a code execution environment. The code execution runtime will run cold-start federated training on the new dataset's training split and then run inference to generate predictions for the forecast target period. Your solution's performance will be measured by evaluating its predictions against the ground truth for the new dataset's target period.\nData Details\nEach synthetic population dataset can be considered as a relational database consisting of eight tables. Each of the eight tables will be provided as a separate CSV file. Each table has data for a particular data schema. The fields and their description for each table are shown below. The eight tables are:\nPerson data - Information about individuals in the dataset\nHousehold data - Information about individual households\nResidence location data - Location information about household residences\nActivity location data - Information about locations where non-home activities take place\nActivity location assignment data - Information about which activities each person was involved in, where it took place, and timing information. Activities are repeated every day, as if it were like the film Groundhog Day\nPopulation contact network data - Information about the contact network between people, including location, time and duration. This table is generated from the Activity location assignment data table, and also repeats every day\nDisease outcome training data - Information about each individual's health status on a particular day\nDisease outcome target data - Binary ground truth infected label for each individual for the forecast period\nTable 1: Person data has the following fields:\nHousehold ID (hid) - An integer identifying a household\nPerson ID (pid) - A unique integer identifying a person\nPerson number (person_number) - Sequence ID of a person within the household. A household of size 3 will have people with person_numbers 1, 2, and 3.\nAge - Age of person\nSex - Indicating the gender of person\nTable 2: Household data has the following fields:\nHousehold ID (hid) - A unique integer identifying the household\nResidence ID (rlid) - An integer identifying the residence\nAdmin 1 - ADCW ID for the admin1 region for U.K. or state FIPS code for Virginia\nAdmin 2 - ADCW ID for the admin1 region for U.K.; Equals to admin1 if for U.K. (ADCW does not assign admin2 region ID for U.K.); 3-digit county FIPS code for Virginia\nHousehold Size (hh_size) - Number of persons in the household\nTable 3: Residence location data has the following fields:\nResidence ID (rlid) - A unique integer identifying the residence\nLongitude - the longitude of the location\nLatitude - the latitude of the location\nAdmin 1 - See household file description\nAdmin 2 - See household file description\nTable 4: Activity location data has the following fields:\nActivity location ID (alid) - Unique integer identifying the location where activity took place\nLongitude - Longitude of the location\nLatitude - Latitude of the location\nAdmin 1 - See household file description\nAdmin 2 - See household file description\nWork - Does location support work activity? (Value is 0 or 1)\nShopping - Does location support shopping activity? (Value is 0 or 1)\nSchool - Does location support school activity? (Value is 0 or 1)\nOther - Does location support other activity? (Value is 0 or 1)\nCollege - Does location support college activity? (Value is 0 or 1)\nReligion - Does location support religion activity? (Value is 0 or 1)\nTable 5: Activity location assignment data has the following fields:\nHousehold ID - Household ID of the person\nPerson ID - Person ID of the person\nActivity number - Number of the activity in the activity sequence to which it belongs\nActivity type - Activity type (1: Home, 2: Work, 3: Shopping, 4: Other, 5: School, 6: College)\nStart Time - Start time of activity in seconds since midnight\nDuration - Duration of the activity in seconds\nLocation ID - Location ID of the location where the activity takes place (rlid or alid)\nTable 6: Population contact network data has the following fields:\nPerson ID 1 (pid1) - Person ID number 1 of this edge\nPerson ID 2 (pid2) - Person ID number 2 of this edge\nLocation ID (lid) - Location ID where contact (edge) arises\nStart time - Start time of the contact between Person ID 1 and Person ID 2 measured in seconds since midnight\nDuration - Duration of the contact measured in seconds\nActivity ID 1 (activity1) - Activity type of Person ID 1 at time of contact (see above)\nActivity ID 2 (activity2) - Activity type of Person ID 2 at time of contact (see above)\nTable 7: Disease outcome training data has the following fields:\nDay - Simulation day\nPerson ID - ID of the person in the Person data or the Contact Network data\nDisease state - Disease-related state of the person on that day with possible values S for \"susceptible\", I for \"infected\", and R for \"recovered\". See previous section for details.\nTable 8: Disease outcome target data has the following fields:\nPerson ID - ID of the person in the Person data or the Contact Network data\nInfected - Binary variable with 1 if the person was infected in the final week of the simulation and 0 if otherwise\nAbove: Overall data model of the synthetic population data. The arrows indicate dependencies between files; for example, the hid element in the person file should be constrained to the values available in the household file.\nThreat Profile\nYour end-to-end solution should preserve privacy across a range of possible threats and attack scenarios, through all stages of the machine learning model lifecycle. You should therefore carefully consider the overall privacy of your solution, focusing on the protection of sensitive information held by all parties involved in the federated learning scenario. The solutions you design and develop should include comprehensive measures to address the threat profiles described below. These measures will provide an appropriate degree of resilience to a wide range of potential attacks defined within the threat profile.\nScope of sensitive data\nYour solution must prevent the unintended disclosure of sensitive information in the population dataset to any other party, including other insider stakeholders (i.e., the other federation units) and outsiders.\nThe following information in the dataset should be treated as sensitive:\nAll personally identifiable information, including the identity of a person, their housing information, their demographics such as age, etc.\nLocation activities of an individual\nThe health state of an individual\nSocial contact information, including the location of any social contact, and the identities of who was involved in the social contact.\nForecasting Individual Risk of Infection\nThe key analytical objective of the PPFL task is to effectively train a model that can predict the risk of infection for individuals in a population over a period of time in a privacy-preserving manner. Such a predictive capability is important from the perspective of an individual; if they know they are likely to be infected in the next week, they may choose to take additional preventative measures (such as wearing a face covering, reducing social contacts, or taking antivirals prophylactically). Additionally, this capability can help inform the measures public health authorities implement to respond to such outbreaks.\nPrediction Target and Evaluation Metric\nThe target variable for the modeling task is a risk score (between 0.0 and 1.0) for each individual in the population. That risk score corresponds to a confidence that that individual enters into an symptomatic infected I disease state at any time during the final week of the simulation (between days 56 and 63).\nNote that, as discussed in the previous section, some individuals become infected asymptomatically. Those individuals count as negative cases (not infected) for the purposes of the ground truth.\nThe evaluation metric will be Area Under the Precision\u2013Recall Curve (AUPRC), also known as average precision (AP), PR-AUC, or AUCPR. This is a commonly used metric for binary classification that summarizes model performance across all operating thresholds. This metric rewards models which can consistently assign positive cases with a higher risk score than negative cases. AUPRC is computed as follows:\nAUPRC=\n\u2211\nn\n(\nR\nn\n\u2212\nR\nn\u22121\n)\nP\nn\nwhere\nP\nn\nand\nR\nn\nare the precision and recall, respectively, when thresholding at the nth individual sorted in order of increasing recall.\nPartitioned Datasets for Federated learning\nOverview\nThe dataset will be horizontally partitioned into local datasets belonging to different federation units. This is intended to mirror how data is distributed across different health districts, hospitals, etc. These federation units have access to the full data tables described above. The dataset assumes that people are in precisely one federation unit, so a health district can see everywhere that one of its individuals goes, but does not have any access to information about other people who reside outside of the health district. As a result, the contacts between individuals from two different federation units are not represented in the population contact network in either of the local datasets.\nThe federated units work jointly to train a global FL model, and can take a common approach to technical design, infrastructure, etc., but are not able to access each other\u2019s raw data. In the real world there are a number of barriers that might prevent this; the parties and the datasets they contribute for PPFL may be subject to a variety of privacy, competition and industry specific regulations (e.g., HIPPA), may be operating in different jurisdictions, and have legitimate commercial and ethical reasons for not sharing customer data with other parties.\nAbove: Diagram comparing the centralized model (MC) with a privacy-preserving federated learning model (MPF).\nPartitioning Details\nFor local development, you were provided a full, unpartitioned dataset in Phase 1. In Phase 2, the evaluation data will be partitioned along administrative boundaries, e.g., by grouped FIPS codes/counties. Any cross-partition edges will be dropped, where an edge is created between two people (nodes) that come into contact as specified by the population contact network table. That is, each partition will only have visibility into edges between people that reside within the same partition.\nMoreover, a partition will have access to all information for all residents within the partition's counties, including residences, persons, and activity assignments. This is true even if an activity occurs in a county outside of that partition. Said another way, a partition knows about all locations within its counties, and all locations outside of its counties where an activity or contact occured.\nCross-partition edges will continue to affect the spread of infection, but will not be visible to any of the partitions. Any partitioning of the data that you might perform in your local development experiments should take this into account. Your solution should be able to handle any number of partitions, and in Phase 2, we may evaluate your solution with a number of partitions between 1 and 10.\nKey Task\nThe key task of this challenge is to design a privacy solution so that the collaborating parties can jointly train and deploy such a model without compromising the privacy requirements (more details on the requirements, and an associated threat model, are described in the problem description).\nFor the purposes of the challenge, you should demonstrate your solution by training two models:\nM\nC\n= a centralized model trained on the datasets in a non-privacy preserving way\nM\nPF\n= a privacy-preserving federated model trained using your privacy solution\nGood luck\nGood luck and enjoy this problem! For more details on submission and evaluation, visit the problem description page. If you have any questions, you can always ask the community by visiting the DrivenData user forum or the cross-U.S.\u2013U.K. public Slack channel. You can request access to the Slack channel here.",
        "evaluation_overview": "Data Track B: Pandemic Forecasting\nTransforming Pandemic Response and Forecasting\nThere are two data use case tracks for the PETs prize challenge. This is the Phase 2 data overview page for the pandemic forecasting track. In this track, innovators will develop end-to-end privacy-preserving federated learning solutions to accurately forecast individual risk of infection.\nBackground\nBackground\nData\nOverview\nThreat Profile\nModeling\nForecasting Risk\nPartitioning Data\nBackground\nFederated learning (FL), or more generally collaborative learning, shows huge promise for machine learning applications derived from sensitive data by enabling training on distributed data sets without sharing the data among the participating parties.\nThe PETs Prize Challenge is focused on enhancing cross-organization, cross-border data sharing to support efforts to enable better public health related forecasting to bolster pandemic response capabilities. The COVID-19 pandemic has taken an immense toll on human lives and had unprecedented level of socio-economic impact on individuals and societies around the globe. As we continue to deal with COVID-19, it has become apparent that better ways to harness the power of data by employing privacy-preserving data sharing and analytics are critical to preparing for such pandemics or public health crises.\nIn this Data Track, you are asked to develop innovative, privacy-preserving FL solutions utilizing dataset of a synthetic population (a.k.a. digital twin) that simulates a population with statistical and dynamical properties similar to a real population. You will design and later develop privacy-preserving federated learning solutions that can predict an individual\u2019s risk for infection. If an individual knows that they are likely to be infected in the next week, they can take more prophylactic measures (masking, changing plans, etc.) than they usually might. Furthermore, this knowledge could help public health officials better predict interventions and staffing needs for specific regions.\nBy focusing on this public health use case, the Challenge aims to drive innovation in privacy-enhancing technologies by exploring their role in a set of high-impact use cases where there are currently challenging trade-offs between enabling sufficient access to data to and limiting the identifiability of individuals or inference of their privacy sensitive information within those data sets. Though novel innovation for this use case alone could achieve significant real world impact, the challenge is designed to incentivize development of privacy technologies that can be applied to other use cases where data is distributed across multiple organizations or jurisdictions, both in public health and elsewhere. The best solutions will deliver meaningful innovation towards deployable solutions in this space, with consideration of how to evidence the privacy guarantees offered to data owners and regulators, but also have the potential to generalize to other situations.\nThe Data\nIn Phase 1, you were provided with datasets representing synthetic population data that includes:\na social contact network, capturing when and where any two people come into contact, and the duration of the contact\ndemographic attributes of individuals\nobservations of individuals\u2019 health state (i.e., whether they are infected or not).\nThese datasets have been generated by the University of Virginia\u2019s Biocomplexity Institute (UVA-BII). Two synthetic population datasets are provided: the first simulates the population of the state of Virginia, USA, and the second the population of the U.K. The two datasets (Virginia and U.K.) have identical structures and schemas (detailed below), but differ in size and scale. The code execution evaluation primarily focuses on the Virginia population. Teams are welcome to use the U.K. dataset for development and local experimentation and to report any such results in their technical paper.\nUsing these datasets and an agent-based outbreak simulation, the UVA-BII team created 63 days of simulated disease outbreak data, subsequently split into 56 days of training data and 7 days of target data. You will be provided with the synthetic populations and the first 56 days of the simulated outbreak datasets for model training. Your task is to predict a risk score for the binary disease state (infected or not infected) of each individual in the final week of the simulation. That is, you should predict the risk of whether each person in the population will be in an infected state on any day in the last 7 days.\nDetails about the size of the synthetic datasets:\nVirginia dataset U.K. dataset\nPopulation size 7.7 million 62 million\nNumber of social contacts 181 million 722 million\nNumber of disease state records (upper bound based on 1 reading per individual per day) 430 million 3.5 billion\nNote: The challenges are based on synthetic data to minimize the security burden placed on participants during the development phase; of course the intent of the challenge is to develop privacy solutions appropriate for use on real datasets with demonstrable privacy guarantees. However, participants must adhere to a data use agreement (see competition rules for more details).\nDisease States and Asymptomatic Infection\nEach individual in the population will have a disease state for each day of the dataset. The disease state acts like a finite-state machine with the following states:\nS: indicates \"susceptible\". This individual is susceptible to infection.\nI: indicates \"infected\". This individual has been infected and is infectious. They can infect other individuals (who are susceptible) through contact.\nR: indicates \"recovered\". This individual has recovered and is no longer infectious or infectable.\n\nState transition diagram for disease state in challenge dataset.\nAn individual disease state can only progress from S to I to R. Once an individual is in the recovered R state, they will not be able to become infected again within this simulation.\nThis dataset also features an additional layer of complexity around asymptomatic infection. Some individuals in the simulation will be asymptomatically infected and will transition to I states, but the I state is hidden from the data. Such an individual is still infectious while in the I state but will appear in the data as being in the S state before transitiong to the R state. This is in constrast to individuals with the normal symptomatic infection whose disease state is transparently visible in the data. Individuals in both normal symptomatic infection and asymptomatic infection states will transition to a visible R state upon recovery.\nState transition diagram for disease state in challenge dataset showing both symptomatic and asymptomatic infections. Asymptomatically infected individuals will have a hidden infected state in the simulation, but appear as susceptible in the dataset. For this reason, asymptomatic infections may appear to transition directly from susceptible to recovered.\nDevelopment and Evaluation Data\nThe datasets provided in Phase 1 are intended for local development use in both Phase 1 and Phase 2. Each dataset has been split in time\u2014the first 56 days of the dataset is the training set, and the final 7 days of the dataset is the forecast target data. The prediction task, as detailed in a later section, is to make predictions for the final 7 days of the dataset. The ground truth is provided for the forecast target period for the development datasets.\nIn Phase 2, a separate and held-out dataset will be used for solution evaluation. You can expect the Phase 2 evaluation dataset be close in size and statistical distributions to the development Virginia dataset, but have a different contact network and an independent disease outbreak simulation. In Phase 2, you will submit code for your solution to a code execution environment. The code execution runtime will run cold-start federated training on the new dataset's training split and then run inference to generate predictions for the forecast target period. Your solution's performance will be measured by evaluating its predictions against the ground truth for the new dataset's target period.\nData Details\nEach synthetic population dataset can be considered as a relational database consisting of eight tables. Each of the eight tables will be provided as a separate CSV file. Each table has data for a particular data schema. The fields and their description for each table are shown below. The eight tables are:\nPerson data - Information about individuals in the dataset\nHousehold data - Information about individual households\nResidence location data - Location information about household residences\nActivity location data - Information about locations where non-home activities take place\nActivity location assignment data - Information about which activities each person was involved in, where it took place, and timing information. Activities are repeated every day, as if it were like the film Groundhog Day\nPopulation contact network data - Information about the contact network between people, including location, time and duration. This table is generated from the Activity location assignment data table, and also repeats every day\nDisease outcome training data - Information about each individual's health status on a particular day\nDisease outcome target data - Binary ground truth infected label for each individual for the forecast period\nTable 1: Person data has the following fields:\nHousehold ID (hid) - An integer identifying a household\nPerson ID (pid) - A unique integer identifying a person\nPerson number (person_number) - Sequence ID of a person within the household. A household of size 3 will have people with person_numbers 1, 2, and 3.\nAge - Age of person\nSex - Indicating the gender of person\nTable 2: Household data has the following fields:\nHousehold ID (hid) - A unique integer identifying the household\nResidence ID (rlid) - An integer identifying the residence\nAdmin 1 - ADCW ID for the admin1 region for U.K. or state FIPS code for Virginia\nAdmin 2 - ADCW ID for the admin1 region for U.K.; Equals to admin1 if for U.K. (ADCW does not assign admin2 region ID for U.K.); 3-digit county FIPS code for Virginia\nHousehold Size (hh_size) - Number of persons in the household\nTable 3: Residence location data has the following fields:\nResidence ID (rlid) - A unique integer identifying the residence\nLongitude - the longitude of the location\nLatitude - the latitude of the location\nAdmin 1 - See household file description\nAdmin 2 - See household file description\nTable 4: Activity location data has the following fields:\nActivity location ID (alid) - Unique integer identifying the location where activity took place\nLongitude - Longitude of the location\nLatitude - Latitude of the location\nAdmin 1 - See household file description\nAdmin 2 - See household file description\nWork - Does location support work activity? (Value is 0 or 1)\nShopping - Does location support shopping activity? (Value is 0 or 1)\nSchool - Does location support school activity? (Value is 0 or 1)\nOther - Does location support other activity? (Value is 0 or 1)\nCollege - Does location support college activity? (Value is 0 or 1)\nReligion - Does location support religion activity? (Value is 0 or 1)\nTable 5: Activity location assignment data has the following fields:\nHousehold ID - Household ID of the person\nPerson ID - Person ID of the person\nActivity number - Number of the activity in the activity sequence to which it belongs\nActivity type - Activity type (1: Home, 2: Work, 3: Shopping, 4: Other, 5: School, 6: College)\nStart Time - Start time of activity in seconds since midnight\nDuration - Duration of the activity in seconds\nLocation ID - Location ID of the location where the activity takes place (rlid or alid)\nTable 6: Population contact network data has the following fields:\nPerson ID 1 (pid1) - Person ID number 1 of this edge\nPerson ID 2 (pid2) - Person ID number 2 of this edge\nLocation ID (lid) - Location ID where contact (edge) arises\nStart time - Start time of the contact between Person ID 1 and Person ID 2 measured in seconds since midnight\nDuration - Duration of the contact measured in seconds\nActivity ID 1 (activity1) - Activity type of Person ID 1 at time of contact (see above)\nActivity ID 2 (activity2) - Activity type of Person ID 2 at time of contact (see above)\nTable 7: Disease outcome training data has the following fields:\nDay - Simulation day\nPerson ID - ID of the person in the Person data or the Contact Network data\nDisease state - Disease-related state of the person on that day with possible values S for \"susceptible\", I for \"infected\", and R for \"recovered\". See previous section for details.\nTable 8: Disease outcome target data has the following fields:\nPerson ID - ID of the person in the Person data or the Contact Network data\nInfected - Binary variable with 1 if the person was infected in the final week of the simulation and 0 if otherwise\nAbove: Overall data model of the synthetic population data. The arrows indicate dependencies between files; for example, the hid element in the person file should be constrained to the values available in the household file.\nThreat Profile\nYour end-to-end solution should preserve privacy across a range of possible threats and attack scenarios, through all stages of the machine learning model lifecycle. You should therefore carefully consider the overall privacy of your solution, focusing on the protection of sensitive information held by all parties involved in the federated learning scenario. The solutions you design and develop should include comprehensive measures to address the threat profiles described below. These measures will provide an appropriate degree of resilience to a wide range of potential attacks defined within the threat profile.\nScope of sensitive data\nYour solution must prevent the unintended disclosure of sensitive information in the population dataset to any other party, including other insider stakeholders (i.e., the other federation units) and outsiders.\nThe following information in the dataset should be treated as sensitive:\nAll personally identifiable information, including the identity of a person, their housing information, their demographics such as age, etc.\nLocation activities of an individual\nThe health state of an individual\nSocial contact information, including the location of any social contact, and the identities of who was involved in the social contact.\nForecasting Individual Risk of Infection\nThe key analytical objective of the PPFL task is to effectively train a model that can predict the risk of infection for individuals in a population over a period of time in a privacy-preserving manner. Such a predictive capability is important from the perspective of an individual; if they know they are likely to be infected in the next week, they may choose to take additional preventative measures (such as wearing a face covering, reducing social contacts, or taking antivirals prophylactically). Additionally, this capability can help inform the measures public health authorities implement to respond to such outbreaks.\nPrediction Target and Evaluation Metric\nThe target variable for the modeling task is a risk score (between 0.0 and 1.0) for each individual in the population. That risk score corresponds to a confidence that that individual enters into an symptomatic infected I disease state at any time during the final week of the simulation (between days 56 and 63).\nNote that, as discussed in the previous section, some individuals become infected asymptomatically. Those individuals count as negative cases (not infected) for the purposes of the ground truth.\nThe evaluation metric will be Area Under the Precision\u2013Recall Curve (AUPRC), also known as average precision (AP), PR-AUC, or AUCPR. This is a commonly used metric for binary classification that summarizes model performance across all operating thresholds. This metric rewards models which can consistently assign positive cases with a higher risk score than negative cases. AUPRC is computed as follows:\nAUPRC=\n\u2211\nn\n(\nR\nn\n\u2212\nR\nn\u22121\n)\nP\nn\nwhere\nP\nn\nand\nR\nn\nare the precision and recall, respectively, when thresholding at the nth individual sorted in order of increasing recall.\nPartitioned Datasets for Federated learning\nOverview\nThe dataset will be horizontally partitioned into local datasets belonging to different federation units. This is intended to mirror how data is distributed across different health districts, hospitals, etc. These federation units have access to the full data tables described above. The dataset assumes that people are in precisely one federation unit, so a health district can see everywhere that one of its individuals goes, but does not have any access to information about other people who reside outside of the health district. As a result, the contacts between individuals from two different federation units are not represented in the population contact network in either of the local datasets.\nThe federated units work jointly to train a global FL model, and can take a common approach to technical design, infrastructure, etc., but are not able to access each other\u2019s raw data. In the real world there are a number of barriers that might prevent this; the parties and the datasets they contribute for PPFL may be subject to a variety of privacy, competition and industry specific regulations (e.g., HIPPA), may be operating in different jurisdictions, and have legitimate commercial and ethical reasons for not sharing customer data with other parties.\nAbove: Diagram comparing the centralized model (MC) with a privacy-preserving federated learning model (MPF).\nPartitioning Details\nFor local development, you were provided a full, unpartitioned dataset in Phase 1. In Phase 2, the evaluation data will be partitioned along administrative boundaries, e.g., by grouped FIPS codes/counties. Any cross-partition edges will be dropped, where an edge is created between two people (nodes) that come into contact as specified by the population contact network table. That is, each partition will only have visibility into edges between people that reside within the same partition.\nMoreover, a partition will have access to all information for all residents within the partition's counties, including residences, persons, and activity assignments. This is true even if an activity occurs in a county outside of that partition. Said another way, a partition knows about all locations within its counties, and all locations outside of its counties where an activity or contact occured.\nCross-partition edges will continue to affect the spread of infection, but will not be visible to any of the partitions. Any partitioning of the data that you might perform in your local development experiments should take this into account. Your solution should be able to handle any number of partitions, and in Phase 2, we may evaluate your solution with a number of partitions between 1 and 10.\nKey Task\nThe key task of this challenge is to design a privacy solution so that the collaborating parties can jointly train and deploy such a model without compromising the privacy requirements (more details on the requirements, and an associated threat model, are described in the problem description).\nFor the purposes of the challenge, you should demonstrate your solution by training two models:\nM\nC\n= a centralized model trained on the datasets in a non-privacy preserving way\nM\nPF\n= a privacy-preserving federated model trained using your privacy solution\nGood luck\nGood luck and enjoy this problem! For more details on submission and evaluation, visit the problem description page. If you have any questions, you can always ask the community by visiting the DrivenData user forum or the cross-U.S.\u2013U.K. public Slack channel. You can request access to the Slack channel here.",
        "url": "https://www.drivendata.org/competitions/145/nist-federated-learning-2-pandemic-forecasting-centralized/"
    },
    {
        "competition_overview": "PETs Prize Challenge: Advancing Privacy-Preserving Federated Learning\nThis is the third in a series of challenge phases with a total prize pool of $800,000! Each phase in the PETs Prize Challenge invites participants to test and apply their innovations in privacy-preserving federated learning. Phase 3 is for Red Teams to put the privacy claims of Blue Teams' solutions to the test.\nBackground\nPrivacy-enhancing technologies (PETs) have the potential to unlock more trustworthy innovation in data analysis and machine learning. Federated learning is one such technology that enables organizations to analyze sensitive data while providing improved privacy protections. These technologies could advance innovation and collaboration in new fields and help harness the power of data to tackle some of our most pressing societal challenges.\nThat\u2019s why the U.S. and U.K. governments are partnering to deliver a set of prize challenges to unleash the potential of these democracy-affirming technologies to make a positive impact. In particular, this challenge will tackle two critical problems via separate data tracks: Data Track A will help with the identification of financial crime, while Data Track B will bolster pandemic responses.\nBy entering the prize challenges, innovators will have the opportunity to compete for cash prizes and engage with regulators and government agencies. Announced at the Summit for Democracy in December 2021, the prize challenges are a product of a collaboration between multiple government departments and agencies on both sides of the Atlantic. Winning solutions will have the opportunity to be profiled at the second Summit for Democracy, to be convened by President Joe Biden, in early 2023.\nObjectives\nThe goal of this prize challenge is to mature federated learning approaches and build trust in their adoption. The challenge organizers hope to accelerate the development of efficient privacy-preserving federated learning solutions that leverage a combination of input and output privacy techniques to:\nDrive innovation in the technological development and application of novel privacy-enhancing technologies\nDeliver strong privacy guarantees against a set of common threats and privacy attacks\nGenerate effective models to accomplish a set of predictive or analytical tasks that support the use cases\nEligibility\nAt the time of entry, the Official Representative (individual or team lead, in the case of a group project) must be age 18 or older and a U.S. citizen or permanent resident of the United States or its territories. In the case of a private entity, the business shall be incorporated in and maintain a place of business in the United States or its territories.\nParticipants in this Challenge, whether they are individuals, entities, or team members, are prohibited from participating in the U.K. prize challenge, and participants in the U.K. prize challenge are likewise prohibited from participating in this Challenge. Individuals, entities, or team members who are found to have entered both the U.S. and the U.K. challenges will be disqualified from participating in either. If you wish to instead participate in the U.K. competition, please register here.\nAny individual who is or has been associated with an active Blue Team is prohibited from participating in the Red Team Phase. A Blue Team may withdraw from Phase 2 of the challenge so that its members are eligible to participate in the Red Team Phase. To withdraw, all registered members of a Blue Team must each email info@drivendata.org and declare unanimous intent of the team's withdrawal.\nIf you're looking to find teammates for this challenge, the community forum or the cross-U.S.\u2013U.K. public Slack channel are great places to start. You can request access to the Slack channel here.\nFor more details, please refer to the official rules.\nPhase 3 Timeline and Prizes\nPhase 3 Key Dates\nLaunch November 10, 2022\nRegistration Deadline December 2, 2022 at 11:59:59 PM UTC\nPreparation Period December 9, 2022\u2013February 13, 2023\nAttack Period February 13\u2013February 28, 2023\nWinners Announced March 30, 2023\nPhase 3 Prizes\nOpen to Red Team participants.\nTop red teams will be evaluated for success and rigor and will be awarded prizes for their performance during this phase.\nPrize Amount\n1st $60,000\n2nd $40,000\n3rd $20,000\nHow to compete (Phase 3)\nClick the \"Compete!\" button in the sidebar to begin the registration process.\nClick on \"Team\" if you need to create or join a team with other participants. To find other participants to form a team with, check out our community forum or the cross-U.S.\u2013U.K. public Slack channel. You can request access to the Slack channel here.\nClick on \"Registration Submission\" in the sidebar to fill out and submit the registration form. Only one submission is needed for a team. You're in!\nGet familiar with the problem through the data overview and problem description pages.\nDuring the preparation period, review the provided blue teams' concept papers to plan your privacy attacks.\nDuring the attack period, conduct privacy attacks on the blue team finalist solutions that you are assigned. Submit all required attack report materials by the deadline.\nNote that registration closes on December 2, 2023 at 11:59 PM UTC. Your team must complete registration by this deadline by submitting the registration form (step #3 above).\nGood luck!\nFull Challenge Timeline and Prizes\nTimeline Overview\nThere are three main phases in the challenge with two types of participants based on a red team/blue team approach. Blue Teams develop privacy-preserving solutions, while Red Teams act as adversaries to test those solutions.\nPhase 1: Concept Development (Jul\u2013Sept 2022): Blue Teams propose privacy-preserving federated learning solution concepts.\nPhase 2: Solution Development (Oct 2022\u2013Jan 2023): Blue Teams develop working prototypes of their solutions.\nPhase 3: Red Teaming (Nov 2022\u2013Feb 2023): Red Teams prepare and test privacy attacks on top blue team solutions from Phase 2.\nPrize Overview\nTrack Prize Pool\nPhase 1: Concept Paper $55,000\nPhase 2: Solution Development $575,000\nPhase 3: Red Teaming $120,000\nOpen Source $140,000*\nTotal $800,000\n* UPDATE March 22, 2023: Open Source award amounts have been increased to $20,000 per award and the number of awards have been increased to 7, for a total of up to $140,000. The increase is reallocated prize moneys that were not awarded in earlier phases, and the total pool of prize awards for the Challenge remains $800,000.\nAdditional Phase Details and Prizes\nPlace Prize Amount\n1st $30,000\n2nd $15,000\n3rd $10,000\nPhase 1: Concept Paper\nJuly 20\u2013September 19, 2022\nParticipants will produce an abstract and technical concept paper laying out their proposed solution. Concept papers will be evaluated by a panel of judges across a set of weighted criteria. Participants will be eligible to win prizes awarded to the top technical papers, ranked by points awarded. Participants must complete a paper in Phase 1 in order to be eligible to compete in Phase 2.\nOpen to Blue Team participants.\nData Track A: Financial Crime Prevention\nPrize Amount\n1st $100,000\n2nd $50,000\n3rd $25,000\nData Track B: Pandemic Response and Forecasting\nPrize Amount\n1st $100,000\n2nd $50,000\n3rd $25,000\nGeneralized Solutions\nPrize Amount\n1st $100,000\n2nd $50,000\n3rd $25,000\nSpecial Recognition\nPrize Amount\nPool $50,000\nPhase 2: Solution Development\nOctober 5, 2022\u2013January 26, 2023\nPut your papers into action! Registered teams from Phase 1 will develop working prototypes and submit them to a remote execution environment for federated training and evaluation. These solutions are expected to be functional, i.e., capable of training a model and predicting against the evaluation data set with measurement of relevant performance and accuracy metrics. Solutions will be evaluated by a panel of judges across a set of weighted criteria. The top solutions, ranked by points awarded, will have their final rankings determined by incorporating red team evaluation from the red teams from Phase 3.\nTeams can qualify for one or multiple of Data Track A: Financial Crime Prevention, Data Track B: Pandemic Response and Forecasting, and Generalized Solutions prize categories depending on how their solutions address the two privacy-preserving federated learning tasks in the challenge.\nA separate Special Recognition prize pool is set aside to award up to five solutions that do not win prizes from the three main prize categories but demonstrate excellence in specific areas of privacy innovation: novelty, advancement in a specific privacy technology, usability, and efficiency.\nOpen to Blue Team participants.\nPrize Amount\nPool $140,000\nOpen Source\nSubmissions due April 20, 2023\nUp to 7 of the final blue team winners from Phase 2 will be invited to release their solutions as open-source software. Each verified participating blue team will be awarded an Open Source prize of $20,000.\nOpen to top Blue Team winners from Phase 2.\n\nThis challenge is sponsored by the National Institute for Standards and Technology (NIST) and the National Science Foundation (NSF)\n\n        \nWith additional collaboration from the Office of Science and Technology Policy (OSTP), NASA, SWIFT, and the University of Virginia's Biocomplexity Institute.\nThis prize challenge and its U.K. counterpart have been developed as part of a joint collaboration between the United Kingdom and the United States.",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/139/nist-federated-learning-3-red-teams/"
    },
    {
        "competition_overview": "Overview\nYou are on the home page for PREPARE Phase 3, where the winners from Phase 2 will work to improve their models.\nThis competition is only open to winners of the Phase 2 Overall and Special Recognition prizes. If you believe you should have access, reach out to us at info@drivendata.org.\nAlzheimer's disease and Alzheimer's disease related dementias (AD/ADRD) are a set of brain disorders affecting more than 6 million Americans. Early intervention is crucial for successful disease modification, but detecting early signs of cognitive decline and AD/ADRD remains challenging. Current clinical methods often lack the sensitivity needed for early prediction, especially in underrepresented groups.\nThe objective of this challenge is to improve early prediction of Alzheimer's disease and related dementias (AD/ADRD). Through this initiative, the National Institute on Aging (NIA) aims to improve accuracy across diverse populations and explore understudied factors that may indicate early AD/ADRD.\nSubmission deadline\nJuly 15, 2025, 11:59 p.m. UTC\nPrizes\nPlace Amount\n1st $100,000\n2nd $50,000\nRunners-up (2) $25,000 each\nOverall Prizes\nOverall prizes will be awarded based on the overall prize evaluation criteria. Four overall prizes will be awarded: 1st Place, 2nd Place, and two Runners-up.\n1st and 2nd Place winners will be invited to an in-person winner showcase. Additional participants may be invited to the event depending on capacity.\nNumber Awarded Amount\nUp to 3 $7,500 each\nClean Code Bonus Prizes\nClean code bonus prizes will be awarded based on the clarity, reproducibility, and usability of each submitted codebase. See the problem description page for details.\n\nKey events\nRefinement Period (April 23 \u2013 July 15): Participants improve and demonstrate the potential of their Phase 2 winning models\nSubmission Deadline (July 15, 2025, 11:59 p.m. UTC): Participants submit their refined codebase and supporting materials\nVirtual Pitch Event (Late July or early August 2025): Participants present their refined solutions\nIn-Person Event (Late September 2025): 1st and 2nd place Overall Winners share insights and engage with NIH stakeholders\nHow to compete\nClick the \"Compete!\" button in the sidebar to enroll in the competition. Note that you will need to be granted access. If you believe you should have access but do not, send an email to info@drivendata.org.\nGet familiar with the challenge task through the problem description, particularly the competition objectives.\nCreate a submission according to the submission format.\nClick \"Submit\" in the sidebar, and upload your submission ZIP file. You can change your submission at any point before the final deadline by removing your previous submission, and then re-submitting a new file.\nPresent your findings at the virtual pitch event.\nCompetition rules\nThe challenge rules are in place to promote fair competition and useful solutions. If you are ever unsure whether your solution meets the competition rules, ask the challenge organizers in the competition forum or send an email to info@drivendata.org. A few key rules are highlighted below. For more details, see the \"Official rules\" page.\nExternal data usage\nExternal data is permitted in this competition provided participants have all rights, licenses, and permissions to use it as contemplated in the Competition Rules. If you aren't sure whether a specific dataset is allowable, please reach out to the challenge organizers in the competition forum or send an email to info@drivendata.org\nAdditional tools\nParticipants may use any additional or external tools as long as they are publicly available and free to use, including open weight pre-trained models. If you want to use a tool that is not clearly designated as open source, you must reach out to competition organizers for approval at info@drivendata.org.\nThis rule exists to ensure that participants have all rights, licenses, and permissions to include the tool as part of their submission as contemplated in the Competition Rules (e.g., see here).\nParticipation in PREPARE Phase 2\nParticipants must compete as part of the same group that received an award in the PREPARE Challenge - Phase 2. Participants who received a Phase 2 award as part of a team cannot add new team members. If only a subset of the Phase 2 team members compete, the team is still eligible.\nData terms of use\nParticipants must follow all applicable terms of use for any data source, including using data for permitted purposes, limiting sharing to authorized parties, deleting data when no longer needed, and properly crediting the data providers.\nCertain datasets, such as MHAS and DementiaBank, prohibit users from sharing competition data with any other person. This includes uploading competition data to any third-party service that will retain the data. For example, participants cannot upload MHAS or DementiaBank competition data to ChatGPT or Gemini, but can download open-source model weights and run a model locally.\nAcoustic Track: Users of DementiaBank data must follow the DementiaBank ground rules. Users are not permitted to share competition data with any other person, must delete competition data after the competition concludes, and must properly credit DementiaBank as follows:\nLanzi, A. M., Saylor, A. K., Fromm, D., Liu, H., MacWhinney, B., & Cohen, M. L. (2023). DementiaBank: Theoretical rationale, protocol, and illustrative analyses. American Journal of Speech-Language Pathology, 32(2), 426-438.\nSocial Determinants Track: Users of MHAS data must agree to the MHAS terms of use. Users are not permitted to share competition data with any other person, must delete competition data after the competition concludes, and must properly credit MHAS as follows:\nThe MHAS (Mexican Health and Aging Study) is partly sponsored by the National Institutes of Health/National Institute on Aging (grant number NIH R01AG018016) in the United States and the Instituto Nacional de Estad\u00edstica y Geograf\u00eda (INEGI) in Mexico. Data files and documentation are public use and available at www.MHASweb.org.\nChallenge sponsor\nThis challenge is sponsored by the National Institute on Aging (NIA), an institute of the National Institutes of Health (NIH)\n\nwith support from NASA",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/304/prepare-challenge-phase-3/"
    },
    {
        "competition_overview": "Overview\nAlzheimer's disease and Alzheimer's disease related dementias (AD/ADRD) are a set of brain disorders affecting more than 6 million Americans. Early intervention is crucial for successful disease modification, but detecting early signs of cognitive decline and AD/ADRD remains challenging. Current clinical methods often lack the sensitivity needed for early prediction, especially in underrepresented groups.\nThe objective of this challenge track is to improve early prediction of Alzheimer's disease and related dementias (AD/ADRD) using acoustic biomarkers from voice recordings. Through this initiative, the National Institute on Aging (NIA) aims to improve accuracy across diverse populations and explore understudied factors that may indicate early AD/ADRD.\nPrizes\nSubmission Deadline:\nDec. 19, 2024, 11:59 p.m. UTC\nPrize Amount\nOverall Prizes $160,000\nSpecial Recognition Prizes $50,000\nExplainability Bonus Prizes $40,000\nCommunity Code Bonus Prizes $10,000\nTotal $260,000\nTracks and Stages\nPhase 2 will occur over two stages, each with its own arena:\nModel Arena (Oct 22 - Dec 19, 2024): Participants submit predictions in the Model Arena, and scores are displayed on the public leaderboard.\nReport Arena (Dec 20, 2024 - Jan 22, 2025): The top 15 leaderboard finalists from each Model Arena (Social Determinants Track + Acoustic Track) are invited to submit reports in the pre-screened Report Arena.\nAll prizes, with the exception of community code, will be awarded based on a combination of leaderboard score and model report. For more details on the competition arenas and timeline, please see the Problem Description.\nIn addition, Phase 2 has two data tracks. You are currently on the page for the Acoustic Track, focused on identifying acoustic biomarkers of AD/ADRD using voice data. You can find the Social Determinants Track here.\nBreakdown\nPlace Amount\n1st $40,000\n2nd $25,000\n3rd $15,000\nOverall Prizes\nWho is eligible? Top 15 finalists on the Model Arena leaderboard who submit a model report.\nPrizes will be awarded to the top three submissions in this track, based on a combination of Model Arena leaderboard score and model report.\nNumber Awarded Amount\nUp to 5 $10,000\nSpecial Recognition Prizes\nWho is eligible? Top 15 finalists on the Model Arena leaderboard who (1) submit a model report and (2) do not receive an Overall Prize.\nUp to five Special Recognition prizes of $10,000 each will be awarded in total across both tracks to recognize excellence in addressing core challenge goals. Possible categories include: (1) most innovative methodology, (2) best early prediction, (3) best consideration of bias, (4) potential to generalize to populations disproportionately impacted by AD/ADRD, (5) for the social determinants track only, best approach to predicting decline between 2016 and 2021.\nNumber Awarded Amount\nUp to 4 $10,000\nExplainability Bonus Prizes\nWho is eligible? Top 15 finalists on the Model Arena leaderboard who submit an explainability report. Submitting a model report is not required.\nModel Arena finalists have the option of completing a bonus task to create outputs that can be used by patients or care providers to interpret and understand a prediction for a given individual. Up to 4 Explainability Bonus Prizes of $10,000 each will be awarded in total across both tracks for reports that explain local model predictions.\nNumber Awarded Amount\nUp to 4 $2,500\nCommunity Code Bonus Prizes\nWho is eligible? Anyone who submits a community code post in the Model Arena, regardless of whether they made a formal submission.\nUp to four prizes of $2,500 each will be awarded to the most helpful contributions to the community code board. In particular, we are looking for community code contributions related to: (1) Getting started and EDA; (2) Bias measurement and mitigation; (3) Feature selection and importance; (4) Model interpretability and explainability. Judges will make final selections that consider post content, clarity and documentation, use of code by others, and the number of upvotes and downloads.\n\nOverall Prize winners will be invited to compete for an additional $200,000 in prizes in the PREPARE Challenge - Phase 3: Proof of Principle Demonstration, where solvers will refine their solutions. Invitations may also be extended to Special Recognition prize winners depending on participation.\nHow to compete\nClick the \"Compete!\" button in the sidebar to enroll in the competition.\nConsider forming a team. You can find teammates using the competition forum.\nGet familiar with the problem through the overview and problem description. You might also want to reference additional resources available on the about page.\nDownload the data from the data tab.\nCreate and train your own model. Check out the community code page for inspiration, or add your own notebook for a chance to win the community code bonus prize!\nUse your model to generate predictions that match the submission format.\nClick \u201cSubmit\u201d in the sidebar, and then \u201cMake new submission\u201d. You\u2019re in!\nThe challenge rules are in place to promote fair competition and useful solutions. If you are ever unsure whether your solution meets the competition rules, ask the challenge organizers in the competition forum or send an email to info@drivendata.org.\nCompetition rules\nThe challenge rules are in place to promote fair competition and useful solutions. If you are ever unsure whether your solution meets the competition rules, ask the challenge organizers in the competition forum or send an email to info@drivendata.org. A few key rules are highlighted below. For more details, see the \"Official rules\" page.\nExternal data usage\nExternal data is not allowed in this competition. However, participants can use pre-trained computer vision models as long as they were (1) available freely and openly in that form at the start of the competition and (2) not trained on any data associated with the ground truth data for this challenge.\nEligibility and Participation Guidelines\nTo be eligible to win a prize under this challenge, participants:\nmust be 18 years of age or older at the time of submission\nmust be citizens or permanent residents of the United States (in the case of a Team, the Team must identify a Team Captain who is a citizen or permanent resident of the United States; in the case of an Entity, the Entity shall be incorporated in and maintain a primary place of business in the US)\nmust not be a federal entity or federal employee acting within the scope of their employment\nmust not be an employee of the Department of Health and Human Services (HHS, or any other component of HHS) acting in their personal capacity\nif employed by a federal agency or entity other than HHS (or any component of HHS), should consult with an agency ethics official to determine prize eligibility\nParticipants who receive federal funds from a grant award or cooperative agreement must either not use their funds to develop their Challenge submissions or to fund efforts in support of their Challenge submissions, or (if use of such funds is consistent with the purpose, terms, and conditions of the grant award or cooperative agreement), must participate in the Challenge as an Entity on behalf of the awardee institution, organization, or entity.\nFor a full list of eligibility and participation rules, please refer to the rules page and the challenge announcement.\nParticipation in PREPARE Phase 1\nAnyone is eligible to compete in the PREPARE Challenge - Phase 2. Participants did not have to compete in the PREPARE Challenge: Data for Early Prediction (Phase 1) to be eligible.\nData sharing and retention\nParticipants are not permitted to share competition data with any other person. Competition data must be deleted after the competition concludes.\nChallenge sponsor\nThis challenge is sponsored by the National Institute on Aging (NIA), an institute of the National Institute of Health (NIH)\n\nwith support from NASA",
        "dataset_overview": "Challenge data\nThe data for this challenge came from DementiaBank, a database for the study of communication progression in dementia that combines data from different research studies. Access to DementiaBank is password protected and restricted to university-affiliated researchers and clinicians who are part of the DementiaBank consortium group (see instructions for joining at dementia.talkbank.org).\nPlease note that we won't be able to field questions about the data, but we wanted to share it here for the benefit of the community. If you have a question, feel free to post it to the forum.\nUsers of DementiaBank data must properly credit DementiaBank with the following citation:\nLanzi, A. M., Saylor, A. K., Fromm, D., Liu, H., MacWhinney, B., & Cohen, M. (2023). DementiaBank: Theoretical rationale, protocol, and illustrative analyses. American Journal of Speech-Language Pathology, 32(2), 426-438. doi.org/10.1044/2022_AJSLP-22-00281\nLearn about the challenge data source: DementiaBank",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/299/competition-nih-alzheimers-acoustic-2/"
    },
    {
        "competition_overview": "Overview\nAlzheimer's disease and Alzheimer's disease related dementias (AD/ADRD) are a set of brain disorders affecting more than 6 million Americans. Early intervention is crucial for successful disease modification, but detecting early signs of cognitive decline and AD/ADRD remains challenging. Current clinical methods often lack the sensitivity needed for early prediction, especially in underrepresented groups.\nThe objective of this challenge track is to improve early prediction of Alzheimer's disease and related dementias (AD/ADRD) using social determinants of health. Through this initiative, the National Institute on Aging (NIA) aims to improve accuracy across diverse populations and explore understudied factors that may indicate early AD/ADRD.\nPrizes\nSubmission Deadline:\nDec. 19, 2024, 11:59 p.m. UTC\nPrize Amount\nOverall Prizes $160,000\nSpecial Recognition Prizes $50,000\nExplainability Bonus Prizes $40,000\nCommunity Code Bonus Prizes $10,000\nTotal $260,000\nTracks and Stages\nPhase 2 will occur over two stages, each with its own arena:\nModel Arena (Oct 22 - Dec 19, 2024): Participants submit predictions in the Model Arena, and scores are displayed on the public leaderboard.\nReport Arena (Dec 20, 2024 - Jan 22, 2025): The top 15 leaderboard finalists from each Model Arena (Social Determinants Track + Acoustic Track) are invited to submit reports in the pre-screened Report Arena.\nAll prizes, with the exception of community code, will be awarded based on a combination of leaderboard score and model report. For more details on the competition arenas and timeline, please see the Problem Description.\nIn addition, Phase 2 has two data tracks. You are currently on the page for the Social Determinants Track, focused on using social determinants of health from survey data to predict AD/ADRD. You can find the Acoustic Track here.\nBreakdown\nPlace Amount\n1st $40,000\n2nd $25,000\n3rd $15,000\nOverall Prizes\nWho is eligible? Top 15 finalists on the Model Arena leaderboard who submit a model report.\nPrizes will be awarded to the top three submissions in this track, based on a combination of Model Arena leaderboard score and model report.\nNumber Awarded Amount\nUp to 5 $10,000\nSpecial Recognition Prizes\nWho is eligible? Top 15 finalists on the Model Arena leaderboard who (1) submit a model report and (2) do not receive an Overall Prize.\nUp to five Special Recognition prizes of $10,000 each will be awarded in total across both tracks to recognize excellence in addressing core challenge goals. Possible categories include: (1) most innovative methodology, (2) best early prediction, (3) best consideration of bias, (4) potential to generalize to populations disproportionately impacted by AD/ADRD, (5) for the social determinants track only, best approach to predicting decline between 2016 and 2021.\nNumber Awarded Amount\nUp to 4 $10,000\nExplainability Bonus Prizes\nWho is eligible? Top 15 finalists on the Model Arena leaderboard who submit an explainability report. Submitting a model report is not required.\nModel Arena finalists have the option of completing a bonus task to create outputs that can be used by patients or care providers to interpret and understand a prediction for a given individual. Up to 4 Explainability Bonus Prizes of $10,000 each will be awarded in total across both tracks for reports that explain local model predictions.\nNumber Awarded Amount\nUp to 4 $2,500\nCommunity Code Bonus Prizes\nWho is eligible? Anyone who submits a community code post in the Model Arena, regardless of whether they made a formal submission.\nUp to four prizes of $2,500 each will be awarded to the most helpful contributions to the community code board. In particular, we are looking for community code contributions related to: (1) Getting started and EDA; (2) Bias measurement and mitigation; (3) Feature selection and importance; (4) Model interpretability and explainability. Judges will make final selections that consider post content, clarity and documentation, use of code by others, and the number of upvotes and downloads.\n\nOverall Prize winners will be invited to compete for an additional $200,000 in prizes in the PREPARE Challenge - Phase 3: Proof of Principle Demonstration, where solvers will refine their solutions. Invitations may also be extended to Special Recognition prize winners depending on participation.\nHow to compete\nClick the \"Compete!\" button in the sidebar to enroll in the competition.\nConsider forming a team. You can find teammates using the competition forum.\nGet familiar with the problem through the overview and problem description. You might also want to reference additional resources available on the about page.\nDownload the data following instructions on the data tab.\nCreate and train your own model. Check out the community code page for inspiration, or add your own notebook for a chance to win the community code bonus prize!\nUse your model to generate predictions that match the submission format.\nClick \u201cSubmit\u201d in the sidebar, and then \u201cMake new submission\u201d. You\u2019re in!\nThe challenge rules are in place to promote fair competition and useful solutions. If you are ever unsure whether your solution meets the competition rules, ask the challenge organizers in the competition forum or send an email to info@drivendata.org.\nCompetition rules\nThe challenge rules are in place to promote fair competition and useful solutions. If you are ever unsure whether your solution meets the competition rules, ask the challenge organizers in the competition forum or send an email to info@drivendata.org. A few key rules are highlighted below. For more details, see the \"Official rules\" page.\nExternal data usage\nExternal data is not allowed in this competition. However, participants can use pre-trained computer vision models as long as they were (1) available freely and openly in that form at the start of the competition and (2) not trained on any data associated with the ground truth data for this challenge.\nEligibility and Participation Guidelines\nTo be eligible to win a prize under this challenge, participants:\nmust be 18 years of age or older at the time of submission\nmust be citizens or permanent residents of the United States (in the case of a Team, the Team must identify a Team Captain who is a citizen or permanent resident of the United States; in the case of an Entity, the Entity shall be incorporated in and maintain a primary place of business in the US)\nmust not be a federal entity or federal employee acting within the scope of their employment\nmust not be an employee of the Department of Health and Human Services (HHS, or any other component of HHS) acting in their personal capacity\nif employed by a federal agency or entity other than HHS (or any component of HHS), should consult with an agency ethics official to determine prize eligibility\nParticipants who receive federal funds from a grant award or cooperative agreement must either not use their funds to develop their Challenge submissions or to fund efforts in support of their Challenge submissions, or (if use of such funds is consistent with the purpose, terms, and conditions of the grant award or cooperative agreement), must participate in the Challenge as an Entity on behalf of the awardee institution, organization, or entity.\nFor a full list of eligibility and participation rules, please refer to the rules page and the challenge announcement.\nParticipation in PREPARE Phase 1\nAnyone is eligible to compete in the PREPARE Challenge - Phase 2. Participants did not have to compete in the PREPARE Challenge: Data for Early Prediction (Phase 1) to be eligible.\nMHAS credit\nUsers of MHAS data must properly credit MHAS with the following text:\nThe MHAS (Mexican Health and Aging Study) is partly sponsored by the National Institutes of Health/National Institute on Aging (grant number NIH R01AG018016) in the United States and the Instituto Nacional de Estad\u00edstica y Geograf\u00eda (INEGI) in Mexico. Data files and documentation are public use and available at www.MHASweb.org.\nChallenge sponsor\nThis challenge is sponsored by the National Institute on Aging (NIA), an institute of the National Institute of Health (NIH)\n\nwith support from NASA",
        "dataset_overview": "Challenge data\nThe data for this challenge came from the Mexican Health and Aging Study (MHAS) Cognitive Aging Ancillary Study (Mex-Cog). MHAS is a longitudinal, population-representative national survey of older adults in Mexico. MHAS makes data products publicly available, including the data used to create the competition dataset. To access the data, people must register (free of charge) and agree to terms of use.\nPlease note that we won't be able to field questions about the data, but we wanted to share it here for the benefit of the community. If you have a question, feel free to post it to the forum.\nUsers of MHAS data must properly credit MHAS with the following text:\nThe MHAS (Mexican Health and Aging Study) is partly sponsored by the National Institutes of Health/National Institute on Aging (grant number NIH R01AG018016) in the United States and the Instituto Nacional de Estad\u00edstica y Geograf\u00eda (INEGI) in Mexico. Data files and documentation are public use and available at www.MHASweb.org.\nLearn about the challenge data source: MHAS",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/300/competition-nih-alzheimers-sdoh-2/"
    },
    {
        "competition_overview": "Overview\nYou are on the home page for the PREPARE Phase 2 Report Arena, where the top 15 leaderboard finalists from both Phase 2 Model Arena tracks will submit reports to compete for Overall, Special Recognition, and Explainability bonus prizes.\nAlzheimer's disease and Alzheimer's disease related dementias (AD/ADRD) are a set of brain disorders affecting more than 6 million Americans. Early intervention is crucial for successful disease modification, but detecting early signs of cognitive decline and AD/ADRD remains challenging. Current clinical methods often lack the sensitivity needed for early prediction, especially in underrepresented groups.\nThe objective of this challenge track is to improve early prediction of Alzheimer's disease and related dementias (AD/ADRD) using acoustic biomarkers from voice recordings or using social determinants of health. Through this initiative, the National Institute on Aging (NIA) aims to improve accuracy across diverse populations and explore understudied factors that may indicate early AD/ADRD.\nPrizes\nSubmission Deadline:\nJan. 22, 2025, 11:59 p.m. UTC\nPrize Amount\nOverall Prizes $160,000\nSpecial Recognition Prizes $50,000\nExplainability Bonus Prizes $40,000\nCommunity Code Bonus Prizes $10,000\nTotal $260,000\nTracks and Stages\nPhase 2 will occur over two stages, each with its own arena:\nModel Arena (Oct 22 - Dec 19, 2024): Participants submit predictions in the Model Arena, and scores are displayed on the public leaderboard.\nReport Arena (Dec 20, 2024 - Jan 22, 2025): The top 15 leaderboard finalists from each Model Arena (Social Determinants Track + Acoustic Track) are invited to submit reports in the pre-screened Report Arena.\nYou are currently on the page for the Report Arena, which is only open to the top 15 leaderboard finalists from each Model Arena (Social Determinants Track + Acoustic Track). Eligible teams are automatically registered. If you need to make changes to your team, please contact info@drivendata.org.\nBreakdown\nPlace Amount\n1st $40,000\n2nd $25,000\n3rd $15,000\nOverall Prizes\nWho is eligible? Top 15 finalists on the Model Arena leaderboard who submit a model report.\nPrizes will be awarded to the top three submissions in this track, based on a combination of Model Arena leaderboard score and model report.\nNumber Awarded Amount\nUp to 5 $10,000\nSpecial Recognition Prizes\nWho is eligible? Top 15 finalists on the Model Arena leaderboard who (1) submit a model report and (2) do not receive an Overall Prize.\nUp to five Special Recognition prizes of $10,000 each will be awarded in total across both tracks to recognize excellence in addressing core challenge goals. Possible categories include: (1) most innovative methodology, (2) best early prediction, (3) best consideration of bias, (4) potential to generalize to populations disproportionately impacted by AD/ADRD, (5) for the social determinants track only, best approach to predicting decline between 2016 and 2021.\nNumber Awarded Amount\nUp to 4 $10,000\nExplainability Bonus Prizes\nWho is eligible? Top 15 finalists on the Model Arena leaderboard who submit an explainability report. Submitting a model report is not required.\nModel Arena finalists have the option of completing a bonus task to create outputs that can be used by patients or care providers to interpret and understand a prediction for a given individual. Up to 4 Explainability Bonus Prizes of $10,000 each will be awarded in total across both tracks for reports that explain local model predictions.\nNumber Awarded Amount\nUp to 4 $2,500\nCommunity Code Bonus Prizes\nWho is eligible? Anyone who submits a community code post in the Model Arena, regardless of whether they made a formal submission.\nUp to four prizes of $2,500 each will be awarded to the most helpful contributions to the community code board. In particular, we are looking for community code contributions related to: (1) Getting started and EDA; (2) Bias measurement and mitigation; (3) Feature selection and importance; (4) Model interpretability and explainability. Judges will make final selections that consider post content, clarity and documentation, use of code by others, and the number of upvotes and downloads.\n\nOverall Prize winners will be invited to compete for an additional $200,000 in prizes in the PREPARE Challenge - Phase 3: Proof of Principle Demonstration, where solvers will refine their solutions. Invitations may also be extended to Special Recognition prize winners depending on participation.\nHow to compete\nThe Report Arena is only open to the top 15 teams on the final, private leaderboard in the Acoustic Track Model Arena or the Social Determinants Track Model Arena. Eligible teams are automatically added to the list of approved participants. If you need to make changes to your team, please contact info@drivendata.org.\nDecide whether to submit a model report for an Overall or Special Recognition Prize, an Explainability submission for an Explainability Bonus Prize, or both.\nTo make a model report submission, follow the instructions on the problem description page. If you had a top 15 leaderboard score in both model tracks (Social Determinants + Acoustic), you may submit two reports (one for each track). Use the \"SDoH report submission\" or \"Acoustic report submission\" in the sidebar to submit.\nTo make an explainability bonus track submission, follow the instructions on the Explainability bonus track page. If you had a top 15 leaderboard score in both tracks (Social Determinants + Acoustic), you may submit two submissions (one for each track). Use the \"SDoH bonus submission\" or \"Acoustic bonus submission\" in the sidebar to submit.\nCompetition rules\nThe challenge rules are in place to promote fair competition and useful solutions. If you are ever unsure whether your solution meets the competition rules, ask the challenge organizers in the competition forum or send an email to info@drivendata.org. A few key rules are highlighted below. For more details, see the \"Official rules\" page.\nModel Updates\nIn the Report Arena, participants are not allowed to change the model that was submitted to a model arena. If you believe a small correction is necessary \u2014 for example, to fix a bug revealed by the test set labels \u2014 please send an email to info@drivendata.org to confirm the change.\nData sharing and retention\nParticipants are not permitted to share competition data with any other person. Competition data must be deleted after the competition concludes.\nEligibility and Participation Guidelines\nTo be eligible to win a prize under this challenge, participants:\nmust be 18 years of age or older at the time of submission\nmust be citizens or permanent residents of the United States (in the case of a Team, the Team must identify a Team Captain who is a citizen or permanent resident of the United States; in the case of an Entity, the Entity shall be incorporated in and maintain a primary place of business in the US)\nmust not be a federal entity or federal employee acting within the scope of their employment\nmust not be an employee of the Department of Health and Human Services (HHS, or any other component of HHS) acting in their personal capacity\nif employed by a federal agency or entity other than HHS (or any component of HHS), should consult with an agency ethics official to determine prize eligibility\nParticipants who receive federal funds from a grant award or cooperative agreement must either not use their funds to develop their Challenge submissions or to fund efforts in support of their Challenge submissions, or (if use of such funds is consistent with the purpose, terms, and conditions of the grant award or cooperative agreement), must participate in the Challenge as an Entity on behalf of the awardee institution, organization, or entity.\nFor a full list of eligibility and participation rules, please refer to the rules page.\nChallenge sponsor\nThis challenge is sponsored by the National Institute on Aging (NIA), an institute of the National Institute of Health (NIH)\n\nwith support from NASA",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/301/prepare-challenge-phase-2-report-arena/"
    },
    {
        "competition_overview": "PREPARE: Pioneering Research for Early Prediction of Alzheimer's and Related Dementias EUREKA Challenge\nPhase 1 [Find IT!]: Data for Early Prediction\nOverview\nAlzheimer's disease and Alzheimer's disease related dementias (AD/ADRD) are a set of brain disorders affecting more than 6 million Americans. The main clinical features of AD/ADRD are progressive impairments of cognition and function and changes in behavior. Early intervention is important for successful disease modification, but detecting early signs of cognitive decline and Alzheimer's disease and related dementias (AD/ADRD) is challenging. Current clinical methods lack the sensitivity needed for early prediction. Alternative approaches (e.g. neuroimaging, fluid biomarkers, neuropsychological tasks, and digital and passive measures) have drawbacks, including cost, complexity, and accessibility. This is especially true for underrepresented groups in research.\nThe National Institute on Aging (NIA), a component of the National Institutes of Health (NIH), aims to diversify data resources, considering under-resourced communities disproportionately burdened by AD/ADRD. For example, factors other than the amyloid protein \u2013 which has long been considered a biomarker for AD \u2013 may play a bigger role in cognitive impairment for Asian, Black, or Hispanic older adults (Wilkins et al., 2022; Dark and Walker, 2022). Identifying new biomarkers and social determinants of health is crucial to improve early detection in these groups and to address racial and ethnic disparities in diagnoses.\nObjectives\nThe goal of the PREPARE Challenge (Pioneering Research for Early Prediction of Alzheimer's and Related Dementias EUREKA Challenge) is to inform novel approaches to early detection that might ultimately lead to more accurate tests, tools, and methodologies for clinical and research purposes. Advances in artificial intelligence (AI), machine learning (ML), and computing ecosystems increase possibilities of intelligent data collection and analysis, including better algorithms and methods that could be leveraged for the prediction of biological, psychological (cognitive), socio-behavioral, functional, and clinical changes related to AD/ADRD.\nTo make progress, the challenge aims to address the need for:\nData from a wider set of sources and types, including data relevant to low-resourced, underserved communities disproportionately impacted by AD/ADRD to better understand and address biases in existing data sources;\nOpen, shareable data, stored in trusted repositories to determine \u201cdistributional robustness\u201d of predictive algorithms; and\nAlgorithms that meet \"right to explanation\" mandates (i.e., if an AI algorithm impacts people, people have a right to an explanation of how AI conclusions were reached).\nChallenge structure and phases\nThe goal of this challenge is to spur and reward the development of solutions for accurate, innovative, and representative early prediction of AD/ADRD. To achieve this goal, the challenge will feature three phases that successively build on each other.\nThis first phase, Find IT!: Data for Early Prediction, is focused on finding, curating, or contributing data to create representative and open datasets that can be used for the early prediction of AD/ADRD. For more details on the different phases and associated prizes, visit the challenge home page.\nPhase overview\nPhase Anticipated Date Description\nPhase 1 [Find IT!]: Data for Early Prediction (YOU ARE HERE) September 2023 Find, curate, or contribute data to create representative and open datasets that can be used for early prediction of AD/ADRD.\nPhase 2 [Build IT!]: Algorithms and Approaches October 2024 Advance algorithms and analytic approaches for early prediction of AD/ADRD, with an emphasis on explainability of predictions.\nPhase 3 [Put IT All Together!]: Proof of Principle Demonstration April 2025 Top solvers from Phase 2 demonstrate algorithmic approaches on diverse datasets and share their results at an innovation event .\nEligibility and Participation Guidelines\nTo be eligible to win a prize under this challenge, participants:\nmust be 18 years of age or older at the time of submission\nmust be citizens or permanent residents of the United States (in the case of a Team, the Team must identify a Team Captain who is a citizen or permanent resident of the United States; in the case of an Entity, the Entity shall be incorporated in and maintain a primary place of business in the US)\nmust not be a federal entity or federal employee acting within the scope of their employment\nmust not be an employee of the Department of Health and Human Services (HHS, or any other component of HHS) acting in their personal capacity\nif employed by a federal agency or entity other than HHS (or any component of HHS), should consult with an agency ethics official to determine prize eligibility\nParticipants who receive federal funds from a grant award or cooperative agreement must either not use their funds to develop their Challenge submissions or to fund efforts in support of their Challenge submissions, or (if use of such funds is consistent with the purpose, terms, and conditions of the grant award or cooperative agreement), must participate in the Challenge as an Entity on behalf of the awardee institution, organization, or entity.\nFor a full list of eligibility and participation rules, please refer to the rules page and the challenge announcement.\nPhase 1 key dates\nChallenge launch September 1, 2023\n(Optional) Midpoint review deadline for executive summary drafts\n(does not apply to data collection ideas) November 15, 2023 at 11:59:59 PM UTC\nWebinar Event\nPresentation by the NIH team and challenge Q&A December 13, 2023 at 3:00 PM ET\nExecutive summary drafts due\n(does not apply to data collection ideas) January 17, 2024 at 11:59:59 PM UTC\nFinal submissions due January 31, 2024 at 11:59:59 PM UTC\nFinalists notified March 18, 2024\nFinalists provide data access for verification June 1, 2024\nPhase 1 prizes\nThe total prize purse for Phase 1 is $200,000. A pool of $150,000 will reward the strongest submissions of representative, inclusive, open, and shareable datasets that can be used for a data science competition focused on the early prediction of AD/ADRD. A bonus prize of $25,000 may be awarded to the submission that best addresses populations disproportionately impacted by AD/ADRD. Solvers may also compete for one of five $5,000 prizes by submiting an idea for new data collection aligned with challenge aims.\nPhase 1 Competition End Date: Jan. 31, 2024, 11:59 p.m. UTC\nAward Prize Amount\n1st $50,000\n2nd $40,000\n3rd $30,000\n4th $20,000\n5th $10,000\nDisproportionate Impact Bonus Prize $25,000\nData Collection Proposal Prizes $25,000\nTotal $200,000\nDisproportionate Impact Bonus Prize ($25,000)\nTeams awarded prizes for their primary submissions in Phase 1 will be eligible for the Disproportionate Impact Bonus Prize. This prize recognizes the submission with the most potential to support algorithms that generalize to populations disproportionately impacted by AD/ADRD.\nPrizes for Data Collection Ideas (subtotal $25,000)\nParticipants who submit ideas for data collection will be eligible for prizes recognizing proposals to collect new representative and open datasets that can be used for early prediction of AD/ADRD, with an emphasis on addressing biases in existing data sources. Up to five total Data Collection Proposal Prizes ($5,000 each) may be awarded from a prize pool of $25,000.\nHow to compete (Phase 1)\nClick the \"Compete!\" button in the sidebar to enroll in the competition.\nClick on \"Team\" if you need to create or join a team with other participants.\nGet familiar with the problem through the Problem description and About pages.\nSubmit a draft of your executive summary by January 17, 2024 at the latest by clicking on Executive summary draft in the sidebar and filling out the form. To receive feedback based on your submission idea, submit your executive summary draft by the midpoint review deadline, November 15, 2023. Partway there!\nPost questions you have about the challenge to the challenge forum. Questions may be answered directly or at a Webinar event with DrivenData and NIH on December 13, 2023.\nDive into the details of your data! Make sure to prepare your data and materials according to the submission format. You can refer to the evaluation criteria and submission template for guidance.\nSubmit a detailed description of your data along with an updated executive summary by January 31, 2024. Submit by clicking on Final submission in the sidebar and filling out the form. You\u2019re in! Note that the last submission before challenge close will be considered the final submission.\nThe primary goal of the challenge is to find an existing dataset to be used in later challenge phases; however, solvers may have ideas for collecting or building out datasets that align with the challenge aims but would not be ready to submit for this challenge. For more information about how to compete for a data collection proposal prize, see the ideas for data collection page.\nThe challenge rules are in place to promote fair competition and useful solutions. If you are ever unsure whether your solution meets the competition rules, ask the challenge organizers in the competition forum or send an email to info@drivendata.org.\nChallenge sponsor\nThis challenge is sponsored by the National Institute on Aging (NIA), an institute of the National Institute of Health (NIH)\n\nwith support from NASA",
        "dataset_overview": "Ideas for Data Collection\nIn addition to the main challenge, solvers are invited to propose new data collection that will result in an open, shareable dataset that can support novel machine learning approaches for early prediction of AD/ADRD with an emphasis on addressing biases in existing data sources. Solvers who propose ideas for new datasets will be eligible to win prizes from a smaller pool of $25,000.\nUnlike in the main challenge where solvers submit existing data, this alternate track invites solvers to propose an idea for future data collection efforts that the solvers intend to undertake. The proposed data need not be ready by the end of the challenge, and indeed, more ambitious proposals may take much longer to collect. The proposed data must include a target variable that validly indicates AD/ADRD, along with predictor variables that can be used as input features for predicting the target variable. Visit the Problem description for more about the problem framing.\nSubmission format\nProposal Submissions must include a data collection proposal section and a team introduction section (subheadings encouraged but optional):\nData collection proposal (maximum 4 pages, excluding references)\nBackground: Clearly articulate your understanding of the problem and goals of the challenge, and how the proposed dataset will address those problems.\nBasic information: Summarize the proposed activities and describe the resulting dataset, including the data sources and relevant study methodology, and which if any parts of the dataset already exist. The description of the resulting dataset should include its sample size (or target sample size, with justification), definitions of the proposed target variable(s) and proposed predictor variables, the amount of time required to complete activities, and major challenges of the proposed plan.\nUtility & rigor: Provide a definition of the proposed target variable(s), information about its measurement and its distribution in the sample. Present evidence that the target reliably and validly indicates AD/ADRD. Describe the predictor variables in the dataset and explain any theoretical or empirical links to AD/ADRD. Include any potential for validation or generalization to other data.\nInnovation: Describe to what extent the proposed data push forward the state of the field and present novel insights and directions for further research by, for example, enabling higher accuracy, earlier predictions, lower cost, and/or greater accessibility. Describe other similar datasets, if any, and how the proposed dataset provides advancements over existing ones. Explain the potential of machine learning on the data to improve early AD/ADRD prediction.\nSample characteristics and representation: Describe how the proposed activities and resulting data would address current biases in research and diagnosis of AD/ADRD in populations disproportionately impacted by AD/ADRD. Include relevant information about the sampling approach, such as whether participants will be compensated for participation, the geographic location of participants, how participants will be contacted and recruited, and any other aspects of the study methodology designed to enhance sample representativeness.\nFeasibility: Describe the major challenges of the proposed plan and strategies to mitigate them. Describe the experiences and abilities of the team to complete the proposed data collection with or without additional support.\nTeam Introduction (maximum 1 page): Describe the submitting team, including members' roles and expertise with respect to the submission. For example, this may include information about a team member\u2019s role on the project, job title, career stage, institutional affiliation of the team members, and relevant education, training, and professional or personal experiences.\nWritten submissions must:\nConsist of a single PDF file with page size set to 8.5\u201d x 11\u201d and at least 1-inch margins.\nUse a font no smaller than 11-point Arial and line spacing no less than 1.0.\nBe written in English.\nNot use the HHS logo or official seal or the logo of NIH or NIA in the entries and must not claim federal government endorsement.\nData collection proposals must be uploaded to the Ideas for data submission page by the Final submission due date, January 31, 2024 at 11:59:59 PM UTC.\nA template for the data collection proposal containing the required sections and descriptions of each section is provided on the Data downloads page.\nEvaluation criteria\nEntries that are responsive and comply with the entry requirements will be scored by technical reviewers in accordance with the criteria outlined below.\nUtility & Rigor - Predictor(s) (20%): What is the potential for the proposed predictor data to provide useful signal for early prediction of AD/ADRD? What are the benefits for using this information beyond what exists today (e.g., wider population, gaps in coverage, earlier identification, lower cost, etc.)?\nUtility & Rigor - Target(s) (20%): How well defined is the proposed AD/ADRD target variable? How well do applicants justify their choice of a target variable? How well do the proposed data include demonstrated links between the predictors and target variable(s)?\nInnovation (20%): To what extent do the proposed data push forward the state of the field and present novel insights and directions for further research?\nDisproportionate Impact (20%): To what extent do the proposed data help address current biases in research and diagnosis of AD/ADRD in populations disproportionately impacted by AD/ADRD?\nFeasibility (20%): How feasible is the data collection proposal? How well positioned is the team to successfully complete the proposed approach with additional support?\nGood luck\nGood luck and have fun engaging with this challenge! If you have any questions, send an email to the challenge organizers at info@drivendata.org or post on the forum!",
        "evaluation_overview": "Ideas for Data Collection\nIn addition to the main challenge, solvers are invited to propose new data collection that will result in an open, shareable dataset that can support novel machine learning approaches for early prediction of AD/ADRD with an emphasis on addressing biases in existing data sources. Solvers who propose ideas for new datasets will be eligible to win prizes from a smaller pool of $25,000.\nUnlike in the main challenge where solvers submit existing data, this alternate track invites solvers to propose an idea for future data collection efforts that the solvers intend to undertake. The proposed data need not be ready by the end of the challenge, and indeed, more ambitious proposals may take much longer to collect. The proposed data must include a target variable that validly indicates AD/ADRD, along with predictor variables that can be used as input features for predicting the target variable. Visit the Problem description for more about the problem framing.\nSubmission format\nProposal Submissions must include a data collection proposal section and a team introduction section (subheadings encouraged but optional):\nData collection proposal (maximum 4 pages, excluding references)\nBackground: Clearly articulate your understanding of the problem and goals of the challenge, and how the proposed dataset will address those problems.\nBasic information: Summarize the proposed activities and describe the resulting dataset, including the data sources and relevant study methodology, and which if any parts of the dataset already exist. The description of the resulting dataset should include its sample size (or target sample size, with justification), definitions of the proposed target variable(s) and proposed predictor variables, the amount of time required to complete activities, and major challenges of the proposed plan.\nUtility & rigor: Provide a definition of the proposed target variable(s), information about its measurement and its distribution in the sample. Present evidence that the target reliably and validly indicates AD/ADRD. Describe the predictor variables in the dataset and explain any theoretical or empirical links to AD/ADRD. Include any potential for validation or generalization to other data.\nInnovation: Describe to what extent the proposed data push forward the state of the field and present novel insights and directions for further research by, for example, enabling higher accuracy, earlier predictions, lower cost, and/or greater accessibility. Describe other similar datasets, if any, and how the proposed dataset provides advancements over existing ones. Explain the potential of machine learning on the data to improve early AD/ADRD prediction.\nSample characteristics and representation: Describe how the proposed activities and resulting data would address current biases in research and diagnosis of AD/ADRD in populations disproportionately impacted by AD/ADRD. Include relevant information about the sampling approach, such as whether participants will be compensated for participation, the geographic location of participants, how participants will be contacted and recruited, and any other aspects of the study methodology designed to enhance sample representativeness.\nFeasibility: Describe the major challenges of the proposed plan and strategies to mitigate them. Describe the experiences and abilities of the team to complete the proposed data collection with or without additional support.\nTeam Introduction (maximum 1 page): Describe the submitting team, including members' roles and expertise with respect to the submission. For example, this may include information about a team member\u2019s role on the project, job title, career stage, institutional affiliation of the team members, and relevant education, training, and professional or personal experiences.\nWritten submissions must:\nConsist of a single PDF file with page size set to 8.5\u201d x 11\u201d and at least 1-inch margins.\nUse a font no smaller than 11-point Arial and line spacing no less than 1.0.\nBe written in English.\nNot use the HHS logo or official seal or the logo of NIH or NIA in the entries and must not claim federal government endorsement.\nData collection proposals must be uploaded to the Ideas for data submission page by the Final submission due date, January 31, 2024 at 11:59:59 PM UTC.\nA template for the data collection proposal containing the required sections and descriptions of each section is provided on the Data downloads page.\nEvaluation criteria\nEntries that are responsive and comply with the entry requirements will be scored by technical reviewers in accordance with the criteria outlined below.\nUtility & Rigor - Predictor(s) (20%): What is the potential for the proposed predictor data to provide useful signal for early prediction of AD/ADRD? What are the benefits for using this information beyond what exists today (e.g., wider population, gaps in coverage, earlier identification, lower cost, etc.)?\nUtility & Rigor - Target(s) (20%): How well defined is the proposed AD/ADRD target variable? How well do applicants justify their choice of a target variable? How well do the proposed data include demonstrated links between the predictors and target variable(s)?\nInnovation (20%): To what extent do the proposed data push forward the state of the field and present novel insights and directions for further research?\nDisproportionate Impact (20%): To what extent do the proposed data help address current biases in research and diagnosis of AD/ADRD in populations disproportionately impacted by AD/ADRD?\nFeasibility (20%): How feasible is the data collection proposal? How well positioned is the team to successfully complete the proposed approach with additional support?\nGood luck\nGood luck and have fun engaging with this challenge! If you have any questions, send an email to the challenge organizers at info@drivendata.org or post on the forum!",
        "url": "https://www.drivendata.org/competitions/253/competition-nih-alzheimers-adrd-1/"
    },
    {
        "competition_overview": "U.K. PETs Prize Challenge: Advancing Privacy-Preserving Federated Learning\nThis is the second in a series of challenge phases with a total prize pool of \u00a3700,000! Each phase in the PETs Prize Challenge invites participants to test and apply their innovations in privacy-preserving federated learning. Phase 2 is for Blue Teams to implement and submit working code for their solutions.\nYou are on the Phase 2 home page for the Financial Crime Data Track, for the U.K. side of the challenge. You can find the U.K. Pandemic Forecasting Track here. The U.S. challenge can be found here.\nBackground\nPrivacy-enhancing technologies (PETs) have the potential to unlock more trustworthy innovation in data analysis and machine learning. Federated learning is one such technology that enables organisations to analyse sensitive data while providing improved privacy protections. These technologies could advance innovation and collaboration in new fields and help harness the power of data to tackle some of our most pressing societal challenges.\nThat\u2019s why the U.S. and U.K. governments are partnering to deliver a set of prize challenges to unleash the potential of these democracy-affirming technologies to make a positive impact. In particular, this challenge will tackle two critical problems via separate data tracks: Data Track A will help with the identification of financial crime, while Data Track B will bolster pandemic responses.\nBy entering the prize challenges, innovators will have the opportunity to compete for cash prizes and engage with regulators and government agencies. Announced at the Summit for Democracy in December 2021, the prize challenges are a product of a collaboration between multiple government departments and agencies on both sides of the Atlantic. Winning solutions will have the opportunity to be profiled at the second Summit for Democracy, to be convened by President Joe Biden, in early 2023.\nObjectives\nThe goal of this prize challenge is to mature federated learning approaches and build trust in their adoption. The challenge organisers hope to accelerate the development of efficient privacy-preserving federated learning solutions that leverage a combination of input and output privacy techniques to:\nDrive innovation in the technological development and application of novel privacy-enhancing technologies\nDeliver strong privacy guarantees against a set of common threats and privacy attacks\nGenerate effective models to accomplish a set of predictive or analytical tasks that support the use cases\nChallenge Structure\nThe Solution Development Phase will have two data use case tracks matching those previously from the White Paper Phase\u2014Track A: Financial Crime Prevention and Track B: Pandemic Response and Forecasting. Teams can elect to participate in either or both tracks, with solutions that apply to one track individually or with a generalised solution. Each solution must correspond to a white paper that met the requirements from the White Paper Phase.\nData Track A \u2013 Financial crime prevention \u2013 The United Nations estimates that up to $2 trillion of cross-border money laundering takes place each year, financing organised crime and undermining economic prosperity. Financial institutions such as banks and credit agencies, along with organisations that process transactions between institutions, such as the SWIFT global financial messaging provider, must protect personal and financial data, while also trying to report and deter illicit financial activities. Using synthetic datasets provided by SWIFT, you will design and later develop innovative privacy-preserving federated learning solutions that facilitate cross-institution and cross-border anomaly detection to combat financial crime. This use case features both vertical and horizontal data partitioning. Check out the Financial Crime overview page for more information.\nData Track B \u2013 Pandemic response and forecasting \u2013 As we continue to deal with COVID-19, it has become apparent that better ways to harness the power of data through analytics are critical for preparing for and responding to public health crises. Federated learning approaches could allow for responsible use of sensitive data to develop cross-organisation and cross-border data analysis that would result in more robust forecasting and pandemic response capabilities. Using synthetic population datasets, you will design and later develop privacy-preserving federated learning solutions that can predict an individual\u2019s risk for infection. This use case features horizontal data partitioning. Check out the Pandemic Forecasting overview page for more information.\nGeneralisable Solutions \u2013 Cross-organisation, cross-border use cases are certainly not limited to the financial or public health domains. Developing out-of-the-box generalised models that can be adapted for use with specific data or problem sets has great potential to advance the adoption and widespread use of privacy-preserving federated learning for public- and private-sector organisations in multiple sectors. Participants may submit generalisable solutions to both tracks. Solutions that are able to perform well on both datasets are likely to score highly in the \u2018Adaptability\u2019 section of the evaluation rubric.\nMore information on how to get started can be found in the Problem Description.\nEligibility\nPhase 2 is open to Blue Teams who submitted white papers to the U.K. side of the challenge during Phase 1. All participants are eligible for prizes at the end of the competition, regardless of whether they received funding after the completion of Phase 1. Individual team members should create accounts on the DrivenData platform. Accounts with email addresses matching those used to register to the Innovate UK competition during Phase 1 will then automatically be registered to this competition. If you have any issues registering, please contact info@drivendata.org.\nPhase 2 Timeline and Prizes\nPhase 2 Key Dates\nLaunch October 25, 2022\nDeadline to Open Pull Requests for Runtime Environment January 11, 2023 at 11:59:59 PM UTC\nSubmissions Due January 26, 2023 at 11:59:59 PM UTC\nAnnouncement of Finalists to be Tested in Phase 3 Red Teaming February 7, 2023\nWinners Announced March 30, 2023\nPhase 2 Prizes\nOpen to Blue Team participants.\nPrizes are awarded agnostic of which use case participants are tackling.\nTop Solutions\nPrize Amount\n1st place \u00a350,000\n2nd place \u00a345,000\n3rd place \u00a335,000\n4th place \u00a320,000\nSpecial Recognition\nPrize Amount\nPool \u00a360,000\nPut your papers into action! Registered teams from Phase 1 will develop working prototypes and submit them to a remote execution environment for federated training and evaluation. These solutions are expected to be functional, i.e., capable of training a model and predicting against the evaluation data set with measurement of relevant performance and accuracy metrics. Solutions will be evaluated by a panel of judges across a set of weighted criteria. The top solutions, ranked by points awarded, will have their final rankings determined by incorporating red team evaluation from the red teams from Phase 3.\nPrizes will be awarded to the four best solutions (this will be agnostic of whether they are tackling the health or financial crime use case). Additional \u201cSpecial Recognition\u201d prizes will be awarded from a pool of \u00a360,000. These awards may include, for example:\nRegulators\u2019 Award: for a team that is able to most effectively demonstrate compatibility with relevant regulatory regimes and principles, such as data protection and finance/healthcare regulation.\nAdvancement in a Specific Technology: for a team that is able to demonstrate significant advancement against the state-of-the-art for a specific privacy technology.\nFull Challenge Timeline and Prizes\nTimeline Overview\nThere are three main phases in the challenge with two types of participants based on a red team/blue team approach. Blue Teams develop privacy-preserving solutions, while Red Teams act as adversaries to test those solutions.\nPhase 1: White Paper Development (Jul\u2013Sept 2022): Blue Teams propose privacy-preserving federated learning solution concepts.\nPhase 2: Solution Development (Oct 2022\u2013Jan 2023): Blue Teams develop working prototypes of their solutions.\nPhase 3: Red Teaming (Nov 2022\u2013Feb 2023): Red Teams prepare and test privacy attacks on top blue team solutions from Phase 2.\nPrize Overview\nTrack Prize Pool\nPhase 1: White Paper \u00a3100,000\nPhase 2: Solution Development Grants \u00a3300,000\nPhase 2: Solution Prizes \u00a3210,000\nPhase 3: Red Teaming \u00a390,000\nTotal \u00a3700,000\nAdditional Phase Details and Prizes\nPlace Prize Amount\nTop 6 Up to \u00a350,000 grant funding\nTop 10 \u00a310,000\nPhase 1: White Paper\nJuly 20\u2013September 21, 2022\nParticipants will produce an abstract and technical white paper laying out their proposed solution. White papers will be evaluated by a panel of judges across a set of weighted criteria. Participants must complete a paper in Phase 1 in order to be eligible to compete in Phase 2.\nOpen to Blue Team participants.\nPrize Amount\n1st \u00a340,000\n2nd \u00a330,000\n3rd \u00a320,000\nPhase 3: Red Teaming\nNovember 10, 2022\u2013February 28, 2023\nPrivacy researchers are invited to form red teams to put the privacy claims of Phase 2's blue team finalists to the test. The red teams will prepare and test privacy attacks on the Phase 2 finalist solutions, which will be incorporated into the final Phase 2 rankings. Top red teams will be evaluated for success and rigor and will be awarded prizes for their performance during this phase.\nOpen to Red Team participants.\n\nHow to compete (Phase 2)\nOnly blue teams who participated in Phase 1 are eligible to participate in Phase 2. Eligible team members should create a DrivenData account, and provide their email addresses to petsprizechallenges@cdei.gov.uk. They will then be automatically registered for this competition on the DrivenData platform.\nGet familiar with the problem through the problem description and code submission format pages.\nDevelop and submit your centralised solution. Check out the centralised code submission format page for more details.\nDevelop and submit your federated solution. Check out the federated code submission format page for more details.\nGood luck!\nThis challenge is sponsored by the Centre for Data Ethics and Innovation (CDEI) and Innovate UK\n\n        \nWith additional collaboration from SWIFT and the University of Virginia's Biocomplexity Institute.\nThis prize challenge and its U.S. counterpart have been developed as part of a joint collaboration between the United Kingdom and the United States.",
        "dataset_overview": "Data Track A: Financial Crime\nTransforming Financial Crime Prevention\nThere are two data use case tracks for the PETs prize challenge. This is the U.K. Phase 2 data overview page for the financial crime prevention track. In this track, innovators will develop end-to-end privacy-preserving federated learning solutions to detect potentially anomalous payments, leveraging a combination of input- and output-privacy techniques.\nBackground on this track is provided below. For a more detailed overview, please refer to the Financial Crime Prevention Technical Brief.\nBackground\nBackground\nData\nOverview\nThreat Profile\nModeling\nAnomaly Detection\nPartitioning Data\nExample Centralised Baseline\nBackground\nThe financial crime track is focused on enhancing cross-organisation, cross-border data access to support efforts to combat fraud, money laundering and other financial crime. You are asked to develop innovative, privacy-preserving solutions to enable detection of potentially anomalous payments, utilising synthetic datasets representing data held by the SWIFT payments network and datasets held by partner banks.\n\u201cAnomalous transactions\u201d covers a range of payments that vary significantly from the norms seen in the dataset, and thus may be indicative of fraud, money laundering, or other financial crime. Examples include a transaction that is of an unexpected amount or currency, uses unusual corridors (senders/receivers), has unusual timestamps, or contains other unusual fields. In the scope of this challenge, the problem is framed as a classification task. The training datasets are labeled with anomalies, and therefore you do not need a detailed understanding of financial crime issues.\nThis is a high-impact and exciting use case for novel privacy-enhancing technologies. There are currently challenging trade-offs between enabling sufficient access to data to build tools to effectively detect illegal financial activity, and limiting the identifiability of innocent individuals or inference of their sensitive information within those data sets. The scale of the problem is vast: the UN estimates that US$800-2000bn is laundered each year, representing 2-5% of global GDP.\nThough novel innovation for this use case alone could achieve significant real-world impact, the challenge is designed to incentivise development of privacy technologies that can be applied to other use cases where data is distributed across multiple organisations or jurisdictions, both in financial services and elsewhere. The best solutions will deliver meaningful innovation towards deployable solutions in this space, with consideration of how to evidence the privacy guarantees offered to data owners and regulators, but also have the potential to generalise to other situations.\nData Overview\nInnovators will use synthetic datasets representing data held by the SWIFT global payments network and by its partner banks. In Phase 1, you were provided with two development datasets:\nDataset 1: A synthetic dataset representing transaction data from the SWIFT global payment network\nDataset 2: Synthetic customer / account metadata, including flags, from SWIFT's partner banks\nThere are approximately 4 million rows across the two development datasets.\nAdditional development data may be released for Phase 2.\nNote: The challenges are based on synthetic data to minimise the security burden placed on competitors during the development phase; of course the intent of the challenge is that privacy solutions are developed that would be appropriate for use on real data sets with demonstrable privacy guarantees. However, competitors must adhere to a data use agreement (see competition rules for more details).\nDataset 1: Transaction data held by SWIFT\nIn Phase 1, you were provided a synthetic dataset derived from data from the SWIFT global payment network. Each row in this dataset is an individual transaction, representing a payment from one sending bank to one receiving bank. The dataset will:\nContain data elements as defined in the ISO20022 pacs.008 / MT103 message format\nComprise transactions between fictitious originators and beneficiaries, sender and receiving banks, payment corridor, amount and timestamps\nExpertise in financial crime or ISO20022 messaging is not an expected prerequisite for entering the challenge, and the assessment process will not focus on detailed understanding of the use case itself. The details in the sections below should be sufficient for understanding the data within the scope of the challenge. However, participants unfamiliar with this space may find it informative to consult a general introduction to ISO20022. You may also find the ISO20022 message definitions informative.\nThe dataset reflects a snapshot of transactions sent by an ordering customer or institution to credit a beneficiary customer or institution. The dataset covers roughly a month\u2019s worth of transactions involving 50 institutions.\nThe synthetic data is not generated based on any real traffic and will not contain any statistical properties of the real SWIFT transaction data (SWIFT has applied normal and uniform distributions).\nDataset 1 details\nDataset 1 contains the following fields:\nMessageId - Globally unique identifier within this dataset for individual transactions\nUETR - The Unique End-to-end Transaction Reference\u2014a 36-character string enabling traceability of all individual transactions associated with a single end-to-end transaction\nTransactionReference - Unique identifier for an individual transaction\nTimestamp - Time at which the individual transaction was initiated\nSender - Institution (bank) initiating/sending the individual transaction\nReceiver - Institution (bank) receiving the individual transaction\nOrderingAccount - Account identifier for the originating ordering entity (individual or organisation) for end-to-end transaction,\nOrderingName - Name for the originating ordering entity\nOrderingStreet - Street address for the originating ordering entity\nOrderingCountryCityZip - Remaining address details for the originating ordering entity\nBeneficiaryAccount - Account identifier for the final beneficiary entity (individual or organisation) for end-to-end transaction\nBeneficiaryName - Name for the final beneficiary entity\nBeneficiaryStreet - Street address for the final beneficiary entity\nBeneficiaryCountryCityZip - Remaining address details for the final beneficiary entity\nSettlementDate - Date the individual transaction was settled\nSettlementCurrency - Currency used for transaction\nSettlementAmount - Value of the transaction net of fees/transfer charges/forex\nInstructedCurrency - Currency of the individual transaction as instructed to be paid by the Sender\nInstructedAmount - Value of the individual transaction as instructed to be paid by the Sender\nLabel - Boolean indicator of whether the transaction is anomalous or not. This is the target variable for the prediction task.\nEnd-to-end transactions\nEach row in this dataset is an individual transaction, representing a payment from a sender bank to a receiver bank. An end-to-end transaction is a transaction from an originating ordering entity (a.k.a. ultimate debtor) to a final beneficiary entity (a.k.a. ultimate creditor) and may involve one or more individual transactions. The end-to-end transaction is one individual transaction in the case where the originating orderer's bank sends payment directly to the final beneficiary's bank. However, it may be the case where the payment is not directly sent, but is instead routed through one or more intermediary banks. In such a case, there are multiple individual transactions belonging to the single end-to-end transaction, with each individual transaction representing a bank-to-bank payment. Each end-to-end transaction is uniquely identified by the UETR field. In the case of a sequence of multiple individual transactions for one end-to-end transaction, all individual transactions share a value for UETR, and the Sender and Receiver banks form a chain from the originating ordering bank through one or more intermediary banks to the final beneficiary bank.\nBecause each end-to-end transaction is defined by one originating orderer and one final beneficiary, this means the Ordering* columns for the orderer and Beneficiary* columns for the beneficiary have been included in this dataset in a denormalised fashion\u2014the values are duplicated across all the individual transactions (rows) belonging to the same end-to-end transaction. Additionally, this means that the OrderingAccount and BeneficiaryAccount in a given row may not necessarily belong to the bank in that row's Sender and the bank in that row's Receiver, respectively. The correct way to associate an OrderingAccount to the correct bank is to identify the Sender bank in the originating (first) individual transaction in that end-to-end transaction, and the correct way to associate a BeneficiaryAccount to the correct bank is to identify the Receiver bank in the final (last) individual transaction in that end-to-end transaction.\nMessageId UETR Sender Receiver OrderingAccount BeneficiaryAccount ...\n... ... ... ... ... ... ...\n10 00012345-... A B 111 222 ...\n11 00012345-... B C 111 222\n12 00012345-... C D 111 222\n... ... ... ... ... ... ...\nIllustrative example showing the how to associate the originating orderer and final beneficiary information with the correct banks for one end-to-end transaction made up of three individual transactions. The orderer and beneficiary account information is duplicated across all rows in this group, and the sender and receiver banks form a chain. The bank and account information of the originating orderer is highlighted in blue, and the bank and account information for the final beneficiary is highlighted in yellow.\nDataset 2: Account data held by banks\nParticipants were provided access to account-related data representative of that held by banks. This dataset contains account-level information, including flags signaling whether the account is valid, suspended, etc.\nData can be linked using the Account field in the bank data and the OrderingAccount or BeneficiaryAccount in the SWIFT transaction data. Please see the previous section on end-to-end transactions for details on how to identify which bank an OrderingAccount or BeneficiaryAccount should be linked to.\nNote that bank nodes will not have access to data on the SWIFT node and vice-versa\u2014a case of vertical data partitioning. It is up to you to determine how to exchange this information in a secure and private way.\nDataset 2 details\nDataset 2 will contain the following fields:\nBank - Identifier for the bank\nAccount - Identifier for the account\nName - Name of the account\nStreet - Street address associated with the account\nCountryCityZip - Remaining address details associated with the account\nFlags - Enumerated data type indicating potential issues or special features that have been associated with an account. Flag definitions are provided below:\n00 - No flags\n01 - Account closed\n03 - Account recently opened\n04 - Name mismatch\n05 - Account under monitoring\n06 - Account suspended\n07 - Account frozen\n08 - Non-transaction amount\n09 - Beneficiary deceased\n10 - Invalid company ID\n11 - Invalid individual ID\nNote that this dataset is provided unpartitioned, with all banks' data in one table.\nNote that the flags may not be representative of real-world practices. For example, in the real world, banks may use different flags and may interpret or weight them differently based on appetite for risk.\nDevelopment and Evaluation Data\nThe datasets being provided are intended for local development use in both Phase 1 and Phase 2. The transaction dataset has been split in time\u2013the bulk of the dataset is the training set, and the final week of the dataset is a test set. The prediction task, as detailed in a later section, is to predict a confidence score for each individual transaction in the test set as to whether it is an anomalous transaction. The ground truth is provided for both the training and set sets in the development dataset.\nIn Phase 2, a separate and held-out dataset will be used for solution evaluation. Some aspects of the Phase 2 evaluation data may be changed that should be learnable by your model. You will submit code for your solution to a code execution environment. The code execution runtime will run cold-start federated training on the new dataset's training split and then run inference to generate predictions for the new dataset's test split. Your solution's performance will be measured by evaluating its predictions against the ground truth for the new dataset's test split.\nNew Development Dataset\nNew development dataset published December 3, 2022.\nA new development dataset has been provided by our data partners at SWIFT and is available on the data download page, indicated by the [NEW] tag. This new development dataset is a synthetic dataset that is largely similar to the previously-provided synthetic development dataset (also still available on the data download page), but has some differences that better represent what is observed in the real world. You should expect that the evaluation dataset (held out and will never be made available) will have similar distributions as this new dataset. Local development and self-reported results in the technical paper should primarily use the new development dataset.\nThreat Profile\nYou will design and develop end-to-end solutions that preserve privacy across a range of possible threats and attack scenarios, through all stages of the machine learning model lifecycle. You should therefore carefully consider the overall privacy of your solution, focusing on the protection of sensitive information held by all parties involved in the federated learning scenario. The solutions you design and develop should include comprehensive measures to address the threat profiles described below. These measures will provide an appropriate degree of resilience to a wide range of potential attacks defined within the threat profile. For more information on threat profiles, please visit the privacy threat profile section of the problem description.\nScope of sensitive data\nYour solution must prevent the unintended disclosure of\na) sensitive information in the SWIFT transaction dataset\nb) sensitive information in the bank dataset, to any other party, including other insider stakeholders (for example, SWIFT and other financial institutions) and outsiders.\nThe sensitive information for the SWIFT dataset is all personally-identifiable information about the originating orderer (a.k.a. ultimate debtor) and final beneficiary (a.k.a. ultimate creditor) parties, including personal details like names and addresses, and group membership information. This includes but is not limited to the raw private data about the orderer/beneficiary stored directly in the account number, name, and address fields, and the transaction identifiers and timestamps.\nThe sensitive information for the bank datasets include all personally identifiable information about parties involved in the transactions, including names and addresses and group membership information. This includes but is not limited to the raw private / business data reflected in account numbers/names/addresses and flags.\nAnomaly detection models\nThe analytical objective is to train a model that enables SWIFT to identify anomalous transactions. In the context of this challenge, this is a classification model to be trained on provided training data with ground truth labels. In real-world deployments, such transactions might be subject to additional verification actions or flagged for further investigation, dependent on context.\nPrediction Target and Evaluation Metric\nThe target variable for the modeling task is a confidence score (between 0.0 and 1.0) for whether each individual transaction is anomalous. As discussed previously, anomalous is not precisely defined and should be learned by your model via supervised learning on provided training data.\nThe evaluation metric will be Area Under the Precision\u2013Recall Curve (AUPRC), also known as average precision (AP), PR-AUC, or AUCPR. This is a commonly used metric for binary classification that summarises model performance across all operating thresholds. This metric rewards models which can consistently assign anomalous transactions with a higher confidence score than negative non-anomalous transactions. AUPRC is computed as follows:\nAUPRC=\n\u2211\nn\n(\nR\nn\n\u2212\nR\nn\u22121\n)\nP\nn\nwhere\nP\nn\nand\nR\nn\nare the precision and recall, respectively, when thresholding at the nth individual transaction sorted in order of increasing recall.\nPartitioned Datasets for Federated Learning\nPartitioning Overview\nA number of banks are working with SWIFT to collaboratively train a model. The parties are working jointly to do this, and can take a common approach to technical design, infrastructure, etc., but are not able to enable access to each other's data. In the real world there are a number of barriers that might prevent this; banks are subject to a variety of privacy, competition and financial industry regulations, may be operating in different jurisdictions, and have legitimate commercial and ethical reasons for not sharing customer data with competitors.\nThe key task of this challenge is to design a privacy solution so that SWIFT can safely train and deploy such a model without compromising the privacy requirements (more details on the requirements, and an associated threat model, are described in the problem description).\nAbove: Diagram comparing the centralised model (MC) with a privacy-preserving federated learning model (MPF).\nPartitioning Details\nThis use case features both vertical and horizontal data partitioning. Data is vertically partitioned between Dataset 1 (SWIFT) and Dataset 2 (partner banks), and it is horizontally partitioned within Dataset 2 (between each partner bank).\nFor local development, you were provided a full, unpartitioned dataset. In Phase 2 evaluation, evaluation will occur with predetermined partitioning along institutional boundaries. The SWIFT data will always belong a single federation unit who represents the SWIFT Data Store and only has access to the SWIFT data. Banks will be split up among federation units such that one bank's account data entirely belongs within one partition. In cases where there are fewer bank partitions than the number of banks, each bank partition may contain data from more than one bank.\nAny partitioning of the data that you might perform in your local development experiments should take this into account. Your solution should be able to handle any number of bank partitions, and in Phase 2, we may evaluate your solution with a number of bank partitions between 1 and 10.\nKey Task\nFor the purposes of the challenge, you should demonstrate your solution by training two models:\nM\nC\n= a centralised model trained on datasets 1 and 2 in a non-privacy preserving way\nM\nPF\n= a privacy-preserving federated model trained using your privacy solution\nExample Centralised Baseline\nIn Phase 1, SWIFT provided sample Python code for training a centralised anomaly detection model (\nM\nC\n) on the ISO20022 training data. This code snippet used the dataset provided as input and trained a simple anomaly detection model using the XGBoost Classifier.\nThe core of the evaluation will be assessing the comparison between a centralised model\nM\nC\n, and an alternative model\nM\nPF\nthat combines a federated learning approach with innovative privacy-preserving techniques.\nIn the real world, SWIFT may wish to train a model collaboratively with a number of banks, in order to increase the volume and variety of data being used to train the model. You should therefore aim to develop scalable solutions that enable additional nodes to be integrated into the federated network whilst incurring an acceptable additional performance overhead.\nThe federated learning scenario thus consists of one node hosting the SWIFT dataset, and N nodes hosting bank data. We may evaluate solutions for values of N between 1 and 10, in order to assess how well solutions scale as more banks are added to the network. During solution development, you may have full autonomy over how you partition the bank dataset in order to understand the scalability of your solution.\nFull details on evaluation criteria can be found in the problem description.\nFor additional reference, here is a technical brief for this use case track provided through the U.K. challenge. This brief has been assembled in collaboration by the U.K. and U.S. challenge organisers. This may help to give a sense for the use case and capabilities expected, though note that details in the brief may not match exactly how the U.S. challenge will operate.\nGood luck\nGood luck and enjoy this problem! For more details on the code submission format, visit the code submission page. If you have any questions, you can always ask the community by visiting the DrivenData user forum or the cross-U.S.\u2013U.K. public Slack channel. You can request access to the Slack channel here. You can also reach out to CDEI at petsprizechallenges@cdei.gov.uk or Innovate UK at support@iuk.ukri.org..",
        "evaluation_overview": "Data Track A: Financial Crime\nTransforming Financial Crime Prevention\nThere are two data use case tracks for the PETs prize challenge. This is the U.K. Phase 2 data overview page for the financial crime prevention track. In this track, innovators will develop end-to-end privacy-preserving federated learning solutions to detect potentially anomalous payments, leveraging a combination of input- and output-privacy techniques.\nBackground on this track is provided below. For a more detailed overview, please refer to the Financial Crime Prevention Technical Brief.\nBackground\nBackground\nData\nOverview\nThreat Profile\nModeling\nAnomaly Detection\nPartitioning Data\nExample Centralised Baseline\nBackground\nThe financial crime track is focused on enhancing cross-organisation, cross-border data access to support efforts to combat fraud, money laundering and other financial crime. You are asked to develop innovative, privacy-preserving solutions to enable detection of potentially anomalous payments, utilising synthetic datasets representing data held by the SWIFT payments network and datasets held by partner banks.\n\u201cAnomalous transactions\u201d covers a range of payments that vary significantly from the norms seen in the dataset, and thus may be indicative of fraud, money laundering, or other financial crime. Examples include a transaction that is of an unexpected amount or currency, uses unusual corridors (senders/receivers), has unusual timestamps, or contains other unusual fields. In the scope of this challenge, the problem is framed as a classification task. The training datasets are labeled with anomalies, and therefore you do not need a detailed understanding of financial crime issues.\nThis is a high-impact and exciting use case for novel privacy-enhancing technologies. There are currently challenging trade-offs between enabling sufficient access to data to build tools to effectively detect illegal financial activity, and limiting the identifiability of innocent individuals or inference of their sensitive information within those data sets. The scale of the problem is vast: the UN estimates that US$800-2000bn is laundered each year, representing 2-5% of global GDP.\nThough novel innovation for this use case alone could achieve significant real-world impact, the challenge is designed to incentivise development of privacy technologies that can be applied to other use cases where data is distributed across multiple organisations or jurisdictions, both in financial services and elsewhere. The best solutions will deliver meaningful innovation towards deployable solutions in this space, with consideration of how to evidence the privacy guarantees offered to data owners and regulators, but also have the potential to generalise to other situations.\nData Overview\nInnovators will use synthetic datasets representing data held by the SWIFT global payments network and by its partner banks. In Phase 1, you were provided with two development datasets:\nDataset 1: A synthetic dataset representing transaction data from the SWIFT global payment network\nDataset 2: Synthetic customer / account metadata, including flags, from SWIFT's partner banks\nThere are approximately 4 million rows across the two development datasets.\nAdditional development data may be released for Phase 2.\nNote: The challenges are based on synthetic data to minimise the security burden placed on competitors during the development phase; of course the intent of the challenge is that privacy solutions are developed that would be appropriate for use on real data sets with demonstrable privacy guarantees. However, competitors must adhere to a data use agreement (see competition rules for more details).\nDataset 1: Transaction data held by SWIFT\nIn Phase 1, you were provided a synthetic dataset derived from data from the SWIFT global payment network. Each row in this dataset is an individual transaction, representing a payment from one sending bank to one receiving bank. The dataset will:\nContain data elements as defined in the ISO20022 pacs.008 / MT103 message format\nComprise transactions between fictitious originators and beneficiaries, sender and receiving banks, payment corridor, amount and timestamps\nExpertise in financial crime or ISO20022 messaging is not an expected prerequisite for entering the challenge, and the assessment process will not focus on detailed understanding of the use case itself. The details in the sections below should be sufficient for understanding the data within the scope of the challenge. However, participants unfamiliar with this space may find it informative to consult a general introduction to ISO20022. You may also find the ISO20022 message definitions informative.\nThe dataset reflects a snapshot of transactions sent by an ordering customer or institution to credit a beneficiary customer or institution. The dataset covers roughly a month\u2019s worth of transactions involving 50 institutions.\nThe synthetic data is not generated based on any real traffic and will not contain any statistical properties of the real SWIFT transaction data (SWIFT has applied normal and uniform distributions).\nDataset 1 details\nDataset 1 contains the following fields:\nMessageId - Globally unique identifier within this dataset for individual transactions\nUETR - The Unique End-to-end Transaction Reference\u2014a 36-character string enabling traceability of all individual transactions associated with a single end-to-end transaction\nTransactionReference - Unique identifier for an individual transaction\nTimestamp - Time at which the individual transaction was initiated\nSender - Institution (bank) initiating/sending the individual transaction\nReceiver - Institution (bank) receiving the individual transaction\nOrderingAccount - Account identifier for the originating ordering entity (individual or organisation) for end-to-end transaction,\nOrderingName - Name for the originating ordering entity\nOrderingStreet - Street address for the originating ordering entity\nOrderingCountryCityZip - Remaining address details for the originating ordering entity\nBeneficiaryAccount - Account identifier for the final beneficiary entity (individual or organisation) for end-to-end transaction\nBeneficiaryName - Name for the final beneficiary entity\nBeneficiaryStreet - Street address for the final beneficiary entity\nBeneficiaryCountryCityZip - Remaining address details for the final beneficiary entity\nSettlementDate - Date the individual transaction was settled\nSettlementCurrency - Currency used for transaction\nSettlementAmount - Value of the transaction net of fees/transfer charges/forex\nInstructedCurrency - Currency of the individual transaction as instructed to be paid by the Sender\nInstructedAmount - Value of the individual transaction as instructed to be paid by the Sender\nLabel - Boolean indicator of whether the transaction is anomalous or not. This is the target variable for the prediction task.\nEnd-to-end transactions\nEach row in this dataset is an individual transaction, representing a payment from a sender bank to a receiver bank. An end-to-end transaction is a transaction from an originating ordering entity (a.k.a. ultimate debtor) to a final beneficiary entity (a.k.a. ultimate creditor) and may involve one or more individual transactions. The end-to-end transaction is one individual transaction in the case where the originating orderer's bank sends payment directly to the final beneficiary's bank. However, it may be the case where the payment is not directly sent, but is instead routed through one or more intermediary banks. In such a case, there are multiple individual transactions belonging to the single end-to-end transaction, with each individual transaction representing a bank-to-bank payment. Each end-to-end transaction is uniquely identified by the UETR field. In the case of a sequence of multiple individual transactions for one end-to-end transaction, all individual transactions share a value for UETR, and the Sender and Receiver banks form a chain from the originating ordering bank through one or more intermediary banks to the final beneficiary bank.\nBecause each end-to-end transaction is defined by one originating orderer and one final beneficiary, this means the Ordering* columns for the orderer and Beneficiary* columns for the beneficiary have been included in this dataset in a denormalised fashion\u2014the values are duplicated across all the individual transactions (rows) belonging to the same end-to-end transaction. Additionally, this means that the OrderingAccount and BeneficiaryAccount in a given row may not necessarily belong to the bank in that row's Sender and the bank in that row's Receiver, respectively. The correct way to associate an OrderingAccount to the correct bank is to identify the Sender bank in the originating (first) individual transaction in that end-to-end transaction, and the correct way to associate a BeneficiaryAccount to the correct bank is to identify the Receiver bank in the final (last) individual transaction in that end-to-end transaction.\nMessageId UETR Sender Receiver OrderingAccount BeneficiaryAccount ...\n... ... ... ... ... ... ...\n10 00012345-... A B 111 222 ...\n11 00012345-... B C 111 222\n12 00012345-... C D 111 222\n... ... ... ... ... ... ...\nIllustrative example showing the how to associate the originating orderer and final beneficiary information with the correct banks for one end-to-end transaction made up of three individual transactions. The orderer and beneficiary account information is duplicated across all rows in this group, and the sender and receiver banks form a chain. The bank and account information of the originating orderer is highlighted in blue, and the bank and account information for the final beneficiary is highlighted in yellow.\nDataset 2: Account data held by banks\nParticipants were provided access to account-related data representative of that held by banks. This dataset contains account-level information, including flags signaling whether the account is valid, suspended, etc.\nData can be linked using the Account field in the bank data and the OrderingAccount or BeneficiaryAccount in the SWIFT transaction data. Please see the previous section on end-to-end transactions for details on how to identify which bank an OrderingAccount or BeneficiaryAccount should be linked to.\nNote that bank nodes will not have access to data on the SWIFT node and vice-versa\u2014a case of vertical data partitioning. It is up to you to determine how to exchange this information in a secure and private way.\nDataset 2 details\nDataset 2 will contain the following fields:\nBank - Identifier for the bank\nAccount - Identifier for the account\nName - Name of the account\nStreet - Street address associated with the account\nCountryCityZip - Remaining address details associated with the account\nFlags - Enumerated data type indicating potential issues or special features that have been associated with an account. Flag definitions are provided below:\n00 - No flags\n01 - Account closed\n03 - Account recently opened\n04 - Name mismatch\n05 - Account under monitoring\n06 - Account suspended\n07 - Account frozen\n08 - Non-transaction amount\n09 - Beneficiary deceased\n10 - Invalid company ID\n11 - Invalid individual ID\nNote that this dataset is provided unpartitioned, with all banks' data in one table.\nNote that the flags may not be representative of real-world practices. For example, in the real world, banks may use different flags and may interpret or weight them differently based on appetite for risk.\nDevelopment and Evaluation Data\nThe datasets being provided are intended for local development use in both Phase 1 and Phase 2. The transaction dataset has been split in time\u2013the bulk of the dataset is the training set, and the final week of the dataset is a test set. The prediction task, as detailed in a later section, is to predict a confidence score for each individual transaction in the test set as to whether it is an anomalous transaction. The ground truth is provided for both the training and set sets in the development dataset.\nIn Phase 2, a separate and held-out dataset will be used for solution evaluation. Some aspects of the Phase 2 evaluation data may be changed that should be learnable by your model. You will submit code for your solution to a code execution environment. The code execution runtime will run cold-start federated training on the new dataset's training split and then run inference to generate predictions for the new dataset's test split. Your solution's performance will be measured by evaluating its predictions against the ground truth for the new dataset's test split.\nNew Development Dataset\nNew development dataset published December 3, 2022.\nA new development dataset has been provided by our data partners at SWIFT and is available on the data download page, indicated by the [NEW] tag. This new development dataset is a synthetic dataset that is largely similar to the previously-provided synthetic development dataset (also still available on the data download page), but has some differences that better represent what is observed in the real world. You should expect that the evaluation dataset (held out and will never be made available) will have similar distributions as this new dataset. Local development and self-reported results in the technical paper should primarily use the new development dataset.\nThreat Profile\nYou will design and develop end-to-end solutions that preserve privacy across a range of possible threats and attack scenarios, through all stages of the machine learning model lifecycle. You should therefore carefully consider the overall privacy of your solution, focusing on the protection of sensitive information held by all parties involved in the federated learning scenario. The solutions you design and develop should include comprehensive measures to address the threat profiles described below. These measures will provide an appropriate degree of resilience to a wide range of potential attacks defined within the threat profile. For more information on threat profiles, please visit the privacy threat profile section of the problem description.\nScope of sensitive data\nYour solution must prevent the unintended disclosure of\na) sensitive information in the SWIFT transaction dataset\nb) sensitive information in the bank dataset, to any other party, including other insider stakeholders (for example, SWIFT and other financial institutions) and outsiders.\nThe sensitive information for the SWIFT dataset is all personally-identifiable information about the originating orderer (a.k.a. ultimate debtor) and final beneficiary (a.k.a. ultimate creditor) parties, including personal details like names and addresses, and group membership information. This includes but is not limited to the raw private data about the orderer/beneficiary stored directly in the account number, name, and address fields, and the transaction identifiers and timestamps.\nThe sensitive information for the bank datasets include all personally identifiable information about parties involved in the transactions, including names and addresses and group membership information. This includes but is not limited to the raw private / business data reflected in account numbers/names/addresses and flags.\nAnomaly detection models\nThe analytical objective is to train a model that enables SWIFT to identify anomalous transactions. In the context of this challenge, this is a classification model to be trained on provided training data with ground truth labels. In real-world deployments, such transactions might be subject to additional verification actions or flagged for further investigation, dependent on context.\nPrediction Target and Evaluation Metric\nThe target variable for the modeling task is a confidence score (between 0.0 and 1.0) for whether each individual transaction is anomalous. As discussed previously, anomalous is not precisely defined and should be learned by your model via supervised learning on provided training data.\nThe evaluation metric will be Area Under the Precision\u2013Recall Curve (AUPRC), also known as average precision (AP), PR-AUC, or AUCPR. This is a commonly used metric for binary classification that summarises model performance across all operating thresholds. This metric rewards models which can consistently assign anomalous transactions with a higher confidence score than negative non-anomalous transactions. AUPRC is computed as follows:\nAUPRC=\n\u2211\nn\n(\nR\nn\n\u2212\nR\nn\u22121\n)\nP\nn\nwhere\nP\nn\nand\nR\nn\nare the precision and recall, respectively, when thresholding at the nth individual transaction sorted in order of increasing recall.\nPartitioned Datasets for Federated Learning\nPartitioning Overview\nA number of banks are working with SWIFT to collaboratively train a model. The parties are working jointly to do this, and can take a common approach to technical design, infrastructure, etc., but are not able to enable access to each other's data. In the real world there are a number of barriers that might prevent this; banks are subject to a variety of privacy, competition and financial industry regulations, may be operating in different jurisdictions, and have legitimate commercial and ethical reasons for not sharing customer data with competitors.\nThe key task of this challenge is to design a privacy solution so that SWIFT can safely train and deploy such a model without compromising the privacy requirements (more details on the requirements, and an associated threat model, are described in the problem description).\nAbove: Diagram comparing the centralised model (MC) with a privacy-preserving federated learning model (MPF).\nPartitioning Details\nThis use case features both vertical and horizontal data partitioning. Data is vertically partitioned between Dataset 1 (SWIFT) and Dataset 2 (partner banks), and it is horizontally partitioned within Dataset 2 (between each partner bank).\nFor local development, you were provided a full, unpartitioned dataset. In Phase 2 evaluation, evaluation will occur with predetermined partitioning along institutional boundaries. The SWIFT data will always belong a single federation unit who represents the SWIFT Data Store and only has access to the SWIFT data. Banks will be split up among federation units such that one bank's account data entirely belongs within one partition. In cases where there are fewer bank partitions than the number of banks, each bank partition may contain data from more than one bank.\nAny partitioning of the data that you might perform in your local development experiments should take this into account. Your solution should be able to handle any number of bank partitions, and in Phase 2, we may evaluate your solution with a number of bank partitions between 1 and 10.\nKey Task\nFor the purposes of the challenge, you should demonstrate your solution by training two models:\nM\nC\n= a centralised model trained on datasets 1 and 2 in a non-privacy preserving way\nM\nPF\n= a privacy-preserving federated model trained using your privacy solution\nExample Centralised Baseline\nIn Phase 1, SWIFT provided sample Python code for training a centralised anomaly detection model (\nM\nC\n) on the ISO20022 training data. This code snippet used the dataset provided as input and trained a simple anomaly detection model using the XGBoost Classifier.\nThe core of the evaluation will be assessing the comparison between a centralised model\nM\nC\n, and an alternative model\nM\nPF\nthat combines a federated learning approach with innovative privacy-preserving techniques.\nIn the real world, SWIFT may wish to train a model collaboratively with a number of banks, in order to increase the volume and variety of data being used to train the model. You should therefore aim to develop scalable solutions that enable additional nodes to be integrated into the federated network whilst incurring an acceptable additional performance overhead.\nThe federated learning scenario thus consists of one node hosting the SWIFT dataset, and N nodes hosting bank data. We may evaluate solutions for values of N between 1 and 10, in order to assess how well solutions scale as more banks are added to the network. During solution development, you may have full autonomy over how you partition the bank dataset in order to understand the scalability of your solution.\nFull details on evaluation criteria can be found in the problem description.\nFor additional reference, here is a technical brief for this use case track provided through the U.K. challenge. This brief has been assembled in collaboration by the U.K. and U.S. challenge organisers. This may help to give a sense for the use case and capabilities expected, though note that details in the brief may not match exactly how the U.S. challenge will operate.\nGood luck\nGood luck and enjoy this problem! For more details on the code submission format, visit the code submission page. If you have any questions, you can always ask the community by visiting the DrivenData user forum or the cross-U.S.\u2013U.K. public Slack channel. You can request access to the Slack channel here. You can also reach out to CDEI at petsprizechallenges@cdei.gov.uk or Innovate UK at support@iuk.ukri.org..",
        "url": "https://www.drivendata.org/competitions/140/uk-federated-learning-2-financial-crime-federated/"
    },
    {
        "competition_overview": "U.K. PETs Prize Challenge: Advancing Privacy-Preserving Federated Learning\nThis is the second in a series of challenge phases with a total prize pool of \u00a3700,000! Each phase in the PETs Prize Challenge invites participants to test and apply their innovations in privacy-preserving federated learning. Phase 2 is for Blue Teams to implement and submit working code for their solutions.\nYou are on the Phase 2 home page for the Financial Crime Data Track, for the U.K. side of the challenge. You can find the U.K. Pandemic Forecasting Track here. The U.S. challenge can be found here.\nBackground\nPrivacy-enhancing technologies (PETs) have the potential to unlock more trustworthy innovation in data analysis and machine learning. Federated learning is one such technology that enables organisations to analyse sensitive data while providing improved privacy protections. These technologies could advance innovation and collaboration in new fields and help harness the power of data to tackle some of our most pressing societal challenges.\nThat\u2019s why the U.S. and U.K. governments are partnering to deliver a set of prize challenges to unleash the potential of these democracy-affirming technologies to make a positive impact. In particular, this challenge will tackle two critical problems via separate data tracks: Data Track A will help with the identification of financial crime, while Data Track B will bolster pandemic responses.\nBy entering the prize challenges, innovators will have the opportunity to compete for cash prizes and engage with regulators and government agencies. Announced at the Summit for Democracy in December 2021, the prize challenges are a product of a collaboration between multiple government departments and agencies on both sides of the Atlantic. Winning solutions will have the opportunity to be profiled at the second Summit for Democracy, to be convened by President Joe Biden, in early 2023.\nObjectives\nThe goal of this prize challenge is to mature federated learning approaches and build trust in their adoption. The challenge organisers hope to accelerate the development of efficient privacy-preserving federated learning solutions that leverage a combination of input and output privacy techniques to:\nDrive innovation in the technological development and application of novel privacy-enhancing technologies\nDeliver strong privacy guarantees against a set of common threats and privacy attacks\nGenerate effective models to accomplish a set of predictive or analytical tasks that support the use cases\nChallenge Structure\nThe Solution Development Phase will have two data use case tracks matching those previously from the White Paper Phase\u2014Track A: Financial Crime Prevention and Track B: Pandemic Response and Forecasting. Teams can elect to participate in either or both tracks, with solutions that apply to one track individually or with a generalised solution. Each solution must correspond to a white paper that met the requirements from the White Paper Phase.\nData Track A \u2013 Financial crime prevention \u2013 The United Nations estimates that up to $2 trillion of cross-border money laundering takes place each year, financing organised crime and undermining economic prosperity. Financial institutions such as banks and credit agencies, along with organisations that process transactions between institutions, such as the SWIFT global financial messaging provider, must protect personal and financial data, while also trying to report and deter illicit financial activities. Using synthetic datasets provided by SWIFT, you will design and later develop innovative privacy-preserving federated learning solutions that facilitate cross-institution and cross-border anomaly detection to combat financial crime. This use case features both vertical and horizontal data partitioning. Check out the Financial Crime overview page for more information.\nData Track B \u2013 Pandemic response and forecasting \u2013 As we continue to deal with COVID-19, it has become apparent that better ways to harness the power of data through analytics are critical for preparing for and responding to public health crises. Federated learning approaches could allow for responsible use of sensitive data to develop cross-organisation and cross-border data analysis that would result in more robust forecasting and pandemic response capabilities. Using synthetic population datasets, you will design and later develop privacy-preserving federated learning solutions that can predict an individual\u2019s risk for infection. This use case features horizontal data partitioning. Check out the Pandemic Forecasting overview page for more information.\nGeneralisable Solutions \u2013 Cross-organisation, cross-border use cases are certainly not limited to the financial or public health domains. Developing out-of-the-box generalised models that can be adapted for use with specific data or problem sets has great potential to advance the adoption and widespread use of privacy-preserving federated learning for public- and private-sector organisations in multiple sectors. Participants may submit generalisable solutions to both tracks. Solutions that are able to perform well on both datasets are likely to score highly in the \u2018Adaptability\u2019 section of the evaluation rubric.\nMore information on how to get started can be found in the Problem Description.\nEligibility\nPhase 2 is open to Blue Teams who submitted white papers to the U.K. side of the challenge during Phase 1. All participants are eligible for prizes at the end of the competition, regardless of whether they received funding after the completion of Phase 1. Individual team members should create accounts on the DrivenData platform. Accounts with email addresses matching those used to register to the Innovate UK competition during Phase 1 will then automatically be registered to this competition. If you have any issues registering, please contact info@drivendata.org.\nPhase 2 Timeline and Prizes\nPhase 2 Key Dates\nLaunch October 25, 2022\nDeadline to Open Pull Requests for Runtime Environment January 11, 2023 at 11:59:59 PM UTC\nSubmissions Due January 26, 2023 at 11:59:59 PM UTC\nAnnouncement of Finalists to be Tested in Phase 3 Red Teaming February 7, 2023\nWinners Announced March 30, 2023\nPhase 2 Prizes\nOpen to Blue Team participants.\nPrizes are awarded agnostic of which use case participants are tackling.\nTop Solutions\nPrize Amount\n1st place \u00a350,000\n2nd place \u00a345,000\n3rd place \u00a335,000\n4th place \u00a320,000\nSpecial Recognition\nPrize Amount\nPool \u00a360,000\nPut your papers into action! Registered teams from Phase 1 will develop working prototypes and submit them to a remote execution environment for federated training and evaluation. These solutions are expected to be functional, i.e., capable of training a model and predicting against the evaluation data set with measurement of relevant performance and accuracy metrics. Solutions will be evaluated by a panel of judges across a set of weighted criteria. The top solutions, ranked by points awarded, will have their final rankings determined by incorporating red team evaluation from the red teams from Phase 3.\nPrizes will be awarded to the four best solutions (this will be agnostic of whether they are tackling the health or financial crime use case). Additional \u201cSpecial Recognition\u201d prizes will be awarded from a pool of \u00a360,000. These awards may include, for example:\nRegulators\u2019 Award: for a team that is able to most effectively demonstrate compatibility with relevant regulatory regimes and principles, such as data protection and finance/healthcare regulation.\nAdvancement in a Specific Technology: for a team that is able to demonstrate significant advancement against the state-of-the-art for a specific privacy technology.\nFull Challenge Timeline and Prizes\nTimeline Overview\nThere are three main phases in the challenge with two types of participants based on a red team/blue team approach. Blue Teams develop privacy-preserving solutions, while Red Teams act as adversaries to test those solutions.\nPhase 1: White Paper Development (Jul\u2013Sept 2022): Blue Teams propose privacy-preserving federated learning solution concepts.\nPhase 2: Solution Development (Oct 2022\u2013Jan 2023): Blue Teams develop working prototypes of their solutions.\nPhase 3: Red Teaming (Nov 2022\u2013Feb 2023): Red Teams prepare and test privacy attacks on top blue team solutions from Phase 2.\nPrize Overview\nTrack Prize Pool\nPhase 1: White Paper \u00a3100,000\nPhase 2: Solution Development Grants \u00a3300,000\nPhase 2: Solution Prizes \u00a3210,000\nPhase 3: Red Teaming \u00a390,000\nTotal \u00a3700,000\nAdditional Phase Details and Prizes\nPlace Prize Amount\nTop 6 Up to \u00a350,000 grant funding\nTop 10 \u00a310,000\nPhase 1: White Paper\nJuly 20\u2013September 21, 2022\nParticipants will produce an abstract and technical white paper laying out their proposed solution. White papers will be evaluated by a panel of judges across a set of weighted criteria. Participants must complete a paper in Phase 1 in order to be eligible to compete in Phase 2.\nOpen to Blue Team participants.\nPrize Amount\n1st \u00a340,000\n2nd \u00a330,000\n3rd \u00a320,000\nPhase 3: Red Teaming\nNovember 10, 2022\u2013February 28, 2023\nPrivacy researchers are invited to form red teams to put the privacy claims of Phase 2's blue team finalists to the test. The red teams will prepare and test privacy attacks on the Phase 2 finalist solutions, which will be incorporated into the final Phase 2 rankings. Top red teams will be evaluated for success and rigor and will be awarded prizes for their performance during this phase.\nOpen to Red Team participants.\n\nHow to compete (Phase 2)\nOnly blue teams who participated in Phase 1 are eligible to participate in Phase 2. Eligible team members should create a DrivenData account, and provide their email addresses to petsprizechallenges@cdei.gov.uk. They will then be automatically registered for this competition on the DrivenData platform.\nGet familiar with the problem through the problem description and code submission format pages.\nDevelop and submit your centralised solution. Check out the centralised code submission format page for more details.\nDevelop and submit your federated solution. Check out the federated code submission format page for more details.\nGood luck!\nThis challenge is sponsored by the Centre for Data Ethics and Innovation (CDEI) and Innovate UK\n\n        \nWith additional collaboration from SWIFT and the University of Virginia's Biocomplexity Institute.\nThis prize challenge and its U.S. counterpart have been developed as part of a joint collaboration between the United Kingdom and the United States.",
        "dataset_overview": "Data Track A: Financial Crime\nTransforming Financial Crime Prevention\nThere are two data use case tracks for the PETs prize challenge. This is the U.K. Phase 2 data overview page for the financial crime prevention track. In this track, innovators will develop end-to-end privacy-preserving federated learning solutions to detect potentially anomalous payments, leveraging a combination of input- and output-privacy techniques.\nBackground on this track is provided below. For a more detailed overview, please refer to the Financial Crime Prevention Technical Brief.\nBackground\nBackground\nData\nOverview\nThreat Profile\nModeling\nAnomaly Detection\nPartitioning Data\nExample Centralised Baseline\nBackground\nThe financial crime track is focused on enhancing cross-organisation, cross-border data access to support efforts to combat fraud, money laundering and other financial crime. You are asked to develop innovative, privacy-preserving solutions to enable detection of potentially anomalous payments, utilising synthetic datasets representing data held by the SWIFT payments network and datasets held by partner banks.\n\u201cAnomalous transactions\u201d covers a range of payments that vary significantly from the norms seen in the dataset, and thus may be indicative of fraud, money laundering, or other financial crime. Examples include a transaction that is of an unexpected amount or currency, uses unusual corridors (senders/receivers), has unusual timestamps, or contains other unusual fields. In the scope of this challenge, the problem is framed as a classification task. The training datasets are labeled with anomalies, and therefore you do not need a detailed understanding of financial crime issues.\nThis is a high-impact and exciting use case for novel privacy-enhancing technologies. There are currently challenging trade-offs between enabling sufficient access to data to build tools to effectively detect illegal financial activity, and limiting the identifiability of innocent individuals or inference of their sensitive information within those data sets. The scale of the problem is vast: the UN estimates that US$800-2000bn is laundered each year, representing 2-5% of global GDP.\nThough novel innovation for this use case alone could achieve significant real-world impact, the challenge is designed to incentivise development of privacy technologies that can be applied to other use cases where data is distributed across multiple organisations or jurisdictions, both in financial services and elsewhere. The best solutions will deliver meaningful innovation towards deployable solutions in this space, with consideration of how to evidence the privacy guarantees offered to data owners and regulators, but also have the potential to generalise to other situations.\nData Overview\nInnovators will use synthetic datasets representing data held by the SWIFT global payments network and by its partner banks. In Phase 1, you were provided with two development datasets:\nDataset 1: A synthetic dataset representing transaction data from the SWIFT global payment network\nDataset 2: Synthetic customer / account metadata, including flags, from SWIFT's partner banks\nThere are approximately 4 million rows across the two development datasets.\nAdditional development data may be released for Phase 2.\nNote: The challenges are based on synthetic data to minimise the security burden placed on competitors during the development phase; of course the intent of the challenge is that privacy solutions are developed that would be appropriate for use on real data sets with demonstrable privacy guarantees. However, competitors must adhere to a data use agreement (see competition rules for more details).\nDataset 1: Transaction data held by SWIFT\nIn Phase 1, you were provided a synthetic dataset derived from data from the SWIFT global payment network. Each row in this dataset is an individual transaction, representing a payment from one sending bank to one receiving bank. The dataset will:\nContain data elements as defined in the ISO20022 pacs.008 / MT103 message format\nComprise transactions between fictitious originators and beneficiaries, sender and receiving banks, payment corridor, amount and timestamps\nExpertise in financial crime or ISO20022 messaging is not an expected prerequisite for entering the challenge, and the assessment process will not focus on detailed understanding of the use case itself. The details in the sections below should be sufficient for understanding the data within the scope of the challenge. However, participants unfamiliar with this space may find it informative to consult a general introduction to ISO20022. You may also find the ISO20022 message definitions informative.\nThe dataset reflects a snapshot of transactions sent by an ordering customer or institution to credit a beneficiary customer or institution. The dataset covers roughly a month\u2019s worth of transactions involving 50 institutions.\nThe synthetic data is not generated based on any real traffic and will not contain any statistical properties of the real SWIFT transaction data (SWIFT has applied normal and uniform distributions).\nDataset 1 details\nDataset 1 contains the following fields:\nMessageId - Globally unique identifier within this dataset for individual transactions\nUETR - The Unique End-to-end Transaction Reference\u2014a 36-character string enabling traceability of all individual transactions associated with a single end-to-end transaction\nTransactionReference - Unique identifier for an individual transaction\nTimestamp - Time at which the individual transaction was initiated\nSender - Institution (bank) initiating/sending the individual transaction\nReceiver - Institution (bank) receiving the individual transaction\nOrderingAccount - Account identifier for the originating ordering entity (individual or organisation) for end-to-end transaction,\nOrderingName - Name for the originating ordering entity\nOrderingStreet - Street address for the originating ordering entity\nOrderingCountryCityZip - Remaining address details for the originating ordering entity\nBeneficiaryAccount - Account identifier for the final beneficiary entity (individual or organisation) for end-to-end transaction\nBeneficiaryName - Name for the final beneficiary entity\nBeneficiaryStreet - Street address for the final beneficiary entity\nBeneficiaryCountryCityZip - Remaining address details for the final beneficiary entity\nSettlementDate - Date the individual transaction was settled\nSettlementCurrency - Currency used for transaction\nSettlementAmount - Value of the transaction net of fees/transfer charges/forex\nInstructedCurrency - Currency of the individual transaction as instructed to be paid by the Sender\nInstructedAmount - Value of the individual transaction as instructed to be paid by the Sender\nLabel - Boolean indicator of whether the transaction is anomalous or not. This is the target variable for the prediction task.\nEnd-to-end transactions\nEach row in this dataset is an individual transaction, representing a payment from a sender bank to a receiver bank. An end-to-end transaction is a transaction from an originating ordering entity (a.k.a. ultimate debtor) to a final beneficiary entity (a.k.a. ultimate creditor) and may involve one or more individual transactions. The end-to-end transaction is one individual transaction in the case where the originating orderer's bank sends payment directly to the final beneficiary's bank. However, it may be the case where the payment is not directly sent, but is instead routed through one or more intermediary banks. In such a case, there are multiple individual transactions belonging to the single end-to-end transaction, with each individual transaction representing a bank-to-bank payment. Each end-to-end transaction is uniquely identified by the UETR field. In the case of a sequence of multiple individual transactions for one end-to-end transaction, all individual transactions share a value for UETR, and the Sender and Receiver banks form a chain from the originating ordering bank through one or more intermediary banks to the final beneficiary bank.\nBecause each end-to-end transaction is defined by one originating orderer and one final beneficiary, this means the Ordering* columns for the orderer and Beneficiary* columns for the beneficiary have been included in this dataset in a denormalised fashion\u2014the values are duplicated across all the individual transactions (rows) belonging to the same end-to-end transaction. Additionally, this means that the OrderingAccount and BeneficiaryAccount in a given row may not necessarily belong to the bank in that row's Sender and the bank in that row's Receiver, respectively. The correct way to associate an OrderingAccount to the correct bank is to identify the Sender bank in the originating (first) individual transaction in that end-to-end transaction, and the correct way to associate a BeneficiaryAccount to the correct bank is to identify the Receiver bank in the final (last) individual transaction in that end-to-end transaction.\nMessageId UETR Sender Receiver OrderingAccount BeneficiaryAccount ...\n... ... ... ... ... ... ...\n10 00012345-... A B 111 222 ...\n11 00012345-... B C 111 222\n12 00012345-... C D 111 222\n... ... ... ... ... ... ...\nIllustrative example showing the how to associate the originating orderer and final beneficiary information with the correct banks for one end-to-end transaction made up of three individual transactions. The orderer and beneficiary account information is duplicated across all rows in this group, and the sender and receiver banks form a chain. The bank and account information of the originating orderer is highlighted in blue, and the bank and account information for the final beneficiary is highlighted in yellow.\nDataset 2: Account data held by banks\nParticipants were provided access to account-related data representative of that held by banks. This dataset contains account-level information, including flags signaling whether the account is valid, suspended, etc.\nData can be linked using the Account field in the bank data and the OrderingAccount or BeneficiaryAccount in the SWIFT transaction data. Please see the previous section on end-to-end transactions for details on how to identify which bank an OrderingAccount or BeneficiaryAccount should be linked to.\nNote that bank nodes will not have access to data on the SWIFT node and vice-versa\u2014a case of vertical data partitioning. It is up to you to determine how to exchange this information in a secure and private way.\nDataset 2 details\nDataset 2 will contain the following fields:\nBank - Identifier for the bank\nAccount - Identifier for the account\nName - Name of the account\nStreet - Street address associated with the account\nCountryCityZip - Remaining address details associated with the account\nFlags - Enumerated data type indicating potential issues or special features that have been associated with an account. Flag definitions are provided below:\n00 - No flags\n01 - Account closed\n03 - Account recently opened\n04 - Name mismatch\n05 - Account under monitoring\n06 - Account suspended\n07 - Account frozen\n08 - Non-transaction amount\n09 - Beneficiary deceased\n10 - Invalid company ID\n11 - Invalid individual ID\nNote that this dataset is provided unpartitioned, with all banks' data in one table.\nNote that the flags may not be representative of real-world practices. For example, in the real world, banks may use different flags and may interpret or weight them differently based on appetite for risk.\nDevelopment and Evaluation Data\nThe datasets being provided are intended for local development use in both Phase 1 and Phase 2. The transaction dataset has been split in time\u2013the bulk of the dataset is the training set, and the final week of the dataset is a test set. The prediction task, as detailed in a later section, is to predict a confidence score for each individual transaction in the test set as to whether it is an anomalous transaction. The ground truth is provided for both the training and set sets in the development dataset.\nIn Phase 2, a separate and held-out dataset will be used for solution evaluation. Some aspects of the Phase 2 evaluation data may be changed that should be learnable by your model. You will submit code for your solution to a code execution environment. The code execution runtime will run cold-start federated training on the new dataset's training split and then run inference to generate predictions for the new dataset's test split. Your solution's performance will be measured by evaluating its predictions against the ground truth for the new dataset's test split.\nNew Development Dataset\nNew development dataset published December 3, 2022.\nA new development dataset has been provided by our data partners at SWIFT and is available on the data download page, indicated by the [NEW] tag. This new development dataset is a synthetic dataset that is largely similar to the previously-provided synthetic development dataset (also still available on the data download page), but has some differences that better represent what is observed in the real world. You should expect that the evaluation dataset (held out and will never be made available) will have similar distributions as this new dataset. Local development and self-reported results in the technical paper should primarily use the new development dataset.\nThreat Profile\nYou will design and develop end-to-end solutions that preserve privacy across a range of possible threats and attack scenarios, through all stages of the machine learning model lifecycle. You should therefore carefully consider the overall privacy of your solution, focusing on the protection of sensitive information held by all parties involved in the federated learning scenario. The solutions you design and develop should include comprehensive measures to address the threat profiles described below. These measures will provide an appropriate degree of resilience to a wide range of potential attacks defined within the threat profile. For more information on threat profiles, please visit the privacy threat profile section of the problem description.\nScope of sensitive data\nYour solution must prevent the unintended disclosure of\na) sensitive information in the SWIFT transaction dataset\nb) sensitive information in the bank dataset, to any other party, including other insider stakeholders (for example, SWIFT and other financial institutions) and outsiders.\nThe sensitive information for the SWIFT dataset is all personally-identifiable information about the originating orderer (a.k.a. ultimate debtor) and final beneficiary (a.k.a. ultimate creditor) parties, including personal details like names and addresses, and group membership information. This includes but is not limited to the raw private data about the orderer/beneficiary stored directly in the account number, name, and address fields, and the transaction identifiers and timestamps.\nThe sensitive information for the bank datasets include all personally identifiable information about parties involved in the transactions, including names and addresses and group membership information. This includes but is not limited to the raw private / business data reflected in account numbers/names/addresses and flags.\nAnomaly detection models\nThe analytical objective is to train a model that enables SWIFT to identify anomalous transactions. In the context of this challenge, this is a classification model to be trained on provided training data with ground truth labels. In real-world deployments, such transactions might be subject to additional verification actions or flagged for further investigation, dependent on context.\nPrediction Target and Evaluation Metric\nThe target variable for the modeling task is a confidence score (between 0.0 and 1.0) for whether each individual transaction is anomalous. As discussed previously, anomalous is not precisely defined and should be learned by your model via supervised learning on provided training data.\nThe evaluation metric will be Area Under the Precision\u2013Recall Curve (AUPRC), also known as average precision (AP), PR-AUC, or AUCPR. This is a commonly used metric for binary classification that summarises model performance across all operating thresholds. This metric rewards models which can consistently assign anomalous transactions with a higher confidence score than negative non-anomalous transactions. AUPRC is computed as follows:\nAUPRC=\n\u2211\nn\n(\nR\nn\n\u2212\nR\nn\u22121\n)\nP\nn\nwhere\nP\nn\nand\nR\nn\nare the precision and recall, respectively, when thresholding at the nth individual transaction sorted in order of increasing recall.\nPartitioned Datasets for Federated Learning\nPartitioning Overview\nA number of banks are working with SWIFT to collaboratively train a model. The parties are working jointly to do this, and can take a common approach to technical design, infrastructure, etc., but are not able to enable access to each other's data. In the real world there are a number of barriers that might prevent this; banks are subject to a variety of privacy, competition and financial industry regulations, may be operating in different jurisdictions, and have legitimate commercial and ethical reasons for not sharing customer data with competitors.\nThe key task of this challenge is to design a privacy solution so that SWIFT can safely train and deploy such a model without compromising the privacy requirements (more details on the requirements, and an associated threat model, are described in the problem description).\nAbove: Diagram comparing the centralised model (MC) with a privacy-preserving federated learning model (MPF).\nPartitioning Details\nThis use case features both vertical and horizontal data partitioning. Data is vertically partitioned between Dataset 1 (SWIFT) and Dataset 2 (partner banks), and it is horizontally partitioned within Dataset 2 (between each partner bank).\nFor local development, you were provided a full, unpartitioned dataset. In Phase 2 evaluation, evaluation will occur with predetermined partitioning along institutional boundaries. The SWIFT data will always belong a single federation unit who represents the SWIFT Data Store and only has access to the SWIFT data. Banks will be split up among federation units such that one bank's account data entirely belongs within one partition. In cases where there are fewer bank partitions than the number of banks, each bank partition may contain data from more than one bank.\nAny partitioning of the data that you might perform in your local development experiments should take this into account. Your solution should be able to handle any number of bank partitions, and in Phase 2, we may evaluate your solution with a number of bank partitions between 1 and 10.\nKey Task\nFor the purposes of the challenge, you should demonstrate your solution by training two models:\nM\nC\n= a centralised model trained on datasets 1 and 2 in a non-privacy preserving way\nM\nPF\n= a privacy-preserving federated model trained using your privacy solution\nExample Centralised Baseline\nIn Phase 1, SWIFT provided sample Python code for training a centralised anomaly detection model (\nM\nC\n) on the ISO20022 training data. This code snippet used the dataset provided as input and trained a simple anomaly detection model using the XGBoost Classifier.\nThe core of the evaluation will be assessing the comparison between a centralised model\nM\nC\n, and an alternative model\nM\nPF\nthat combines a federated learning approach with innovative privacy-preserving techniques.\nIn the real world, SWIFT may wish to train a model collaboratively with a number of banks, in order to increase the volume and variety of data being used to train the model. You should therefore aim to develop scalable solutions that enable additional nodes to be integrated into the federated network whilst incurring an acceptable additional performance overhead.\nThe federated learning scenario thus consists of one node hosting the SWIFT dataset, and N nodes hosting bank data. We may evaluate solutions for values of N between 1 and 10, in order to assess how well solutions scale as more banks are added to the network. During solution development, you may have full autonomy over how you partition the bank dataset in order to understand the scalability of your solution.\nFull details on evaluation criteria can be found in the problem description.\nFor additional reference, here is a technical brief for this use case track provided through the U.K. challenge. This brief has been assembled in collaboration by the U.K. and U.S. challenge organisers. This may help to give a sense for the use case and capabilities expected, though note that details in the brief may not match exactly how the U.S. challenge will operate.\nGood luck\nGood luck and enjoy this problem! For more details on the code submission format, visit the code submission page. If you have any questions, you can always ask the community by visiting the DrivenData user forum or the cross-U.S.\u2013U.K. public Slack channel. You can request access to the Slack channel here. You can also reach out to CDEI at petsprizechallenges@cdei.gov.uk or Innovate UK at support@iuk.ukri.org..",
        "evaluation_overview": "Data Track A: Financial Crime\nTransforming Financial Crime Prevention\nThere are two data use case tracks for the PETs prize challenge. This is the U.K. Phase 2 data overview page for the financial crime prevention track. In this track, innovators will develop end-to-end privacy-preserving federated learning solutions to detect potentially anomalous payments, leveraging a combination of input- and output-privacy techniques.\nBackground on this track is provided below. For a more detailed overview, please refer to the Financial Crime Prevention Technical Brief.\nBackground\nBackground\nData\nOverview\nThreat Profile\nModeling\nAnomaly Detection\nPartitioning Data\nExample Centralised Baseline\nBackground\nThe financial crime track is focused on enhancing cross-organisation, cross-border data access to support efforts to combat fraud, money laundering and other financial crime. You are asked to develop innovative, privacy-preserving solutions to enable detection of potentially anomalous payments, utilising synthetic datasets representing data held by the SWIFT payments network and datasets held by partner banks.\n\u201cAnomalous transactions\u201d covers a range of payments that vary significantly from the norms seen in the dataset, and thus may be indicative of fraud, money laundering, or other financial crime. Examples include a transaction that is of an unexpected amount or currency, uses unusual corridors (senders/receivers), has unusual timestamps, or contains other unusual fields. In the scope of this challenge, the problem is framed as a classification task. The training datasets are labeled with anomalies, and therefore you do not need a detailed understanding of financial crime issues.\nThis is a high-impact and exciting use case for novel privacy-enhancing technologies. There are currently challenging trade-offs between enabling sufficient access to data to build tools to effectively detect illegal financial activity, and limiting the identifiability of innocent individuals or inference of their sensitive information within those data sets. The scale of the problem is vast: the UN estimates that US$800-2000bn is laundered each year, representing 2-5% of global GDP.\nThough novel innovation for this use case alone could achieve significant real-world impact, the challenge is designed to incentivise development of privacy technologies that can be applied to other use cases where data is distributed across multiple organisations or jurisdictions, both in financial services and elsewhere. The best solutions will deliver meaningful innovation towards deployable solutions in this space, with consideration of how to evidence the privacy guarantees offered to data owners and regulators, but also have the potential to generalise to other situations.\nData Overview\nInnovators will use synthetic datasets representing data held by the SWIFT global payments network and by its partner banks. In Phase 1, you were provided with two development datasets:\nDataset 1: A synthetic dataset representing transaction data from the SWIFT global payment network\nDataset 2: Synthetic customer / account metadata, including flags, from SWIFT's partner banks\nThere are approximately 4 million rows across the two development datasets.\nAdditional development data may be released for Phase 2.\nNote: The challenges are based on synthetic data to minimise the security burden placed on competitors during the development phase; of course the intent of the challenge is that privacy solutions are developed that would be appropriate for use on real data sets with demonstrable privacy guarantees. However, competitors must adhere to a data use agreement (see competition rules for more details).\nDataset 1: Transaction data held by SWIFT\nIn Phase 1, you were provided a synthetic dataset derived from data from the SWIFT global payment network. Each row in this dataset is an individual transaction, representing a payment from one sending bank to one receiving bank. The dataset will:\nContain data elements as defined in the ISO20022 pacs.008 / MT103 message format\nComprise transactions between fictitious originators and beneficiaries, sender and receiving banks, payment corridor, amount and timestamps\nExpertise in financial crime or ISO20022 messaging is not an expected prerequisite for entering the challenge, and the assessment process will not focus on detailed understanding of the use case itself. The details in the sections below should be sufficient for understanding the data within the scope of the challenge. However, participants unfamiliar with this space may find it informative to consult a general introduction to ISO20022. You may also find the ISO20022 message definitions informative.\nThe dataset reflects a snapshot of transactions sent by an ordering customer or institution to credit a beneficiary customer or institution. The dataset covers roughly a month\u2019s worth of transactions involving 50 institutions.\nThe synthetic data is not generated based on any real traffic and will not contain any statistical properties of the real SWIFT transaction data (SWIFT has applied normal and uniform distributions).\nDataset 1 details\nDataset 1 contains the following fields:\nMessageId - Globally unique identifier within this dataset for individual transactions\nUETR - The Unique End-to-end Transaction Reference\u2014a 36-character string enabling traceability of all individual transactions associated with a single end-to-end transaction\nTransactionReference - Unique identifier for an individual transaction\nTimestamp - Time at which the individual transaction was initiated\nSender - Institution (bank) initiating/sending the individual transaction\nReceiver - Institution (bank) receiving the individual transaction\nOrderingAccount - Account identifier for the originating ordering entity (individual or organisation) for end-to-end transaction,\nOrderingName - Name for the originating ordering entity\nOrderingStreet - Street address for the originating ordering entity\nOrderingCountryCityZip - Remaining address details for the originating ordering entity\nBeneficiaryAccount - Account identifier for the final beneficiary entity (individual or organisation) for end-to-end transaction\nBeneficiaryName - Name for the final beneficiary entity\nBeneficiaryStreet - Street address for the final beneficiary entity\nBeneficiaryCountryCityZip - Remaining address details for the final beneficiary entity\nSettlementDate - Date the individual transaction was settled\nSettlementCurrency - Currency used for transaction\nSettlementAmount - Value of the transaction net of fees/transfer charges/forex\nInstructedCurrency - Currency of the individual transaction as instructed to be paid by the Sender\nInstructedAmount - Value of the individual transaction as instructed to be paid by the Sender\nLabel - Boolean indicator of whether the transaction is anomalous or not. This is the target variable for the prediction task.\nEnd-to-end transactions\nEach row in this dataset is an individual transaction, representing a payment from a sender bank to a receiver bank. An end-to-end transaction is a transaction from an originating ordering entity (a.k.a. ultimate debtor) to a final beneficiary entity (a.k.a. ultimate creditor) and may involve one or more individual transactions. The end-to-end transaction is one individual transaction in the case where the originating orderer's bank sends payment directly to the final beneficiary's bank. However, it may be the case where the payment is not directly sent, but is instead routed through one or more intermediary banks. In such a case, there are multiple individual transactions belonging to the single end-to-end transaction, with each individual transaction representing a bank-to-bank payment. Each end-to-end transaction is uniquely identified by the UETR field. In the case of a sequence of multiple individual transactions for one end-to-end transaction, all individual transactions share a value for UETR, and the Sender and Receiver banks form a chain from the originating ordering bank through one or more intermediary banks to the final beneficiary bank.\nBecause each end-to-end transaction is defined by one originating orderer and one final beneficiary, this means the Ordering* columns for the orderer and Beneficiary* columns for the beneficiary have been included in this dataset in a denormalised fashion\u2014the values are duplicated across all the individual transactions (rows) belonging to the same end-to-end transaction. Additionally, this means that the OrderingAccount and BeneficiaryAccount in a given row may not necessarily belong to the bank in that row's Sender and the bank in that row's Receiver, respectively. The correct way to associate an OrderingAccount to the correct bank is to identify the Sender bank in the originating (first) individual transaction in that end-to-end transaction, and the correct way to associate a BeneficiaryAccount to the correct bank is to identify the Receiver bank in the final (last) individual transaction in that end-to-end transaction.\nMessageId UETR Sender Receiver OrderingAccount BeneficiaryAccount ...\n... ... ... ... ... ... ...\n10 00012345-... A B 111 222 ...\n11 00012345-... B C 111 222\n12 00012345-... C D 111 222\n... ... ... ... ... ... ...\nIllustrative example showing the how to associate the originating orderer and final beneficiary information with the correct banks for one end-to-end transaction made up of three individual transactions. The orderer and beneficiary account information is duplicated across all rows in this group, and the sender and receiver banks form a chain. The bank and account information of the originating orderer is highlighted in blue, and the bank and account information for the final beneficiary is highlighted in yellow.\nDataset 2: Account data held by banks\nParticipants were provided access to account-related data representative of that held by banks. This dataset contains account-level information, including flags signaling whether the account is valid, suspended, etc.\nData can be linked using the Account field in the bank data and the OrderingAccount or BeneficiaryAccount in the SWIFT transaction data. Please see the previous section on end-to-end transactions for details on how to identify which bank an OrderingAccount or BeneficiaryAccount should be linked to.\nNote that bank nodes will not have access to data on the SWIFT node and vice-versa\u2014a case of vertical data partitioning. It is up to you to determine how to exchange this information in a secure and private way.\nDataset 2 details\nDataset 2 will contain the following fields:\nBank - Identifier for the bank\nAccount - Identifier for the account\nName - Name of the account\nStreet - Street address associated with the account\nCountryCityZip - Remaining address details associated with the account\nFlags - Enumerated data type indicating potential issues or special features that have been associated with an account. Flag definitions are provided below:\n00 - No flags\n01 - Account closed\n03 - Account recently opened\n04 - Name mismatch\n05 - Account under monitoring\n06 - Account suspended\n07 - Account frozen\n08 - Non-transaction amount\n09 - Beneficiary deceased\n10 - Invalid company ID\n11 - Invalid individual ID\nNote that this dataset is provided unpartitioned, with all banks' data in one table.\nNote that the flags may not be representative of real-world practices. For example, in the real world, banks may use different flags and may interpret or weight them differently based on appetite for risk.\nDevelopment and Evaluation Data\nThe datasets being provided are intended for local development use in both Phase 1 and Phase 2. The transaction dataset has been split in time\u2013the bulk of the dataset is the training set, and the final week of the dataset is a test set. The prediction task, as detailed in a later section, is to predict a confidence score for each individual transaction in the test set as to whether it is an anomalous transaction. The ground truth is provided for both the training and set sets in the development dataset.\nIn Phase 2, a separate and held-out dataset will be used for solution evaluation. Some aspects of the Phase 2 evaluation data may be changed that should be learnable by your model. You will submit code for your solution to a code execution environment. The code execution runtime will run cold-start federated training on the new dataset's training split and then run inference to generate predictions for the new dataset's test split. Your solution's performance will be measured by evaluating its predictions against the ground truth for the new dataset's test split.\nNew Development Dataset\nNew development dataset published December 3, 2022.\nA new development dataset has been provided by our data partners at SWIFT and is available on the data download page, indicated by the [NEW] tag. This new development dataset is a synthetic dataset that is largely similar to the previously-provided synthetic development dataset (also still available on the data download page), but has some differences that better represent what is observed in the real world. You should expect that the evaluation dataset (held out and will never be made available) will have similar distributions as this new dataset. Local development and self-reported results in the technical paper should primarily use the new development dataset.\nThreat Profile\nYou will design and develop end-to-end solutions that preserve privacy across a range of possible threats and attack scenarios, through all stages of the machine learning model lifecycle. You should therefore carefully consider the overall privacy of your solution, focusing on the protection of sensitive information held by all parties involved in the federated learning scenario. The solutions you design and develop should include comprehensive measures to address the threat profiles described below. These measures will provide an appropriate degree of resilience to a wide range of potential attacks defined within the threat profile. For more information on threat profiles, please visit the privacy threat profile section of the problem description.\nScope of sensitive data\nYour solution must prevent the unintended disclosure of\na) sensitive information in the SWIFT transaction dataset\nb) sensitive information in the bank dataset, to any other party, including other insider stakeholders (for example, SWIFT and other financial institutions) and outsiders.\nThe sensitive information for the SWIFT dataset is all personally-identifiable information about the originating orderer (a.k.a. ultimate debtor) and final beneficiary (a.k.a. ultimate creditor) parties, including personal details like names and addresses, and group membership information. This includes but is not limited to the raw private data about the orderer/beneficiary stored directly in the account number, name, and address fields, and the transaction identifiers and timestamps.\nThe sensitive information for the bank datasets include all personally identifiable information about parties involved in the transactions, including names and addresses and group membership information. This includes but is not limited to the raw private / business data reflected in account numbers/names/addresses and flags.\nAnomaly detection models\nThe analytical objective is to train a model that enables SWIFT to identify anomalous transactions. In the context of this challenge, this is a classification model to be trained on provided training data with ground truth labels. In real-world deployments, such transactions might be subject to additional verification actions or flagged for further investigation, dependent on context.\nPrediction Target and Evaluation Metric\nThe target variable for the modeling task is a confidence score (between 0.0 and 1.0) for whether each individual transaction is anomalous. As discussed previously, anomalous is not precisely defined and should be learned by your model via supervised learning on provided training data.\nThe evaluation metric will be Area Under the Precision\u2013Recall Curve (AUPRC), also known as average precision (AP), PR-AUC, or AUCPR. This is a commonly used metric for binary classification that summarises model performance across all operating thresholds. This metric rewards models which can consistently assign anomalous transactions with a higher confidence score than negative non-anomalous transactions. AUPRC is computed as follows:\nAUPRC=\n\u2211\nn\n(\nR\nn\n\u2212\nR\nn\u22121\n)\nP\nn\nwhere\nP\nn\nand\nR\nn\nare the precision and recall, respectively, when thresholding at the nth individual transaction sorted in order of increasing recall.\nPartitioned Datasets for Federated Learning\nPartitioning Overview\nA number of banks are working with SWIFT to collaboratively train a model. The parties are working jointly to do this, and can take a common approach to technical design, infrastructure, etc., but are not able to enable access to each other's data. In the real world there are a number of barriers that might prevent this; banks are subject to a variety of privacy, competition and financial industry regulations, may be operating in different jurisdictions, and have legitimate commercial and ethical reasons for not sharing customer data with competitors.\nThe key task of this challenge is to design a privacy solution so that SWIFT can safely train and deploy such a model without compromising the privacy requirements (more details on the requirements, and an associated threat model, are described in the problem description).\nAbove: Diagram comparing the centralised model (MC) with a privacy-preserving federated learning model (MPF).\nPartitioning Details\nThis use case features both vertical and horizontal data partitioning. Data is vertically partitioned between Dataset 1 (SWIFT) and Dataset 2 (partner banks), and it is horizontally partitioned within Dataset 2 (between each partner bank).\nFor local development, you were provided a full, unpartitioned dataset. In Phase 2 evaluation, evaluation will occur with predetermined partitioning along institutional boundaries. The SWIFT data will always belong a single federation unit who represents the SWIFT Data Store and only has access to the SWIFT data. Banks will be split up among federation units such that one bank's account data entirely belongs within one partition. In cases where there are fewer bank partitions than the number of banks, each bank partition may contain data from more than one bank.\nAny partitioning of the data that you might perform in your local development experiments should take this into account. Your solution should be able to handle any number of bank partitions, and in Phase 2, we may evaluate your solution with a number of bank partitions between 1 and 10.\nKey Task\nFor the purposes of the challenge, you should demonstrate your solution by training two models:\nM\nC\n= a centralised model trained on datasets 1 and 2 in a non-privacy preserving way\nM\nPF\n= a privacy-preserving federated model trained using your privacy solution\nExample Centralised Baseline\nIn Phase 1, SWIFT provided sample Python code for training a centralised anomaly detection model (\nM\nC\n) on the ISO20022 training data. This code snippet used the dataset provided as input and trained a simple anomaly detection model using the XGBoost Classifier.\nThe core of the evaluation will be assessing the comparison between a centralised model\nM\nC\n, and an alternative model\nM\nPF\nthat combines a federated learning approach with innovative privacy-preserving techniques.\nIn the real world, SWIFT may wish to train a model collaboratively with a number of banks, in order to increase the volume and variety of data being used to train the model. You should therefore aim to develop scalable solutions that enable additional nodes to be integrated into the federated network whilst incurring an acceptable additional performance overhead.\nThe federated learning scenario thus consists of one node hosting the SWIFT dataset, and N nodes hosting bank data. We may evaluate solutions for values of N between 1 and 10, in order to assess how well solutions scale as more banks are added to the network. During solution development, you may have full autonomy over how you partition the bank dataset in order to understand the scalability of your solution.\nFull details on evaluation criteria can be found in the problem description.\nFor additional reference, here is a technical brief for this use case track provided through the U.K. challenge. This brief has been assembled in collaboration by the U.K. and U.S. challenge organisers. This may help to give a sense for the use case and capabilities expected, though note that details in the brief may not match exactly how the U.S. challenge will operate.\nGood luck\nGood luck and enjoy this problem! For more details on the code submission format, visit the code submission page. If you have any questions, you can always ask the community by visiting the DrivenData user forum or the cross-U.S.\u2013U.K. public Slack channel. You can request access to the Slack channel here. You can also reach out to CDEI at petsprizechallenges@cdei.gov.uk or Innovate UK at support@iuk.ukri.org..",
        "url": "https://www.drivendata.org/competitions/146/uk-federated-learning-2-financial-crime-centralised/"
    },
    {
        "competition_overview": "U.K. PETs Prize Challenge: Advancing Privacy-Preserving Federated Learning\nThis is the second in a series of challenge phases with a total prize pool of \u00a3700,000! Each phase in the PETs Prize Challenge invites participants to test and apply their innovations in privacy-preserving federated learning. Phase 2 is for Blue Teams to implement and submit working code for their solutions.\nYou are on the Phase 2 home page for the Pandemic Forecasting Data Track, for the U.K. side of the challenge. You can find the U.K. Financial Crime Track here. The U.S. challenge can be found here.\nBackground\nPrivacy-enhancing technologies (PETs) have the potential to unlock more trustworthy innovation in data analysis and machine learning. Federated learning is one such technology that enables organisations to analyse sensitive data while providing improved privacy protections. These technologies could advance innovation and collaboration in new fields and help harness the power of data to tackle some of our most pressing societal challenges.\nThat\u2019s why the U.S. and U.K. governments are partnering to deliver a set of prize challenges to unleash the potential of these democracy-affirming technologies to make a positive impact. In particular, this challenge will tackle two critical problems via separate data tracks: Data Track A will help with the identification of financial crime, while Data Track B will bolster pandemic responses.\nBy entering the prize challenges, innovators will have the opportunity to compete for cash prizes and engage with regulators and government agencies. Announced at the Summit for Democracy in December 2021, the prize challenges are a product of a collaboration between multiple government departments and agencies on both sides of the Atlantic. Winning solutions will have the opportunity to be profiled at the second Summit for Democracy, to be convened by President Joe Biden, in early 2023.\nObjectives\nThe goal of this prize challenge is to mature federated learning approaches and build trust in their adoption. The challenge organisers hope to accelerate the development of efficient privacy-preserving federated learning solutions that leverage a combination of input and output privacy techniques to:\nDrive innovation in the technological development and application of novel privacy-enhancing technologies\nDeliver strong privacy guarantees against a set of common threats and privacy attacks\nGenerate effective models to accomplish a set of predictive or analytical tasks that support the use cases\nChallenge Structure\nThe Solution Development Phase will have two data use case tracks matching those previously from the White Paper Phase\u2014Track A: Financial Crime Prevention and Track B: Pandemic Response and Forecasting. Teams can elect to participate in either or both tracks, with solutions that apply to one track individually or with a generalised solution. Each solution must correspond to a white paper that met the requirements from the White Paper Phase.\nData Track A \u2013 Financial crime prevention \u2013 The United Nations estimates that up to $2 trillion of cross-border money laundering takes place each year, financing organised crime and undermining economic prosperity. Financial institutions such as banks and credit agencies, along with organisations that process transactions between institutions, such as the SWIFT global financial messaging provider, must protect personal and financial data, while also trying to report and deter illicit financial activities. Using synthetic datasets provided by SWIFT, you will design and later develop innovative privacy-preserving federated learning solutions that facilitate cross-institution and cross-border anomaly detection to combat financial crime. This use case features both vertical and horizontal data partitioning. Check out the Financial Crime overview page for more information.\nData Track B \u2013 Pandemic response and forecasting \u2013 As we continue to deal with COVID-19, it has become apparent that better ways to harness the power of data through analytics are critical for preparing for and responding to public health crises. Federated learning approaches could allow for responsible use of sensitive data to develop cross-organisation and cross-border data analysis that would result in more robust forecasting and pandemic response capabilities. Using synthetic population datasets, you will design and later develop privacy-preserving federated learning solutions that can predict an individual\u2019s risk for infection. This use case features horizontal data partitioning. Check out the Pandemic Forecasting overview page for more information.\nGeneralisable Solutions \u2013 Cross-organisation, cross-border use cases are certainly not limited to the financial or public health domains. Developing out-of-the-box generalised models that can be adapted for use with specific data or problem sets has great potential to advance the adoption and widespread use of privacy-preserving federated learning for public- and private-sector organisations in multiple sectors. Participants may submit generalizable solutions to both tracks. Solutions that are able to perform well on both datasets are likely to score highly in the \u2018Adaptability\u2019 section of the evaluation rubric.\nMore information on how to get started can be found in the Problem Description.\nEligibility\nPhase 2 is open to Blue Teams who submitted white papers to the U.K. side of the challenge during Phase 1. All participants are eligible for prizes at the end of the competition, regardless of whether they received funding after the completion of Phase 1. Individual team members should create accounts on the DrivenData platform. Accounts with email addresses matching those used to register to the Innovate UK competition during Phase 1 will then automatically be registered to this competition. If you have any issues registering, please contact info@drivendata.org.\nPhase 2 Timeline and Prizes\nPhase 2 Key Dates\nLaunch October 25, 2022\nDeadline to Open Pull Requests for Runtime Environment January 11, 2023 at 11:59:59 PM UTC\nSubmissions Due January 26, 2023 at 11:59:59 PM UTC\nAnnouncement of Finalists to be Tested in Phase 3 Red Teaming February 7, 2023\nWinners Announced March 30, 2023\nPhase 2 Prizes\nOpen to Blue Team participants.\nPrizes are awarded agnostic of which use case participants are tackling.\nTop Solutions\nPrize Amount\n1st place \u00a350,000\n2nd place \u00a345,000\n3rd place \u00a335,000\n4th place \u00a320,000\nSpecial Recognition\nPrize Amount\nPool \u00a360,000\nPut your papers into action! Registered teams from Phase 1 will develop working prototypes and submit them to a remote execution environment for federated training and evaluation. These solutions are expected to be functional, i.e., capable of training a model and predicting against the evaluation data set with measurement of relevant performance and accuracy metrics. Solutions will be evaluated by a panel of judges across a set of weighted criteria. The top solutions, ranked by points awarded, will have their final rankings determined by incorporating red team evaluation from the red teams from Phase 3.\nPrizes will be awarded to the four best solutions (this will be agnostic of whether they are tackling the health or financial crime use case). Additional \u201cSpecial Recognition\u201d prizes will be awarded from a pool of \u00a360,000. These awards may include, for example:\nRegulators\u2019 Award: for a team that is able to most effectively demonstrate compatibility with relevant regulatory regimes and principles, such as data protection and finance/healthcare regulation.\nAdvancement in a Specific Technology: for a team that is able to demonstrate significant advancement against the state-of-the-art for a specific privacy technology.\nFull Challenge Timeline and Prizes\nTimeline Overview\nThere are three main phases in the challenge with two types of participants based on a red team/blue team approach. Blue Teams develop privacy-preserving solutions, while Red Teams act as adversaries to test those solutions.\nPhase 1: White Paper Development (Jul\u2013Sept 2022): Blue Teams propose privacy-preserving federated learning solution concepts.\nPhase 2: Solution Development (Oct 2022\u2013Jan 2023): Blue Teams develop working prototypes of their solutions.\nPhase 3: Red Teaming (Nov 2022\u2013Feb 2023): Red Teams prepare and test privacy attacks on top blue team solutions from Phase 2.\nPrize Overview\nTrack Prize Pool\nPhase 1: White Paper \u00a3100,000\nPhase 2: Solution Development Grants \u00a3300,000\nPhase 2: Solution Prizes \u00a3210,000\nPhase 3: Red Teaming \u00a390,000\nTotal \u00a3700,000\nAdditional Phase Details and Prizes\nPlace Prize Amount\nTop 6 Up to \u00a350,000 grant funding\nTop 10 \u00a310,000\nPhase 1: White Paper\nJuly 20\u2013September 21, 2022\nParticipants will produce an abstract and technical white paper laying out their proposed solution. White papers will be evaluated by a panel of judges across a set of weighted criteria. Participants must complete a paper in Phase 1 in order to be eligible to compete in Phase 2.\nOpen to Blue Team participants.\nPrize Amount\n1st \u00a340,000\n2nd \u00a330,000\n3rd \u00a320,000\nPhase 3: Red Teaming\nNovember 10, 2022\u2013February 28, 2023\nPrivacy researchers are invited to form red teams to put the privacy claims of Phase 2's blue team finalists to the test. The red teams will prepare and test privacy attacks on the Phase 2 finalist solutions, which will be incorporated into the final Phase 2 rankings. Top red teams will be evaluated for success and rigor and will be awarded prizes for their performance during this phase.\nOpen to Red Team participants.\n\nHow to compete (Phase 2)\nOnly blue teams who participated in Phase 1 are eligible to participate in Phase 2. Eligible team members should create a DrivenData account, and provide their email addresses to petsprizechallenges@cdei.gov.uk. They will then be automatically registered for this competition on the DrivenData platform.\nGet familiar with the problem through the problem description and code submission format pages.\nDevelop and submit your centralised solution. Check out the centralised code submission format page for more details.\nDevelop and submit your federated solution. Check out the federated code submission format page for more details.\nGood luck!\nThis challenge is sponsored by the Centre for Data Ethics and Innovation (CDEI) and Innovate UK\n\n        \nWith additional collaboration from SWIFT and the University of Virginia's Biocomplexity Institute.\nThis prize challenge and its U.S. counterpart have been developed as part of a joint collaboration between the United Kingdom and the United States.",
        "dataset_overview": "Data Track B: Pandemic Forecasting\nTransforming Pandemic Response and Forecasting\nThere are two data use case tracks for the PETs prize challenge. This is the U.K. Phase 2 data overview page for the pandemic forecasting track. In this track, innovators will develop end-to-end privacy-preserving federated learning solutions to accurately forecast individual risk of infection.\nBackground on this track is provided below. For a more detailed overview, please refer to the Pandemic Response and Forecasting Technical Brief.\nBackground\nBackground\nData\nOverview\nThreat Profile\nModeling\nForecasting Risk\nPartitioning Data\nBackground\nFederated learning (FL), or more generally collaborative learning, shows huge promise for machine learning applications derived from sensitive data by enabling training on distributed data sets without sharing the data among the participating parties.\nThe PETs Prize Challenge is focused on enhancing cross-organisation, cross-border data sharing to support efforts to enable better public health related forecasting to bolster pandemic response capabilities. The COVID-19 pandemic has taken an immense toll on human lives and had unprecedented level of socio-economic impact on individuals and societies around the globe. As we continue to deal with COVID-19, it has become apparent that better ways to harness the power of data by employing privacy-preserving data sharing and analytics are critical to preparing for such pandemics or public health crises.\nIn this Data Track, you are asked to develop innovative, privacy-preserving FL solutions utilising dataset of a synthetic population (a.k.a. digital twin) that simulates a population with statistical and dynamical properties similar to a real population. You will design and later develop privacy-preserving federated learning solutions that can predict an individual\u2019s risk for infection. If an individual knows that they are likely to be infected in the next week, they can take more prophylactic measures (masking, changing plans, etc.) than they usually might. Furthermore, this knowledge could help public health officials better predict interventions and staffing needs for specific regions.\nBy focusing on this public health use case, the Challenge aims to drive innovation in privacy-enhancing technologies by exploring their role in a set of high-impact use cases where there are currently challenging trade-offs between enabling sufficient access to data to and limiting the identifiability of individuals or inference of their privacy sensitive information within those data sets. Though novel innovation for this use case alone could achieve significant real world impact, the challenge is designed to incentivise development of privacy technologies that can be applied to other use cases where data is distributed across multiple organisations or jurisdictions, both in public health and elsewhere. The best solutions will deliver meaningful innovation towards deployable solutions in this space, with consideration of how to evidence the privacy guarantees offered to data owners and regulators, but also have the potential to generalise to other situations.\nThe Data\nIn Phase 1, you were provided with datasets representing synthetic population data that includes:\na social contact network, capturing when and where any two people come into contact, and the duration of the contact\ndemographic attributes of individuals\nobservations of individuals\u2019 health state (i.e., whether they are infected or not).\nThese datasets have been generated by the University of Virginia\u2019s Biocomplexity Institute (UVA-BII). Two synthetic population datasets are provided: the first simulates the population of the state of Virginia, USA, and the second the population of the U.K. The two datasets (Virginia and U.K.) have identical structures and schemas (detailed below), but differ in size and scale. The code execution evaluation primarily focuses on the Virginia population. Teams are welcome to use the U.K. dataset for development and local experimentation and to report any such results in their technical paper.\nUsing these datasets and an agent-based outbreak simulation, the UVA-BII team created 63 days of simulated disease outbreak data, subsequently split into 56 days of training data and 7 days of target data. You will be provided with the synthetic populations and the first 56 days of the simulated outbreak datasets for model training. Your task is to predict a risk score for the binary disease state (infected or not infected) of each individual in the final week of the simulation. That is, you should predict the risk of whether each person in the population will be in an infected state on any day in the last 7 days.\nDetails about the size of the synthetic datasets:\nVirginia dataset U.K. dataset\nPopulation size 7.7 million 62 million\nNumber of social contacts 181 million 722 million\nNumber of disease state records (upper bound based on 1 reading per individual per day) 430 million 3.5 billion\nNote: The challenges are based on synthetic data to minimise the security burden placed on participants during the development phase; of course the intent of the challenge is to develop privacy solutions appropriate for use on real datasets with demonstrable privacy guarantees. However, participants must adhere to a data use agreement (see competition rules for more details).\nDisease States and Asymptomatic Infection\nEach individual in the population will have a disease state for each day of the dataset. The disease state acts like a finite-state machine with the following states:\nS: indicates \"susceptible\". This individual is susceptible to infection.\nI: indicates \"infected\". This individual has been infected and is infectious. They can infect other individuals (who are susceptible) through contact.\nR: indicates \"recovered\". This individual has recovered and is no longer infectious or infectable.\n\nState transition diagram for disease state in challenge dataset.\nAn individual disease state can only progress from S to I to R. Once an individual is in the recovered R state, they will not be able to become infected again within this simulation.\nThis dataset also features an additional layer of complexity around asymptomatic infection. Some individuals in the simulation will be asymptomatically infected and will transition to I states, but the I state is hidden from the data. Such an individual is still infectious while in the I state but will appear in the data as being in the S state before transitiong to the R state. This is in constrast to individuals with the normal symptomatic infection whose disease state is transparently visible in the data. Individuals in both normal symptomatic infection and asymptomatic infection states will transition to a visible R state upon recovery.\nState transition diagram for disease state in challenge dataset showing both symptomatic and asymptomatic infections. Asymptomatically infected individuals will have a hidden infected state in the simulation, but appear as susceptible in the dataset. For this reason, asymptomatic infections may appear to transition directly from susceptible to recovered.\nDevelopment and Evaluation Data\nThe datasets provided in Phase 1 are intended for local development use in both Phase 1 and Phase 2. Each dataset has been split in time\u2014the first 56 days of the dataset is the training set, and the final 7 days of the dataset is the forecast target data. The prediction task, as detailed in a later section, is to make predictions for the final 7 days of the dataset. The ground truth is provided for the forecast target period for the development datasets.\nIn Phase 2, a separate and held-out dataset will be used for solution evaluation. You can expect the Phase 2 evaluation dataset be close in size and statistical distributions to the development Virginia dataset, but have a different contact network and an independent disease outbreak simulation. In Phase 2, you will submit code for your solution to a code execution environment. The code execution runtime will run cold-start federated training on the new dataset's training split and then run inference to generate predictions for the forecast target period. Your solution's performance will be measured by evaluating its predictions against the ground truth for the new dataset's target period.\nData Details\nEach synthetic population dataset can be considered as a relational database consisting of eight tables. Each of the eight tables will be provided as a separate CSV file. Each table has data for a particular data schema. The fields and their description for each table are shown below. The eight tables are:\nPerson data - Information about individuals in the dataset\nHousehold data - Information about individual households\nResidence location data - Location information about household residences\nActivity location data - Information about locations where non-home activities take place\nActivity location assignment data - Information about which activities each person was involved in, where it took place, and timing information. Activities are repeated every day, as if it were like the film Groundhog Day\nPopulation contact network data - Information about the contact network between people, including location, time and duration. This table is generated from the Activity location assignment data table, and also repeats every day\nDisease outcome training data - Information about each individual's health status on a particular day\nDisease outcome target data - Binary ground truth infected label for each individual for the forecast period\nTable 1: Person data has the following fields:\nHousehold ID (hid) - An integer identifying a household\nPerson ID (pid) - A unique integer identifying a person\nPerson number (person_number) - Sequence ID of a person within the household. A household of size 3 will have people with person_numbers 1, 2, and 3.\nAge - Age of person\nSex - Indicating the gender of person\nTable 2: Household data has the following fields:\nHousehold ID (hid) - A unique integer identifying the household\nResidence ID (rlid) - An integer identifying the residence\nAdmin 1 - ADCW ID for the admin1 region for U.K. or state FIPS code for Virginia\nAdmin 2 - ADCW ID for the admin1 region for U.K.; Equals to admin1 if for U.K. (ADCW does not assign admin2 region ID for U.K.); 3-digit county FIPS code for Virginia\nHousehold Size (hh_size) - Number of persons in the household\nTable 3: Residence location data has the following fields:\nResidence ID (rlid) - A unique integer identifying the residence\nLongitude - the longitude of the location\nLatitude - the latitude of the location\nAdmin 1 - See household file description\nAdmin 2 - See household file description\nTable 4: Activity location data has the following fields:\nActivity location ID (alid) - Unique integer identifying the location where activity took place\nLongitude - Longitude of the location\nLatitude - Latitude of the location\nAdmin 1 - See household file description\nAdmin 2 - See household file description\nWork - Does location support work activity? (Value is 0 or 1)\nShopping - Does location support shopping activity? (Value is 0 or 1)\nSchool - Does location support school activity? (Value is 0 or 1)\nOther - Does location support other activity? (Value is 0 or 1)\nCollege - Does location support college activity? (Value is 0 or 1)\nReligion - Does location support religion activity? (Value is 0 or 1)\nTable 5: Activity location assignment data has the following fields:\nHousehold ID - Household ID of the person\nPerson ID - Person ID of the person\nActivity number - Number of the activity in the activity sequence to which it belongs\nActivity type - Activity type (1: Home, 2: Work, 3: Shopping, 4: Other, 5: School, 6: College)\nStart Time - Start time of activity in seconds since midnight\nDuration - Duration of the activity in seconds\nLocation ID - Location ID of the location where the activity takes place (rlid or alid)\nTable 6: Population contact network data has the following fields:\nPerson ID 1 (pid1) - Person ID number 1 of this edge\nPerson ID 2 (pid2) - Person ID number 2 of this edge\nLocation ID (lid) - Location ID where contact (edge) arises\nStart time - Start time of the contact between Person ID 1 and Person ID 2 measured in seconds since midnight\nDuration - Duration of the contact measured in seconds\nActivity ID 1 (activity1) - Activity type of Person ID 1 at time of contact (see above)\nActivity ID 2 (activity2) - Activity type of Person ID 2 at time of contact (see above)\nTable 7: Disease outcome training data has the following fields:\nDay - Simulation day\nPerson ID - ID of the person in the Person data or the Contact Network data\nDisease state - Disease-related state of the person on that day with possible values S for \"susceptible\", I for \"infected\", and R for \"recovered\". See previous section for details.\nTable 8: Disease outcome target data has the following fields:\nPerson ID - ID of the person in the Person data or the Contact Network data\nInfected - Binary variable with 1 if the person was infected in the final week of the simulation and 0 if otherwise\nAbove: Overall data model of the synthetic population data. The arrows indicate dependencies between files; for example, the hid element in the person file should be constrained to the values available in the household file.\nThreat Profile\nYour end-to-end solution should preserve privacy across a range of possible threats and attack scenarios, through all stages of the machine learning model lifecycle. You should therefore carefully consider the overall privacy of your solution, focusing on the protection of sensitive information held by all parties involved in the federated learning scenario. The solutions you design and develop should include comprehensive measures to address the threat profiles described below. These measures will provide an appropriate degree of resilience to a wide range of potential attacks defined within the threat profile.\nScope of sensitive data\nYour solution must prevent the unintended disclosure of sensitive information in the population dataset to any other party, including other insider stakeholders (i.e., the other federation units) and outsiders.\nThe following information in the dataset should be treated as sensitive:\nAll personally identifiable information, including the identity of a person, their housing information, their demographics such as age, etc.\nLocation activities of an individual\nThe health state of an individual\nSocial contact information, including the location of any social contact, and the identities of who was involved in the social contact.\nForecasting Individual Risk of Infection\nThe key analytical objective of the PPFL task is to effectively train a model that can predict the risk of infection for individuals in a population over a period of time in a privacy-preserving manner. Such a predictive capability is important from the perspective of an individual; if they know they are likely to be infected in the next week, they may choose to take additional preventative measures (such as wearing a face covering, reducing social contacts, or taking antivirals prophylactically). Additionally, this capability can help inform the measures public health authorities implement to respond to such outbreaks.\nPrediction Target and Evaluation Metric\nThe target variable for the modeling task is a risk score (between 0.0 and 1.0) for each individual in the population. That risk score corresponds to a confidence that that individual enters into an symptomatic infected I disease state at any time during the final week of the simulation (between days 56 and 63).\nNote that, as discussed in the previous section, some individuals become infected asymptomatically. Those individuals count as negative cases (not infected) for the purposes of the ground truth.\nThe evaluation metric will be Area Under the Precision\u2013Recall Curve (AUPRC), also known as average precision (AP), PR-AUC, or AUCPR. This is a commonly used metric for binary classification that summarises model performance across all operating thresholds. This metric rewards models which can consistently assign positive cases with a higher risk score than negative cases. AUPRC is computed as follows:\nAUPRC=\n\u2211\nn\n(\nR\nn\n\u2212\nR\nn\u22121\n)\nP\nn\nwhere\nP\nn\nand\nR\nn\nare the precision and recall, respectively, when thresholding at the nth individual sorted in order of increasing recall.\nPartitioned Datasets for Federated learning\nOverview\nThe dataset will be horizontally partitioned into local datasets belonging to different federation units. This is intended to mirror how data is distributed across different health districts, hospitals, etc. These federation units have access to the full data tables described above. The dataset assumes that people are in precisely one federation unit, so a health district can see everywhere that one of its individuals goes, but does not have any access to information about other people who reside outside of the health district. As a result, the contacts between individuals from two different federation units are not represented in the population contact network in either of the local datasets.\nThe federated units work jointly to train a global FL model, and can take a common approach to technical design, infrastructure, etc., but are not able to access each other\u2019s raw data. In the real world there are a number of barriers that might prevent this; the parties and the datasets they contribute for PPFL may be subject to a variety of privacy, competition and industry specific regulations (e.g., HIPPA), may be operating in different jurisdictions, and have legitimate commercial and ethical reasons for not sharing customer data with other parties.\nAbove: Diagram comparing the centralised model (MC) with a privacy-preserving federated learning model (MPF).\nPartitioning Details\nFor local development, you were provided a full, unpartitioned dataset in Phase 1. In Phase 2, the evaluation data will be partitioned along administrative boundaries, e.g., by grouped FIPS codes/counties. Any cross-partition edges will be dropped, where an edge is created between two people (nodes) that come into contact as specified by the population contact network table. That is, each partition will only have visibility into edges between people that reside within the same partition.\nMoreover, a partition will have access to all information for all residents within the partition's counties, including residences, persons, and activity assignments. This is true even if an activity occurs in a county outside of that partition. Said another way, a partition knows about all locations within its counties, and all locations outside of its counties where an activity or contact occured.\nCross-partition edges will continue to affect the spread of infection, but will not be visible to any of the partitions. Any partitioning of the data that you might perform in your local development experiments should take this into account. Your solution should be able to handle any number of partitions, and in Phase 2, we may evaluate your solution with a number of partitions between 1 and 10.\nKey Task\nThe key task of this challenge is to design a privacy solution so that the collaborating parties can jointly train and deploy such a model without compromising the privacy requirements (more details on the requirements, and an associated threat model, are described in the problem description).\nFor the purposes of the challenge, you should demonstrate your solution by training two models:\nM\nC\n= a centralised model trained on the datasets in a non-privacy preserving way\nM\nPF\n= a privacy-preserving federated model trained using your privacy solution\nGood luck\nGood luck and enjoy this problem! For more details on the code submission format, visit the code submission page. If you have any questions, you can always ask the community by visiting the DrivenData user forum or the cross-U.S.\u2013U.K. public Slack channel. You can request access to the Slack channel here. You can also reach out to CDEI at petsprizechallenges@cdei.gov.uk or Innovate UK at support@iuk.ukri.org.",
        "evaluation_overview": "Data Track B: Pandemic Forecasting\nTransforming Pandemic Response and Forecasting\nThere are two data use case tracks for the PETs prize challenge. This is the U.K. Phase 2 data overview page for the pandemic forecasting track. In this track, innovators will develop end-to-end privacy-preserving federated learning solutions to accurately forecast individual risk of infection.\nBackground on this track is provided below. For a more detailed overview, please refer to the Pandemic Response and Forecasting Technical Brief.\nBackground\nBackground\nData\nOverview\nThreat Profile\nModeling\nForecasting Risk\nPartitioning Data\nBackground\nFederated learning (FL), or more generally collaborative learning, shows huge promise for machine learning applications derived from sensitive data by enabling training on distributed data sets without sharing the data among the participating parties.\nThe PETs Prize Challenge is focused on enhancing cross-organisation, cross-border data sharing to support efforts to enable better public health related forecasting to bolster pandemic response capabilities. The COVID-19 pandemic has taken an immense toll on human lives and had unprecedented level of socio-economic impact on individuals and societies around the globe. As we continue to deal with COVID-19, it has become apparent that better ways to harness the power of data by employing privacy-preserving data sharing and analytics are critical to preparing for such pandemics or public health crises.\nIn this Data Track, you are asked to develop innovative, privacy-preserving FL solutions utilising dataset of a synthetic population (a.k.a. digital twin) that simulates a population with statistical and dynamical properties similar to a real population. You will design and later develop privacy-preserving federated learning solutions that can predict an individual\u2019s risk for infection. If an individual knows that they are likely to be infected in the next week, they can take more prophylactic measures (masking, changing plans, etc.) than they usually might. Furthermore, this knowledge could help public health officials better predict interventions and staffing needs for specific regions.\nBy focusing on this public health use case, the Challenge aims to drive innovation in privacy-enhancing technologies by exploring their role in a set of high-impact use cases where there are currently challenging trade-offs between enabling sufficient access to data to and limiting the identifiability of individuals or inference of their privacy sensitive information within those data sets. Though novel innovation for this use case alone could achieve significant real world impact, the challenge is designed to incentivise development of privacy technologies that can be applied to other use cases where data is distributed across multiple organisations or jurisdictions, both in public health and elsewhere. The best solutions will deliver meaningful innovation towards deployable solutions in this space, with consideration of how to evidence the privacy guarantees offered to data owners and regulators, but also have the potential to generalise to other situations.\nThe Data\nIn Phase 1, you were provided with datasets representing synthetic population data that includes:\na social contact network, capturing when and where any two people come into contact, and the duration of the contact\ndemographic attributes of individuals\nobservations of individuals\u2019 health state (i.e., whether they are infected or not).\nThese datasets have been generated by the University of Virginia\u2019s Biocomplexity Institute (UVA-BII). Two synthetic population datasets are provided: the first simulates the population of the state of Virginia, USA, and the second the population of the U.K. The two datasets (Virginia and U.K.) have identical structures and schemas (detailed below), but differ in size and scale. The code execution evaluation primarily focuses on the Virginia population. Teams are welcome to use the U.K. dataset for development and local experimentation and to report any such results in their technical paper.\nUsing these datasets and an agent-based outbreak simulation, the UVA-BII team created 63 days of simulated disease outbreak data, subsequently split into 56 days of training data and 7 days of target data. You will be provided with the synthetic populations and the first 56 days of the simulated outbreak datasets for model training. Your task is to predict a risk score for the binary disease state (infected or not infected) of each individual in the final week of the simulation. That is, you should predict the risk of whether each person in the population will be in an infected state on any day in the last 7 days.\nDetails about the size of the synthetic datasets:\nVirginia dataset U.K. dataset\nPopulation size 7.7 million 62 million\nNumber of social contacts 181 million 722 million\nNumber of disease state records (upper bound based on 1 reading per individual per day) 430 million 3.5 billion\nNote: The challenges are based on synthetic data to minimise the security burden placed on participants during the development phase; of course the intent of the challenge is to develop privacy solutions appropriate for use on real datasets with demonstrable privacy guarantees. However, participants must adhere to a data use agreement (see competition rules for more details).\nDisease States and Asymptomatic Infection\nEach individual in the population will have a disease state for each day of the dataset. The disease state acts like a finite-state machine with the following states:\nS: indicates \"susceptible\". This individual is susceptible to infection.\nI: indicates \"infected\". This individual has been infected and is infectious. They can infect other individuals (who are susceptible) through contact.\nR: indicates \"recovered\". This individual has recovered and is no longer infectious or infectable.\n\nState transition diagram for disease state in challenge dataset.\nAn individual disease state can only progress from S to I to R. Once an individual is in the recovered R state, they will not be able to become infected again within this simulation.\nThis dataset also features an additional layer of complexity around asymptomatic infection. Some individuals in the simulation will be asymptomatically infected and will transition to I states, but the I state is hidden from the data. Such an individual is still infectious while in the I state but will appear in the data as being in the S state before transitiong to the R state. This is in constrast to individuals with the normal symptomatic infection whose disease state is transparently visible in the data. Individuals in both normal symptomatic infection and asymptomatic infection states will transition to a visible R state upon recovery.\nState transition diagram for disease state in challenge dataset showing both symptomatic and asymptomatic infections. Asymptomatically infected individuals will have a hidden infected state in the simulation, but appear as susceptible in the dataset. For this reason, asymptomatic infections may appear to transition directly from susceptible to recovered.\nDevelopment and Evaluation Data\nThe datasets provided in Phase 1 are intended for local development use in both Phase 1 and Phase 2. Each dataset has been split in time\u2014the first 56 days of the dataset is the training set, and the final 7 days of the dataset is the forecast target data. The prediction task, as detailed in a later section, is to make predictions for the final 7 days of the dataset. The ground truth is provided for the forecast target period for the development datasets.\nIn Phase 2, a separate and held-out dataset will be used for solution evaluation. You can expect the Phase 2 evaluation dataset be close in size and statistical distributions to the development Virginia dataset, but have a different contact network and an independent disease outbreak simulation. In Phase 2, you will submit code for your solution to a code execution environment. The code execution runtime will run cold-start federated training on the new dataset's training split and then run inference to generate predictions for the forecast target period. Your solution's performance will be measured by evaluating its predictions against the ground truth for the new dataset's target period.\nData Details\nEach synthetic population dataset can be considered as a relational database consisting of eight tables. Each of the eight tables will be provided as a separate CSV file. Each table has data for a particular data schema. The fields and their description for each table are shown below. The eight tables are:\nPerson data - Information about individuals in the dataset\nHousehold data - Information about individual households\nResidence location data - Location information about household residences\nActivity location data - Information about locations where non-home activities take place\nActivity location assignment data - Information about which activities each person was involved in, where it took place, and timing information. Activities are repeated every day, as if it were like the film Groundhog Day\nPopulation contact network data - Information about the contact network between people, including location, time and duration. This table is generated from the Activity location assignment data table, and also repeats every day\nDisease outcome training data - Information about each individual's health status on a particular day\nDisease outcome target data - Binary ground truth infected label for each individual for the forecast period\nTable 1: Person data has the following fields:\nHousehold ID (hid) - An integer identifying a household\nPerson ID (pid) - A unique integer identifying a person\nPerson number (person_number) - Sequence ID of a person within the household. A household of size 3 will have people with person_numbers 1, 2, and 3.\nAge - Age of person\nSex - Indicating the gender of person\nTable 2: Household data has the following fields:\nHousehold ID (hid) - A unique integer identifying the household\nResidence ID (rlid) - An integer identifying the residence\nAdmin 1 - ADCW ID for the admin1 region for U.K. or state FIPS code for Virginia\nAdmin 2 - ADCW ID for the admin1 region for U.K.; Equals to admin1 if for U.K. (ADCW does not assign admin2 region ID for U.K.); 3-digit county FIPS code for Virginia\nHousehold Size (hh_size) - Number of persons in the household\nTable 3: Residence location data has the following fields:\nResidence ID (rlid) - A unique integer identifying the residence\nLongitude - the longitude of the location\nLatitude - the latitude of the location\nAdmin 1 - See household file description\nAdmin 2 - See household file description\nTable 4: Activity location data has the following fields:\nActivity location ID (alid) - Unique integer identifying the location where activity took place\nLongitude - Longitude of the location\nLatitude - Latitude of the location\nAdmin 1 - See household file description\nAdmin 2 - See household file description\nWork - Does location support work activity? (Value is 0 or 1)\nShopping - Does location support shopping activity? (Value is 0 or 1)\nSchool - Does location support school activity? (Value is 0 or 1)\nOther - Does location support other activity? (Value is 0 or 1)\nCollege - Does location support college activity? (Value is 0 or 1)\nReligion - Does location support religion activity? (Value is 0 or 1)\nTable 5: Activity location assignment data has the following fields:\nHousehold ID - Household ID of the person\nPerson ID - Person ID of the person\nActivity number - Number of the activity in the activity sequence to which it belongs\nActivity type - Activity type (1: Home, 2: Work, 3: Shopping, 4: Other, 5: School, 6: College)\nStart Time - Start time of activity in seconds since midnight\nDuration - Duration of the activity in seconds\nLocation ID - Location ID of the location where the activity takes place (rlid or alid)\nTable 6: Population contact network data has the following fields:\nPerson ID 1 (pid1) - Person ID number 1 of this edge\nPerson ID 2 (pid2) - Person ID number 2 of this edge\nLocation ID (lid) - Location ID where contact (edge) arises\nStart time - Start time of the contact between Person ID 1 and Person ID 2 measured in seconds since midnight\nDuration - Duration of the contact measured in seconds\nActivity ID 1 (activity1) - Activity type of Person ID 1 at time of contact (see above)\nActivity ID 2 (activity2) - Activity type of Person ID 2 at time of contact (see above)\nTable 7: Disease outcome training data has the following fields:\nDay - Simulation day\nPerson ID - ID of the person in the Person data or the Contact Network data\nDisease state - Disease-related state of the person on that day with possible values S for \"susceptible\", I for \"infected\", and R for \"recovered\". See previous section for details.\nTable 8: Disease outcome target data has the following fields:\nPerson ID - ID of the person in the Person data or the Contact Network data\nInfected - Binary variable with 1 if the person was infected in the final week of the simulation and 0 if otherwise\nAbove: Overall data model of the synthetic population data. The arrows indicate dependencies between files; for example, the hid element in the person file should be constrained to the values available in the household file.\nThreat Profile\nYour end-to-end solution should preserve privacy across a range of possible threats and attack scenarios, through all stages of the machine learning model lifecycle. You should therefore carefully consider the overall privacy of your solution, focusing on the protection of sensitive information held by all parties involved in the federated learning scenario. The solutions you design and develop should include comprehensive measures to address the threat profiles described below. These measures will provide an appropriate degree of resilience to a wide range of potential attacks defined within the threat profile.\nScope of sensitive data\nYour solution must prevent the unintended disclosure of sensitive information in the population dataset to any other party, including other insider stakeholders (i.e., the other federation units) and outsiders.\nThe following information in the dataset should be treated as sensitive:\nAll personally identifiable information, including the identity of a person, their housing information, their demographics such as age, etc.\nLocation activities of an individual\nThe health state of an individual\nSocial contact information, including the location of any social contact, and the identities of who was involved in the social contact.\nForecasting Individual Risk of Infection\nThe key analytical objective of the PPFL task is to effectively train a model that can predict the risk of infection for individuals in a population over a period of time in a privacy-preserving manner. Such a predictive capability is important from the perspective of an individual; if they know they are likely to be infected in the next week, they may choose to take additional preventative measures (such as wearing a face covering, reducing social contacts, or taking antivirals prophylactically). Additionally, this capability can help inform the measures public health authorities implement to respond to such outbreaks.\nPrediction Target and Evaluation Metric\nThe target variable for the modeling task is a risk score (between 0.0 and 1.0) for each individual in the population. That risk score corresponds to a confidence that that individual enters into an symptomatic infected I disease state at any time during the final week of the simulation (between days 56 and 63).\nNote that, as discussed in the previous section, some individuals become infected asymptomatically. Those individuals count as negative cases (not infected) for the purposes of the ground truth.\nThe evaluation metric will be Area Under the Precision\u2013Recall Curve (AUPRC), also known as average precision (AP), PR-AUC, or AUCPR. This is a commonly used metric for binary classification that summarises model performance across all operating thresholds. This metric rewards models which can consistently assign positive cases with a higher risk score than negative cases. AUPRC is computed as follows:\nAUPRC=\n\u2211\nn\n(\nR\nn\n\u2212\nR\nn\u22121\n)\nP\nn\nwhere\nP\nn\nand\nR\nn\nare the precision and recall, respectively, when thresholding at the nth individual sorted in order of increasing recall.\nPartitioned Datasets for Federated learning\nOverview\nThe dataset will be horizontally partitioned into local datasets belonging to different federation units. This is intended to mirror how data is distributed across different health districts, hospitals, etc. These federation units have access to the full data tables described above. The dataset assumes that people are in precisely one federation unit, so a health district can see everywhere that one of its individuals goes, but does not have any access to information about other people who reside outside of the health district. As a result, the contacts between individuals from two different federation units are not represented in the population contact network in either of the local datasets.\nThe federated units work jointly to train a global FL model, and can take a common approach to technical design, infrastructure, etc., but are not able to access each other\u2019s raw data. In the real world there are a number of barriers that might prevent this; the parties and the datasets they contribute for PPFL may be subject to a variety of privacy, competition and industry specific regulations (e.g., HIPPA), may be operating in different jurisdictions, and have legitimate commercial and ethical reasons for not sharing customer data with other parties.\nAbove: Diagram comparing the centralised model (MC) with a privacy-preserving federated learning model (MPF).\nPartitioning Details\nFor local development, you were provided a full, unpartitioned dataset in Phase 1. In Phase 2, the evaluation data will be partitioned along administrative boundaries, e.g., by grouped FIPS codes/counties. Any cross-partition edges will be dropped, where an edge is created between two people (nodes) that come into contact as specified by the population contact network table. That is, each partition will only have visibility into edges between people that reside within the same partition.\nMoreover, a partition will have access to all information for all residents within the partition's counties, including residences, persons, and activity assignments. This is true even if an activity occurs in a county outside of that partition. Said another way, a partition knows about all locations within its counties, and all locations outside of its counties where an activity or contact occured.\nCross-partition edges will continue to affect the spread of infection, but will not be visible to any of the partitions. Any partitioning of the data that you might perform in your local development experiments should take this into account. Your solution should be able to handle any number of partitions, and in Phase 2, we may evaluate your solution with a number of partitions between 1 and 10.\nKey Task\nThe key task of this challenge is to design a privacy solution so that the collaborating parties can jointly train and deploy such a model without compromising the privacy requirements (more details on the requirements, and an associated threat model, are described in the problem description).\nFor the purposes of the challenge, you should demonstrate your solution by training two models:\nM\nC\n= a centralised model trained on the datasets in a non-privacy preserving way\nM\nPF\n= a privacy-preserving federated model trained using your privacy solution\nGood luck\nGood luck and enjoy this problem! For more details on the code submission format, visit the code submission page. If you have any questions, you can always ask the community by visiting the DrivenData user forum or the cross-U.S.\u2013U.K. public Slack channel. You can request access to the Slack channel here. You can also reach out to CDEI at petsprizechallenges@cdei.gov.uk or Innovate UK at support@iuk.ukri.org.",
        "url": "https://www.drivendata.org/competitions/141/uk-federated-learning-2-pandemic-forecasting-federated/"
    },
    {
        "competition_overview": "U.K. PETs Prize Challenge: Advancing Privacy-Preserving Federated Learning\nThis is the second in a series of challenge phases with a total prize pool of \u00a3700,000! Each phase in the PETs Prize Challenge invites participants to test and apply their innovations in privacy-preserving federated learning. Phase 2 is for Blue Teams to implement and submit working code for their solutions.\nYou are on the Phase 2 home page for the Pandemic Forecasting Data Track, for the U.K. side of the challenge. You can find the U.K. Financial Crime Track here. The U.S. challenge can be found here.\nBackground\nPrivacy-enhancing technologies (PETs) have the potential to unlock more trustworthy innovation in data analysis and machine learning. Federated learning is one such technology that enables organisations to analyse sensitive data while providing improved privacy protections. These technologies could advance innovation and collaboration in new fields and help harness the power of data to tackle some of our most pressing societal challenges.\nThat\u2019s why the U.S. and U.K. governments are partnering to deliver a set of prize challenges to unleash the potential of these democracy-affirming technologies to make a positive impact. In particular, this challenge will tackle two critical problems via separate data tracks: Data Track A will help with the identification of financial crime, while Data Track B will bolster pandemic responses.\nBy entering the prize challenges, innovators will have the opportunity to compete for cash prizes and engage with regulators and government agencies. Announced at the Summit for Democracy in December 2021, the prize challenges are a product of a collaboration between multiple government departments and agencies on both sides of the Atlantic. Winning solutions will have the opportunity to be profiled at the second Summit for Democracy, to be convened by President Joe Biden, in early 2023.\nObjectives\nThe goal of this prize challenge is to mature federated learning approaches and build trust in their adoption. The challenge organisers hope to accelerate the development of efficient privacy-preserving federated learning solutions that leverage a combination of input and output privacy techniques to:\nDrive innovation in the technological development and application of novel privacy-enhancing technologies\nDeliver strong privacy guarantees against a set of common threats and privacy attacks\nGenerate effective models to accomplish a set of predictive or analytical tasks that support the use cases\nChallenge Structure\nThe Solution Development Phase will have two data use case tracks matching those previously from the White Paper Phase\u2014Track A: Financial Crime Prevention and Track B: Pandemic Response and Forecasting. Teams can elect to participate in either or both tracks, with solutions that apply to one track individually or with a generalised solution. Each solution must correspond to a white paper that met the requirements from the White Paper Phase.\nData Track A \u2013 Financial crime prevention \u2013 The United Nations estimates that up to $2 trillion of cross-border money laundering takes place each year, financing organised crime and undermining economic prosperity. Financial institutions such as banks and credit agencies, along with organisations that process transactions between institutions, such as the SWIFT global financial messaging provider, must protect personal and financial data, while also trying to report and deter illicit financial activities. Using synthetic datasets provided by SWIFT, you will design and later develop innovative privacy-preserving federated learning solutions that facilitate cross-institution and cross-border anomaly detection to combat financial crime. This use case features both vertical and horizontal data partitioning. Check out the Financial Crime overview page for more information.\nData Track B \u2013 Pandemic response and forecasting \u2013 As we continue to deal with COVID-19, it has become apparent that better ways to harness the power of data through analytics are critical for preparing for and responding to public health crises. Federated learning approaches could allow for responsible use of sensitive data to develop cross-organisation and cross-border data analysis that would result in more robust forecasting and pandemic response capabilities. Using synthetic population datasets, you will design and later develop privacy-preserving federated learning solutions that can predict an individual\u2019s risk for infection. This use case features horizontal data partitioning. Check out the Pandemic Forecasting overview page for more information.\nGeneralisable Solutions \u2013 Cross-organisation, cross-border use cases are certainly not limited to the financial or public health domains. Developing out-of-the-box generalised models that can be adapted for use with specific data or problem sets has great potential to advance the adoption and widespread use of privacy-preserving federated learning for public- and private-sector organisations in multiple sectors. Participants may submit generalizable solutions to both tracks. Solutions that are able to perform well on both datasets are likely to score highly in the \u2018Adaptability\u2019 section of the evaluation rubric.\nMore information on how to get started can be found in the Problem Description.\nEligibility\nPhase 2 is open to Blue Teams who submitted white papers to the U.K. side of the challenge during Phase 1. All participants are eligible for prizes at the end of the competition, regardless of whether they received funding after the completion of Phase 1. Individual team members should create accounts on the DrivenData platform. Accounts with email addresses matching those used to register to the Innovate UK competition during Phase 1 will then automatically be registered to this competition. If you have any issues registering, please contact info@drivendata.org.\nPhase 2 Timeline and Prizes\nPhase 2 Key Dates\nLaunch October 25, 2022\nDeadline to Open Pull Requests for Runtime Environment January 11, 2023 at 11:59:59 PM UTC\nSubmissions Due January 26, 2023 at 11:59:59 PM UTC\nAnnouncement of Finalists to be Tested in Phase 3 Red Teaming February 7, 2023\nWinners Announced March 30, 2023\nPhase 2 Prizes\nOpen to Blue Team participants.\nPrizes are awarded agnostic of which use case participants are tackling.\nTop Solutions\nPrize Amount\n1st place \u00a350,000\n2nd place \u00a345,000\n3rd place \u00a335,000\n4th place \u00a320,000\nSpecial Recognition\nPrize Amount\nPool \u00a360,000\nPut your papers into action! Registered teams from Phase 1 will develop working prototypes and submit them to a remote execution environment for federated training and evaluation. These solutions are expected to be functional, i.e., capable of training a model and predicting against the evaluation data set with measurement of relevant performance and accuracy metrics. Solutions will be evaluated by a panel of judges across a set of weighted criteria. The top solutions, ranked by points awarded, will have their final rankings determined by incorporating red team evaluation from the red teams from Phase 3.\nPrizes will be awarded to the four best solutions (this will be agnostic of whether they are tackling the health or financial crime use case). Additional \u201cSpecial Recognition\u201d prizes will be awarded from a pool of \u00a360,000. These awards may include, for example:\nRegulators\u2019 Award: for a team that is able to most effectively demonstrate compatibility with relevant regulatory regimes and principles, such as data protection and finance/healthcare regulation.\nAdvancement in a Specific Technology: for a team that is able to demonstrate significant advancement against the state-of-the-art for a specific privacy technology.\nFull Challenge Timeline and Prizes\nTimeline Overview\nThere are three main phases in the challenge with two types of participants based on a red team/blue team approach. Blue Teams develop privacy-preserving solutions, while Red Teams act as adversaries to test those solutions.\nPhase 1: White Paper Development (Jul\u2013Sept 2022): Blue Teams propose privacy-preserving federated learning solution concepts.\nPhase 2: Solution Development (Oct 2022\u2013Jan 2023): Blue Teams develop working prototypes of their solutions.\nPhase 3: Red Teaming (Nov 2022\u2013Feb 2023): Red Teams prepare and test privacy attacks on top blue team solutions from Phase 2.\nPrize Overview\nTrack Prize Pool\nPhase 1: White Paper \u00a3100,000\nPhase 2: Solution Development Grants \u00a3300,000\nPhase 2: Solution Prizes \u00a3210,000\nPhase 3: Red Teaming \u00a390,000\nTotal \u00a3700,000\nAdditional Phase Details and Prizes\nPlace Prize Amount\nTop 6 Up to \u00a350,000 grant funding\nTop 10 \u00a310,000\nPhase 1: White Paper\nJuly 20\u2013September 21, 2022\nParticipants will produce an abstract and technical white paper laying out their proposed solution. White papers will be evaluated by a panel of judges across a set of weighted criteria. Participants must complete a paper in Phase 1 in order to be eligible to compete in Phase 2.\nOpen to Blue Team participants.\nPrize Amount\n1st \u00a340,000\n2nd \u00a330,000\n3rd \u00a320,000\nPhase 3: Red Teaming\nNovember 10, 2022\u2013February 28, 2023\nPrivacy researchers are invited to form red teams to put the privacy claims of Phase 2's blue team finalists to the test. The red teams will prepare and test privacy attacks on the Phase 2 finalist solutions, which will be incorporated into the final Phase 2 rankings. Top red teams will be evaluated for success and rigor and will be awarded prizes for their performance during this phase.\nOpen to Red Team participants.\n\nHow to compete (Phase 2)\nOnly blue teams who participated in Phase 1 are eligible to participate in Phase 2. Eligible team members should create a DrivenData account, and provide their email addresses to petsprizechallenges@cdei.gov.uk. They will then be automatically registered for this competition on the DrivenData platform.\nGet familiar with the problem through the problem description and code submission format pages.\nDevelop and submit your centralised solution. Check out the centralised code submission format page for more details.\nDevelop and submit your federated solution. Check out the federated code submission format page for more details.\nGood luck!\nThis challenge is sponsored by the Centre for Data Ethics and Innovation (CDEI) and Innovate UK\n\n        \nWith additional collaboration from SWIFT and the University of Virginia's Biocomplexity Institute.\nThis prize challenge and its U.S. counterpart have been developed as part of a joint collaboration between the United Kingdom and the United States.",
        "dataset_overview": "Data Track B: Pandemic Forecasting\nTransforming Pandemic Response and Forecasting\nThere are two data use case tracks for the PETs prize challenge. This is the U.K. Phase 2 data overview page for the pandemic forecasting track. In this track, innovators will develop end-to-end privacy-preserving federated learning solutions to accurately forecast individual risk of infection.\nBackground on this track is provided below. For a more detailed overview, please refer to the Pandemic Response and Forecasting Technical Brief.\nBackground\nBackground\nData\nOverview\nThreat Profile\nModeling\nForecasting Risk\nPartitioning Data\nBackground\nFederated learning (FL), or more generally collaborative learning, shows huge promise for machine learning applications derived from sensitive data by enabling training on distributed data sets without sharing the data among the participating parties.\nThe PETs Prize Challenge is focused on enhancing cross-organisation, cross-border data sharing to support efforts to enable better public health related forecasting to bolster pandemic response capabilities. The COVID-19 pandemic has taken an immense toll on human lives and had unprecedented level of socio-economic impact on individuals and societies around the globe. As we continue to deal with COVID-19, it has become apparent that better ways to harness the power of data by employing privacy-preserving data sharing and analytics are critical to preparing for such pandemics or public health crises.\nIn this Data Track, you are asked to develop innovative, privacy-preserving FL solutions utilising dataset of a synthetic population (a.k.a. digital twin) that simulates a population with statistical and dynamical properties similar to a real population. You will design and later develop privacy-preserving federated learning solutions that can predict an individual\u2019s risk for infection. If an individual knows that they are likely to be infected in the next week, they can take more prophylactic measures (masking, changing plans, etc.) than they usually might. Furthermore, this knowledge could help public health officials better predict interventions and staffing needs for specific regions.\nBy focusing on this public health use case, the Challenge aims to drive innovation in privacy-enhancing technologies by exploring their role in a set of high-impact use cases where there are currently challenging trade-offs between enabling sufficient access to data to and limiting the identifiability of individuals or inference of their privacy sensitive information within those data sets. Though novel innovation for this use case alone could achieve significant real world impact, the challenge is designed to incentivise development of privacy technologies that can be applied to other use cases where data is distributed across multiple organisations or jurisdictions, both in public health and elsewhere. The best solutions will deliver meaningful innovation towards deployable solutions in this space, with consideration of how to evidence the privacy guarantees offered to data owners and regulators, but also have the potential to generalise to other situations.\nThe Data\nIn Phase 1, you were provided with datasets representing synthetic population data that includes:\na social contact network, capturing when and where any two people come into contact, and the duration of the contact\ndemographic attributes of individuals\nobservations of individuals\u2019 health state (i.e., whether they are infected or not).\nThese datasets have been generated by the University of Virginia\u2019s Biocomplexity Institute (UVA-BII). Two synthetic population datasets are provided: the first simulates the population of the state of Virginia, USA, and the second the population of the U.K. The two datasets (Virginia and U.K.) have identical structures and schemas (detailed below), but differ in size and scale. The code execution evaluation primarily focuses on the Virginia population. Teams are welcome to use the U.K. dataset for development and local experimentation and to report any such results in their technical paper.\nUsing these datasets and an agent-based outbreak simulation, the UVA-BII team created 63 days of simulated disease outbreak data, subsequently split into 56 days of training data and 7 days of target data. You will be provided with the synthetic populations and the first 56 days of the simulated outbreak datasets for model training. Your task is to predict a risk score for the binary disease state (infected or not infected) of each individual in the final week of the simulation. That is, you should predict the risk of whether each person in the population will be in an infected state on any day in the last 7 days.\nDetails about the size of the synthetic datasets:\nVirginia dataset U.K. dataset\nPopulation size 7.7 million 62 million\nNumber of social contacts 181 million 722 million\nNumber of disease state records (upper bound based on 1 reading per individual per day) 430 million 3.5 billion\nNote: The challenges are based on synthetic data to minimise the security burden placed on participants during the development phase; of course the intent of the challenge is to develop privacy solutions appropriate for use on real datasets with demonstrable privacy guarantees. However, participants must adhere to a data use agreement (see competition rules for more details).\nDisease States and Asymptomatic Infection\nEach individual in the population will have a disease state for each day of the dataset. The disease state acts like a finite-state machine with the following states:\nS: indicates \"susceptible\". This individual is susceptible to infection.\nI: indicates \"infected\". This individual has been infected and is infectious. They can infect other individuals (who are susceptible) through contact.\nR: indicates \"recovered\". This individual has recovered and is no longer infectious or infectable.\n\nState transition diagram for disease state in challenge dataset.\nAn individual disease state can only progress from S to I to R. Once an individual is in the recovered R state, they will not be able to become infected again within this simulation.\nThis dataset also features an additional layer of complexity around asymptomatic infection. Some individuals in the simulation will be asymptomatically infected and will transition to I states, but the I state is hidden from the data. Such an individual is still infectious while in the I state but will appear in the data as being in the S state before transitiong to the R state. This is in constrast to individuals with the normal symptomatic infection whose disease state is transparently visible in the data. Individuals in both normal symptomatic infection and asymptomatic infection states will transition to a visible R state upon recovery.\nState transition diagram for disease state in challenge dataset showing both symptomatic and asymptomatic infections. Asymptomatically infected individuals will have a hidden infected state in the simulation, but appear as susceptible in the dataset. For this reason, asymptomatic infections may appear to transition directly from susceptible to recovered.\nDevelopment and Evaluation Data\nThe datasets provided in Phase 1 are intended for local development use in both Phase 1 and Phase 2. Each dataset has been split in time\u2014the first 56 days of the dataset is the training set, and the final 7 days of the dataset is the forecast target data. The prediction task, as detailed in a later section, is to make predictions for the final 7 days of the dataset. The ground truth is provided for the forecast target period for the development datasets.\nIn Phase 2, a separate and held-out dataset will be used for solution evaluation. You can expect the Phase 2 evaluation dataset be close in size and statistical distributions to the development Virginia dataset, but have a different contact network and an independent disease outbreak simulation. In Phase 2, you will submit code for your solution to a code execution environment. The code execution runtime will run cold-start federated training on the new dataset's training split and then run inference to generate predictions for the forecast target period. Your solution's performance will be measured by evaluating its predictions against the ground truth for the new dataset's target period.\nData Details\nEach synthetic population dataset can be considered as a relational database consisting of eight tables. Each of the eight tables will be provided as a separate CSV file. Each table has data for a particular data schema. The fields and their description for each table are shown below. The eight tables are:\nPerson data - Information about individuals in the dataset\nHousehold data - Information about individual households\nResidence location data - Location information about household residences\nActivity location data - Information about locations where non-home activities take place\nActivity location assignment data - Information about which activities each person was involved in, where it took place, and timing information. Activities are repeated every day, as if it were like the film Groundhog Day\nPopulation contact network data - Information about the contact network between people, including location, time and duration. This table is generated from the Activity location assignment data table, and also repeats every day\nDisease outcome training data - Information about each individual's health status on a particular day\nDisease outcome target data - Binary ground truth infected label for each individual for the forecast period\nTable 1: Person data has the following fields:\nHousehold ID (hid) - An integer identifying a household\nPerson ID (pid) - A unique integer identifying a person\nPerson number (person_number) - Sequence ID of a person within the household. A household of size 3 will have people with person_numbers 1, 2, and 3.\nAge - Age of person\nSex - Indicating the gender of person\nTable 2: Household data has the following fields:\nHousehold ID (hid) - A unique integer identifying the household\nResidence ID (rlid) - An integer identifying the residence\nAdmin 1 - ADCW ID for the admin1 region for U.K. or state FIPS code for Virginia\nAdmin 2 - ADCW ID for the admin1 region for U.K.; Equals to admin1 if for U.K. (ADCW does not assign admin2 region ID for U.K.); 3-digit county FIPS code for Virginia\nHousehold Size (hh_size) - Number of persons in the household\nTable 3: Residence location data has the following fields:\nResidence ID (rlid) - A unique integer identifying the residence\nLongitude - the longitude of the location\nLatitude - the latitude of the location\nAdmin 1 - See household file description\nAdmin 2 - See household file description\nTable 4: Activity location data has the following fields:\nActivity location ID (alid) - Unique integer identifying the location where activity took place\nLongitude - Longitude of the location\nLatitude - Latitude of the location\nAdmin 1 - See household file description\nAdmin 2 - See household file description\nWork - Does location support work activity? (Value is 0 or 1)\nShopping - Does location support shopping activity? (Value is 0 or 1)\nSchool - Does location support school activity? (Value is 0 or 1)\nOther - Does location support other activity? (Value is 0 or 1)\nCollege - Does location support college activity? (Value is 0 or 1)\nReligion - Does location support religion activity? (Value is 0 or 1)\nTable 5: Activity location assignment data has the following fields:\nHousehold ID - Household ID of the person\nPerson ID - Person ID of the person\nActivity number - Number of the activity in the activity sequence to which it belongs\nActivity type - Activity type (1: Home, 2: Work, 3: Shopping, 4: Other, 5: School, 6: College)\nStart Time - Start time of activity in seconds since midnight\nDuration - Duration of the activity in seconds\nLocation ID - Location ID of the location where the activity takes place (rlid or alid)\nTable 6: Population contact network data has the following fields:\nPerson ID 1 (pid1) - Person ID number 1 of this edge\nPerson ID 2 (pid2) - Person ID number 2 of this edge\nLocation ID (lid) - Location ID where contact (edge) arises\nStart time - Start time of the contact between Person ID 1 and Person ID 2 measured in seconds since midnight\nDuration - Duration of the contact measured in seconds\nActivity ID 1 (activity1) - Activity type of Person ID 1 at time of contact (see above)\nActivity ID 2 (activity2) - Activity type of Person ID 2 at time of contact (see above)\nTable 7: Disease outcome training data has the following fields:\nDay - Simulation day\nPerson ID - ID of the person in the Person data or the Contact Network data\nDisease state - Disease-related state of the person on that day with possible values S for \"susceptible\", I for \"infected\", and R for \"recovered\". See previous section for details.\nTable 8: Disease outcome target data has the following fields:\nPerson ID - ID of the person in the Person data or the Contact Network data\nInfected - Binary variable with 1 if the person was infected in the final week of the simulation and 0 if otherwise\nAbove: Overall data model of the synthetic population data. The arrows indicate dependencies between files; for example, the hid element in the person file should be constrained to the values available in the household file.\nThreat Profile\nYour end-to-end solution should preserve privacy across a range of possible threats and attack scenarios, through all stages of the machine learning model lifecycle. You should therefore carefully consider the overall privacy of your solution, focusing on the protection of sensitive information held by all parties involved in the federated learning scenario. The solutions you design and develop should include comprehensive measures to address the threat profiles described below. These measures will provide an appropriate degree of resilience to a wide range of potential attacks defined within the threat profile.\nScope of sensitive data\nYour solution must prevent the unintended disclosure of sensitive information in the population dataset to any other party, including other insider stakeholders (i.e., the other federation units) and outsiders.\nThe following information in the dataset should be treated as sensitive:\nAll personally identifiable information, including the identity of a person, their housing information, their demographics such as age, etc.\nLocation activities of an individual\nThe health state of an individual\nSocial contact information, including the location of any social contact, and the identities of who was involved in the social contact.\nForecasting Individual Risk of Infection\nThe key analytical objective of the PPFL task is to effectively train a model that can predict the risk of infection for individuals in a population over a period of time in a privacy-preserving manner. Such a predictive capability is important from the perspective of an individual; if they know they are likely to be infected in the next week, they may choose to take additional preventative measures (such as wearing a face covering, reducing social contacts, or taking antivirals prophylactically). Additionally, this capability can help inform the measures public health authorities implement to respond to such outbreaks.\nPrediction Target and Evaluation Metric\nThe target variable for the modeling task is a risk score (between 0.0 and 1.0) for each individual in the population. That risk score corresponds to a confidence that that individual enters into an symptomatic infected I disease state at any time during the final week of the simulation (between days 56 and 63).\nNote that, as discussed in the previous section, some individuals become infected asymptomatically. Those individuals count as negative cases (not infected) for the purposes of the ground truth.\nThe evaluation metric will be Area Under the Precision\u2013Recall Curve (AUPRC), also known as average precision (AP), PR-AUC, or AUCPR. This is a commonly used metric for binary classification that summarises model performance across all operating thresholds. This metric rewards models which can consistently assign positive cases with a higher risk score than negative cases. AUPRC is computed as follows:\nAUPRC=\n\u2211\nn\n(\nR\nn\n\u2212\nR\nn\u22121\n)\nP\nn\nwhere\nP\nn\nand\nR\nn\nare the precision and recall, respectively, when thresholding at the nth individual sorted in order of increasing recall.\nPartitioned Datasets for Federated learning\nOverview\nThe dataset will be horizontally partitioned into local datasets belonging to different federation units. This is intended to mirror how data is distributed across different health districts, hospitals, etc. These federation units have access to the full data tables described above. The dataset assumes that people are in precisely one federation unit, so a health district can see everywhere that one of its individuals goes, but does not have any access to information about other people who reside outside of the health district. As a result, the contacts between individuals from two different federation units are not represented in the population contact network in either of the local datasets.\nThe federated units work jointly to train a global FL model, and can take a common approach to technical design, infrastructure, etc., but are not able to access each other\u2019s raw data. In the real world there are a number of barriers that might prevent this; the parties and the datasets they contribute for PPFL may be subject to a variety of privacy, competition and industry specific regulations (e.g., HIPPA), may be operating in different jurisdictions, and have legitimate commercial and ethical reasons for not sharing customer data with other parties.\nAbove: Diagram comparing the centralised model (MC) with a privacy-preserving federated learning model (MPF).\nPartitioning Details\nFor local development, you were provided a full, unpartitioned dataset in Phase 1. In Phase 2, the evaluation data will be partitioned along administrative boundaries, e.g., by grouped FIPS codes/counties. Any cross-partition edges will be dropped, where an edge is created between two people (nodes) that come into contact as specified by the population contact network table. That is, each partition will only have visibility into edges between people that reside within the same partition.\nMoreover, a partition will have access to all information for all residents within the partition's counties, including residences, persons, and activity assignments. This is true even if an activity occurs in a county outside of that partition. Said another way, a partition knows about all locations within its counties, and all locations outside of its counties where an activity or contact occured.\nCross-partition edges will continue to affect the spread of infection, but will not be visible to any of the partitions. Any partitioning of the data that you might perform in your local development experiments should take this into account. Your solution should be able to handle any number of partitions, and in Phase 2, we may evaluate your solution with a number of partitions between 1 and 10.\nKey Task\nThe key task of this challenge is to design a privacy solution so that the collaborating parties can jointly train and deploy such a model without compromising the privacy requirements (more details on the requirements, and an associated threat model, are described in the problem description).\nFor the purposes of the challenge, you should demonstrate your solution by training two models:\nM\nC\n= a centralised model trained on the datasets in a non-privacy preserving way\nM\nPF\n= a privacy-preserving federated model trained using your privacy solution\nGood luck\nGood luck and enjoy this problem! For more details on the code submission format, visit the code submission page. If you have any questions, you can always ask the community by visiting the DrivenData user forum or the cross-U.S.\u2013U.K. public Slack channel. You can request access to the Slack channel here. You can also reach out to CDEI at petsprizechallenges@cdei.gov.uk or Innovate UK at support@iuk.ukri.org.",
        "evaluation_overview": "Data Track B: Pandemic Forecasting\nTransforming Pandemic Response and Forecasting\nThere are two data use case tracks for the PETs prize challenge. This is the U.K. Phase 2 data overview page for the pandemic forecasting track. In this track, innovators will develop end-to-end privacy-preserving federated learning solutions to accurately forecast individual risk of infection.\nBackground on this track is provided below. For a more detailed overview, please refer to the Pandemic Response and Forecasting Technical Brief.\nBackground\nBackground\nData\nOverview\nThreat Profile\nModeling\nForecasting Risk\nPartitioning Data\nBackground\nFederated learning (FL), or more generally collaborative learning, shows huge promise for machine learning applications derived from sensitive data by enabling training on distributed data sets without sharing the data among the participating parties.\nThe PETs Prize Challenge is focused on enhancing cross-organisation, cross-border data sharing to support efforts to enable better public health related forecasting to bolster pandemic response capabilities. The COVID-19 pandemic has taken an immense toll on human lives and had unprecedented level of socio-economic impact on individuals and societies around the globe. As we continue to deal with COVID-19, it has become apparent that better ways to harness the power of data by employing privacy-preserving data sharing and analytics are critical to preparing for such pandemics or public health crises.\nIn this Data Track, you are asked to develop innovative, privacy-preserving FL solutions utilising dataset of a synthetic population (a.k.a. digital twin) that simulates a population with statistical and dynamical properties similar to a real population. You will design and later develop privacy-preserving federated learning solutions that can predict an individual\u2019s risk for infection. If an individual knows that they are likely to be infected in the next week, they can take more prophylactic measures (masking, changing plans, etc.) than they usually might. Furthermore, this knowledge could help public health officials better predict interventions and staffing needs for specific regions.\nBy focusing on this public health use case, the Challenge aims to drive innovation in privacy-enhancing technologies by exploring their role in a set of high-impact use cases where there are currently challenging trade-offs between enabling sufficient access to data to and limiting the identifiability of individuals or inference of their privacy sensitive information within those data sets. Though novel innovation for this use case alone could achieve significant real world impact, the challenge is designed to incentivise development of privacy technologies that can be applied to other use cases where data is distributed across multiple organisations or jurisdictions, both in public health and elsewhere. The best solutions will deliver meaningful innovation towards deployable solutions in this space, with consideration of how to evidence the privacy guarantees offered to data owners and regulators, but also have the potential to generalise to other situations.\nThe Data\nIn Phase 1, you were provided with datasets representing synthetic population data that includes:\na social contact network, capturing when and where any two people come into contact, and the duration of the contact\ndemographic attributes of individuals\nobservations of individuals\u2019 health state (i.e., whether they are infected or not).\nThese datasets have been generated by the University of Virginia\u2019s Biocomplexity Institute (UVA-BII). Two synthetic population datasets are provided: the first simulates the population of the state of Virginia, USA, and the second the population of the U.K. The two datasets (Virginia and U.K.) have identical structures and schemas (detailed below), but differ in size and scale. The code execution evaluation primarily focuses on the Virginia population. Teams are welcome to use the U.K. dataset for development and local experimentation and to report any such results in their technical paper.\nUsing these datasets and an agent-based outbreak simulation, the UVA-BII team created 63 days of simulated disease outbreak data, subsequently split into 56 days of training data and 7 days of target data. You will be provided with the synthetic populations and the first 56 days of the simulated outbreak datasets for model training. Your task is to predict a risk score for the binary disease state (infected or not infected) of each individual in the final week of the simulation. That is, you should predict the risk of whether each person in the population will be in an infected state on any day in the last 7 days.\nDetails about the size of the synthetic datasets:\nVirginia dataset U.K. dataset\nPopulation size 7.7 million 62 million\nNumber of social contacts 181 million 722 million\nNumber of disease state records (upper bound based on 1 reading per individual per day) 430 million 3.5 billion\nNote: The challenges are based on synthetic data to minimise the security burden placed on participants during the development phase; of course the intent of the challenge is to develop privacy solutions appropriate for use on real datasets with demonstrable privacy guarantees. However, participants must adhere to a data use agreement (see competition rules for more details).\nDisease States and Asymptomatic Infection\nEach individual in the population will have a disease state for each day of the dataset. The disease state acts like a finite-state machine with the following states:\nS: indicates \"susceptible\". This individual is susceptible to infection.\nI: indicates \"infected\". This individual has been infected and is infectious. They can infect other individuals (who are susceptible) through contact.\nR: indicates \"recovered\". This individual has recovered and is no longer infectious or infectable.\n\nState transition diagram for disease state in challenge dataset.\nAn individual disease state can only progress from S to I to R. Once an individual is in the recovered R state, they will not be able to become infected again within this simulation.\nThis dataset also features an additional layer of complexity around asymptomatic infection. Some individuals in the simulation will be asymptomatically infected and will transition to I states, but the I state is hidden from the data. Such an individual is still infectious while in the I state but will appear in the data as being in the S state before transitiong to the R state. This is in constrast to individuals with the normal symptomatic infection whose disease state is transparently visible in the data. Individuals in both normal symptomatic infection and asymptomatic infection states will transition to a visible R state upon recovery.\nState transition diagram for disease state in challenge dataset showing both symptomatic and asymptomatic infections. Asymptomatically infected individuals will have a hidden infected state in the simulation, but appear as susceptible in the dataset. For this reason, asymptomatic infections may appear to transition directly from susceptible to recovered.\nDevelopment and Evaluation Data\nThe datasets provided in Phase 1 are intended for local development use in both Phase 1 and Phase 2. Each dataset has been split in time\u2014the first 56 days of the dataset is the training set, and the final 7 days of the dataset is the forecast target data. The prediction task, as detailed in a later section, is to make predictions for the final 7 days of the dataset. The ground truth is provided for the forecast target period for the development datasets.\nIn Phase 2, a separate and held-out dataset will be used for solution evaluation. You can expect the Phase 2 evaluation dataset be close in size and statistical distributions to the development Virginia dataset, but have a different contact network and an independent disease outbreak simulation. In Phase 2, you will submit code for your solution to a code execution environment. The code execution runtime will run cold-start federated training on the new dataset's training split and then run inference to generate predictions for the forecast target period. Your solution's performance will be measured by evaluating its predictions against the ground truth for the new dataset's target period.\nData Details\nEach synthetic population dataset can be considered as a relational database consisting of eight tables. Each of the eight tables will be provided as a separate CSV file. Each table has data for a particular data schema. The fields and their description for each table are shown below. The eight tables are:\nPerson data - Information about individuals in the dataset\nHousehold data - Information about individual households\nResidence location data - Location information about household residences\nActivity location data - Information about locations where non-home activities take place\nActivity location assignment data - Information about which activities each person was involved in, where it took place, and timing information. Activities are repeated every day, as if it were like the film Groundhog Day\nPopulation contact network data - Information about the contact network between people, including location, time and duration. This table is generated from the Activity location assignment data table, and also repeats every day\nDisease outcome training data - Information about each individual's health status on a particular day\nDisease outcome target data - Binary ground truth infected label for each individual for the forecast period\nTable 1: Person data has the following fields:\nHousehold ID (hid) - An integer identifying a household\nPerson ID (pid) - A unique integer identifying a person\nPerson number (person_number) - Sequence ID of a person within the household. A household of size 3 will have people with person_numbers 1, 2, and 3.\nAge - Age of person\nSex - Indicating the gender of person\nTable 2: Household data has the following fields:\nHousehold ID (hid) - A unique integer identifying the household\nResidence ID (rlid) - An integer identifying the residence\nAdmin 1 - ADCW ID for the admin1 region for U.K. or state FIPS code for Virginia\nAdmin 2 - ADCW ID for the admin1 region for U.K.; Equals to admin1 if for U.K. (ADCW does not assign admin2 region ID for U.K.); 3-digit county FIPS code for Virginia\nHousehold Size (hh_size) - Number of persons in the household\nTable 3: Residence location data has the following fields:\nResidence ID (rlid) - A unique integer identifying the residence\nLongitude - the longitude of the location\nLatitude - the latitude of the location\nAdmin 1 - See household file description\nAdmin 2 - See household file description\nTable 4: Activity location data has the following fields:\nActivity location ID (alid) - Unique integer identifying the location where activity took place\nLongitude - Longitude of the location\nLatitude - Latitude of the location\nAdmin 1 - See household file description\nAdmin 2 - See household file description\nWork - Does location support work activity? (Value is 0 or 1)\nShopping - Does location support shopping activity? (Value is 0 or 1)\nSchool - Does location support school activity? (Value is 0 or 1)\nOther - Does location support other activity? (Value is 0 or 1)\nCollege - Does location support college activity? (Value is 0 or 1)\nReligion - Does location support religion activity? (Value is 0 or 1)\nTable 5: Activity location assignment data has the following fields:\nHousehold ID - Household ID of the person\nPerson ID - Person ID of the person\nActivity number - Number of the activity in the activity sequence to which it belongs\nActivity type - Activity type (1: Home, 2: Work, 3: Shopping, 4: Other, 5: School, 6: College)\nStart Time - Start time of activity in seconds since midnight\nDuration - Duration of the activity in seconds\nLocation ID - Location ID of the location where the activity takes place (rlid or alid)\nTable 6: Population contact network data has the following fields:\nPerson ID 1 (pid1) - Person ID number 1 of this edge\nPerson ID 2 (pid2) - Person ID number 2 of this edge\nLocation ID (lid) - Location ID where contact (edge) arises\nStart time - Start time of the contact between Person ID 1 and Person ID 2 measured in seconds since midnight\nDuration - Duration of the contact measured in seconds\nActivity ID 1 (activity1) - Activity type of Person ID 1 at time of contact (see above)\nActivity ID 2 (activity2) - Activity type of Person ID 2 at time of contact (see above)\nTable 7: Disease outcome training data has the following fields:\nDay - Simulation day\nPerson ID - ID of the person in the Person data or the Contact Network data\nDisease state - Disease-related state of the person on that day with possible values S for \"susceptible\", I for \"infected\", and R for \"recovered\". See previous section for details.\nTable 8: Disease outcome target data has the following fields:\nPerson ID - ID of the person in the Person data or the Contact Network data\nInfected - Binary variable with 1 if the person was infected in the final week of the simulation and 0 if otherwise\nAbove: Overall data model of the synthetic population data. The arrows indicate dependencies between files; for example, the hid element in the person file should be constrained to the values available in the household file.\nThreat Profile\nYour end-to-end solution should preserve privacy across a range of possible threats and attack scenarios, through all stages of the machine learning model lifecycle. You should therefore carefully consider the overall privacy of your solution, focusing on the protection of sensitive information held by all parties involved in the federated learning scenario. The solutions you design and develop should include comprehensive measures to address the threat profiles described below. These measures will provide an appropriate degree of resilience to a wide range of potential attacks defined within the threat profile.\nScope of sensitive data\nYour solution must prevent the unintended disclosure of sensitive information in the population dataset to any other party, including other insider stakeholders (i.e., the other federation units) and outsiders.\nThe following information in the dataset should be treated as sensitive:\nAll personally identifiable information, including the identity of a person, their housing information, their demographics such as age, etc.\nLocation activities of an individual\nThe health state of an individual\nSocial contact information, including the location of any social contact, and the identities of who was involved in the social contact.\nForecasting Individual Risk of Infection\nThe key analytical objective of the PPFL task is to effectively train a model that can predict the risk of infection for individuals in a population over a period of time in a privacy-preserving manner. Such a predictive capability is important from the perspective of an individual; if they know they are likely to be infected in the next week, they may choose to take additional preventative measures (such as wearing a face covering, reducing social contacts, or taking antivirals prophylactically). Additionally, this capability can help inform the measures public health authorities implement to respond to such outbreaks.\nPrediction Target and Evaluation Metric\nThe target variable for the modeling task is a risk score (between 0.0 and 1.0) for each individual in the population. That risk score corresponds to a confidence that that individual enters into an symptomatic infected I disease state at any time during the final week of the simulation (between days 56 and 63).\nNote that, as discussed in the previous section, some individuals become infected asymptomatically. Those individuals count as negative cases (not infected) for the purposes of the ground truth.\nThe evaluation metric will be Area Under the Precision\u2013Recall Curve (AUPRC), also known as average precision (AP), PR-AUC, or AUCPR. This is a commonly used metric for binary classification that summarises model performance across all operating thresholds. This metric rewards models which can consistently assign positive cases with a higher risk score than negative cases. AUPRC is computed as follows:\nAUPRC=\n\u2211\nn\n(\nR\nn\n\u2212\nR\nn\u22121\n)\nP\nn\nwhere\nP\nn\nand\nR\nn\nare the precision and recall, respectively, when thresholding at the nth individual sorted in order of increasing recall.\nPartitioned Datasets for Federated learning\nOverview\nThe dataset will be horizontally partitioned into local datasets belonging to different federation units. This is intended to mirror how data is distributed across different health districts, hospitals, etc. These federation units have access to the full data tables described above. The dataset assumes that people are in precisely one federation unit, so a health district can see everywhere that one of its individuals goes, but does not have any access to information about other people who reside outside of the health district. As a result, the contacts between individuals from two different federation units are not represented in the population contact network in either of the local datasets.\nThe federated units work jointly to train a global FL model, and can take a common approach to technical design, infrastructure, etc., but are not able to access each other\u2019s raw data. In the real world there are a number of barriers that might prevent this; the parties and the datasets they contribute for PPFL may be subject to a variety of privacy, competition and industry specific regulations (e.g., HIPPA), may be operating in different jurisdictions, and have legitimate commercial and ethical reasons for not sharing customer data with other parties.\nAbove: Diagram comparing the centralised model (MC) with a privacy-preserving federated learning model (MPF).\nPartitioning Details\nFor local development, you were provided a full, unpartitioned dataset in Phase 1. In Phase 2, the evaluation data will be partitioned along administrative boundaries, e.g., by grouped FIPS codes/counties. Any cross-partition edges will be dropped, where an edge is created between two people (nodes) that come into contact as specified by the population contact network table. That is, each partition will only have visibility into edges between people that reside within the same partition.\nMoreover, a partition will have access to all information for all residents within the partition's counties, including residences, persons, and activity assignments. This is true even if an activity occurs in a county outside of that partition. Said another way, a partition knows about all locations within its counties, and all locations outside of its counties where an activity or contact occured.\nCross-partition edges will continue to affect the spread of infection, but will not be visible to any of the partitions. Any partitioning of the data that you might perform in your local development experiments should take this into account. Your solution should be able to handle any number of partitions, and in Phase 2, we may evaluate your solution with a number of partitions between 1 and 10.\nKey Task\nThe key task of this challenge is to design a privacy solution so that the collaborating parties can jointly train and deploy such a model without compromising the privacy requirements (more details on the requirements, and an associated threat model, are described in the problem description).\nFor the purposes of the challenge, you should demonstrate your solution by training two models:\nM\nC\n= a centralised model trained on the datasets in a non-privacy preserving way\nM\nPF\n= a privacy-preserving federated model trained using your privacy solution\nGood luck\nGood luck and enjoy this problem! For more details on the code submission format, visit the code submission page. If you have any questions, you can always ask the community by visiting the DrivenData user forum or the cross-U.S.\u2013U.K. public Slack channel. You can request access to the Slack channel here. You can also reach out to CDEI at petsprizechallenges@cdei.gov.uk or Innovate UK at support@iuk.ukri.org.",
        "url": "https://www.drivendata.org/competitions/147/uk-federated-learning-2-pandemic-forecasting-centralized/"
    },
    {
        "competition_overview": "U.K. PETs Prize Challenge: Advancing Privacy-Preserving Federated Learning\nThis is the third in a series of challenge phases with a total prize pool of \u00a3700,000! Each phase in the PETs Prize Challenge invites participants to test and apply their innovations in privacy-preserving federated learning. Phase 3 is for Red Teams to put the privacy claims of Blue Teams' solutions to the test.\nBackground\nPrivacy-enhancing technologies (PETs) have the potential to unlock more trustworthy innovation in data analysis and machine learning. Federated learning is one such technology that enables organisations to analyse sensitive data while providing improved privacy protections. These technologies could advance innovation and collaboration in new fields and help harness the power of data to tackle some of our most pressing societal challenges.\nThat\u2019s why the U.S. and U.K. governments are partnering to deliver a set of prize challenges to unleash the potential of these democracy-affirming technologies to make a positive impact. In particular, this challenge will tackle two critical problems via separate data tracks: Data Track A will help with the identification of financial crime, while Data Track B will bolster pandemic responses.\nBy entering the prize challenges, innovators will have the opportunity to compete for cash prizes and engage with regulators and government agencies. Announced at the Summit for Democracy in December 2021, the prize challenges are a product of a collaboration between multiple government departments and agencies on both sides of the Atlantic. Winning solutions will have the opportunity to be profiled at the second Summit for Democracy, to be convened by President Joe Biden, in early 2023.\nObjectives\nThe goal of this prize challenge is to mature federated learning approaches and build trust in their adoption. The challenge organisers hope to accelerate the development of efficient privacy-preserving federated learning solutions that leverage a combination of input and output privacy techniques to:\nDrive innovation in the technological development and application of novel privacy-enhancing technologies\nDeliver strong privacy guarantees against a set of common threats and privacy attacks\nGenerate effective models to accomplish a set of predictive or analytical tasks that support the use cases\nEligibility\nRed teams must first register through Innovate UK here, where full eligibility criteria can also be found.\nParticipants in this Challenge, whether they are individuals, entities, or team members, are prohibited from participating in the U.K. prize challenge, and participants in the U.S. prize challenge are likewise prohibited from participating in this Challenge. Individuals, entities, or team members who are found to have entered both the U.S. and the U.K. challenges will be disqualified from participating in either. If you wish to instead participate in the U.S. competition, please register here.\nAny individual who is or has been associated with an active Blue Team is prohibited from participating in the Red Team Phase. A Blue Team may withdraw from Phase 2 of the challenge so that its members are eligible to participate in the Red Team Phase. To withdraw, teams should email petsprizechallenges@cdei.gov.uk by 11.59pm on 2nd December 2022.\nIf you're looking to find teammates for this challenge, the community forum or the cross-U.S.\u2013U.K. public Slack channel are great places to start. You can request access to the Slack channel here.\nPhase 3 Timeline and Prizes\nPhase 3 Key Dates\nLaunch 10 November 2022\nRegistration Deadline 2 December 2022 at 11:59:59 PM UTC\nPreparation Period 9 December 2022\u20137 February 2023\nAttack Period 7 February\u201328 February 2023\nWinners Announced 30 March 2023\nPhase 3 Prizes\nOpen to Red Team participants.\nTop red teams will be evaluated for success and rigor and will be awarded prizes for their performance during this phase.\nPrize Amount\n1st \u00a340,000\n2nd \u00a330,000\n3rd \u00a320,000\nHow to compete (Phase 3)\nComplete the registration form on the Innovate UK platform.\nRegister for a DrivenData account here using the same email.\nOnce the registration period has concluded, applicants who have registered through Innovate UK will be assigned to the DrivenData competition, enabling them to download additional resources including the relevant SWIFT and UVA datasets.\nGet familiar with the problem through the data overview and problem description pages.\nDuring the preparation period, review the provided blue teams' white papers to plan your privacy attacks.\nDuring the attack period, conduct privacy attacks on the blue team finalist solutions that you are assigned. Submit all required attack report materials by the deadline.\nNote that registration closes on 2 December 2023 at 11:59 PM UTC.\nGood luck!\nFull Challenge Timeline and Prizes\nTimeline Overview\nThere are three main phases in the challenge with two types of participants based on a red team/blue team approach. Blue Teams develop privacy-preserving solutions, while Red Teams act as adversaries to test those solutions.\nPhase 1: White Paper Development (Jul\u2013Sept 2022): Blue Teams propose privacy-preserving federated learning solution concepts.\nPhase 2: Solution Development (Oct 2022\u2013Jan 2023): Blue Teams develop working prototypes of their solutions.\nPhase 3: Red Teaming (Nov 2022\u2013Feb 2023): Red Teams prepare and test privacy attacks on top blue team solutions from Phase 2.\nPrize Overview\nTrack Prize Pool\nPhase 1: White Paper \u00a3100,000\nPhase 2: Solution Development Grants \u00a3300,000\nPhase 2: Solution Prizes \u00a3210,000\nPhase 3: Red Teaming \u00a390,000\nTotal \u00a3700,000\nAdditional Phase Details and Prizes\nPlace Prize Amount\nTop 6 Up to \u00a350,000 grant funding\nTop 10 \u00a310,000\nPhase 1: White Paper\n20 July\u201321 September 2022\nParticipants will produce an abstract and technical white paper laying out their proposed solution. White papers will be evaluated by a panel of judges across a set of weighted criteria. Participants must complete a paper in Phase 1 in order to be eligible to compete in Phase 2.\nOpen to Blue Team participants.\nTop Solutions\nPrize Amount\n1st place \u00a350,000\n2nd place \u00a345,000\n3rd place \u00a335,000\n4th place \u00a320,000\nSpecial Recognition\nPrize Amount\nPool \u00a360,000\nPhase 2: Solution Development\n25 October 2022\u201326 January 2023\nPut your papers into action! Registered teams from Phase 1 will develop working prototypes and submit them to a remote execution environment for federated training and evaluation. These solutions are expected to be functional, i.e., capable of training a model and predicting against the evaluation data set with measurement of relevant performance and accuracy metrics. Solutions will be evaluated by a panel of judges across a set of weighted criteria. The top solutions, ranked by points awarded, will have their final rankings determined by incorporating red team evaluation from the red teams from Phase 3.\nPrizes will be awarded to the four best solutions (this will be agnostic of whether they are tackling the health or financial crime use case). Additional \u201cSpecial Recognition\u201d prizes will be awarded from a pool of \u00a360,000.\nOpen to Blue Team participants.\nThis challenge is sponsored by the Centre for Data Ethics and Innovation (CDEI) and Innovate UK\n\n        \nWith additional collaboration from SWIFT and the University of Virginia's Biocomplexity Institute.\nThis prize challenge and its U.S. counterpart have been developed as part of a joint collaboration between the United Kingdom and the United States.",
        "dataset_overview": "With additional collaboration from SWIFT and the University of Virginia's Biocomplexity Institute.\nThis prize challenge and its U.S. counterpart have been developed as part of a joint collaboration between the United Kingdom and the United States.",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/142/uk-federated-learning-3/"
    },
    {
        "competition_overview": "Overview\nAccurate seasonal water supply forecasts are crucial for effective water resources management in the Western United States. This region faces dry conditions and high demand for water, and these forecasts are essential for making informed decisions. They guide everything from water supply management and flood control to hydropower generation and environmental objectives.\nYet, hydrological modeling is a complex task that depends on natural processes marked by inherent uncertainties, such as antecedent streamflow, snowpack accumulation, soil moisture dynamics, and rainfall patterns. To maximize the utility of these forecasts, it's essential to provide not just accurate predictions, but also comprehensive ranges of values that effectively convey the inherent uncertainties in the predictions.\nTask\nThe goal of this challenge is to develop probabilistic forecast models that predict seasonal water supply at the 0.10, 0.50, and 0.90 quantiles. The seasonal water supply refers to the naturalized cumulative streamflow volume over specified seasonal periods. The challenge is occuring over multiple stages: the Hindcast Stage evaluated models on historical data simulating real-time forecasting, the Forecast Stage will run in real-time during the 2024 season, and a final prize stage will ask solvers to submit additional materials for the overall and bonus prizes, like model reports and cross-validation predictions, which will be judged by a panel of technical experts.\nBy improving the accuracy, explainability, and uncertainty characterization of seasonal streamflow forecasts, water resources managers will be better equipped to operate facilities during high flows, mitigate impacts of drought, improve hydropower generation, and meet environmental targets.\nChallenge overview\nTimeline Overview\nThe challenge is occurring over multiple stages, each with its own prizes:\nHindcast Stage: Models were evaluated on historical ground truth data to simulate forecasts that would have been made in the past.\nForecast Stage: Models are be run four times each month from January through July 2024 to issue real-time forecasts for the 2024 season.\nFinal Prize Stage: In a final stage, solvers will submit updated models and additional materials to compete for Overall Prizes and for the Explainability and Communication Bonus Track prizes.\nPrize Overview\nCategory Prize Pool\nOverall $325,000\nHindcast Stage $50,000\nForecast Stage $50,000\nExplainability and Communication Bonus Track $75,000\nTotal $500,000\nForecast Stage Details\nYou are currently in the Forecast Stage for the challenge. In the Forecast Stage, you will submit models to forecast the seasonal water supply for the 2024 season. Models will be run automatically at scheduled times through July 2024.\nHaving your code submission successfully run during the Forecast evaluation period is required to be eligible for the Final Prize Stage (Overall prizes and Explainability bonus prizes).\nForecast Stage Key Dates\nForecast Stage opens, additional training data available December 22, 2023\n    First scheduled trial issue date January 1, 2024\n    Data and runtime change request deadline January 5, 2024\n    Second scheduled trial issue date January 8, 2024\nDeadline for code submission January 11, 2024 at 11:59 pm UTC\nEvaluation period begins January 15, 2024\nForecasts evaluated against ground truth August 14, 2024\nPrizes Breakdown\nPlace Prize Amount\n1st $100,000\n2nd $75,000\n3rd $50,000\n4th $30,000\n5th $20,000\nRegional and Lead Time Bonus Prizes $50,000\nOverall Prizes\nOverall prizes will be awarded based on an evaluation by a panel of technical experts. The evaluation will consider the overall performance across all stages of the challenge as well as a final model report detailing solutions.\nBonus prizes will also be awarded for best performance in forecast subcategories, including regional and long lead time. Additional details about the subcategories will be shared later in the challenge.\nYou must participate in both Hindcast and Forecast Stages by submitting successfully executed code to be eligible for overall prizes.\nPlace Prize Amount\n1st $25,000\n2nd $15,000\n3rd $10,000\nHindcast Stage Prizes\nOctober\u2013December 2023\nTest your model against historical ground truth data! Train your models and submit code. Your code will be executed to perform inference on a held-out test set. Prizes will be awarded based on a combination of your leaderboard performance and an evaluation of your model report by a panel of judges.\nPlace Prize Amount\n1st $25,000\n2nd $15,000\n3rd $10,000\nForecast Stage Prizes\nDecember 2023\u2013July 2024\nFind out how your model performs with live forecasts for the 2024 season! DrivenData will execute your model at a regular cadence from January through July 2024. After July, forecasts will be evaluated against the true water supply measurements and top leaderboard performers will win Forecast Stage prizes.\nYou must participate in the Hindcast Stage by submitting successfully executed code in order to be registered for the Forecast Stage.\nPlace Prize Amount\n1st $25,000\n2nd $20,000\n3rd $15,000\n4th $10,000\n5th $5,000\nForecast Explainability and Communication Prizes\nAugment your model to produce forecast explanation outputs alongside its normal predictions. Understanding how predictors drive forecasts and changes in forecasts is important to operational decision makers. Submissions will be judged by a panel of technical experts to select the winners.\nYou must participate in both Hindcast and Forecast Stages and submit a final model report to be eligible.\nHow to compete\nOnly teams that made successful code submissions in the Hindcast Stage Evaluation Arena are eligible to participate in the Forecast Stage. Eligible teams are automatically registered. If you need to make changes to your team, please contact info@drivendata.org.\nReview the problem description page and read the submission requirements on the code submission format page.\nUpdate your model with the newly released training data on the data download page.\nBundle your trained model and prediction code in a ZIP archive for submission.\nClick \"Submissions\" in the sidebar, and then \"Make new submission\" during the open submission period. You can make as many submissions as you like (subject to a daily limit). Your latest submission at the deadline will be locked in as your official submission.\nKeep an eye out for email notifications of failed jobs during the evaluation period. Submit a fix if you have a failed job.\nThe challenge rules are in place to promote fair competition and useful solutions. If you are ever unsure whether your solution meets the rules, ask the challenge organizers in the competition forum or send an email to info@drivendata.org.\nNote on prize eligibility: The term Competition Sponsor in the rules includes the Bureau of Reclamation as well as all federal employees acting within the scope of their employment and federally-funded researchers acting within the scope of their funding. These parties are not eligible to win a prize in this challenge.\nSponsors\nThis challenge is sponsored by the Bureau of Reclamation\n\nWith support from NASA\n\nAnd with collaborators from the USDA Natural Resources Conservation Service and the U.S. Army Corps of Engineers",
        "dataset_overview": "Approved data sources\nIn this challenge, you must only use approved data sources as feature data (i.e., predictor or input data) for your forecast models.\nSee the Approved data sources page in the Hindcast Stage Development Arena to view the up-to-date list of approved data sources.",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/259/reclamation-water-supply-forecast/"
    },
    {
        "competition_overview": "Overview\nAccurate seasonal water supply forecasts are crucial for effective water resources management in the Western United States. This region faces dry conditions and high demand for water, and these forecasts are essential for making informed decisions. They guide everything from water supply management and flood control to hydropower generation and environmental objectives.\nYet, hydrological modeling is a complex task that depends on natural processes marked by inherent uncertainties, such as antecedent streamflow, snowpack accumulation, soil moisture dynamics, and rainfall patterns. To maximize the utility of these forecasts, it's essential to provide not just accurate predictions, but also comprehensive ranges of values that effectively convey the inherent uncertainties in the predictions.\nTask\nThe goal of this challenge is to develop probabilistic forecast models that predict seasonal water supply at the 0.10, 0.50, and 0.90 quantiles. The seasonal water supply refers to the naturalized cumulative streamflow volume over specified seasonal periods. The challenge is occuring over multiple stages: the Hindcast Stage evaluated models on historical data simulating real-time forecasting, the Forecast Stage will run in real-time during the 2024 season, and a final prize stage will ask solvers to submit additional materials for the overall and bonus prizes, like model reports and cross-validation predictions, which will be judged by a panel of technical experts.\nBy improving the accuracy, explainability, and uncertainty characterization of seasonal streamflow forecasts, water resources managers will be better equipped to operate facilities during high flows, mitigate impacts of drought, improve hydropower generation, and meet environmental targets.\nChallenge overview\nTimeline Overview\nThe challenge is occurring over multiple stages, each with its own prizes:\nHindcast Stage: Models were evaluated on historical ground truth data to simulate forecasts that would have been made in the past.\nForecast Stage: Models are be run four times each month from January through July 2024 to issue real-time forecasts for the 2024 season.\nFinal Prize Stage: In a final stage, solvers will submit updated models and additional materials to compete for Overall Prizes and for the Explainability and Communication Bonus Track prizes.\nPrize Overview\nCategory Prize Pool\nOverall $325,000\nHindcast Stage $50,000\nForecast Stage $50,000\nExplainability and Communication Bonus Track $75,000\nTotal $500,000\nFinal Prize Stage Details\nYou are currently in the Final Prize Stage for the challenge. In the Final Prize Stage, you will submit cross-validation results on the historical data and a final model report. Your submission will be evaluated by a panel of technical experts and compete for the challenge's Overall Prizes.\nThe Final Prize Stage will also include the Forecast Explainability and Communication Bonus Track. The details for the Explainability Bonus Track are still being finalized and will be announced at a later time.\nFinal Stage Key Dates\nFinal Prize Stage opens February 16, 2024\nCross-validation predictions and final model report due March 28, 2024 at 11:59 pm UTC\nExplainability Bonus Track submissions opens April 9, 2024\nExplainability Bonus Track submissions due May 16, 2024 at 11:59 pm UTC\nPrizes Breakdown\nPlace Prize Amount\n1st $100,000\n2nd $75,000\n3rd $50,000\n4th $30,000\n5th $20,000\nBonus Prize: Cascades $10,000\nBonus Prize: Sierra Nevada $10,000\nBonus Prize: Colorado Headwaters $10,000\nBonus Prize: Challenging Basins $10,000\nBonus Prize: Early Lead Time $10,000\nOverall Prizes\nOverall prizes will be awarded based on an evaluation by a panel of technical experts. The evaluation will consider the overall performance across all stages of the challenge as well as a final model report detailing solutions.\nBonus prizes will also be awarded for best performance in forecast subcategories. See the problem description page for additional details.\nYou must participate in both Hindcast and Forecast Stages by submitting successfully executed code to be eligible for overall prizes.\nPlace Prize Amount\n1st $25,000\n2nd $15,000\n3rd $10,000\nHindcast Stage Prizes\nOctober\u2013December 2023\nTest your model against historical ground truth data! Train your models and submit code. Your code will be executed to perform inference on a held-out test set. Prizes will be awarded based on a combination of your leaderboard performance and an evaluation of your model report by a panel of judges.\nPlace Prize Amount\n1st $25,000\n2nd $15,000\n3rd $10,000\nForecast Stage Prizes\nDecember 2023\u2013July 2024\nFind out how your model performs with live forecasts for the 2024 season! DrivenData will execute your model at a regular cadence from January through July 2024. After July, forecasts will be evaluated against the true water supply measurements and top leaderboard performers will win Forecast Stage prizes.\nYou must participate in the Hindcast Stage by submitting successfully executed code in order to be registered for the Forecast Stage.\nPlace Prize Amount\n1st $25,000\n2nd $20,000\n3rd $15,000\n4th $10,000\n5th $5,000\nForecast Explainability and Communication Prizes\nAugment your model to produce forecast explanation outputs alongside its normal predictions. Understanding how predictors drive forecasts and changes in forecasts is important to operational decision makers. Submissions will be judged by a panel of technical experts to select the winners.\nYou must participate in both Hindcast and Forecast Stages and submit a final model report to be eligible.\nHow to compete\nOnly teams that made successful code submissions in the Hindcast and Forecast Stages are eligible to participate in the Final Prize Stage. Eligible teams are automatically registered. If you need to make changes to your team, please contact info@drivendata.org.\nReview the problem description page to refresh yourself on the challenge setup.\nEvaluate your model using cross-validation as instructed on the cross-validation submission format page. Click \"Submissions\" in the sidebar, and then \"Make new submission\" to submit your cross-validation predictions.\nWrite your final model report according to the instructions on the final model report format page. Click \"Report submissions\" in the sidebar to submit your report.\nThe challenge rules are in place to promote fair competition and useful solutions. If you are ever unsure whether your solution meets the rules, ask the challenge organizers in the competition forum or send an email to info@drivendata.org.\nNote on prize eligibility: The term Competition Sponsor in the rules includes the Bureau of Reclamation as well as all federal employees acting within the scope of their employment and federally-funded researchers acting within the scope of their funding. These parties are not eligible to win a prize in this challenge.\nSponsors\nThis challenge is sponsored by the Bureau of Reclamation\n\nWith support from NASA\n\nAnd with collaborators from the USDA Natural Resources Conservation Service and the U.S. Army Corps of Engineers",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/262/reclamation-water-supply-forecast-final/"
    },
    {
        "competition_overview": "Overview\nAccurate seasonal water supply forecasts are crucial for effective water resources management in the Western United States. This region faces dry conditions and high demand for water, and these forecasts are essential for making informed decisions. They guide everything from water supply management and flood control to hydropower generation and environmental objectives.\nYet, hydrological modeling is a complex task that depends on natural processes marked by inherent uncertainties, such as antecedent streamflow, snowpack accumulation, soil moisture dynamics, and rainfall patterns. To maximize the utility of these forecasts, it's essential to provide not just accurate predictions, but also comprehensive ranges of values that effectively convey the inherent uncertainties in the predictions.\nTask\nThe goal of this challenge is to develop probabilistic forecast models that predict naturalized cumulative streamflow volume at the 0.10, 0.50, and 0.90 quantiles. The challenge will occur over multiple stages: the Hindcast Stage evaluated models on historical data simulating real-time forecasting, the Forecast Stage will run in real-time during the 2024 season, and a final prize stage will ask solvers to submit additional materials for the overall and bonus prizes, like model reports and cross-validation prediction, which will be judged by a panel of technical experts.\nBy improving the accuracy, explainability, and uncertainty characterization of seasonal streamflow forecasts, water resources managers will be better equiped to operate facilities for high flows, mitigate impacts of drought, improve hydropower generation, and meet environmental targets.\nChallenge overview\nTimeline Overview\nThe challenge is occurring over multiple stages, each with its own prizes:\nHindcast Stage: Models will be evaluated on historical ground truth data to simulate forecasts that would be made in the past.\nForecast Stage: Models will be run in real-time on a regular cadence from January through July 2024 to issue forecasts for the 2024 season.\nFinal Prize Stage: In a final stage, solvers will submit updated models and additional materials to compete for Overall Prizes and for the Explainability and Communication Bonus Track prizes.\nPrize Overview\nCategory Prize Pool\nOverall $325,000\nHindcast Stage $50,000\nForecast Stage $50,000\nExplainability and Communication Bonus Track $75,000\nTotal $500,000\nHindcast Stage Details\nYou are currently in the Hindcast Stage for the challenge. In the Hindcast Stage, you will develop your models, and they will have their performance evaluated based on historical data.\nThe Hindcast Stage has two arenas for submissions.\nDevelopment Arena: For developing your water supply forecasting models. You will be able to submit hindcast predictions for the historical test set to the challenge platform for evaluation in order to get feedback on your model. Submissions in the Development Arena will not count for prizes.\nEvaluation Arena (YOU ARE HERE): For the official evaluation of your models on the historical test set. You will submit model code for remote execution as well as a model report. Your test set performance and model report will be evaluated by a panel of technical experts to determine Hindcast Stage prizes.\nMaking a successful code submission in the Hindcast Evaluation Arena is required to participate in later stages of the challenge, where additional prizes may be won.\nHindcast Stage Key Dates\nChallenge launch October 17, 2023\nHindcast Evaluation Arena opens for test submissions November 29, 2023\nData source request deadline December 5, 2023\nRuntime dependency request deadline December 7, 2023\nHindcast Evaluation Arena full submissions available December 11, 2023\nHindcast code execution submission deadline December 21, 2023\nHindcast model report deadline January 26, 2024\nPrizes Breakdown\nPlace Prize Amount\n1st $100,000\n2nd $75,000\n3rd $50,000\n4th $30,000\n5th $20,000\nRegional and Lead Time Bonus Prizes $50,000\nOverall Prizes\nOverall prizes will be awarded based on an evaluation by a panel of technical experts. The evaluation will consider the overall performance across all stages of the challenge as well as a final model report detailing solutions.\nBonus prizes will also be awarded for best performance in forecast subcategories, including regional and long lead time. Additional details about the subcategories will be shared later in the challenge.\nYou must participate in both Hindcast and Forecast Stages by submitting successfully executed code to be eligible for overall prizes.\nPlace Prize Amount\n1st $25,000\n2nd $15,000\n3rd $10,000\nHindcast Stage Prizes\nOctober\u2013December 2023\nTest your model against historical ground truth data! Train your models and submit code. Your code will be executed to perform inference on a held-out test set. Prizes will be awarded based on a combination of your leaderboard performance and an evaluation of your model report by a panel of judges.\nPlace Prize Amount\n1st $25,000\n2nd $15,000\n3rd $10,000\nForecast Stage Prizes\nDecember 2023\u2013July 2024\nFind out how your model performs with live forecasts for the 2024 season! DrivenData will execute your model at a regular cadence from January through July 2024. After July, forecasts will be evaluated against the true water supply measurements and top leaderboard performers will win Forecast Stage prizes.\nYou must participate in the Hindcast Stage by submitting successfully executed code to be eligible.\nPlace Prize Amount\n1st $25,000\n2nd $20,000\n3rd $15,000\n4th $10,000\n5th $5,000\nForecast Explainability and Communication Prizes\nAugment your model to produce forecast explanation outputs alongside its normal predictions. Understanding how predictors drive forecasts and changes in forecasts is important to operational decision makers. Submissions will be judged by a panel of technical experts to select the winners.\nYou must participate in both Hindcast and Forecast Stages and submit a final model report to be eligible.\nHow to compete\nClick the \"Compete!\" button in the sidebar to enroll in the competition.\nGet familiar with the problem through the overview and problem description. You might also want to reference additional resources available on the About page.\nDownload the training ground truth data from the Data download page.\nCreate and train your own model.\nBundle your trained model and prediction code for evaluation in our cloud runtime. See the Code submission format page for more detail.\nClick \u201cSubmission\u201d in the sidebar, and then \u201cMake new submission\u201d.\nTo be considered for prizes in the Hindcast Stage, don't forget to submit a model report detailing your methodology. See the Report submission format page for more detail. Once complete, click on \"Report Submission\" in the sidebar, upload your PDF, and then click \"Submit\". You're in!\nThe challenge rules are in place to promote fair competition and useful solutions. If you are ever unsure whether your solution meets the rules, ask the challenge organizers in the competition forum or send an email to info@drivendata.org.\nNote on prize eligibility: The term Competition Sponsor in the rules includes the Bureau of Reclamation as well as all federal employees acting within the scope of their employment and federally-funded researchers acting within the scope of their funding. These parties are not eligible to win a prize in this challenge.\nSponsors\nThis challenge is sponsored by the Bureau of Reclamation\n\nWith support from NASA\n\nAnd with collaborators from the USDA Natural Resources Conservation Service and the U.S. Army Corps of Engineers",
        "dataset_overview": "Approved data sources\nIn this challenge, you must only use approved data sources as feature data (i.e., predictor or input data) for your forecast models.\nSee the Approved data sources page in the Development Arena to view the up-to-date list of approved data sources.",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/257/reclamation-water-supply-forecast-hindcast/"
    },
    {
        "competition_overview": "Overview\nAccurate seasonal water supply forecasts are crucial for effective water resources management in the Western United States. This region faces dry conditions and high demand for water, and these forecasts are essential for making informed decisions. They guide everything from water supply management and flood control to hydropower generation and environmental objectives.\nYet, hydrological modeling is a complex task that depends on natural processes marked by inherent uncertainties, such as antecedent streamflow, snowpack accumulation, soil moisture dynamics, and rainfall patterns. To maximize the utility of these forecasts, it's essential to provide not just accurate predictions, but also comprehensive ranges of values that effectively convey the inherent uncertainties in the predictions.\nTask\nThe goal of this challenge is to develop probabilistic forecast models that predict naturalized cumulative streamflow volume at the 0.10, 0.50, and 0.90 quantiles. The challenge will occur over multiple stages: the Hindcast Stage evaluated models on historical data simulating real-time forecasting, the Forecast Stage will run in real-time during the 2024 season, and a final prize stage will ask solvers to submit additional materials for the overall and bonus prizes, like model reports and cross-validation predictions, which will be judged by a panel of technical experts.\nBy improving the accuracy, explainability, and uncertainty characterization of seasonal streamflow forecasts, water resources managers will be better equiped to operate facilities for high flows, mitigate impacts of drought, improve hydropower generation, and meet environmental targets.\nChallenge overview\nTimeline Overview\nThe challenge is occurring over multiple stages, each with its own prizes:\nHindcast Stage: Models will be evaluated on historical ground truth data to simulate forecasts that would be made in the past.\nForecast Stage: Models will be run in real-time on a regular cadence from January through July 2024 to issue forecasts for the 2024 season.\nFinal Prize Stage: In a final stage, solvers will submit updated models and additional materials to compete for Overall Prizes and for the Explainability and Communication Bonus Track prizes.\nPrize Overview\nCategory Prize Pool\nOverall $325,000\nHindcast Stage $50,000\nForecast Stage $50,000\nExplainability and Communication Bonus Track $75,000\nTotal $500,000\nHindcast Stage Details\nYou are currently in the Hindcast Stage for the challenge. In the Hindcast Stage, you will develop your models, and they will have their performance evaluated based on historical data.\nThe Hindcast Stage will have two arenas for submissions.\nDevelopment Arena (YOU ARE HERE): For developing your water supply forecasting models. You will be able to submit hindcast predictions for the historical test set to the challenge platform for evaluation in order to get feedback on your model. Submissions in the Development Arena will not count for prizes.\nEvaluation Arena: For the official evaluation of your models on the historical test set. You will submit model code for remote execution as well as a model report. Your test set performance and model report will be evaluated by a panel of technical experts to determine Hindcast Stage prizes.\nHindcast Stage Key Dates\nChallenge launch October 17, 2023\nHindcast Evaluation Arena opens for test submissions November 29, 2023\nData source request deadline December 5, 2023\nRuntime dependency request deadline December 7, 2023\nHindcast Evaluation Arena full submissions available December 11, 2023\nHindcast code execution submission deadline December 21, 2023\nHindcast model report deadline January 26, 2024\nUpdated on November 29, 2023. See announcements: 1, 2.\nPrizes Breakdown\nPlace Prize Amount\n1st $100,000\n2nd $75,000\n3rd $50,000\n4th $30,000\n5th $20,000\nRegional and Lead Time Bonus Prizes $50,000\nOverall Prizes\nOverall prizes will be awarded based on an evaluation by a panel of technical experts. The evaluation will consider the overall performance across all stages of the challenge as well as a final model report detailing solutions.\nBonus prizes will also be awarded for best performance in forecast subcategories, including regional and long lead time. Additional details about the subcategories will be shared later in the challenge.\nYou must participate in both Hindcast and Forecast Stages to be eligible for overall prizes.\nPlace Prize Amount\n1st $25,000\n2nd $15,000\n3rd $10,000\nHindcast Stage Prizes\nOctober\u2013December 2023\nTest your model against historical ground truth data! Train your models and submit code. Your code will be executed to perform inference on a held-out test set. Prizes will be awarded based on a combination of your leaderboard performance and an evaluation of your model report by a panel of judges.\nPlace Prize Amount\n1st $25,000\n2nd $15,000\n3rd $10,000\nForecast Stage Prizes\nDecember 2023\u2013July 2024\nFind out how your model performs with live forecasts for the 2024 season! DrivenData will execute your model at a regular cadence from January through July 2024. After July, forecasts will be evaluated against the true water supply measurements and top leaderboard performers will win Forecast Stage prizes.\nYou must participate in the Hindcast Stage to be eligible.\nPlace Prize Amount\n1st $25,000\n2nd $20,000\n3rd $15,000\n4th $10,000\n5th $5,000\nForecast Explainability and Communication Prizes\nAugment your model to produce forecast explanation outputs alongside its normal predictions. Understanding how predictors drive forecasts and changes in forecasts is important to operational decision makers. Submissions will be judged by panel of technical experts to select the winners.\nYou must participate in both Hindcast and Forecast Stages and submit a final model report to be eligible.\nHow to compete\nClick the \"Compete!\" button in the sidebar to enroll in the competition.\nGet familiar with the problem through the overview and problem description. You might also want to reference additional resources available on the about page.\nDownload the training ground truth data from the data download page. Coming soon!\nCreate and train your own model. Keep an eye out for additional resources to help you get started.\nUse your model to generate predictions that match the submission format.\nClick \u201cSubmit\u201d in the sidebar, and then \u201cMake new submission\u201d. You\u2019re in!\nThe challenge rules are in place to promote fair competition and useful solutions. If you are ever unsure whether your solution meets the rules, ask the challenge organizers in the competition forum or send an email to info@drivendata.org.\nNote on prize eligibility: The term Competition Sponsor in the rules includes the Bureau of Reclamation as well as all federal employees acting within the scope of their employment and federally-funded researchers acting within the scope of their funding. These parties are not eligible to win a prize in this challenge.\nSponsors\nThis challenge is sponsored by the Bureau of Reclamation\n\nWith support from NASA\n\nAnd with collaborators from the USDA Natural Resources Conservation Service and the U.S. Army Corps of Engineers",
        "dataset_overview": "Approved data sources\nLast updated: December 14, 2023\nThis page documents all approved data sources for use as feature data (i.e., predictor or input data) for your forecast models. Valid submissions must use only approved data sources as features. If you need additional data from an already approved source (e.g., from a longer time range), or if you would like to use data from another source, please refer to the section on requesting approval for additional data for more details.\nAs a reminder, for code execution in the Hindcast and Forecast Stage evaluations, you will be required to use copies of the feature data available within the runtime environment unless otherwise specified. For each data source, the following will be documented:\nApproved data source \u2014 specific source that approved data will be downloaded from\nHindcast data available \u2014 details about what specific parameters of data will be available in code execution, if applicable.\nDirect API access permitted \u2014 only if direct API access to pull data during test set inference is permitted for this data source\nData download code \u2014 the code used by DrivenData to download data for the code execution runtime.\nSample data reading code \u2014 sample code that can be used to load the data. This will be made available as part of an installed package wsfr-read within the runtime environment.\nAdditional data sources, availability details for code execution, and code will be updated as they get approved by challenge organizers. Changes will be documented in the announcements.\nAntecedent streamflow\nNRCS and RFCs monthly naturalized flow\nDescription\nNaturalized flow at the forecast sites from NRCS and the RFCs. These are the past monthly time series observations for the forecast target variable. See the problem description page for additional discussion.\nApproved data source\nSee test_monthly_naturalized_flow.csv on the data download page.\nHindcast data available\nPreceding October 1 through May or June for each forecast year in Hindcast test set, depending on the site's forecast season. Monthly data is only available for 23 of the 26 forecast sites.\nUSGS streamflow\nDescription\nDaily observed streamflow measurements from the U.S. Geological Survey (USGS) recorded at USGS streamgages. These measurements represent actual observed flow of water at specific locations, and not the naturalized flow being forecasted. Solutions should not be attempting to model the adjustments for calculating naturalized flow, as these are impacted by water management operations that may in general be influenced by forecasts. However, observed streamflow at the forecast sites or at other locations may still be useful predictors of the overall drainage basin condition. The runtime environment will include daily measurements for the specific USGS streamgages located at the forecast sites (available for 25 of the 26 sites, see provided metadata.csv), but you are permitted to use data from any other location by directly accessing the USGS Water Services APIs. View additional details via the USGS Water Services API documentation.\nApproved data source\nUSGS Water Services: (https://waterservices.usgs.gov/nwis)\nHindcast data available\nPreceding October 1 through July 21 for each forecast year in Hindcast test set for gages at 25 of the 26 forecast sites.\nData download code (for gages at forecast sites)\nVia runtime repository: drivendataorg/water-supply-forecast-rodeo-runtime\nDirect API access permitted\nFor other locations available from USGS, you are permitted to directly query the USGS Water Services APIs for data from other locations.\nSample data reading code\nVia runtime repository: drivendataorg/water-supply-forecast-rodeo-runtime\nUSBR reservoir inflow\nDescription\nMetered or calculated data on inflow into U.S. Bureau of Reclamation (USBR) reservoirs. These inflow measurements represent actual observed flow of water, and not the naturalized flow being forecasted. Solutions should not be attempting to model the adjustments for calculating naturalized flow, as these are impacted by water management operations that may in general be influenced by forecasts. However, observed flow at some locations may still be useful predictors of the overall drainage basin condition. View additional details via the the USBR RISE API documentation.\nApproved data source\nUSBR RISE API: (https://data.usbr.gov/rise/api)\nDirect API access permitted\nYou are permitted to directly query reservoir inflow measurements from the USBR RISE API\nSnowpack\nNRCS SNOTEL\nDescription\nThe Snow Telemetry (SNOTEL) network is composed of over 900 automated data collection sites located in remote, high-elevation mountain watersheds in the Western U.S. They are used to monitor snowpack, precipitation, temperature, and other climatic conditions. You can read more about the SNOTEL network here. You can read more about the NRCS Air-Water Database (AWDB) Web Service here.\nApproved data source\nNRCS AWDB Web Service: SOAP (https://wcc.sc.egov.usda.gov/awdbWebService/services?WSDL), REST (https://wcc.sc.egov.usda.gov/awdbRestApi/services)\nHindcast data available\nPreceding October 1 through July 21 for each forecast year in Hindcast test set for stations with 40 miles of the forecast site drainage basins\nData download code\nVia runtime repository: drivendataorg/water-supply-forecast-rodeo-runtime\nDirect API access permitted\nFor other SNOTEL stations whose data is not downloaded, you are permitted to directly query the NRCS AWDB APIs for data.\nCDEC Snow Sensor Network\nDescription\nThe California Data Exchange Center (CDEC) facilitates the collection, storage, and exchange of hydrologic and climate information to support real-time flood management and water supply needs in California. CDEC operates snowpack monitoring stations similar to SNOTEL within California. Station metadata for snow monitoring stations is available on the data download page (cdec_snow_stations.csv).\nApproved data source\nCDEC APIs (https://cdec.water.ca.gov/)\nHindcast data available\nPreceding October 1 through July 21 for each forecast year in Hindcast test set for stations with 40 miles of the forecast site drainage basins\nData download code\nVia runtime repository: drivendataorg/water-supply-forecast-rodeo-runtime (requires cdec_snow_stations.csv from data download page)\nDirect API access permitted\nFor other CDEC stations whose data is not downloaded, you are permitted to directly query the CDEC APIs for data.\nSNODAS\nDescription\nThe SNOw Data Assimilation System (SNODAS) estimates snow cover, snow water equivalent, and other snow parameters to support hydrologic modeling. It integrates observations from satellite and airborn platforms and from ground stations with physics models. This is a daily 1 km by 1 km data product that covers the contiguous U.S. You can read more about SNODAS here.\nApproved data source\nNational Snow and Ice Data Center (NSIDC) (masked)\nHindcast data available\nPreceding October 1 through July 21 for each forecast year in Hindcast test set\nData download code\nVia runtime repository: drivendataorg/water-supply-forecast-rodeo-runtime\nUA/SWANN\nDescription\nThe Snow Water Artificial Neural Network (SWANN) system developed at the University of Arizona produces a gridded snow water equivalent data product by assimilating SNOTEL ground-based observations with temperature and precipitation data. This daily data source is available as two products: 4 km gridded data over the contiguous United States, and spatially averaged over USGS HUC regions.\nApproved data source\nUniversity of Arizona: 4 km gridded data (https://climate.arizona.edu/data/UA_SWE/); HUC spatially averaged data (https://climate.arizona.edu/snowview/csv/Download/Watersheds/)\nDirect API access permitted\nYou are permitted to directly download data from the University of Arizona file servers\nMODIS Snow Cover\nDescription\nThe Moderate Resolution Imaging Spectroradiometer (MODIS) is an instrument on the Terra and Aqua spacecraft. Snow cover gridded data products with 500-meter resolution are available as daily composites (MOD10A1-061, MYD10A1-061) and 8-day composites (MOD10A2-061, MYD10A2-061). For more information, see the product catalog entries (daily; 8-day) and example notebooks (daily; 8-day) from the Microsoft Planetary Computer.\nApproved source\nMicrosoft Planetary Computer (https://planetarycomputer.microsoft.com/api/stac/v1/collections/modis-10A1-061; https://planetarycomputer.microsoft.com/api/stac/v1/collections/modis-10A2-061)\nDirect API access permitted\nYou are permitted to directly download data via the Planetary Computer API\nWeather and climate products\nObserved and forecasted weather and climate data products can provide relevant information on the environmental conditions that affect streamflow.\nRCC-ACIS\nDescription\nThe Applied Climate Information System (ACIS), maintained by NOAA Regional Climate Centers (RCCs), provides access to historical and near real-time climate observation data from a variety of sources in order to support operational users with one high quality system. Climate data products like daily temperature and precipitation observations may be especially relevant to water supply forecasting. You can read more about the ACIS Web Services APIs here.\nApproved data source\nACIS Web Services (https://data.rcc-acis.org)\nDirect API access permitted\nYou are permitted to directly query any API endpoint from ACIS Web Services\nCPC Seasonal Outlooks\nDescription\nThe Climate Prediction Center (CPC), a part of the National Weather Service, issues seasonal temperature and precipitation forecasts with up to 13 months lead time. These forecasts are issued for 102 geographical regions called \"climate divisions\" (also called \"forecast divisions\") defined by the CPC. You can read more about the forecasts here. Geospatial vector data for the climate divisions is available on the data download page (cpc_climate_divisions.gpkg) and can be joined to downloaded data on the CD identifier column.\nApproved data source\nCPC Outlook Archive\nData download code\nVia runtime repository: drivendataorg/water-supply-forecast-rodeo-runtime\nSample data reading code\nVia runtime repository: drivendataorg/water-supply-forecast-rodeo-runtime\nSeasonal meteorological forecasts from Copernicus\nDescription\nA multi-system seasonal forecast service that integrates global seasonal (long-range) gridded forecast products from several Europrean forecast centers. An overview of this dataset and additional documentation from the Copernicus Climate Change Service is available here. Read more about the CDS API and cdsapi Python client here.\nApproved data source\nCopernicus Climate Date Store\nDirect API access permitted\nYou are permitted to directly query data needed using the cdsapi Python client\nSeasonal fire danger indices forecasts from CEMS\nDescription\nLong-range forecasts of global daily fire danger produced by the Copernicus Emergency Management Service (CEMS) using the Global ECMWF Fire Forecast (GEFF) model. Daily forecasts are made with a lead time of up to 216 days (approximately 7 months). This dataset includes many different fire danger indices used by different countries. An overview of this dataset and additional documentation from the Copernicus Climate Change Service is available here. Read more about the CDS API and cdsapi Python client here.\nApproved data source\nCopernicus Climate Date Store\nDirect API access permitted\nYou are permitted to directly query data needed using the cdsapi Python client\nERA5-Land and ERA5-Land-T reanalysis\nUpdated on December 13, 2023 to include monthly averaged data as approved.\nDescription\nERA5-Land is a global reanalysis dataset of land variables. This data source includes both ERA5-Land, which is published with a 2\u20133 month lag, and ERA5-Land-T, which is a non-checked version published in near-real-time. This data is available at hourly resolution and as monthly averages. An overview of this dataset and additional documentation from the Copernicus Climate Change Service is available here: hourly, monthly averaged. Read more about the CDS API and cdsapi Python client here.\nApproved data source\nCopernicus Climate Date Store: hourly, monthly averaged\nDirect API access permitted\nYou are permitted to directly query data needed using the cdsapi Python client\nNLDAS-2 forcing data\nDescription\nNorth American Land Data Assimilation System (NLDAS) uses numerical physics models integrated with ground- and space-based observing systems to produce fields of water and energy states and fluxes. The forcing data includes a variety of meteorological variables that are inputs to this model, such as precipitation, wind speed, average air temperature, incoming radiation, and surface pressure. You can read more about the NLDAS-2 forcing dataset here. You can view the datasets from the Goddard Earth Sciences Data and Information Services Center (GES DISC) here. A Python client PyNLDAS2 is available.\nApproved data source\nGES DISC (https://hydro1.gesdisc.eosdis.nasa.gov/)\nDirect API access permitted\nYou are permitted to directly download data from GES DISC\nNCEP/NCAR Reanalysis 1\nDescription\nThe NCEP/NCAR Reanalysis 1 is a reanalysis dataset. It is a gridded data product of atmospheric and land variables produced by data assimilation of numerical weather models with observational climate data. You can read more about the dataset here.\nApproved data source\nNOAA PSL Downloads Server (https://downloads.psl.noaa.gov/Datasets/ncep.reanalysis) or THREDDS Server (https://psl.noaa.gov/thredds/catalog/Datasets/ncep.reanalysis)\nDirect API access permitted\nYou are permitted to directly download data from NOAA PSL\nUSGS SSEBop Evapotranspiration\nDescription\nEvapotranspiration (ET) is the movement of water into the atmosphere that combines evaporation and transpiration. USGS provides ET data products based on remote sensing data and an operational Simplified Surface Energy Balance (SSEBop) model. The v5 data product uses MODIS thermal imagery and covers 2003\u20132022, while the v6 data product uses VIIRS thermal imagery and covers 2012 to present. Since neither of the versions cover the full period of the challenge, you will need to use both. Note that you will need to adjust the v5 MODIS values to be comparable to the v6 VIIRS values, as they are derived from different remote sensing data and are not the same. Please document your adjustment methodology in the model report. Both versions are available as dekadal (10-day) and monthly products. You can read more about these products from USGS (v5 MODIS; v6 VIIRS).\nApproved data source\nUSGS FEWS Net file servers (https://edcintl.cr.usgs.gov): v5 MODIS dekadal, v5 MODIS monthly, v6 VIIRS dekadal, v6 VIIRS monthly\nDirect API access permitted\nYou are permitted to directly download data from USGS FEWS Net file servers\nDrought and moisture conditions\nPalmer Drought Severity Index (PDSI) from gridMET\nDescription\nThe Palmer Drought Severity Index (PDSI) is a measure of drought based on a soil moisture model applied to precipitation and temperature data. This particular data source is a gridded pentad (every 5 days) PDSI product produced from gridMET meteorological data and USDA STATSGO soil data. You can read more about this PDSI data product here.\nApproved data source\nUniversity of Idaho Northwest Knowledge Network (NetCDF format\u2014see \"NetcdfSubset\")\nHindcast data available\nPreceding October 1 through July 21 for each forecast year in Hindcast test set\nData download code\nVia runtime repository: drivendataorg/water-supply-forecast-rodeo-runtime\nGRACE-based Soil Moisture and Groundwater Drought Indicators\nSoil moisture and groundwater drought indicators derived from GRACE-FO satellite data. GRACE-FO is a satellite mission that maps the Earth's gravitational field, a measurement of spatial mass concentration. This data product incorporates GRACE-FO observations with other data and a numerical model. There are three indicators: surface soil moisture (top 2 cm of soil), root zone soil moisture (top 1 m of soil), and shallow groundwater. You can read more about the data here.\nApproved data source\nNational Drought Mitigation Center (CONUS, NetCDF4 format)\nHindcast data available\nPreceding October 1 through July 21 for each forecast year in Hindcast test set\nData download code\nVia runtime repository: drivendataorg/water-supply-forecast-rodeo-runtime\nClimate teleconnection indices\nTeleconnection refers to climate patterns or anomalies in one region of the world that are correlated with and often influence weather patterns in distant parts of the globe.\nOceanic Ni\u00f1o Index (ONI)\nDescription\n3-month running average of sea surface temperature anomalies in the Ni\u00f1o 3.4 region. This measure is used as an indicator of the El Ni\u00f1o\u2013Southern Oscillation phenomenon. Warm (El Ni\u00f1o) and cold (La Ni\u00f1a) phases are defined as a minimum of five consecutive ONI values surpassing a threshold of +/- 0.5\u00b0C. You can read more about the ONI here.\nApproved data source\nNational Centers for Environmental Information (NCEI)\nData download code\nVia runtime repository: drivendataorg/water-supply-forecast-rodeo-runtime\nSample data reading code\nVia runtime repository: drivendataorg/water-supply-forecast-rodeo-runtime\nNi\u00f1o Regions Sea Surface Temperatures\nDescription\nMonthly sea surface temperature anomalies in Ni\u00f1o regions. These are additional measures related to the El Ni\u00f1o\u2013Southern Oscillation phenomenon. You can read more about these measures here.\nApproved source\nNational Centers for Environmental Information (NCEI)\nData download code\nVia runtime repository: drivendataorg/water-supply-forecast-rodeo-runtime\nSample data reading code\nVia runtime repository: drivendataorg/water-supply-forecast-rodeo-runtime\nSouthern Oscillation Index (SOI)\nDescription\nStandardized sea level pressure differences between Tahiti and Darwin, Australia. This measure is used as an indicator of the El Ni\u00f1o\u2013Southern Oscillation phenomenon. The index is negative when there is below-normal air pressure at Tahiti and above-normal air pressure at Darwin, and vice versa when the index is positive. Periods of negative values coincide with El Ni\u00f1o and positive values coincide with La Ni\u00f1a. You can read more about the SOI here.\nApproved source\nNational Centers for Environmental Information (NCEI)\nData download code\nVia runtime repository: drivendataorg/water-supply-forecast-rodeo-runtime\nSample data reading code\nVia runtime repository: drivendataorg/water-supply-forecast-rodeo-runtime\nMadden-Julian Oscillation (MJO) Pentad Indices\nDescription\nThe Madden-Julian Oscillation is an eastward moving weather pattern with a typical period of 30 to 60 days. The pentad indices are normalized projections of pentad velocity potential on patterns from extended empirical orthogonal function analysis on historical reference data from 1979 to 2000. You can read more about the Madden-Julian Oscillation from Climate.gov, and the methodology for the indices from the CPC.\nApproved source\nClimate Prediction Center (CPC)\nData download code\nVia runtime repository: drivendataorg/water-supply-forecast-rodeo-runtime\nSample data reading code\nVia runtime repository: drivendataorg/water-supply-forecast-rodeo-runtime\nPacific North American (PNA) Index\nDescription\nThe Pacific North American (PNA) pattern is a large-scale weather in the atmospheric circulation over the Pacific Ocean and North America. The index is the projection of the air pressure field on a particular mode from empirical orthogonal function analysis of reference data from 1950 to 2000. You can read more about the PNA pattern from Climate.gov and from NCEI.\nApproved source\nClimate Prediction Center (CPC)\nData download code\nVia runtime repository: drivendataorg/water-supply-forecast-rodeo-runtime\nSample data reading code\nVia runtime repository: drivendataorg/water-supply-forecast-rodeo-runtime\nPacific Decadal Oscillation (PDO) Index\nDescription\nThe Pacific Decadal Oscillation (PDO) is a climate pattern of the Pacific Ocean that is characterized by warm and cool phases in sea surface temperature. It is similar to the El Ni\u00f1o\u2013Southern Oscillation but has a longer time scale, with phases that can persist for 20 to 30 years. The index is calculated from projecting sea surface temperatures on the first principal component of reference data from 1900 to 1993. You can read more about the PDO index from JISAO or from NCEI.\nApproved source\nNational Centers for Environmental Information (NCEI)\nData download code\nVia runtime repository: drivendataorg/water-supply-forecast-rodeo-runtime\nSample data reading code\nVia runtime repository: drivendataorg/water-supply-forecast-rodeo-runtime\nVegetation conditions\nMODIS Vegetation Indices\nDescription\nThe Moderate Resolution Imaging Spectroradiometer (MODIS) is an instrument on the Terra and Aqua spacecraft. The Vegetation Indices 16-day data product includes global Normalized Difference Vegetation Index (NDVI) and Enhanced Vegetation Index (EVI) measures of vegetation. For more information, see this product's catalog entry and example notebook from the Microsoft Planetary Computer.\nApproved source\nMicrosoft Planetary Computer (https://planetarycomputer.microsoft.com/api/stac/v1/collections/modis-13A1-061)\nHindcast data available\nPreceding October 1 through July 21 for each forecast year in Hindcast test set for STAC items that spatially intersect with the forecast site drainage basins\nData download code\nVia runtime repository: drivendataorg/water-supply-forecast-rodeo-runtime\nDirect API access permitted\nFor additional items, you are permitted to directly download via the Planetary Computer API\nLand and Elevation\nCopernicus DEM GLO-90\nDescription\nThe Copernicus Digital Elevation Model (DEM) is an elevation dataset that represents the surface of the Earth, including buildings, infrastructure, and vegetation. The data comes from the TanDEM-X mission. The GLO-90 data product has a horizontal resolution of approximately 90 meters. For more information, see this product's catalog entry and example notebook from the Microsoft Planetary Computer.\nApproved source\nMicrosoft Planetary Computer (https://planetarycomputer.microsoft.com/api/stac/v1/collections/cop-dem-glo-90)\nDirect API access permitted\nYou are permitted to directly download data via the Planetary Computer API\nNational Land Cover Database (NLCD) Urban Imperviousness\nDescription\nThe National Land Cover Database (NLCD) is a set of data products on land cover and land cover change for the contiguous United States published by USGS and MRLC. Urban imperviousness refers to surfaces which are water resistant. The 2021 NLCD urban imperviousness product is approved for use in the challenge. You can read more about this product here. Important: NLCD is an update-based data product with releases corresponding to the year of the source imagery: 2001, 2006, 2011, 2016, 2019, and 2021. NLCD releases take many years to prepare, and the actual release date is typically several years later than the source imagery year. For example, the 2011 product was not available until 2014-03-31. In order to reflect operational conditions when performing inference, you must use only the latest epoch available based on release date. For example, if making predictions for the 2013-01-01 issue date, the latest release available at that time was the 2006 product (released on 2011-02-16), and so the latest available epoch would have been 2006. A CSV file nlcd_release_dates.csv containing release dates for each version is available on the data download page.\nApproved source\nMRLC (19.54 GB, ZIP archive)\nHindcast data available\nThe 2021 NLCD imperviousness ZIP archive (NLCD_impervious_2021_release_all_files_20230630.zip) and companion nlcd_release_dates.csv are directly available in the mounted data drive\nBasinATLAS Basin Attributes\nDescription\nThe BasinATLAS dataset, a part of the HydroSHEDS database, is a collection of hydrological, physiographic, climate, land cover, geological, and anthropogenic variables describing sub-basins globally. Version 10 of the BasinATLAS dataset is approved for use in the challenge. You can read more about BasinATLAS here and see the detailed catalog of variables here. Important: BasinATLAS variables are derived from a diverse set of source datasets each with different time coverage. For any BasinATLAS variables used, you should clearly document in your model report the source data provenance and justify why the variable does not leverage future data in an unrealistic way for operational use or leak information about test set.\nApproved source\nHydroSHEDS via Figshare (2.7 GB, geodatabase format)\nHindcast data available\nBasinATLAS v10 compressed geodatabase (BasinATLAS_Data_v10.gdb.zip) is directly available in the mounted data drive\nDon't see a data source you want to use? Please see the documentation on requesting approval for additional data.",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/254/reclamation-water-supply-forecast-dev/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/90/competition-reclamation-snow-water-eval/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/86/competition-reclamation-snow-water-dev/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/79/competition-image-similarity-1-dev/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/80/competition-image-similarity-2-dev/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/84/competition-image-similarity-1-final/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/85/competition-image-similarity-2-final/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/68/competition-differential-privacy-maps-1/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/69/deid2-sprint-1-prescreened/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/74/competition-differential-privacy-maps-2/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/75/deid2-sprint-2-prescreened/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/76/competition-differential-privacy-maps-3/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/77/deid2-sprint-3-prescreened/"
    },
    {
        "competition_overview": "Welcome to the Video Similarity Challenge! In this competition, you will build models that detect whether a query video contains a possibly manipulated clip from one or more videos in a reference set.\nThe ability to identify and track content on social media platforms, called content tracing, is crucial to the experience of users on these platforms. Previously, Meta AI and DrivenData hosted the Image Similarity Challenge, in which participants developed state-of-the-art models capable of accurately detecting when an image was derived from a known image. The motivation for detecting copies and manipulations with videos is similar\u2014enforcing copyright protections, identifying misinformation, and removing violent or objectionable content.\nManual content moderation has challenges scaling to meet the large volume of content on platforms like Instagram and Facebook, where tens of thousands of hours of video are uploaded each day. Accurate and performant algorithms are critical in flagging and removing inappropriate content. This competition allows you to test your skills in building a key part of that content tracing system, and in so doing contribute to making social media more trustworthy and safe for the people who use it.",
        "dataset_overview": "Video Similarity Dataset License Agreement\nThis Video Similarity Dataset License Agreement (\u201cAgreement\u201d) contains the terms and conditions that govern your access to and use of the Video Similarity Dataset (as defined below), you (as defined below). You may not use the Video Similarity Dataset if you do not accept this Agreement. This Agreement is effective upon the earlier of the date that you first access the Video Similarity Dataset or accept this Agreement (\u201cEffective Date\u201d), and is entered into by and between Meta Platforms, Inc. (\u201cMeta\u201d), and you, or your employer or other entity (if you are entering into this agreement on behalf of your employer or other entity) (\u201cParticipant\u201d or \u201cyou\u201d). The Video Similarity Dataset includes the videos, software, tools, documentation or other materials created, derived or licensed by Meta and provided to you for use in connection with the purposes set forth below. By clicking to accept, accessing the Video Similarity Dataset, or both, you hereby agree to the terms of the Agreement. If you are agreeing to be bound by the Agreement on behalf of your employer or other entity, you represent and warrant to Meta that you have full legal authority to bind your employer or such entity to this Agreement. If you do not have the requisite authority, you may not accept the Agreement or access the Video Similarity Dataset on behalf of your employer or other entity.\n(1) Subject to Participant\u2019s compliance with the terms and conditions of this Agreement, Meta hereby grants to Participant, a limited, non-exclusive, non-transferable, non-sublicensable, revocable license to: (a) use the Video Similarity Dataset to research software, algorithms, machine learning models, techniques and technologies designed to detect manipulated media, images, audio and videos (the \u201cPurpose\u201d); (b) distribute and reproduce up to one hundred (100) videos from the Video Similarity Dataset per Participant for research or academic publication related to the Purpose. If you include videos from the Video Similarity Dataset in a research or academic publication, then you shall include proper attribution to the videos as detailed for each video in the README file.\n(2) Subject to Participant\u2019s compliance with the terms and conditions of this Agreement, Participant retains its intellectual property rights in and to all algorithms, software, machine learning models, techniques and technologies developed or otherwise derived by Participant from the use of the Video Similarity Dataset. Such algorithms, software, machine learning models, techniques and technologies may be used for academic and commercial purposes.\n(3) As between Meta and Participant, Meta retains all intellectual property rights in and to the Video Similarity Dataset, except where indicated otherwise. All rights not expressly granted under this Agreement by Meta are reserved.\n(4) At any time, Meta may require Participant to delete all copies of the Video Similarity Dataset (in whole or in part) in Participant\u2019s possession and control. Participant will promptly comply with any and all such requests. Upon Meta\u2019s request, Participant shall provide Meta with written confirmation of Participant\u2019s compliance with such requirement.\n(5) In the event that (a) Meta reasonably believes (as determined at Meta\u2019s sole discretion) that you are or are likely to be in violation of the terms of this Agreement or (b) of any claim, investigation, or any legal, administrative, regulatory, judicial action or proceeding by any third party (including, without limitation, a governmental or regulatory agency) involving Meta or its affiliates or you or your affiliates in connection with this Agreement or subject matter thereof, then Meta itself or through a designee may audit your use, storage and/or distribution of the Video Similarity Dataset, including, without limitation, any and all records, files associated with the Video Similarity Dataset, and this Agreement. Solely with respect to (a), you may elect to have Meta have such audit performed by an independent registered public accounting firm of nationally recognized standing approved by Meta provided that in such case you hereby agree to pay all costs and expenses of such audit. You hereby agree to cooperate with any such audit, including, without limitation, providing any certifications, declarations, or other documentation and any data or information as may be necessary or reasonably requested by Meta or any designee.\n(6) Participant will not:\nmodify, translate, or create any derivative works based upon the Video Similarity Dataset, other than as contemplated herein;\ndistribute, copy, disclose, assign, sublicense, embed, host or otherwise transfer the Video Similarity Dataset to any third party, except as described in Section 1(b) above;\nremove or alter any copyright, trademark or other proprietary notices appearing on or in copies of the Video Similarity Dataset;\nuse the Video Similarity Dataset in a pornographic, defamatory or other unlawful manner, or in violation of any applicable regulations or laws;\nincorporate the Video Similarity Dataset into any other program, dataset, or product;\nuse the Video Similarity Dataset to distribute manipulated images or videos (except as described in Section 1(b) above); or\nuse the Video Similarity Dataset for any purpose other than the Purpose specified in this Agreement.\n(7) If you use the Video Similarity Dataset (or any portion thereof) in a manner that features models or property in connection with a subject that would be unflattering or unduly controversial to a reasonable person, you must indicate: (1) that the content is being used for illustrative purposes only, and (2) any person depicted in the content is a model. For example, you could say: \u201cStock photo. Posed by model.\u201d\n(8) Meta always appreciates your feedback and other suggestions about the Video Similarity Dataset. However, you should know and you hereby agree that we may use your feedback and suggestions without any restriction or obligation, including, without limitation, that Meta has no obligation to compensate you for your feedback or to keep such feedback confidential.\n(9) Upon the termination of this Agreement, Participant will immediately stop using the Video Similarity Dataset and destroy all copies of the Video Similarity Dataset and related materials in Participant\u2019s possession and control. Additionally, Meta may, at any time, for any reason or for no reason, terminate this Agreement, effective immediately upon notice to the Participant. Upon termination, the license granted to Participant hereunder will immediately terminate and Participant will immediately stop using the Video Similarity Dataset and destroy all copies of the Video Similarity Dataset and related materials in Participant\u2019s possession or control. Except for the licenses granted to Participant, the other provisions of this Agreement will survive any termination.\n(10) THE VIDEO SIMILARITY DATASET IS PROVIDED \u201cAS IS\u201d WITHOUT ANY EXPRESS OR IMPLIED WARRANTY OF ANY KIND, INCLUDING WARRANTIES OF MERCHANTABILITY, TITLE, NON-INFRINGEMENT, OR FITNESS FOR ANY PARTICULAR PURPOSE.\n(11) IN NO EVENT WILL META, ITS CONTRACTORS AND ITS LICENSORS BE LIABLE FOR ANY CONSEQUENTIAL, INCIDENTAL, EXEMPLARY, PUNITIVE, SPECIAL, OR INDIRECT DAMAGES (INCLUDING DAMAGES FOR LOSS OF PROFITS, BUSINESS INTERRUPTION, OR LOSS OF INFORMATION) ARISING OUT OF OR RELATING TO THIS AGREEMENT OR ITS SUBJECT MATTER, EVEN IF META HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n(12) META'S, ITS LICENSOR'S AND ITS CONTRACTOR'S TOTAL LIABILITY ARISING FROM OR RELATING TO THIS AGREEMENT AND ITS SUBJECT MATTER WILL NOT EXCEED ONE HUNDRED DOLLARS ($100).\n(13) Either party may terminate this Agreement if the other is in material breach of this Agreement and such breach remains uncured for thirty (30) days following receipt of written notice of the breach.\n(14) Participant will comply with all applicable export controls, import controls and trade sanctions applicable to the Video Similarity Dataset. You shall obtain, at your sole cost and expense, any export and import (temporary and permanent) license and other official authorization applicable to the Video Similarity Dataset. This Agreement, and your relationship with Meta under this Agreement, shall be governed by the laws of the State of California without regard to its conflict of laws provisions. You and Meta agree to submit to the exclusive jurisdiction of the courts located within the county of Santa Clara, California to resolve any legal matter arising from the Agreement. Notwithstanding this, you agree that Meta shall still be allowed to apply for injunctive remedies (or an equivalent type of urgent legal relief) in any jurisdiction. Meta may make changes to this Agreement at any time with notice to Participant and the opportunity to decline further use of the Video Similarity Dataset. You should look at the Agreement and check for notice of any changes regularly. Changes will not be retroactive. Changes will become effective, and will be deemed accepted by Participant, (a) immediately for those who become Participants after the notification is posted; or (b) for pre-existing Participants, on the date specified in the notice, which will be no sooner than 30 days after the changes are posted (except changes required by law which will be effective immediately). If You do not agree with the changes to the Agreement, You may terminate Your use of Video Similarity Dataset, which will be Your sole and exclusive remedy. You agree that Your continued use of Video Similarity Dataset constitutes Your agreement to the modified terms of this Agreement. No failure to exercise and no delay in exercising any right, remedy or power hereunder will operate as a waiver thereof, nor will any single or partial exercise of any right, remedy or power hereunder preclude any other or further exercise thereof or the exercise of any other right, remedy or power provided herein or by law or in equity. Participant may not assign its rights and obligations hereunder without prior written consent of Meta. Meta may assign its rights and obligations hereunder at any time to any party without Participant\u2019s consent. If any provision of this Agreement is found by a court of competent jurisdiction to be void, invalid or unenforceable, the same will be reformed to comply with applicable law or stricken if not so conformable, so as not to affect the validity or enforceability of the remainder of this Agreement. This Agreement constitutes the entire agreement between the parties concerning the subject matter hereof and supersedes all prior or contemporaneous representations, discussions, negotiations, conditions, and agreements between the parties relating to the subject matter hereof.",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/101/meta-video-similarity-descriptor/"
    },
    {
        "competition_overview": "Welcome to the Video Similarity Challenge! In this competition, you will build models that detect whether a query video contains a possibly manipulated clip from one or more videos in a reference set.\nThe ability to identify and track content on social media platforms, called content tracing, is crucial to the experience of users on these platforms. Previously, Meta AI and DrivenData hosted the Image Similarity Challenge in which participants developed state-of-the-art models capable of accurately detecting when an image was derived from a known image. The motivation for detecting copies and manipulations with videos is similar -- enforcing copyright protections, identifying misinformation, and removing violent or objectionable content.\nManual content moderation has challenges scaling to meet the large volume of content on platforms like Instagram and Facebook, where tens of thousands of hours of video are uploaded each day. Accurate and performant algorithms are critical in flagging and removing inappropriate content. This competition allows you to test your skills in building a key part of that content tracing system, and in so doing contribute to making social media more trustworthy and safe for the people who use it.",
        "dataset_overview": "Video Similarity Dataset License Agreement\nThis Video Similarity Dataset License Agreement (\u201cAgreement\u201d) contains the terms and conditions that govern your access to and use of the Video Similarity Dataset (as defined below), you (as defined below). You may not use the Video Similarity Dataset if you do not accept this Agreement. This Agreement is effective upon the earlier of the date that you first access the Video Similarity Dataset or accept this Agreement (\u201cEffective Date\u201d), and is entered into by and between Meta Platforms, Inc. (\u201cMeta\u201d), and you, or your employer or other entity (if you are entering into this agreement on behalf of your employer or other entity) (\u201cParticipant\u201d or \u201cyou\u201d). The Video Similarity Dataset includes the videos, software, tools, documentation or other materials created, derived or licensed by Meta and provided to you for use in connection with the purposes set forth below. By clicking to accept, accessing the Video Similarity Dataset, or both, you hereby agree to the terms of the Agreement. If you are agreeing to be bound by the Agreement on behalf of your employer or other entity, you represent and warrant to Meta that you have full legal authority to bind your employer or such entity to this Agreement. If you do not have the requisite authority, you may not accept the Agreement or access the Video Similarity Dataset on behalf of your employer or other entity.\n(1) Subject to Participant\u2019s compliance with the terms and conditions of this Agreement, Meta hereby grants to Participant, a limited, non-exclusive, non-transferable, non-sublicensable, revocable license to: (a) use the Video Similarity Dataset to research software, algorithms, machine learning models, techniques and technologies designed to detect manipulated media, images, audio and videos (the \u201cPurpose\u201d); (b) distribute and reproduce up to one hundred (100) videos from the Video Similarity Dataset per Participant for research or academic publication related to the Purpose. If you include videos from the Video Similarity Dataset in a research or academic publication, then you shall include proper attribution to the videos as detailed for each video in the README file.\n(2) Subject to Participant\u2019s compliance with the terms and conditions of this Agreement, Participant retains its intellectual property rights in and to all algorithms, software, machine learning models, techniques and technologies developed or otherwise derived by Participant from the use of the Video Similarity Dataset. Such algorithms, software, machine learning models, techniques and technologies may be used for academic and commercial purposes.\n(3) As between Meta and Participant, Meta retains all intellectual property rights in and to the Video Similarity Dataset, except where indicated otherwise. All rights not expressly granted under this Agreement by Meta are reserved.\n(4) At any time, Meta may require Participant to delete all copies of the Video Similarity Dataset (in whole or in part) in Participant\u2019s possession and control. Participant will promptly comply with any and all such requests. Upon Meta\u2019s request, Participant shall provide Meta with written confirmation of Participant\u2019s compliance with such requirement.\n(5) In the event that (a) Meta reasonably believes (as determined at Meta\u2019s sole discretion) that you are or are likely to be in violation of the terms of this Agreement or (b) of any claim, investigation, or any legal, administrative, regulatory, judicial action or proceeding by any third party (including, without limitation, a governmental or regulatory agency) involving Meta or its affiliates or you or your affiliates in connection with this Agreement or subject matter thereof, then Meta itself or through a designee may audit your use, storage and/or distribution of the Video Similarity Dataset, including, without limitation, any and all records, files associated with the Video Similarity Dataset, and this Agreement. Solely with respect to (a), you may elect to have Meta have such audit performed by an independent registered public accounting firm of nationally recognized standing approved by Meta provided that in such case you hereby agree to pay all costs and expenses of such audit. You hereby agree to cooperate with any such audit, including, without limitation, providing any certifications, declarations, or other documentation and any data or information as may be necessary or reasonably requested by Meta or any designee.\n(6) Participant will not:\nmodify, translate, or create any derivative works based upon the Video Similarity Dataset, other than as contemplated herein;\ndistribute, copy, disclose, assign, sublicense, embed, host or otherwise transfer the Video Similarity Dataset to any third party, except as described in Section 1(b) above;\nremove or alter any copyright, trademark or other proprietary notices appearing on or in copies of the Video Similarity Dataset;\nuse the Video Similarity Dataset in a pornographic, defamatory or other unlawful manner, or in violation of any applicable regulations or laws;\nincorporate the Video Similarity Dataset into any other program, dataset, or product;\nuse the Video Similarity Dataset to distribute manipulated images or videos (except as described in Section 1(b) above); or\nuse the Video Similarity Dataset for any purpose other than the Purpose specified in this Agreement.\n(7) If you use the Video Similarity Dataset (or any portion thereof) in a manner that features models or property in connection with a subject that would be unflattering or unduly controversial to a reasonable person, you must indicate: (1) that the content is being used for illustrative purposes only, and (2) any person depicted in the content is a model. For example, you could say: \u201cStock photo. Posed by model.\u201d\n(8) Meta always appreciates your feedback and other suggestions about the Video Similarity Dataset. However, you should know and you hereby agree that we may use your feedback and suggestions without any restriction or obligation, including, without limitation, that Meta has no obligation to compensate you for your feedback or to keep such feedback confidential.\n(9) Upon the termination of this Agreement, Participant will immediately stop using the Video Similarity Dataset and destroy all copies of the Video Similarity Dataset and related materials in Participant\u2019s possession and control. Additionally, Meta may, at any time, for any reason or for no reason, terminate this Agreement, effective immediately upon notice to the Participant. Upon termination, the license granted to Participant hereunder will immediately terminate and Participant will immediately stop using the Video Similarity Dataset and destroy all copies of the Video Similarity Dataset and related materials in Participant\u2019s possession or control. Except for the licenses granted to Participant, the other provisions of this Agreement will survive any termination.\n(10) THE VIDEO SIMILARITY DATASET IS PROVIDED \u201cAS IS\u201d WITHOUT ANY EXPRESS OR IMPLIED WARRANTY OF ANY KIND, INCLUDING WARRANTIES OF MERCHANTABILITY, TITLE, NON-INFRINGEMENT, OR FITNESS FOR ANY PARTICULAR PURPOSE.\n(11) IN NO EVENT WILL META, ITS CONTRACTORS AND ITS LICENSORS BE LIABLE FOR ANY CONSEQUENTIAL, INCIDENTAL, EXEMPLARY, PUNITIVE, SPECIAL, OR INDIRECT DAMAGES (INCLUDING DAMAGES FOR LOSS OF PROFITS, BUSINESS INTERRUPTION, OR LOSS OF INFORMATION) ARISING OUT OF OR RELATING TO THIS AGREEMENT OR ITS SUBJECT MATTER, EVEN IF META HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n(12) META'S, ITS LICENSOR'S AND ITS CONTRACTOR'S TOTAL LIABILITY ARISING FROM OR RELATING TO THIS AGREEMENT AND ITS SUBJECT MATTER WILL NOT EXCEED ONE HUNDRED DOLLARS ($100).\n(13) Either party may terminate this Agreement if the other is in material breach of this Agreement and such breach remains uncured for thirty (30) days following receipt of written notice of the breach.\n(14) Participant will comply with all applicable export controls, import controls and trade sanctions applicable to the Video Similarity Dataset. You shall obtain, at your sole cost and expense, any export and import (temporary and permanent) license and other official authorization applicable to the Video Similarity Dataset. This Agreement, and your relationship with Meta under this Agreement, shall be governed by the laws of the State of California without regard to its conflict of laws provisions. You and Meta agree to submit to the exclusive jurisdiction of the courts located within the county of Santa Clara, California to resolve any legal matter arising from the Agreement. Notwithstanding this, you agree that Meta shall still be allowed to apply for injunctive remedies (or an equivalent type of urgent legal relief) in any jurisdiction. Meta may make changes to this Agreement at any time with notice to Participant and the opportunity to decline further use of the Video Similarity Dataset. You should look at the Agreement and check for notice of any changes regularly. Changes will not be retroactive. Changes will become effective, and will be deemed accepted by Participant, (a) immediately for those who become Participants after the notification is posted; or (b) for pre-existing Participants, on the date specified in the notice, which will be no sooner than 30 days after the changes are posted (except changes required by law which will be effective immediately). If You do not agree with the changes to the Agreement, You may terminate Your use of Video Similarity Dataset, which will be Your sole and exclusive remedy. You agree that Your continued use of Video Similarity Dataset constitutes Your agreement to the modified terms of this Agreement. No failure to exercise and no delay in exercising any right, remedy or power hereunder will operate as a waiver thereof, nor will any single or partial exercise of any right, remedy or power hereunder preclude any other or further exercise thereof or the exercise of any other right, remedy or power provided herein or by law or in equity. Participant may not assign its rights and obligations hereunder without prior written consent of Meta. Meta may assign its rights and obligations hereunder at any time to any party without Participant\u2019s consent. If any provision of this Agreement is found by a court of competent jurisdiction to be void, invalid or unenforceable, the same will be reformed to comply with applicable law or stricken if not so conformable, so as not to affect the validity or enforceability of the remainder of this Agreement. This Agreement constitutes the entire agreement between the parties concerning the subject matter hereof and supersedes all prior or contemporaneous representations, discussions, negotiations, conditions, and agreements between the parties relating to the subject matter hereof.",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/106/meta-video-similarity-matching/"
    },
    {
        "competition_overview": "Welcome to Phase 2 of the Meta AI Video Similarity Challenge! In this competition, you will build models that detect whether a query video contains a possibly manipulated clip from one or more videos in a reference set.\nThe ability to identify and track content on social media platforms, called content tracing, is crucial to the experience of users on these platforms. Previously, Meta AI and DrivenData hosted the Image Similarity Challenge, in which participants developed state-of-the-art models capable of accurately detecting when an image was derived from a known image. The motivation for detecting copies and manipulations with videos is similar\u2014enforcing copyright protections, identifying misinformation, and removing violent or objectionable content.\nManual content moderation has challenges scaling to meet the large volume of content on platforms like Instagram and Facebook, where tens of thousands of hours of video are uploaded each day. Accurate and performant algorithms are critical in flagging and removing inappropriate content. This competition allows you to test your skills in building a key part of that content tracing system, and in so doing contribute to making social media more trustworthy and safe for the people who use it.",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/184/competition-meta-vsc-descriptor-p2/"
    },
    {
        "competition_overview": "Welcome to Phase 2 of the Meta AI Video Similarity Challenge! In this competition, you will build models that detect whether a query video contains a possibly manipulated clip from one or more videos in a reference set.\nThe ability to identify and track content on social media platforms, called content tracing, is crucial to the experience of users on these platforms. Previously, Meta AI and DrivenData hosted the Image Similarity Challenge, in which participants developed state-of-the-art models capable of accurately detecting when an image was derived from a known image. The motivation for detecting copies and manipulations with videos is similar\u2014enforcing copyright protections, identifying misinformation, and removing violent or objectionable content.\nManual content moderation has challenges scaling to meet the large volume of content on platforms like Instagram and Facebook, where tens of thousands of hours of video are uploaded each day. Accurate and performant algorithms are critical in flagging and removing inappropriate content. This competition allows you to test your skills in building a key part of that content tracing system, and in so doing contribute to making social media more trustworthy and safe for the people who use it.",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/185/competition-meta-vsc-matching-p2/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Video Similarity Dataset License Agreement\nThis Video Similarity Dataset License Agreement (\u201cAgreement\u201d) contains the terms and conditions that govern your access to and use of the Video Similarity Dataset (as defined below), you (as defined below). You may not use the Video Similarity Dataset if you do not accept this Agreement. This Agreement is effective upon the earlier of the date that you first access the Video Similarity Dataset or accept this Agreement (\u201cEffective Date\u201d), and is entered into by and between Meta Platforms, Inc. (\u201cMeta\u201d), and you, or your employer or other entity (if you are entering into this agreement on behalf of your employer or other entity) (\u201cParticipant\u201d or \u201cyou\u201d). The Video Similarity Dataset includes the videos, software, tools, documentation or other materials created, derived or licensed by Meta and provided to you for use in connection with the purposes set forth below. By clicking to accept, accessing the Video Similarity Dataset, or both, you hereby agree to the terms of the Agreement. If you are agreeing to be bound by the Agreement on behalf of your employer or other entity, you represent and warrant to Meta that you have full legal authority to bind your employer or such entity to this Agreement. If you do not have the requisite authority, you may not accept the Agreement or access the Video Similarity Dataset on behalf of your employer or other entity.\n(1) Subject to Participant\u2019s compliance with the terms and conditions of this Agreement, Meta hereby grants to Participant, a limited, non-exclusive, non-transferable, non-sublicensable, revocable license to: (a) use the Video Similarity Dataset to research software, algorithms, machine learning models, techniques and technologies designed to detect manipulated media, images, audio and videos (the \u201cPurpose\u201d); (b) distribute and reproduce up to one hundred (100) videos from the Video Similarity Dataset per Participant for research or academic publication related to the Purpose. If you include videos from the Video Similarity Dataset in a research or academic publication, then you shall include proper attribution to the videos as detailed for each video in the README file.\n(2) Subject to Participant\u2019s compliance with the terms and conditions of this Agreement, Participant retains its intellectual property rights in and to all algorithms, software, machine learning models, techniques and technologies developed or otherwise derived by Participant from the use of the Video Similarity Dataset. Such algorithms, software, machine learning models, techniques and technologies may be used for academic and commercial purposes.\n(3) As between Meta and Participant, Meta retains all intellectual property rights in and to the Video Similarity Dataset, except where indicated otherwise. All rights not expressly granted under this Agreement by Meta are reserved.\n(4) At any time, Meta may require Participant to delete all copies of the Video Similarity Dataset (in whole or in part) in Participant\u2019s possession and control. Participant will promptly comply with any and all such requests. Upon Meta\u2019s request, Participant shall provide Meta with written confirmation of Participant\u2019s compliance with such requirement.\n(5) In the event that (a) Meta reasonably believes (as determined at Meta\u2019s sole discretion) that you are or are likely to be in violation of the terms of this Agreement or (b) of any claim, investigation, or any legal, administrative, regulatory, judicial action or proceeding by any third party (including, without limitation, a governmental or regulatory agency) involving Meta or its affiliates or you or your affiliates in connection with this Agreement or subject matter thereof, then Meta itself or through a designee may audit your use, storage and/or distribution of the Video Similarity Dataset, including, without limitation, any and all records, files associated with the Video Similarity Dataset, and this Agreement. Solely with respect to (a), you may elect to have Meta have such audit performed by an independent registered public accounting firm of nationally recognized standing approved by Meta provided that in such case you hereby agree to pay all costs and expenses of such audit. You hereby agree to cooperate with any such audit, including, without limitation, providing any certifications, declarations, or other documentation and any data or information as may be necessary or reasonably requested by Meta or any designee.\n(6) Participant will not:\nmodify, translate, or create any derivative works based upon the Video Similarity Dataset, other than as contemplated herein;\ndistribute, copy, disclose, assign, sublicense, embed, host or otherwise transfer the Video Similarity Dataset to any third party, except as described in Section 1(b) above;\nremove or alter any copyright, trademark or other proprietary notices appearing on or in copies of the Video Similarity Dataset;\nuse the Video Similarity Dataset in a pornographic, defamatory or other unlawful manner, or in violation of any applicable regulations or laws;\nincorporate the Video Similarity Dataset into any other program, dataset, or product;\nuse the Video Similarity Dataset to distribute manipulated images or videos (except as described in Section 1(b) above); or\nuse the Video Similarity Dataset for any purpose other than the Purpose specified in this Agreement.\n(7) If you use the Video Similarity Dataset (or any portion thereof) in a manner that features models or property in connection with a subject that would be unflattering or unduly controversial to a reasonable person, you must indicate: (1) that the content is being used for illustrative purposes only, and (2) any person depicted in the content is a model. For example, you could say: \u201cStock photo. Posed by model.\u201d\n(8) Meta always appreciates your feedback and other suggestions about the Video Similarity Dataset. However, you should know and you hereby agree that we may use your feedback and suggestions without any restriction or obligation, including, without limitation, that Meta has no obligation to compensate you for your feedback or to keep such feedback confidential.\n(9) Upon the termination of this Agreement, Participant will immediately stop using the Video Similarity Dataset and destroy all copies of the Video Similarity Dataset and related materials in Participant\u2019s possession and control. Additionally, Meta may, at any time, for any reason or for no reason, terminate this Agreement, effective immediately upon notice to the Participant. Upon termination, the license granted to Participant hereunder will immediately terminate and Participant will immediately stop using the Video Similarity Dataset and destroy all copies of the Video Similarity Dataset and related materials in Participant\u2019s possession or control. Except for the licenses granted to Participant, the other provisions of this Agreement will survive any termination.\n(10) THE VIDEO SIMILARITY DATASET IS PROVIDED \u201cAS IS\u201d WITHOUT ANY EXPRESS OR IMPLIED WARRANTY OF ANY KIND, INCLUDING WARRANTIES OF MERCHANTABILITY, TITLE, NON-INFRINGEMENT, OR FITNESS FOR ANY PARTICULAR PURPOSE.\n(11) IN NO EVENT WILL META, ITS CONTRACTORS AND ITS LICENSORS BE LIABLE FOR ANY CONSEQUENTIAL, INCIDENTAL, EXEMPLARY, PUNITIVE, SPECIAL, OR INDIRECT DAMAGES (INCLUDING DAMAGES FOR LOSS OF PROFITS, BUSINESS INTERRUPTION, OR LOSS OF INFORMATION) ARISING OUT OF OR RELATING TO THIS AGREEMENT OR ITS SUBJECT MATTER, EVEN IF META HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n(12) META'S, ITS LICENSOR'S AND ITS CONTRACTOR'S TOTAL LIABILITY ARISING FROM OR RELATING TO THIS AGREEMENT AND ITS SUBJECT MATTER WILL NOT EXCEED ONE HUNDRED DOLLARS ($100).\n(13) Either party may terminate this Agreement if the other is in material breach of this Agreement and such breach remains uncured for thirty (30) days following receipt of written notice of the breach.\n(14) Participant will comply with all applicable export controls, import controls and trade sanctions applicable to the Video Similarity Dataset. You shall obtain, at your sole cost and expense, any export and import (temporary and permanent) license and other official authorization applicable to the Video Similarity Dataset. This Agreement, and your relationship with Meta under this Agreement, shall be governed by the laws of the State of California without regard to its conflict of laws provisions. You and Meta agree to submit to the exclusive jurisdiction of the courts located within the county of Santa Clara, California to resolve any legal matter arising from the Agreement. Notwithstanding this, you agree that Meta shall still be allowed to apply for injunctive remedies (or an equivalent type of urgent legal relief) in any jurisdiction. Meta may make changes to this Agreement at any time with notice to Participant and the opportunity to decline further use of the Video Similarity Dataset. You should look at the Agreement and check for notice of any changes regularly. Changes will not be retroactive. Changes will become effective, and will be deemed accepted by Participant, (a) immediately for those who become Participants after the notification is posted; or (b) for pre-existing Participants, on the date specified in the notice, which will be no sooner than 30 days after the changes are posted (except changes required by law which will be effective immediately). If You do not agree with the changes to the Agreement, You may terminate Your use of Video Similarity Dataset, which will be Your sole and exclusive remedy. You agree that Your continued use of Video Similarity Dataset constitutes Your agreement to the modified terms of this Agreement. No failure to exercise and no delay in exercising any right, remedy or power hereunder will operate as a waiver thereof, nor will any single or partial exercise of any right, remedy or power hereunder preclude any other or further exercise thereof or the exercise of any other right, remedy or power provided herein or by law or in equity. Participant may not assign its rights and obligations hereunder without prior written consent of Meta. Meta may assign its rights and obligations hereunder at any time to any party without Participant\u2019s consent. If any provision of this Agreement is found by a court of competent jurisdiction to be void, invalid or unenforceable, the same will be reformed to comply with applicable law or stricken if not so conformable, so as not to affect the validity or enforceability of the remainder of this Agreement. This Agreement constitutes the entire agreement between the parties concerning the subject matter hereof and supersedes all prior or contemporaneous representations, discussions, negotiations, conditions, and agreements between the parties relating to the subject matter hereof.",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/219/competition-meta-vsc-desc-open/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Video Similarity Dataset License Agreement\nThis Video Similarity Dataset License Agreement (\u201cAgreement\u201d) contains the terms and conditions that govern your access to and use of the Video Similarity Dataset (as defined below), you (as defined below). You may not use the Video Similarity Dataset if you do not accept this Agreement. This Agreement is effective upon the earlier of the date that you first access the Video Similarity Dataset or accept this Agreement (\u201cEffective Date\u201d), and is entered into by and between Meta Platforms, Inc. (\u201cMeta\u201d), and you, or your employer or other entity (if you are entering into this agreement on behalf of your employer or other entity) (\u201cParticipant\u201d or \u201cyou\u201d). The Video Similarity Dataset includes the videos, software, tools, documentation or other materials created, derived or licensed by Meta and provided to you for use in connection with the purposes set forth below. By clicking to accept, accessing the Video Similarity Dataset, or both, you hereby agree to the terms of the Agreement. If you are agreeing to be bound by the Agreement on behalf of your employer or other entity, you represent and warrant to Meta that you have full legal authority to bind your employer or such entity to this Agreement. If you do not have the requisite authority, you may not accept the Agreement or access the Video Similarity Dataset on behalf of your employer or other entity.\n(1) Subject to Participant\u2019s compliance with the terms and conditions of this Agreement, Meta hereby grants to Participant, a limited, non-exclusive, non-transferable, non-sublicensable, revocable license to: (a) use the Video Similarity Dataset to research software, algorithms, machine learning models, techniques and technologies designed to detect manipulated media, images, audio and videos (the \u201cPurpose\u201d); (b) distribute and reproduce up to one hundred (100) videos from the Video Similarity Dataset per Participant for research or academic publication related to the Purpose. If you include videos from the Video Similarity Dataset in a research or academic publication, then you shall include proper attribution to the videos as detailed for each video in the README file.\n(2) Subject to Participant\u2019s compliance with the terms and conditions of this Agreement, Participant retains its intellectual property rights in and to all algorithms, software, machine learning models, techniques and technologies developed or otherwise derived by Participant from the use of the Video Similarity Dataset. Such algorithms, software, machine learning models, techniques and technologies may be used for academic and commercial purposes.\n(3) As between Meta and Participant, Meta retains all intellectual property rights in and to the Video Similarity Dataset, except where indicated otherwise. All rights not expressly granted under this Agreement by Meta are reserved.\n(4) At any time, Meta may require Participant to delete all copies of the Video Similarity Dataset (in whole or in part) in Participant\u2019s possession and control. Participant will promptly comply with any and all such requests. Upon Meta\u2019s request, Participant shall provide Meta with written confirmation of Participant\u2019s compliance with such requirement.\n(5) In the event that (a) Meta reasonably believes (as determined at Meta\u2019s sole discretion) that you are or are likely to be in violation of the terms of this Agreement or (b) of any claim, investigation, or any legal, administrative, regulatory, judicial action or proceeding by any third party (including, without limitation, a governmental or regulatory agency) involving Meta or its affiliates or you or your affiliates in connection with this Agreement or subject matter thereof, then Meta itself or through a designee may audit your use, storage and/or distribution of the Video Similarity Dataset, including, without limitation, any and all records, files associated with the Video Similarity Dataset, and this Agreement. Solely with respect to (a), you may elect to have Meta have such audit performed by an independent registered public accounting firm of nationally recognized standing approved by Meta provided that in such case you hereby agree to pay all costs and expenses of such audit. You hereby agree to cooperate with any such audit, including, without limitation, providing any certifications, declarations, or other documentation and any data or information as may be necessary or reasonably requested by Meta or any designee.\n(6) Participant will not:\nmodify, translate, or create any derivative works based upon the Video Similarity Dataset, other than as contemplated herein;\ndistribute, copy, disclose, assign, sublicense, embed, host or otherwise transfer the Video Similarity Dataset to any third party, except as described in Section 1(b) above;\nremove or alter any copyright, trademark or other proprietary notices appearing on or in copies of the Video Similarity Dataset;\nuse the Video Similarity Dataset in a pornographic, defamatory or other unlawful manner, or in violation of any applicable regulations or laws;\nincorporate the Video Similarity Dataset into any other program, dataset, or product;\nuse the Video Similarity Dataset to distribute manipulated images or videos (except as described in Section 1(b) above); or\nuse the Video Similarity Dataset for any purpose other than the Purpose specified in this Agreement.\n(7) If you use the Video Similarity Dataset (or any portion thereof) in a manner that features models or property in connection with a subject that would be unflattering or unduly controversial to a reasonable person, you must indicate: (1) that the content is being used for illustrative purposes only, and (2) any person depicted in the content is a model. For example, you could say: \u201cStock photo. Posed by model.\u201d\n(8) Meta always appreciates your feedback and other suggestions about the Video Similarity Dataset. However, you should know and you hereby agree that we may use your feedback and suggestions without any restriction or obligation, including, without limitation, that Meta has no obligation to compensate you for your feedback or to keep such feedback confidential.\n(9) Upon the termination of this Agreement, Participant will immediately stop using the Video Similarity Dataset and destroy all copies of the Video Similarity Dataset and related materials in Participant\u2019s possession and control. Additionally, Meta may, at any time, for any reason or for no reason, terminate this Agreement, effective immediately upon notice to the Participant. Upon termination, the license granted to Participant hereunder will immediately terminate and Participant will immediately stop using the Video Similarity Dataset and destroy all copies of the Video Similarity Dataset and related materials in Participant\u2019s possession or control. Except for the licenses granted to Participant, the other provisions of this Agreement will survive any termination.\n(10) THE VIDEO SIMILARITY DATASET IS PROVIDED \u201cAS IS\u201d WITHOUT ANY EXPRESS OR IMPLIED WARRANTY OF ANY KIND, INCLUDING WARRANTIES OF MERCHANTABILITY, TITLE, NON-INFRINGEMENT, OR FITNESS FOR ANY PARTICULAR PURPOSE.\n(11) IN NO EVENT WILL META, ITS CONTRACTORS AND ITS LICENSORS BE LIABLE FOR ANY CONSEQUENTIAL, INCIDENTAL, EXEMPLARY, PUNITIVE, SPECIAL, OR INDIRECT DAMAGES (INCLUDING DAMAGES FOR LOSS OF PROFITS, BUSINESS INTERRUPTION, OR LOSS OF INFORMATION) ARISING OUT OF OR RELATING TO THIS AGREEMENT OR ITS SUBJECT MATTER, EVEN IF META HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n(12) META'S, ITS LICENSOR'S AND ITS CONTRACTOR'S TOTAL LIABILITY ARISING FROM OR RELATING TO THIS AGREEMENT AND ITS SUBJECT MATTER WILL NOT EXCEED ONE HUNDRED DOLLARS ($100).\n(13) Either party may terminate this Agreement if the other is in material breach of this Agreement and such breach remains uncured for thirty (30) days following receipt of written notice of the breach.\n(14) Participant will comply with all applicable export controls, import controls and trade sanctions applicable to the Video Similarity Dataset. You shall obtain, at your sole cost and expense, any export and import (temporary and permanent) license and other official authorization applicable to the Video Similarity Dataset. This Agreement, and your relationship with Meta under this Agreement, shall be governed by the laws of the State of California without regard to its conflict of laws provisions. You and Meta agree to submit to the exclusive jurisdiction of the courts located within the county of Santa Clara, California to resolve any legal matter arising from the Agreement. Notwithstanding this, you agree that Meta shall still be allowed to apply for injunctive remedies (or an equivalent type of urgent legal relief) in any jurisdiction. Meta may make changes to this Agreement at any time with notice to Participant and the opportunity to decline further use of the Video Similarity Dataset. You should look at the Agreement and check for notice of any changes regularly. Changes will not be retroactive. Changes will become effective, and will be deemed accepted by Participant, (a) immediately for those who become Participants after the notification is posted; or (b) for pre-existing Participants, on the date specified in the notice, which will be no sooner than 30 days after the changes are posted (except changes required by law which will be effective immediately). If You do not agree with the changes to the Agreement, You may terminate Your use of Video Similarity Dataset, which will be Your sole and exclusive remedy. You agree that Your continued use of Video Similarity Dataset constitutes Your agreement to the modified terms of this Agreement. No failure to exercise and no delay in exercising any right, remedy or power hereunder will operate as a waiver thereof, nor will any single or partial exercise of any right, remedy or power hereunder preclude any other or further exercise thereof or the exercise of any other right, remedy or power provided herein or by law or in equity. Participant may not assign its rights and obligations hereunder without prior written consent of Meta. Meta may assign its rights and obligations hereunder at any time to any party without Participant\u2019s consent. If any provision of this Agreement is found by a court of competent jurisdiction to be void, invalid or unenforceable, the same will be reformed to comply with applicable law or stricken if not so conformable, so as not to affect the validity or enforceability of the remainder of this Agreement. This Agreement constitutes the entire agreement between the parties concerning the subject matter hereof and supersedes all prior or contemporaneous representations, discussions, negotiations, conditions, and agreements between the parties relating to the subject matter hereof.",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/220/competition-meta-vsc-match-open/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Accessing the Hateful Memes Dataset\nThe Hateful Memes Dataset is being made available under the License Agreement below. For this challenge, all participants will need to confirm their acceptance of this agreement in addition to the Competition Rules in order to access the data.\nIf you have not already done so, you will have the chance to indicate agreement and get access to the data by clicking Compete to the right and following the prompts.\nHateful Memes Dataset License Agreement\nIn order to access the Hateful Memes Dataset (as defined below), you (as defined below) must first agree to this Hateful Memes Dataset (\u201cHM Dataset\u201d) License Agreement (\u201cAgreement\u201d). You may not use the HM Dataset if you do not accept this Agreement. By clicking to accept, accessing the HM Dataset, or both, you hereby agree to the terms of the Agreement. If you are agreeing to be bound by the Agreement on behalf of your employer or other entity, you represent and warrant to Facebook that you have full legal authority to bind your employer or such entity to this Agreement. If you do not have the requisite authority, you may not accept the Agreement or access the HM Dataset on behalf of your employer or other entity.\nThis Agreement is effective upon the earlier of the date that you first access the HM Dataset or accept this Agreement (\u201cEffective Date\u201d), and is entered into by and between Facebook, Inc. (\u201cFacebook\u201d), and you, or your employer or other entity (if you are entering into this agreement on behalf of your employer or other entity) (\u201cParticipant\u201d or \u201cyou\u201d).\n(1) Subject to Participant\u2019s compliance with the terms and conditions of this Agreement, Facebook hereby grants to Participant, a limited, non-exclusive, non-transferable, non-sublicensable license to: (a) use the HM Dataset to research, develop and improve software, algorithms, machine learning models, techniques and technologies designed to detect manipulated media, images, audio and videos (the \u201cPurpose\u201d) and (b) distribute and reproduce up to a total of one hundred (100) images from the HM Dataset per Participant for research or academic publications related to the Purpose. If you include images from the HM Dataset in a research or academic publications, then you shall include attribution to Getty Images in one of the following formats:\n\u201cImage above is a compilation of assets, including \u00a9Getty Images/[photographer name]\u201d. If no photographer name is listed, attribution shall be as follows:\n\u201cImage above is a compilation of assets, including \u00a9Getty Images/[collection name]\u201d. If no collection name is listed, attribution shall be as follows:\n\u201c\u00a9Getty Images\u201d.\n(2) Subject to Participant\u2019s compliance with the terms and conditions of this Agreement, Participant retains its intellectual property rights in and to all algorithms, software, machine learning models, techniques and technologies developed or otherwise derived by Participant from the use of the HM Dataset. Such algorithms, software, machine learning models, techniques and technologies may be used for academic and commercial purposes.\n(3) As between Facebook and Participant, Facebook retains all intellectual property rights in and to the HM Dataset. All rights not expressly granted under this Agreement by Facebook are reserved.\n(4) At any time, Facebook may require Participant to delete all copies of the HM Dataset (in whole or in part) in Participant\u2019s possession and control. Participant will promptly comply with any and all such requests. Upon Facebook\u2019s request, Participant shall provide Facebook with written confirmation of Participant\u2019s compliance with such requirement.\n(5) If Facebook reasonably believes (as determined at Facebook\u2019s sole discretion) that you are or are likely to be in violation of the terms of this Agreement, then Facebook or Facebook\u2019s designee (at Facebook\u2019s sole expense) may audit your use, storage and distribution of the HM Dataset, including, without limitation, any and all records, files associated with the HM Dataset, and this Agreement. You hereby agree to cooperate with such audit.\n(6) Participant will not:\nmodify, translate, or create any derivative works based upon the HM Dataset;\ndistribute, copy, disclose, assign, sublicense, embed, host or otherwise transfer the HM Dataset to any third party, except as described in Section 1(b) above;\nremove or alter any copyright, trademark or other proprietary notices appearing on or in copies of the HM Dataset;\nuse the HM Dataset in a pornographic, defamatory or other unlawful manner, or in violation of any applicable regulations or laws;\nincorporate the HM Dataset into any other program, dataset, or product;\nuse the HM Dataset to distribute manipulated images or videos (except as described in Section 1(b) above); or\nuse the HM Dataset for any purpose other than the Purpose specified in this Agreement.\n(7) If you use the HM Dataset (or any portion thereof) in a manner that features models or property in connection with a subject that would be unflattering or unduly controversial to a reasonable person, you must indicate: (1) that the content is being used for illustrative purposes only, and (2) any person depicted in the content is a model. For example, you could say: \u201cStock photo. Posed by model.\u201d\n(8) Facebook always appreciates your feedback and other suggestions about the HM Dataset. However, you should know and you hereby agree that we may use your feedback and suggestions without any restriction or obligation, including, without limitation, to compensate you or to keep them confidential.\n(9) Upon the termination of this Agreement, Participant will immediately stop using the HM Dataset and destroy all copies of the HM Dataset and related materials in Participant\u2019s possession and control. Additionally, Facebook may, at any time, for any reason or for no reason, terminate this Agreement, effective immediately upon notice to the Participant. Upon termination, the license granted to Participant hereunder will immediately terminate and Participant will immediately stop using the HM Dataset and destroy all copies of the HM Dataset and related materials in Participant\u2019s possession or control. Except for the licenses granted to Participant, the other provisions of this Agreement will survive any termination.\n(10) THE HM DATASET IS PROVIDED \u201cAS IS\u201d WITHOUT ANY EXPRESS OR IMPLIED WARRANTY OF ANY KIND, INCLUDING WARRANTIES OF MERCHANTABILITY, TITLE, NON-INFRINGEMENT, OR FITNESS FOR ANY PARTICULAR PURPOSE.\n(11) IN NO EVENT WILL FACEBOOK, ITS CONTRACTORS AND ITS LICENSORS BE LIABLE FOR ANY CONSEQUENTIAL, INCIDENTAL, EXEMPLARY, PUNITIVE, SPECIAL, OR INDIRECT DAMAGES (INCLUDING DAMAGES FOR LOSS OF PROFITS, BUSINESS INTERRUPTION, OR LOSS OF INFORMATION) ARISING OUT OF OR RELATING TO THIS AGREEMENT OR ITS SUBJECT MATTER, EVEN IF FACEBOOK HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n(12) FACEBOOK, ITS LICENSORS AND ITS CONTRACTOR\u2019S TOTAL LIABILITY ARISING FROM OR RELATING TO THIS AGREEMENT AND ITS SUBJECT MATTER WILL NOT EXCEED ONE HUNDRED DOLLARS ($100).\n(13) Either party may terminate this Agreement if the other is in material breach of this Agreement and such breach remains uncured for thirty (30) days following receipt of written notice of the breach.\n(14) From time to time, Facebook may provide your name and the fact that you are a licensee of the HM Dataset to our licensors.\n(15) Participant will comply with all applicable export controls, import controls and trade sanctions applicable to the HM Dataset. You shall obtain, at your sole cost and expense, any export and import (temporary and permanent) license and other official authorization applicable to the HM Dataset. This Agreement, and your relationship with Facebook under this Agreement, shall be governed by the laws of the State of California without regard to its conflict of laws provisions. You and Facebook agree to submit to the exclusive jurisdiction of the courts located within the county of Santa Clara, California to resolve any legal matter arising from the Agreement. Notwithstanding this, you agree that Facebook shall still be allowed to apply for injunctive remedies (or an equivalent type of urgent legal relief) in any jurisdiction. Facebook may make changes to this Agreement at any time with notice to Participant and the opportunity to decline further use of the HM Dataset. You should look at the Agreement and check for notice of any changes regularly. Changes will not be retroactive. They will become effective, and will be deemed accepted by Participant, (a) immediately for those who become Participants after the notification is posted; or (b) for pre-existing Participants, on the date specified in the notice, which will be no sooner than 30 days after the changes are posted (except changes required by law which will be effective immediately). If You do not agree with the modifications to the Agreement, You may terminate Your use of HM Dataset, which will be Your sole and exclusive remedy. You agree that Your continued use of HM Dataset constitutes Your agreement to the modified terms of this Agreement. No failure to exercise and no delay in exercising any right, remedy or power hereunder will operate as a waiver thereof, nor will any single or partial exercise of any right, remedy or power hereunder preclude any other or further exercise thereof or the exercise of any other right, remedy or power provided herein or by law or in equity. Participant may not assign its rights and obligations hereunder without prior written consent of Facebook. Facebook may assign its rights and obligations hereunder at any time to any party without Participant\u2019s consent. If any provision of this Agreement is found by a court of competent jurisdiction to be void, invalid or unenforceable, the same will be reformed to comply with applicable law or stricken if not so conformable, so as not to affect the validity or enforceability of the remainder of this Agreement. This Agreement constitutes the entire agreement between the parties concerning the subject matter hereof and supersedes all prior or contemporaneous representations, discussions, negotiations, conditions, and agreements between the parties relating to the subject matter hereof.",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/64/hateful-memes/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Accessing the Hateful Memes Dataset\nThe Hateful Memes Dataset is being made available under the License Agreement below. For this challenge, all participants will need to confirm their acceptance of this agreement in addition to the Competition Rules in order to access the data.\nIf you have not already done so, you will have the chance to indicate agreement and get access to the data by clicking Compete to the right and following the prompts.\nHateful Memes Dataset License Agreement\nIn order to access the Hateful Memes Dataset (as defined below), you (as defined below) must first agree to this Hateful Memes Dataset (\u201cHM Dataset\u201d) License Agreement (\u201cAgreement\u201d). You may not use the HM Dataset if you do not accept this Agreement. By clicking to accept, accessing the HM Dataset, or both, you hereby agree to the terms of the Agreement. If you are agreeing to be bound by the Agreement on behalf of your employer or other entity, you represent and warrant to Facebook that you have full legal authority to bind your employer or such entity to this Agreement. If you do not have the requisite authority, you may not accept the Agreement or access the HM Dataset on behalf of your employer or other entity.\nThis Agreement is effective upon the earlier of the date that you first access the HM Dataset or accept this Agreement (\u201cEffective Date\u201d), and is entered into by and between Facebook, Inc. (\u201cFacebook\u201d), and you, or your employer or other entity (if you are entering into this agreement on behalf of your employer or other entity) (\u201cParticipant\u201d or \u201cyou\u201d).\n(1) Subject to Participant\u2019s compliance with the terms and conditions of this Agreement, Facebook hereby grants to Participant, a limited, non-exclusive, non-transferable, non-sublicensable license to: (a) use the HM Dataset to research, develop and improve software, algorithms, machine learning models, techniques and technologies designed to detect manipulated media, images, audio and videos (the \u201cPurpose\u201d) and (b) distribute and reproduce up to a total of one hundred (100) images from the HM Dataset per Participant for research or academic publications related to the Purpose. If you include images from the HM Dataset in a research or academic publications, then you shall include attribution to Getty Images in one of the following formats:\n\u201cImage above is a compilation of assets, including \u00a9Getty Images/[photographer name]\u201d. If no photographer name is listed, attribution shall be as follows:\n\u201cImage above is a compilation of assets, including \u00a9Getty Images/[collection name]\u201d. If no collection name is listed, attribution shall be as follows:\n\u201c\u00a9Getty Images\u201d.\n(2) Subject to Participant\u2019s compliance with the terms and conditions of this Agreement, Participant retains its intellectual property rights in and to all algorithms, software, machine learning models, techniques and technologies developed or otherwise derived by Participant from the use of the HM Dataset. Such algorithms, software, machine learning models, techniques and technologies may be used for academic and commercial purposes.\n(3) As between Facebook and Participant, Facebook retains all intellectual property rights in and to the HM Dataset. All rights not expressly granted under this Agreement by Facebook are reserved.\n(4) At any time, Facebook may require Participant to delete all copies of the HM Dataset (in whole or in part) in Participant\u2019s possession and control. Participant will promptly comply with any and all such requests. Upon Facebook\u2019s request, Participant shall provide Facebook with written confirmation of Participant\u2019s compliance with such requirement.\n(5) If Facebook reasonably believes (as determined at Facebook\u2019s sole discretion) that you are or are likely to be in violation of the terms of this Agreement, then Facebook or Facebook\u2019s designee (at Facebook\u2019s sole expense) may audit your use, storage and distribution of the HM Dataset, including, without limitation, any and all records, files associated with the HM Dataset, and this Agreement. You hereby agree to cooperate with such audit.\n(6) Participant will not:\nmodify, translate, or create any derivative works based upon the HM Dataset;\ndistribute, copy, disclose, assign, sublicense, embed, host or otherwise transfer the HM Dataset to any third party, except as described in Section 1(b) above;\nremove or alter any copyright, trademark or other proprietary notices appearing on or in copies of the HM Dataset;\nuse the HM Dataset in a pornographic, defamatory or other unlawful manner, or in violation of any applicable regulations or laws;\nincorporate the HM Dataset into any other program, dataset, or product;\nuse the HM Dataset to distribute manipulated images or videos (except as described in Section 1(b) above); or\nuse the HM Dataset for any purpose other than the Purpose specified in this Agreement.\n(7) If you use the HM Dataset (or any portion thereof) in a manner that features models or property in connection with a subject that would be unflattering or unduly controversial to a reasonable person, you must indicate: (1) that the content is being used for illustrative purposes only, and (2) any person depicted in the content is a model. For example, you could say: \u201cStock photo. Posed by model.\u201d\n(8) Facebook always appreciates your feedback and other suggestions about the HM Dataset. However, you should know and you hereby agree that we may use your feedback and suggestions without any restriction or obligation, including, without limitation, to compensate you or to keep them confidential.\n(9) Upon the termination of this Agreement, Participant will immediately stop using the HM Dataset and destroy all copies of the HM Dataset and related materials in Participant\u2019s possession and control. Additionally, Facebook may, at any time, for any reason or for no reason, terminate this Agreement, effective immediately upon notice to the Participant. Upon termination, the license granted to Participant hereunder will immediately terminate and Participant will immediately stop using the HM Dataset and destroy all copies of the HM Dataset and related materials in Participant\u2019s possession or control. Except for the licenses granted to Participant, the other provisions of this Agreement will survive any termination.\n(10) THE HM DATASET IS PROVIDED \u201cAS IS\u201d WITHOUT ANY EXPRESS OR IMPLIED WARRANTY OF ANY KIND, INCLUDING WARRANTIES OF MERCHANTABILITY, TITLE, NON-INFRINGEMENT, OR FITNESS FOR ANY PARTICULAR PURPOSE.\n(11) IN NO EVENT WILL FACEBOOK, ITS CONTRACTORS AND ITS LICENSORS BE LIABLE FOR ANY CONSEQUENTIAL, INCIDENTAL, EXEMPLARY, PUNITIVE, SPECIAL, OR INDIRECT DAMAGES (INCLUDING DAMAGES FOR LOSS OF PROFITS, BUSINESS INTERRUPTION, OR LOSS OF INFORMATION) ARISING OUT OF OR RELATING TO THIS AGREEMENT OR ITS SUBJECT MATTER, EVEN IF FACEBOOK HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n(12) FACEBOOK, ITS LICENSORS AND ITS CONTRACTOR\u2019S TOTAL LIABILITY ARISING FROM OR RELATING TO THIS AGREEMENT AND ITS SUBJECT MATTER WILL NOT EXCEED ONE HUNDRED DOLLARS ($100).\n(13) Either party may terminate this Agreement if the other is in material breach of this Agreement and such breach remains uncured for thirty (30) days following receipt of written notice of the breach.\n(14) From time to time, Facebook may provide your name and the fact that you are a licensee of the HM Dataset to our licensors.\n(15) Participant will comply with all applicable export controls, import controls and trade sanctions applicable to the HM Dataset. You shall obtain, at your sole cost and expense, any export and import (temporary and permanent) license and other official authorization applicable to the HM Dataset. This Agreement, and your relationship with Facebook under this Agreement, shall be governed by the laws of the State of California without regard to its conflict of laws provisions. You and Facebook agree to submit to the exclusive jurisdiction of the courts located within the county of Santa Clara, California to resolve any legal matter arising from the Agreement. Notwithstanding this, you agree that Facebook shall still be allowed to apply for injunctive remedies (or an equivalent type of urgent legal relief) in any jurisdiction. Facebook may make changes to this Agreement at any time with notice to Participant and the opportunity to decline further use of the HM Dataset. You should look at the Agreement and check for notice of any changes regularly. Changes will not be retroactive. They will become effective, and will be deemed accepted by Participant, (a) immediately for those who become Participants after the notification is posted; or (b) for pre-existing Participants, on the date specified in the notice, which will be no sooner than 30 days after the changes are posted (except changes required by law which will be effective immediately). If You do not agree with the modifications to the Agreement, You may terminate Your use of HM Dataset, which will be Your sole and exclusive remedy. You agree that Your continued use of HM Dataset constitutes Your agreement to the modified terms of this Agreement. No failure to exercise and no delay in exercising any right, remedy or power hereunder will operate as a waiver thereof, nor will any single or partial exercise of any right, remedy or power hereunder preclude any other or further exercise thereof or the exercise of any other right, remedy or power provided herein or by law or in equity. Participant may not assign its rights and obligations hereunder without prior written consent of Facebook. Facebook may assign its rights and obligations hereunder at any time to any party without Participant\u2019s consent. If any provision of this Agreement is found by a court of competent jurisdiction to be void, invalid or unenforceable, the same will be reformed to comply with applicable law or stricken if not so conformable, so as not to affect the validity or enforceability of the remainder of this Agreement. This Agreement constitutes the entire agreement between the parties concerning the subject matter hereof and supersedes all prior or contemporaneous representations, discussions, negotiations, conditions, and agreements between the parties relating to the subject matter hereof.",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/70/hateful-memes-phase-2/"
    },
    {
        "competition_overview": "Overview\nWelcome to the Artificial Intelligence for Advancing Instruction (AIAI) Data Science Challenge! In this challenge, you will build models that classify instructional activities using multimodal classroom data.\nClassroom observation videos provide valuable insights into a teacher's instruction, student interactions, and classroom dynamics. Over the past 15 years, their use in teacher preparation and the study of teacher quality has increased significantly. Classroom videos are also a common source of data for educational researchers studying classroom interactions as well as a resource for professional development. Despite this growth, using video at scale remains challenging due to the time and resources required for processing and analysis. In this challenge, you will build models to help automate classroom observation so that it can be offered at scale and inform future teaching.\nPhases\nThere are two phases to this challenge:\nPhase 1: Model Development (June 9th, 2025 \u2013 August 5th, 2025)\u202f\u2190\u202fyou are here\nParticipants develop and refine their models using the training dataset, then generate predictions for the Phase 1 test set. Submissions are evaluated against the test set labels and ranked on the Phase 1 leaderboard. These scores contribute 25% to final prize rankings.\nPhase 2: Final Scoring (August 7th, 2025 \u2013 August 11th, 2025)\nParticipants make one submission against a new, unseen test set. Submissions are evaluated against the Phase 2 test set labels and ranked on the Phase 2 leaderboard. These scores contribute 75% to final prize rankings.\nPrizes\nPhase 1 End Date:\nAug. 5, 2025, 11:59 p.m. UTC\nPlace Prize Amount (Phase 2)\n1st $40,000\n2nd $20,000\n3rd $10,000\nHow to compete (Phase 1)\nGet permission to access the data and participate in the competition by requesting, completing, and submitting a data use agreement.\nClick the \"Compete!\" button in the sidebar to enroll in the competition.\nGet familiar with the problem through the overview and problem description. You might also want to reference additional resources available on the about page.\nAccess the Training and Phase 1 competition data via Globus to train your own model.\nUse your model to generate predictions that match the submission format.\nClick \u201cSubmit\u201d in the sidebar, and then \u201cMake new submission\u201d. You\u2019re in!\nChallenge rules\nIn addition to abiding to the terms of the data use agreement required to participate in this competition, participants must also adhere to the challenge rules. The challenge rules are in place to promote fair competition and useful solutions. If you are ever unsure whether your solution meets the challenge rules, ask the challenge organizers in the competition forum or send an email to info@drivendata.org.\nUse of pre-trained models\nUpdated on July 1st, 2025 in response to a question on the forum\nParticipants may use pre-trained models provided that the models were publicly available under a free and open source license at the beginning of the competition. If you want to use a tool that is not clearly designated as open source, you must reach out to competition organizers for approval at info@drivendata.org.\nThis rule exists to ensure that participants have all rights, licenses, and permissions to include the tool as part of their submission as stated in the Competition Rules.\nChallenge sponsor\nPrize generously supplied by our friends from the Artificial Intelligence for Advancing Instruction (AIAI) project at the University of Virginia.\nImages courtesy of the University of Virginia.",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/302/competition-uva-aiai-p1/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "NVDRS Data Sharing Agreement\nIn order to access the competition dataset, you must first agree to this National Violent Death Reporting System (NVDRS) Data Sharing Agreement (DSA). You may not use the competition dataset if you do not accept this DSA. By signing up for the competition, accessing the competition dataset, or both, you hereby agree to the terms of the DSA.\nThe National Violent Death Reporting System (NVDRS) Data Sharing Agreement (DSA) was created to govern the protection and use of sensitive or potentially identifiable NVDRS data, as required by the NVDRS Data Re-release Plan. Prior to release of NVDRS Restricted Access Data (RAD) by the Centers for Disease Control and Prevention (CDC), a DSA must be established for external users.\n1. Terms Governing Use, Protection, and Reporting of Data\nPrior to the release of any data, requestors, their collaborators, and an authorized institutional business official must agree to comply with all of the terms and conditions described below. The requestor\u2019s home institution might also require approval from the body of the requestor\u2019s home institution that is charged with the ethical review and approval of research projects. It is the requestor\u2019s responsibility to check with his or her institution and obtain any necessary approvals (e.g., IRB approval) for your records. CDC requires institutional IRB approval only when the proposal includes linking of NVDRS data with other data that contains personally identifiable information.\na. Use Limited to Research Project\nThe principal investigator(s) and all collaborators agree that the data will be used solely for the purpose approved by CDC. Furthermore, the principal investigator(s) and all collaborators agree to refrain from any attempt to link NVDRS RAD to any other dataset without prior permission from the CDC (any intention to link NVDRS RAD with other data should be specified in the initial request or follow-up request for the RAD and approved by the CDC). Researchers seeking to conduct additional analyses not specified in the approved proposal or to receive additional data years or variables that do not substantively change the scope of the approved project may contact the program at nvdrs-rad@cdc.gov to complete a project amendment request.\nb. Non-transferability of Agreement\nSubstantive changes made to the project (including, but not limited to, the appointment of a new principal investigator to complete the project, the inclusion of additional collaborators who will have access to the data, or a change in the principal investigator\u2019s institutional affiliation) require the execution of a new DSA, or an amendment to the existing DSA. It is the sole responsibility of the principal investigator to alert the CDC of such changes (within 21 days of the change). If the principal investigator of NVDRS RAD changes positions or leaves an agency or institution, CDC should be notified within a week, and the principal investigator will be required to destroy the data as directed by the CDC.\nc. No Disclosure of Data\nThe principal investigator(s) and all collaborators agree to employ all reasonable efforts to maintain the confidentiality of the data, with such efforts to be no less than the degree of care to preserve and safeguard its own data. The principal investigator(s) and all collaborators further agree not to disclose, reveal, or give the data, with or without charge, to any entity or any individual not listed in section III.k. without prior approval from the CDC.\nIn the event that the principal investigator is required by judicial or administrative process to disclose the data, the principal investigator must: (1) immediately notify the CDC\u2019s National Center for Injury Prevention and Control (NCIPC) and allow CDC a reasonable time to oppose the process; and (2) work in collaboration with the CDC to maintain confidentiality of the data. (The NVDRS Data Re-release Plan calls for expedited review and fast-track processing in the event of selected public health emergencies).\nd. Non-identification of Subjects\nThe principal investigator(s) and all collaborators of NVDRS RAD agree to the following confidentiality restrictions:\nNVDRS data will be used solely for statistical analyses related to the approved project. No attempt will be made to identify specific individuals, households, or institutions. Data lists at the individual level will not be published or distributed.\nIn the event of inadvertent discovery of the identity of any person during the course of the proposed project, the principal investigator(s) will (1) send an email to the RAD Help Desk at nvdrs-rad@cdc.gov which will then be routed by CDC to notify the NCIPC Associate Director for Science; (2) safeguard or destroy the identifying information as directed by the CDC; and (3) make no use of knowledge of the discovery. The identifying information must not be disclosed to any other individual or party.\nState VDRS data provided to CDC are protected under an Assurance of Confidentiality pursuant to Section 308(d) of the Public Health Service Act. An Assurance of Confidentiality is a formal confidentiality protection for data maintained by CDC authorized under Section 308(d) of the Public Health Service Act.\nIf the data use agreement is violated, the principal investigator will be restricted from using NVDRS data in the future. (https://www.cdc.gov/rdc/Data/b4/section308.pdf)\nThe data files provided to the researcher must be stored on, and accessed from, the secure computer system of the researcher\u2019s affiliated organization or institution. This means that the researcher should store files on a server that is behind a firewall, has data encryption, permits file access only to the approved researchers, and has encrypted network communications. The researcher would then access the files from a password-protected institutional desktop or laptop. If a secure computer system is not available, the researcher should store files on an encrypted, password-protected, stand-alone computer or laptop protected by anti-virus and anti-malware software.\nThe inadvertent disclosure of potentially identifying information is to be avoided by using the following guidelines for the release of statistics derived from the requested dataset. For any data release format:\nAnnual counts and rates must be suppressed for cities or counties of fewer than 100,000 people.\nCells showing or derived from fewer than 10 deaths must be suppressed, but \u201czero\u201d cells may be shown. Cell \u201csuppression\u201d will take one of two approaches: 1) combining row or column categories so as to eliminate the small cells, or 2) suppressing the small cell, another cell in the same row, another cell in the same column, and a fourth cell at the intersection of the row and column containing the second and third suppressed cells. Suppression of the second and third additional cells is necessary to prevent derivation of the small cell by subtraction from the row or column totals. Suppression of the fourth cell is necessary to prevent derivation of the second or third cells by subtraction. Beyond these specific guidelines, it must not otherwise be possible to derive identifying information by subtraction or other calculation from a table, or combination of tables, in any release format.\nRates are not to be computed for cells containing fewer than 20 deaths (or cases).\nThe disclosed data should never permit identification when used in combination with other known data.\ne. Maintenance of Data Security and Oversight\nThe principal investigator(s) and all collaborators must ensure that data security measures to secure the data, preserve confidentiality, and prevent unauthorized access are enforced and maintained at all times during possession of NVDRS RAD. The principal investigator shall ensure that no unauthorized person has access to the contents of NVDRS RAD files or to any files derived from RAD. Upon request, the principal investigator agrees to permit the inspection by the CDC of the physical storage, management, and handling of RAD files (at reasonable hours) and any other information relating to the DSA.\nf. Notification of Pending Publications\nThe principal investigator agrees to notify the CDC in advance as to when and where a publication of a report (or other public disclosure) from the project will appear. In addition, the principal investigator agrees to provide the CDC, in advance of its appearance, a copy of any manuscript or other public disclosure document. CDC will respect the embargoed information and requests that this information is provided for awareness and record keeping purposes.\ng. Non-endorsement Liability\nThe principal investigator(s) and all collaborators agree not to claim or imply Governmental endorsement of the research project, the entity, or personnel conducting the research project. Any published material derived from NVDRS data must acknowledge the CDC as the provider of the data and participating NVDRS states as the sources of the data. Published materials must also include a disclaimer that credits any analyses, interpretations, or conclusions reached by the author (i.e., the principal investigator(s) and any collaborators who received the data) to that author and not to the original sources of the data (i.e., NVDRS participating states) or to the CDC. The disclaimer should take the following form: \u201cThe National Violent Death Reporting System (NVDRS) is administered by the Centers for Disease Control and Prevention (CDC) by participating NVDRS states. The findings and conclusions of this study are those of the authors alone and do not necessarily represent the official position of the CDC or of participating NVDRS states.\u201d\nh. Termination and Disqualification\nThe CDC, in its sole discretion, may terminate the DSA if it determines that the principal investigator(s) and/or any collaborators are in violation of any condition of the DSA and such violation is not remedied within 30 days after the date of written notice of the violation. Furthermore, failure to comply with the DSA may result in the disqualification of the principal investigator(s) and collaborators from having access to the NVDRS data. Violations should be addressed by sending an e-mail to nvdrs-rad@cdc.gov.\ni. Duplication of Research\nThe principal investigator(s) and all collaborators of NVDRS RAD acknowledge that other researchers have access to NVDRS data in the form of public-use datasets and RAD and that duplication of research is a distinct possibility.\nj. Destruction of All Sensitive Files at Project Completion\nThe principal investigator agrees to destroy all NVDRS RAD files, and all derived files three years from the receipt of the data unless otherwise specified. Before the 3 year period expires, the principal investigator can apply to extend this destruction date. Researchers can renew after the allowable access period for up to 2 one-year extensions.\nThe Participant warrants and represents that he/she/they has the requisite power and authority to enter into this DSA and to perform according to its terms, and that the Participant agreeing to this DSA by signing up for the competition has authority to do so.",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/295/cdc-automated-abstraction/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "NVDRS Data Sharing Agreement\nIn order to access the competition dataset, you must first agree to this National Violent Death Reporting System (NVDRS) Data Sharing Agreement (DSA). You may not use the competition dataset if you do not accept this DSA. By signing up for the competition, accessing the competition dataset, or both, you hereby agree to the terms of the DSA.\nThe National Violent Death Reporting System (NVDRS) Data Sharing Agreement (DSA) was created to govern the protection and use of sensitive or potentially identifiable NVDRS data, as required by the NVDRS Data Re-release Plan. Prior to release of NVDRS Restricted Access Data (RAD) by the Centers for Disease Control and Prevention (CDC), a DSA must be established for external users.\n1. Terms Governing Use, Protection, and Reporting of Data\nPrior to the release of any data, requestors, their collaborators, and an authorized institutional business official must agree to comply with all of the terms and conditions described below. The requestor\u2019s home institution might also require approval from the body of the requestor\u2019s home institution that is charged with the ethical review and approval of research projects. It is the requestor\u2019s responsibility to check with his or her institution and obtain any necessary approvals (e.g., IRB approval) for your records. CDC requires institutional IRB approval only when the proposal includes linking of NVDRS data with other data that contains personally identifiable information.\na. Use Limited to Research Project\nThe principal investigator(s) and all collaborators agree that the data will be used solely for the purpose approved by CDC. Furthermore, the principal investigator(s) and all collaborators agree to refrain from any attempt to link NVDRS RAD to any other dataset without prior permission from the CDC (any intention to link NVDRS RAD with other data should be specified in the initial request or follow-up request for the RAD and approved by the CDC). Researchers seeking to conduct additional analyses not specified in the approved proposal or to receive additional data years or variables that do not substantively change the scope of the approved project may contact the program at nvdrs-rad@cdc.gov to complete a project amendment request.\nb. Non-transferability of Agreement\nSubstantive changes made to the project (including, but not limited to, the appointment of a new principal investigator to complete the project, the inclusion of additional collaborators who will have access to the data, or a change in the principal investigator\u2019s institutional affiliation) require the execution of a new DSA, or an amendment to the existing DSA. It is the sole responsibility of the principal investigator to alert the CDC of such changes (within 21 days of the change). If the principal investigator of NVDRS RAD changes positions or leaves an agency or institution, CDC should be notified within a week, and the principal investigator will be required to destroy the data as directed by the CDC.\nc. No Disclosure of Data\nThe principal investigator(s) and all collaborators agree to employ all reasonable efforts to maintain the confidentiality of the data, with such efforts to be no less than the degree of care to preserve and safeguard its own data. The principal investigator(s) and all collaborators further agree not to disclose, reveal, or give the data, with or without charge, to any entity or any individual not listed in section III.k. without prior approval from the CDC.\nIn the event that the principal investigator is required by judicial or administrative process to disclose the data, the principal investigator must: (1) immediately notify the CDC\u2019s National Center for Injury Prevention and Control (NCIPC) and allow CDC a reasonable time to oppose the process; and (2) work in collaboration with the CDC to maintain confidentiality of the data. (The NVDRS Data Re-release Plan calls for expedited review and fast-track processing in the event of selected public health emergencies).\nd. Non-identification of Subjects\nThe principal investigator(s) and all collaborators of NVDRS RAD agree to the following confidentiality restrictions:\nNVDRS data will be used solely for statistical analyses related to the approved project. No attempt will be made to identify specific individuals, households, or institutions. Data lists at the individual level will not be published or distributed.\nIn the event of inadvertent discovery of the identity of any person during the course of the proposed project, the principal investigator(s) will (1) send an email to the RAD Help Desk at nvdrs-rad@cdc.gov which will then be routed by CDC to notify the NCIPC Associate Director for Science; (2) safeguard or destroy the identifying information as directed by the CDC; and (3) make no use of knowledge of the discovery. The identifying information must not be disclosed to any other individual or party.\nState VDRS data provided to CDC are protected under an Assurance of Confidentiality pursuant to Section 308(d) of the Public Health Service Act. An Assurance of Confidentiality is a formal confidentiality protection for data maintained by CDC authorized under Section 308(d) of the Public Health Service Act.\nIf the data use agreement is violated, the principal investigator will be restricted from using NVDRS data in the future. (https://www.cdc.gov/rdc/Data/b4/section308.pdf)\nThe data files provided to the researcher must be stored on, and accessed from, the secure computer system of the researcher\u2019s affiliated organization or institution. This means that the researcher should store files on a server that is behind a firewall, has data encryption, permits file access only to the approved researchers, and has encrypted network communications. The researcher would then access the files from a password-protected institutional desktop or laptop. If a secure computer system is not available, the researcher should store files on an encrypted, password-protected, stand-alone computer or laptop protected by anti-virus and anti-malware software.\nThe inadvertent disclosure of potentially identifying information is to be avoided by using the following guidelines for the release of statistics derived from the requested dataset. For any data release format:\nAnnual counts and rates must be suppressed for cities or counties of fewer than 100,000 people.\nCells showing or derived from fewer than 10 deaths must be suppressed, but \u201czero\u201d cells may be shown. Cell \u201csuppression\u201d will take one of two approaches: 1) combining row or column categories so as to eliminate the small cells, or 2) suppressing the small cell, another cell in the same row, another cell in the same column, and a fourth cell at the intersection of the row and column containing the second and third suppressed cells. Suppression of the second and third additional cells is necessary to prevent derivation of the small cell by subtraction from the row or column totals. Suppression of the fourth cell is necessary to prevent derivation of the second or third cells by subtraction. Beyond these specific guidelines, it must not otherwise be possible to derive identifying information by subtraction or other calculation from a table, or combination of tables, in any release format.\nRates are not to be computed for cells containing fewer than 20 deaths (or cases).\nThe disclosed data should never permit identification when used in combination with other known data.\ne. Maintenance of Data Security and Oversight\nThe principal investigator(s) and all collaborators must ensure that data security measures to secure the data, preserve confidentiality, and prevent unauthorized access are enforced and maintained at all times during possession of NVDRS RAD. The principal investigator shall ensure that no unauthorized person has access to the contents of NVDRS RAD files or to any files derived from RAD. Upon request, the principal investigator agrees to permit the inspection by the CDC of the physical storage, management, and handling of RAD files (at reasonable hours) and any other information relating to the DSA.\nf. Notification of Pending Publications\nThe principal investigator agrees to notify the CDC in advance as to when and where a publication of a report (or other public disclosure) from the project will appear. In addition, the principal investigator agrees to provide the CDC, in advance of its appearance, a copy of any manuscript or other public disclosure document. CDC will respect the embargoed information and requests that this information is provided for awareness and record keeping purposes.\ng. Non-endorsement Liability\nThe principal investigator(s) and all collaborators agree not to claim or imply Governmental endorsement of the research project, the entity, or personnel conducting the research project. Any published material derived from NVDRS data must acknowledge the CDC as the provider of the data and participating NVDRS states as the sources of the data. Published materials must also include a disclaimer that credits any analyses, interpretations, or conclusions reached by the author (i.e., the principal investigator(s) and any collaborators who received the data) to that author and not to the original sources of the data (i.e., NVDRS participating states) or to the CDC. The disclaimer should take the following form: \u201cThe National Violent Death Reporting System (NVDRS) is administered by the Centers for Disease Control and Prevention (CDC) by participating NVDRS states. The findings and conclusions of this study are those of the authors alone and do not necessarily represent the official position of the CDC or of participating NVDRS states.\u201d\nh. Termination and Disqualification\nThe CDC, in its sole discretion, may terminate the DSA if it determines that the principal investigator(s) and/or any collaborators are in violation of any condition of the DSA and such violation is not remedied within 30 days after the date of written notice of the violation. Furthermore, failure to comply with the DSA may result in the disqualification of the principal investigator(s) and collaborators from having access to the NVDRS data. Violations should be addressed by sending an e-mail to nvdrs-rad@cdc.gov.\ni. Duplication of Research\nThe principal investigator(s) and all collaborators of NVDRS RAD acknowledge that other researchers have access to NVDRS data in the form of public-use datasets and RAD and that duplication of research is a distinct possibility.\nj. Destruction of All Sensitive Files at Project Completion\nThe principal investigator agrees to destroy all NVDRS RAD files, and all derived files three years from the receipt of the data unless otherwise specified. Before the 3 year period expires, the principal investigator can apply to extend this destruction date. Researchers can renew after the allowable access period for up to 2 one-year extensions.\nThe Participant warrants and represents that he/she/they has the requisite power and authority to enter into this DSA and to perform according to its terms, and that the Participant agreeing to this DSA by signing up for the competition has authority to do so.",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/296/cdc-novel-variables/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/217/cdc-fall-narratives/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/63/genetic-engineering-attribution/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Note: This is Phase 2 of the Pushback to the Future challenge and is only open to finalists from Phase 1 of the challenge.\nCongratulations and welcome to Phase 2!\nYou developed one of the best submissions for Phase 1 of the Pushback to the Future: Predict Pushback Time at US Airports challenge and are now invited to help NASA explore the use of federated learning to improve pushback predictions!\nThe goal of Phase 2 is to explore federated learning techniques that enable airlines to safely contribute private data towards a centralized model without sharing or pooling that data. Airlines collect a lot of information that is relevant to pushback time but valuable or sensitive, like the number of passengers that have checked in for a flight or the number of bags that have been loaded onto a plane. In Phase 2, we're asking you to translate your winning Phase 1 model into a model that can be trained in a federated manner.\nTo complete Phase 2:\nDownload the data.\nDevelop your Phase 1 model into a federated model (see the Problem description for more detail).\nUpload your code and documentation to the Development submission page during the development period.\nSubmit predictions during the evaluation period. (We will add an upload page at the start of the evaluation period.)\nThe deadlines for each period are listed below.\nHere are a few ways to get started:\nRead the Problem description.\nLog into the forum and post an introduction.\nMark the office hours dates on your calendar.\nCheck out the simple pushback federated learning example.\nTimeline\nPeriod Start date End date\nDevelopment June 1st July 17th\nEvaluation July 19th July 31st\nDevelopment period\nThe first period of Phase 2 will be a development period during which you will focus on translating your Phase 1 model to a federated model. Visit the Data download page to download a training dataset partitioned into public and private data. You can read more about the data format in the Problem description. By no later than July 17th, upload your submission in the Development submission page. Development submissions need to include:\nFederated training code, inference code, and model assets.\nA write-up documenting how you approached federating your model.\nEvaluation period\nAt the start of the evaluation period, we will release test features so you can generate predictions. The test data will cover the same time period and flights as the Phase 1 test period (subsetting to only the selected airlines) and will have the same format as the train features.\nAfter the development period, participants will not be allowed to change their model training code or retrain their model. If any minor updates are needed to run your code, you will need to submit a detailed change log documenting each change and why it was necessary. By no later than July 31st, upload your evaluation submission (the upload page will be added during the evaluation period). Evaluation submissions need to include:\nTest predictions matching the submission format, which will be released at the beginning of the evaluation period.\nIf any changes were needed to run your code, the updated code and detailed change log.\nForum\nWe've created a discussion forum for the finalists, the NASA team, and DrivenData to discuss Phase 2. You can use the forum to request help, share insights, confirm what is allowed, and generally discuss your experiments with federated learning.\nThe Phase 2 forum is private, and all finalists who already had a forum account have been added. To request access if you have not yet been added:\nLog in to the forum at https://community.drivendata.org/. Your forum user is linked to your DrivenData user, so sign in with your DrivenData username and password.\nClick \u2630 to see available Groups, and select the \"Pushback to the Future Phase 2 Participants\" tile.\nClick \"Request\". You should be approved within a few days at most.\nOffice hours\nNASA will host three optional office hours for open discussion about the challenge, data, federated learning, and whatever else is on your mind! Office hours are scheduled for:\nWed, June 21st 2:30 - 4 PM EST\nThu, July 13th 2:30 - 4 PM EST\nWed, July 26th 2:30 - 4 PM EST\nWe'll post the meeting links in the forum a few days prior to the event. You are encouraged to reach out to NASA on the forum and in office hours as you develop your solutions. They are looking forward to hearing what you find out!\nThis challenge is in collaboration with NASA\nHeader Image: \"Pushback...\" by Cory W. Watts is licensed under CC BY-SA 2.0",
        "url": "https://www.drivendata.org/competitions/218/competition-nasa-airport-pushback-phase2/"
    },
    {
        "competition_overview": "Coordinating our nation\u2019s airways is the role of the National Airspace System (NAS). The NAS is arguably the most complex transportation system in the world. Operational changes can save or cost airlines, taxpayers, consumers, and the economy at large thousands to millions of dollars on a regular basis. It is critical that decisions to change procedures are done with as much lead time and certainty as possible. The NAS is investing in new ways to bring vast amounts of data together with state-of-the-art machine learning to improve air travel for everyone.\nIn order to optimize commercial aircraft flights, air traffic management systems need to be able to predict as many details about a flight as possible. One significant source of uncertainty comes right at the beginning of a flight: the pushback time. A more accurate pushback time can lead to better predictability of take off time from the runway.\nPredicting pushback time depends upon factors like passenger loading, cargo loading, weather, aircraft type, and operator procedures. While available data can be used to improve these predictions, the combination of public and private sources can make it difficult to get access to all of the information needed to make the best predictions. Federated learning (FL) offers immense promise here as an approach to training central ML models using private data held by separate organizations.",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/149/competition-nasa-airport-pushback/"
    },
    {
        "competition_overview": "Coordinating our nation\u2019s airways is the role of the National Airspace System (NAS). The NAS is arguably the most complex transportation system in the world. Operational changes can save or cost airlines, taxpayers, consumers, and the economy at large thousands to millions of dollars on a regular basis. It is critical that decisions to change procedures are done with as much lead time and certainty as possible. The NAS is investing in new ways to bring vast amounts of data together with state-of-the-art machine learning to improve air travel for everyone.\nIn order to optimize commercial aircraft flights, air traffic management systems need to be able to predict as many details about a flight as possible. One significant source of uncertainty comes right at the beginning of a flight: the pushback time. A more accurate pushback time can lead to better predictability of take off time from the runway.\nPredicting pushback time depends upon factors like passenger loading, cargo loading, weather, aircraft type, and operator procedures. While available data can be used to improve these predictions, the combination of public and private sources can make it difficult to get access to all of the information needed to make the best predictions. Federated learning (FL) offers immense promise here as an approach to training central ML models using private data held by separate organizations.",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/182/competition-nasa-airport-pushback-prescreened/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/88/competition-air-quality-pm/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/91/competition-air-quality-no2/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Good news! The research team from this competition has made the challenge data available on the IEEE DataPort for ongoing use, practice and learning.\nThis data is maintained by the research team and released at their discretion.\nData link: Urban Semantic 3D Dataset\nPlease note that we won't be able to field questions about the data, but we wanted to share it here for the benefit of the community. If you have a question, feel free to post it to the forum.\nApproved for public release, 21-810",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/78/overhead-geopose-challenge/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Good news! The project team from this competition has made the training data for this competition available on Academic Torrents for ongoing use, practice and learning. This data is maintained by the project team and released at their discretion.\nDataset: N+1 fish, N+2 fish dataset (train_videos)\nAdditional training data using electronic monitoring systems for fisheries can be found at Fishnet.AI.\nPlease note that we won't be able to field questions about the data, but we wanted to share it here for the benefit of the community. If you have a question, feel free to post it to the forum.\nFor more details on the results and winners of the competition, including links to open source solutions, check out the results blog post.",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/48/identify-fish-challenge/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/260/spacecraft-detection/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/261/spacecraft-pose-estimation/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/89/competition-nasa-airport-configuration/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/92/competition-nasa-airport-configuration-prescreened/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Open Data\nGood news! The project team from this competition has made the challenge data available for ongoing use, practice and learning. This data will be maintained by the project team and released at their discretion.\nFor additional information about the data, see the Problem Description page.\nPlease note that we won't be able to field questions about the data, but we wanted to share it here for the benefit of the community. If you have a question, feel free to post it to the forum.\nAccess the data: Beluga ID 2022",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/96/beluga-whales/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/298/literacy-screening/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/252/ai-research-assistants/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Open data\nGood news! The dataset from this challenge has been approved for sharing for ongoing learning and development. The data are available to use outside of the challenge with attribution.\nThe cyanobacteria density dataset provided to participants is available through the SeaBASS data archive. The archive includes:\nA .tgz TAR file with dataset documentation in PDF format\nA folder called archive, which contains the full challenge data in .sb format\nSee the SeaBASS website for instructions on working with TAR files and .sb files. For additional information about the data, see the Problem description page.\nIf you publish your work, please cite the challenge data and SeaBASS.\nS. Gupta, E. Gelbart, R. Gupta, K. Wetstone, and E. Dorne (2024). Cyanobacteria Aggregated Manual Labels Dataset (NASA and DrivenData). SeaBASS. http://dx.doi.org/10.5067/SeaBASS/CAML/DATA001\nPlease note that we won't be able to field questions about the data, but we wanted to share it here for the benefit of the community. If you have a question, feel free to post it to the forum.\nAccess the data:\nSeaBASS archive\nExplore the data in the GEE community catalog",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/143/tick-tick-bloom/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Open Data\nGood news! The project team from this competition has made data from this challenge available for ongoing use, practice and learning. Data from the previous Mars Spectrometry challenge is available here. This data will be maintained by the project team and released at their discretion.\nFor additional information about the data, see the Problem Description page.\nPlease note that we won't be able to field questions about the data, but we wanted to share it here for the benefit of the community. If you have a question, feel free to post it to the forum.\nDataset links\nMars Spectrometry 2: Gas Chromatography for the Sample Analysis at Mars Data (SAM) Instrument\nMars Spectrometry: Detect Evidence for Past Habitability",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/97/nasa-mars-gcms/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Open Data\nGood news! The project team from this competition has made some of the data from this challenge available for ongoing use, practice and learning. Data from the Mars Spectrometry GCMS challenge is also available here. This data will be maintained by the project team and released at their discretion.\nFor additional information about the data, see the Problem Description page.\nPlease note that we won't be able to field questions about the data, but we wanted to share it here for the benefit of the community. If you have a question, feel free to post it to the forum.\nDataset links\nMars Spectrometry: Detect Evidence for Past Habitability\nMars Spectrometry 2: Gas Chromatography for the Sample Analysis at Mars Data (SAM) Instrument",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/93/nasa-mars-spectrometry/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Good news! The research team from this competition has made the challenge data available on the NCEI website for ongoing use, practice and learning.\nThis data is maintained by the research team and released at their discretion.\nData link: MagNet: Model the Geomagnetic Field Dataset\nPlease note that we won't be able to field questions about the data, but we wanted to share it here for the benefit of the community. If you have a question, feel free to post it to the forum.",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/73/noaa-magnetic-forecasting/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Good news! Schneider Electric has made the training data for this competition available on their Data Exchange for ongoing use, practice and learning.\nIn order to access the data, you\u2019ll first need to create an account on the Schneider Exchange. Then follow the link below to add the competition dataset to your Digital Library, where you\u2019ll be able to freely access the data. These datasets are maintained by Schneider Electric and released at their discretion.\nDataset: Detecting anomalies in building energy usage\nPlease note that we won't be able to field questions about the data, but we wanted to share it here for the benefit of the community. If you have a question, feel free to post it to the forum.\nFor more details on the results and winners of the competition, including links to open source solutions, check out the results blog post.",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/52/anomaly-detection-electricity/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Data access instructions\nReal-world patient data is highly sensitive and difficult to share safely. The data for this competition has identifying factors removed, but still details the care of vulnerable and still-living people.\nTo access the competition data, each participant will need to register with PhysioNet under MIT\u2019s agreement and complete an online training course. Then you will have access to the world\u2019s largest publicly available repository of patient data!\nDetailed steps to register for data access are below:\nCreate an account on the PhysioNet platform by visiting https://physionet.org/register/\nThe online training course is provided by the CITI Program. Go to https://about.citiprogram.org/ and create an account there there using the following steps:\nClick on \u201cRegister\u201d in the upper right hand corner\nOn the registration page, click the \u201cSelect your organization affiliation\u201d button on the left. When asked to enter your affiliation during this process, type/select \u201cMassachusetts Institute of Technology Affiliates\u201d as your organization affiliation.\nAgree to the terms of service, the privacy policy, and affirm that you are an affiliate by checking the appropriate checkboxes\nEnter your information (name, email) in step 2, select your username/password in step 3, and answer questions in step 4\nOn the subsequent \u201cYour CE Credit Status\u201d page, you may respond \u201cNO\u201d to the CE Credit Status prompt\nOn the subsequent \u201cAffiliate with an Institution\u201d page, fill out all required information. You do not need to use an institutional email address\nEnter \u201cSNOMED Entity Linking Challenge\u201d as your \u201cDepartment\u201d\nEnter \u201cStatistician\u201d as your \u201cRole\u201d\nOn the \u201cSelect Curriculum\u201d page, answer \u201cBasic IRB Data or Specimens Only Research\u201d to question 1 and fill out any other required fields.\nYou should now see the \u201cData or Specimens Only Research\u201d course in your active courses. Complete this course, which contains 9 segments. You need to achieve an overall score of 90% or higher.\nOnce you have completed the course, click \u201cView/Print\u201d and save a copy of the Completion Report (not the certificate).\nGo to https://physionet.org/settings/training/ and upload the report, then click \u201cSubmit Training\u201d.\nGo to https://physionet.org/credential-application/ to submit a credential application\nFor \u201cResearcher Category\u201d, select the \u201cIndependent Researcher\u201d\nFor the \u201cReference\u201d, fill out the following fields\nReference Category: Other\nReference name: SNOMED CT Entity Linking Challenge\nReference email: challenge@snomed.org\nReference organization: SNOMED\nReference job title or position: Challenge\nResearch topic: I will be using the MIMIC-IV-Note dataset to participate in the SNOMED CT Entity Linking Challenge hosted on the DrivenData platform.\nAt this point the PhysioNet team will process your credentialing and training applications. The process is normally complete within 24-48 hours.\nOnce you have received email notifications that each of your \u201ccredentialing\u201d and \u201ctraining\u201d applications have been accepted, there is a final step: complete the Data Usage Agreement (DUA). To do this, log into your PhysioNet account and navigate to the PhysioNet challenge page.\nScroll to the very bottom of the page and you will see a red box reading: \u201csign the data use agreement for the project\u201d. Click that to agree.\nAt this point, you should be able to download the training notes and annotations files for this competition. If you encounter any issues, please post to the forum or send an email to challenge@snomed.org.",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/258/competition-snomed-ct/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Data resources\nDigital pathologists must leverage specialized knowledge about medical pathology and the specific digital data formats used to represent microscope slides in their analyses. We've provided information about these domains to help you get started. If you want to dive deeper into the field of digital pathology, check the bottom of the page for additional references. Let's get started!\nSubject matter resources\nMelanoma pathology\nAnnotated slides\nImage artifacts\nWhole slide image resources\nPyramidal TIFs\nLibraries\nTips and tricks\nMelanoma pathology\nPathologists examine portions of the tissue, structure, or organ with the melanoma with a microscope to diagnose a patient with melanoma and to predict the probability that they will relapse. Physicians also take clinical factors into account when assessing a patient's likelihood of relapse. These include age, sex, and medical history of melanoma.\nIn order to make a prediction about the likelihood of relapse, pathologists rely primarily on two factors:\nBreslow depth: the thickness of the tumor measured in millimeters. Melanomas that are less than 1 millimeter thick are called \"thin\" melanomas, and are usually associated with a good prognosis (>95% five-year survival rate)\u2014however, these melanomas also comprise a large and poorly understood proportion of relapses and deaths. Melanomas of intermediate thickness (between 1 and 4 millimeters) have a higher risk of relapse, and are also not well understood.1 Thicker melanomas (over 4 millimeters) have a 50% risk of relapse within 5 years.\nUlceration: the presence of a lesion that has total loss of epidermal tissue.\nExamples of breslow measurements and ulceration are provided in a set of 16 annotated slides (described below) provided by VisioMel to help finalists learn what pathologists pay attention to when looking at slides.\nAnnotated slides\nOur partners at VisioMel have provided a set of 16 annotated slides that help illustrate a pathologist's analysis. These slides also appear in the training data without any annotations. These annotations are available on the data download page in the file annotations.csv, which provides the geometries of the annotations in well-known text (WKT) and openCV representations and their respective labels. The origin for the WKT geometries is the bottom-left corner and the origin for the openCV geometries is the top-left corner. The page also contains visualized_annotations.pdf, which overlays these annotations on their respective tifs.\nThe slide images are very large and contain a mixture of lesioned and otherwise normal tissue. They vary substantially in terms of the amount of white space, coloration, and the number of slides or samples pictured. Your solution will need to handle these variations.\nIn looking at the annotated slides, it is important to keep a few points in mind:\nPathologists have annotated images to point out regions that represent melanoma and normal areas. However, annotated regions do not necessarily include all lesioned tissue in the slide. An unannotated region is not necessarily normal tissue.\nThe lesion may fall entirely within the square, or may extend beyond the annotation boundaries.\nAs images have different resolutions in pixels/micrometer, annotations will have different dimensions in terms of pixels. When using the geometries, it is important to know the origin of the coordinate system.\nAbout two-thirds of skin melanomas occur in a healthy skin free of blemishes or lesions. However, in 30% of cases, melanoma develops from an existing beauty spot or \"naevus,\" a benign lesion. In this case, the malignant melanoma tissue and the healthy nevus tissue are closely associated.\nArtifacts in images\nSome images in this challenge may have a few anomalies that participants might want to look out for. These quirks are not relevant to the task of predicting relapse.\nSome slides may be partially out of focus or highly zoomed:\nSome slides have black spots around the edges. These are from the slide holder used during the scanning process and are not relevant to the melanoma.\nWorking with whole slide images\nWhole slide images (WSIs) are digital formats that allow glass slides to be viewed, managed, shared, and analyzed on a computer monitor. These are extremely high resolution images that are provided to you as pyramidal TIFs.\nPyramidal TIFs\nPyramidal TIFs are a multi-resolution, tiled format. Each resolution is stored as a separate layer or \"page\" within the TIF. Page 0 contains the image at the full resolution of the WSI. Subsequent pages contain lower resolution images\u2015each layer is the previous layer downsampled by a factor of two. In other words, relative to the full resolution, the pages are downsampled 2x, 4x, 8x, 16x, and so on.\nAn illustration of the multi-scale property of the pyramidal TIF image format. Source: Cmglee - Own work, CC BY-SA 3.0, Link\nThe full-resolution slide images are enormous, and you may not be able to load even a single full-resolution image into memory. You will need to get creative in your use of multiple scales! For example, one of the first things you will notice browsing the images is that much of the slide is blank space with no tissue whatsoever. It would be a waste of your limited compute time to analyze those parts at full resolution. One simple solution is to segment the image into tissue/non-tissue at a lower resolution and analyze only the tissue regions at higher resolutions.\nLibraries\nPyVips\nPyVips is a Python binding for libvips, a low-level library for working with large images. PyVips can be used to read and manipulate pyramidal TIF formats. Here's a code snippet to perform some basic image operations using PyVips:\nimport pyvips\n\n# load the full-resolution image\npath = \"slide1.tif\"\nslide = pyvips.Image.new_from_file(path, page=0)\nprint(slide.width, slide.height)\n>> 67456 84224\n\n# load the 16x downsampled image from page 4 (2 ** 4 = 16)\nslide = pyvips.Image.new_from_file(path, page=4)\nprint(slide.width, slide.height)\n>> 4216 5264\n\n# read an image region in as a PIL Image\nx, y = 100, 200\nlevel = 2\nregion_width = 500\nregion_height = 500\nregion = slide.crop(x, y, region_width, region_height)\n\n# convert the PIL Image to a numpy array\nimport numpy\n\narray = np.ndarray(\n    buffer=region.write_to_memory(),\n    dtype=np.uint8,\n    shape=(region.height, region.width, region.bands)\n)\n# red, green, blue channels\nprint(array.shape)\n>> (500, 500, 3)\n\n# also look into pyvips.Region.fetch for faster region read\nregion = pyvips.Region.new(slide).fetch(x, y, region_width, region_height)\n\nbands = 3\narray = np.ndarray(\n    buffer=region,\n    dtype=np.uint8,\n    shape=(region_height, region_width, bands)\n)\nprint(array.shape)\n>> (500, 500, 3)\nCytomine\nCytomine allows you to display and explore native whole slide images and pyramidal TIF formats in a web browser. It also supports adding annotations and executing scripts from inside Cytomine or from any computing server using the dedicated Cytomine Python client. Cytomine can be installed locally or on any Linux server. The Cytomine GitHub repository includes examples of Python scripts demonstrating how to interact with your Cytomine instance, as well as examples of ready-to-use machine learning scripts (all S_ prefixed repos, such as S_CellDetect_Stardist_HE_ROI).\nTips and tricks\nHere are a few tips to working with the data:\nSpend time learning how to efficiently process these enormous images. Due to the time limit on submissions, you cannot process every part of the image at the highest resolution available. Instead, consider methods to first predict which parts of the image are most important for an accurate diagnosis. The pyramidal TIF format contains multiple zoom levels that you can access with the page keyword argument to pyvips.Image.new_from_file. You might try a \"multiscale\" approach\u2015detect the most important parts of the image from lower resolutions versions (pyvips.Image.new_from_file(path, page=5)) and then analyze just those parts at higher resolutions.\nUse pyvips.Region.fetch instead of pyvips.Image.crop to read in small image regions. According to a PyVips developer and our own tests, pyvips.Region.new(image).fetch(...) is much faster than pyvips.Image.crop(...). (Requires libvips version 0.8.6 or greater.)\nAvoid OpenSlide-based data loaders (pyvips.Image.openslideload and openslide.OpenSlide) in your machine learning pipelines. OpenSlide is no longer under active development, and several issues have arisen that have not been addressed.\nAdditional reading and research\nHere are a few papers and tutorials that talk about machine learning with WSI that you may find helpful:\nAn attention-based multi-resolution model for prostate whole slide image classification and localization\nUsing deep convolutional neural networks to identify and classify tumor-associated stroma in diagnostic breast biopsies\nAssessment of machine learning of breast pathology structures for automated differentiation of breast cancer and high-risk proliferative lesions\nHistologic tissue components provide major cues for machine learning-based prostate cancer detection and grading on prostatectomy specimens\nAssessment of machine learning detection of environmental enteropathy and celiac disease in children",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/148/visiomel-melanoma/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Data resources\nWhole slide images are digital formats that allow glass slides to be viewed, managed, shared, and analyzed on a computer monitor. These extremely high resolution images require special software to be able to read and manipulate in memory. A few different packages exist to do this in Python, and we will cover the main ones here. Each WSI image in the train set is also available as a pyramidal TIF, so we'll cover some helpful tips for that format too. All test images are pyramidal TIFs.\nFormats\nNative whole slide images\nThe native whole slide image (WSI) formats present in our dataset are:\n.mrxs (MIRAX)\n.svs (Aperio)\n.ndpi (Hamamatsu)\nThese formats are related to the scanner used to create the images. These are provided in the native resolution and magnification. You can determine the exact resolution and magnification by referring to train_metadata.csv or test_metadata.csv. While the pyramidal TIF format has standardized, precomputed zoom levels, the available levels for WSI can vary. You can determine the zoom levels for each image using OpenSlide.\nPyramidal TIF\nWe recommend working from the pyramidal TIF versions of the slides in your machine learning workflow. The software tools for this format are better maintained and less prone to error than those available for WSIs.\nPyramidal TIFs are a multi-resolution, tiled format. Each resolution is stored as a separate layer or \"page\" within the TIF. Page 0 contains the image at the full resolution of the WSI. Subsequent pages contain lower resolution images\u2015each layer is the previous layer downsampled by a factor of two, until the largest dimension of the downsampled image is less than 512 pixels. In other words, relative to the full resolution, the pages are downsampled 2x, 4x, 8x, 16x, and so on. Each layer is stored as individual 512x512 \"tiles\" where each tile is compressed using JPEG with quality (Q) 75.\nAn illustration of the multi-scale property of the pyramidal TIF image format. Source: Cmglee - Own work, CC BY-SA 3.0, Link\nUsing the full-resolution provided in the metadata csv, you can calculate the resolution of each layer as full_image_resolution * (2 ** page). The resolution reported in the metadata CSVs is in the units microns/pixel, so larger numbers are more zoomed out.\nGiven the enormous size of the full-resolution images, which makes it impossible to load even a single image into memory, you will need to get creative in your use of multiple scales! For example, one of the first things you will notice browsing the images is that much of the slide is blank space with no tissue whatsoever. It would be a waste of your limited compute time to analyze those parts at full resolution. One simple solution is to segment the image into tissue/non-tissue at a lower resolution and analyze only the tissue regions at higher resolutions.\nLibraries\nOpenSlide\nOpenSlide supports all native whole slide image formats, including:\n.mrxs (MIRAX)\n.svs (Aperio)\n.ndpi (Hamamatsu)\nHere's a code snippet to perform some basic image operations using OpenSlide:\nimport openslide\n\n# load the slide\nslide = openslide.OpenSlide(\"slide1.ndpi\")\n\n# number of levels\nprint(slide.level_count)\n>> 8\n\n# dimensions for each level in the image\nprint(slide.level_dimensions)\n>> ((67456, 84224), (33728, 42112), (16864, 21056), (8432, 10528), (4216, 5264), (2108, 2632), (1054, 1316), (527, 658))\n\n# magnification for each level\nprint(slide.level_downsamples)\n\n# highest resolution available in micrometers per pixel\nprint(slide.properties[\"openslide.mpp-x\"], slide.properties[\"openslide.mpp-y\"])\n>> 0.22717462913741793 0.22717462913741793\n\n# generate thumbnail\nthumbnail = slide.get_thumbnail((output_height, output_width))\n\n# read an image region in as a PIL Image\nx, y = 100, 200\nlevel = 2\nregion_width = 500\nregion_height = 500\nregion = slide.read_region((x, y), level, (region_width, region_height))\n\n# convert the PIL Image to a numpy array\narray = np.array(region)\n\n# red, green, blue, alpha channels\nprint(array.shape)\n>> (500, 500, 4)\nOpenSlide Python is unable to read in the pyramidal TIF formats. To do that, you should use PyVips.\nPyVips\nPyVips is a Python binding for libvips, a low-level library for working with large images. PyVips can be used to read and manipulate the pyramidal TIF formats. Here's a code snippet to perform some basic image operations using PyVips:\nimport pyvips\n\n# load the full-resolution image\npath = \"slide1.tif\"\nslide = pyvips.Image.new_from_file(path, page=0)\nprint(slide.width, slide.height)\n>> 67456 84224\n\n# load the 16x downsampled image from page 4 (2 ** 4 = 16)\nslide = pyvips.Image.new_from_file(path, page=4)\nprint(slide.width, slide.height)\n>> 4216 5264\n\n# read an image region in as a PIL Image\nx, y = 100, 200\nlevel = 2\nregion_width = 500\nregion_height = 500\nregion = slide.crop(x, y, region_width, region_height)\n\n# convert the PIL Image to a numpy array\nimport numpy\n\narray = np.ndarray(\n    buffer=region.write_to_memory(),\n    dtype=np.uint8,\n    shape=(region.height, region.width, region.bands)\n)\n# red, green, blue channels\nprint(array.shape)\n>> (500, 500, 3)\n\n# also look into pyvips.Region.fetch for faster region read\nregion = pyvips.Region.new(slide).fetch(x, y, region_width, region_height)\n\nbands = 3\narray = np.ndarray(\n    buffer=region,\n    dtype=np.uint8,\n    shape=(region_height, region_width, bands)\n)\nprint(array.shape)\n>> (500, 500, 3)\nIf OpenSlide is installed on your machine, PyVips can also read in native WSI formats. In fact, this is how we converted WSIs to pyramidal TIFs for the competition. An example of the conversion is below:\nimport pyvips\n\nnative_path = \"image_001.svs\"  # path to the native WSI file\nnative_image = pyvips.Image.openslideload(native_path)\nnative_image.tiffsave(\n    \"image_001.tif\",  # path to the output pyramidal TIF file\n    compression=\"jpeg\",\n    Q=75,\n    tile=True,\n    tile_width=512,\n    tile_height=512,\n    pyramid=True,\n)\nDeep Zoom Viewer\nOpenSlide Python provides support for Deep Zoom. This allows you to display images in a web browser without converting slides to the Deep Zoom format. Deep Zoom is useful if you want to efficiently view, pan around, and zoom whole slide images. Read more on the OpenSlide docs.\nCytomine\nCytomine allows you to display and explore native whole slide images and pyramidal TIF formats in a web browser. It also supports adding annotations and executing scripts from inside Cytomine or from any computing server using the dedicated Cytomine Python client. Cytomine can be installed locally or on any Linux server. The Cytomine GitHub repository includes examples of Python scripts demonstrating how to interact with your Cytomine instance, as well as examples of ready-to-use machine learning scripts (all S_ prefixed repos, such as S_CellDetect_Stardist_HE_ROI).\nTips and tricks\nHere are a few tricks we discovered while working with the data:\nA critical part of the challenge of this competition is how to efficiently process these enormous images. Due to the time limit on submissions, you will not have time to process every part of the image at the highest resolution available. Instead, you might consider methods to first predict which parts of the image are most important for an accurate diagnosis. The pyramidal TIF format contains multiple zoom levels that you can access with the page keyword argument to pyvips.Image.new_from_file. You might try a \"multiscale\" approach\u2015detect the most important parts of the image from lower resolutions versions (pyvips.Image.new_from_file(path, page=5)) and then analyze just those parts at higher resolutions.\nUse pyvips.Region.fetch instead of pyvips.Image.crop to read in small image regions. According to a PyVips developer and our own tests, pyvips.Region.new(image).fetch(...) is much faster than pyvips.Image.crop(...). (Requires libvips version 0.8.6 or greater.)\nAvoid OpenSlide-based data loaders (pyvips.Image.openslideload and openslide.OpenSlide) in your machine learning pipelines. OpenSlide is no longer under active development, and several issues have arisen that have not been addressed. For example, we ran into errors reading certain SVS files (likely related to this open issue) in which parts of the image were blank. Even installing openslide-python with pip requires downgrading setuptools. We found that reading the pyramidal TIFs using PyVips was a reliable way to work with the images in our machine learning pipelines. (OpenSlide Python's Deep Zoom viewer is still helpful for manually browsing the slides.)\nAdditional reading and research\nHere are a few papers and tutorials that talk about machine learning with WSI that you may find helpful:\nWhole slide image preprocessing in Python\nAssessment of Machine Learning of Breast Pathology Structures for Automated Differentiation of Breast Cancer and High-Risk Proliferative Lesions - PubMed\nUsing deep convolutional neural networks to identify and classify tumor-associated stroma in diagnostic breast biopsies\nAssessment of Machine Learning of Breast Pathology Structures for Automated Differentiation of Breast Cancer and High-Risk Proliferative Lesions\nHistologic tissue components provide major cues for machine learning-based prostate cancer detection and grading on prostatectomy specimens\nAssessment of Machine Learning Detection of Environmental Enteropathy and Celiac Disease in Children",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/67/competition-cervical-biopsy/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Good news! Schneider Electric has made the training data for this competition available on their Data Exchange for ongoing use, practice and learning.\nIn order to access the data, you\u2019ll first need to create an account on the Schneider Exchange. Then follow the link below to add the competition dataset to your Digital Library, where you\u2019ll be able to freely access the data. These datasets are maintained by Schneider Electric and released at their discretion.\nDataset: Forecasting cold start building energy consumption\nPlease note that we won't be able to field questions about the data, but we wanted to share it here for the benefit of the community. If you have a question, feel free to post it to the forum.\nFor more details on the results and winners of the competition, including links to open source solutions, check out the results blog post.",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/55/schneider-cold-start/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Good news! Schneider Electric has made the training data for this competition available on their Data Exchange for ongoing use, practice and learning.\nIn order to access the data, you\u2019ll first need to create an account on the Schneider Exchange. Then follow the link below to add the competition dataset to your Digital Library, where you\u2019ll be able to freely access the data. These datasets are maintained by Schneider Electric and released at their discretion.\nDataset: Optimizing storage demand-side strategies\nPlease note that we won't be able to field questions about the data, but we wanted to share it here for the benefit of the community. If you have a question, feel free to post it to the forum.\nFor more details on the results and winners of the competition, including links to open source solutions, check out the results blog post.",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/53/optimize-photovoltaic-battery/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Good news! Schneider Electric has made the training data for this competition available on their Data Exchange for ongoing use, practice and learning.\nIn order to access the data, you\u2019ll first need to create an account on the Schneider Exchange. Then follow the link below to add the competition dataset to your Digital Library, where you\u2019ll be able to freely access the data. These datasets are maintained by Schneider Electric and released at their discretion.\nDataset: Forecasting building energy consumption\nPlease note that we won't be able to field questions about the data, but we wanted to share it here for the benefit of the community. If you have a question, feel free to post it to the forum.\nFor more details on the results and winners of the competition, including links to open source solutions, check out the results blog post.",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/51/electricity-prediction-machine-learning/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/83/cloud-cover/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/81/detect-flood-water/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/59/camera-trap-serengeti/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Good news! Schneider Electric has made the training data for this competition available on their Data Exchange for ongoing use, practice and learning.\nIn order to access the data, you\u2019ll first need to create an account on the Schneider Exchange. Then follow the link below to add the competition dataset to your Digital Library, where you\u2019ll be able to freely access the data. These datasets are maintained by Schneider Electric and released at their discretion.\nDataset: Prediction of cleaning production equipment\nPlease note that we won't be able to field questions about the data, but we wanted to share it here for the benefit of the community. If you have a question, feel free to post it to the forum.\nFor more details on the results and winners of the competition, including links to open source solutions, check out the results blog post.",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/56/predict-cleaning-time-series/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/49/deep-learning-camera-trap-animals/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/47/penguins/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/255/kelp-forest-segmentation/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/60/building-segmentation-disaster-resilience/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/50/worldbank-poverty-prediction/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Algorithm challenge description\nYour goal is to predict the yield of fog nets in the Anti-Atlas mountains in Southwestern Morocco. We provide you with measurements from weather stations both at the site of the fog nets and for the larger geographical region.\nAvailable data\nFor this competition, we have both weather data from a sensor co-located with the fog nets (microclimate data) and data from weather stations in three nearby cities in Morocco (macroclimate data). Various metorological measures are recorded at these different weather stations. Your goal is to use these data sources to predict the yield of a collection of fog nets. The yield of these nets is measured every two hours. For the test set, windows of 4 days have been removed from the training data at regular intervals, and competitors will attempt to predict most accurately the yield during these intervals. For some of these intervals, concurrent microclimate data is provided. For others, only macroclimate data is available.\nMap of locations\nMicroclimate weather data\nData is collected by a sensor array co-located with the fog nets. Measurements of meteorological variables such as temperature, humidity and wind are created at 5 minute intervals. We provide at both the 5-minute time scale, and an aggregated 2-hour time scale. Yield predictions are made for every two hour interval during the test periods.\n\n\npercip_mm - Perciptitation (mm)\nhumidity - A measure of the humidity in the air\ntemp - The temperature\nleafwet450_min - Leaf wetness (a measure of the presence of dew) sensor 1\nleafwet460_min - Leaf wetness (a measure of the presence of dew) sensor 2\nleafwet_lwscnt - Leaf wetness (a measure of the presence of dew) sensor 3\ngusts_ms - A measure of the highest gust during the reporting interval\nwind_dir - The dominant direction the wind is blowing in\nwind_ms - A measure of the current wind speed\nDates and times available\nWe provide microclimate data on two time scales: The sensor array natively records measurements every 5 minutes. This is too noisy to predict the yield effectively, so we are asking you to make predictions for the yield every 2 hours. For convenience, we provide you with both the native (5 minute interval) data and resampled (every 2 hours) microclimate measurements. It should be less complicated to use the microclimate data that matches the prediction interval (every 2 hours), but you may be able to extract more information from the finer-grained 5 minute interval data.\nNote: not all of the measurements are available for the entire time period. Data is unavailable for November 2014 and March - July 2015. At different intervals, measurements may be missing from the microclimate data. For some time windows in the test set, the microclimate weather data has been intentionally withheld. Competitors may want to interpolate these missing values either using the microclimate or macroclimate data.\nMacroclimate data\nThe macroclimate data consists of measures from one weather station on the coast of Morocco in Sidi Ifni and two airport weather stations in the cities of Agadir and Guelmim. These weather stations collect many metorological measurements. The variables recorded differ between the Sidi Ifni weather station and the two airports (Agadir and Guelmim). A description of these variables is below.\nWeather Station Variables (Sidi Ifni)\nNh - Amount of all the CL cloud present or, if no CL cloud is present, the amount of all the CM cloud present\nTx - Maximum air temperature (degrees Celsius) during the past period (not exceeding 12 hours)\nDD - Mean wind direction (compass points) at a height of 10-12 metres above the earth's surface over the 10-minute period immediately preceding the observation\ntR - The period of time during which the specified amount of precipitation was accumulated\nTn - Minimum air temperature (degrees Celsius) during the past period (not exceeding 12 hours)\nff10 - Maximum gust value at a height of 10-12 metres above the earth's surface over the 10-minute period immediately preceding the observation (meters per second)\nTg - The minimum soil surface temperature at night. (degrees Celsius)\nTd - Dewpoint temperature at a height of 2 metres above the earth's surface (degrees Celsius)\nDate / Local time - Local time in this location. Summer time (Daylight Saving Time) is taken into consideration\nPo - Atmospheric pressure at weather station level (millimeters of mercury)\nE' - State of the ground with snow or measurable ice cover.\nFf - Mean wind speed at a height of 10-12 metres above the earth's surface over the 10-minute period immediately preceding the observation (meters per second)\nRRR - Amount of precipitation (millimeters)\nE - State of the ground without snow or measurable ice cover\nH - Height of the base of the lowest clouds (m)\nff3 - Maximum gust value at a height of 10-12 metres above the earth's surface between the periods of observations (meters per second)\nsss - Snow depth (cm)\nN - Total cloud cover\nP - Atmospheric pressure reduced to mean sea level (millimeters of mercury)\nU - Relative humidity (%) at a height of 2 metres above the earth\nT - Air temperature (degrees Celsius) at 2 metre height above the earth's surface\nVV - Horizontal visibility (km)\nWW - Present weather reported from a weather station.\nCh - Clouds of the genera Cirrus, Cirrocumulus and Cirrostratus\nCm - Clouds of the genera Altocumulus, Altostratus and Nimbostratus\nCl - Clouds of the genera Stratocumulus, Stratus, Cumulus and Cumulonimbus\nPa - Pressure tendency: changes in atmospheric pressure over the last three hours (millimeters of mercury).\nW2 - Past weather (weather between the periods of observation) 2\nW1 - Past weather (weather between the periods of observation) 1\nAirport Weather Variables (Agadir, Guelmim)\nW'W' - Recent weather phenomena of operational significance\nc - Total cloud cover\nVV - Horizontal visibility (km)\nDD - Mean wind direction (compass points) at a height of 10-12 metres above the earth's surface over the 10-minute period immediately preceding the observation\nWW - Special present weather phenomena observed at or near the aerodrome\nP - Atmospheric pressure reduced to mean sea level (millimeters of mercury)\nff10 - Maximum gust value at a height of 10-12 metres above the earth's surface over the 10-minute period immediately preceding the observation (meters per second)\nU - Relative humidity (%) at a height of 2 metres above the earth\nT - Air temperature (degrees Celsius) at 2 metre height above the earth's surface\nFf - Mean wind speed at a height of 10-12 metres above the earth's surface over the 10-minute period immediately preceding the observation (meters per second)\nTd - Dewpoint temperature at a height of 2 metres above the earth's surface (degrees Celsius)\nDate / Local time - Local time in this location. Summer time (Daylight Saving Time) is taken into consideration\nPo - Atmospheric pressure at weather station level (millimeters of mercury)\nDates and times available\nFor the macroclimate data, historical measurements are provided for the length of the period of interest. The interval depends on the weather station and there are some brief missing time periods in the dataset.\nNote: The macroclimate data has not been divided into test/training sets. The data is provided thanks to rp5.ru.\nTarget variable\nThe target variable for this competition is the yield of the fog net array. This measures how much water the nets collect. Water that condenses on the net runs down a gutter to a tipping bucket. Every time the bucket fills, a counter increases. The yield measures the number of tippings across the net system for each 2-hour period.\nyield - the (rescaled) amount of water that the system yielded as measured at a particular time\nSubmission format\nThe submission format is the same as the target variable. You must predict a float value for each of the time periods in the submission format file.\nyield\n2013-11-24 00:00:00 0.0\n2013-11-24 02:00:00 0.0\n2013-11-24 04:00:00 0.0\n2013-11-24 06:00:00 0.0\n2013-11-24 08:00:00 0.0\nOne last word\nRemember, there is a time-based component to this problem. The most useful algorithms for generalizing will not pollute their predictions with data from the future that wouldn't be available at prediction time.\nGood luck and enjoy this problem! If you have any questions you can always visit the user forum!",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/9/from-fog-nets-to-neural-nets/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Open Data\nGood news! The project team from this competition has made the challenge data available for ongoing use, practice and learning. This data will be maintained by the project team and released at their discretion.\nFor additional information about the data, see the Problem Description page.\nPlease note that we won't be able to field questions about the data, but we wanted to share it here for the benefit of the community. If you have a question, feel free to post it to the forum.\nDataset link\nTropical Cyclone Wind Estimation Competition Dataset\nAbout Radiant MLHub\nRadiant MLHub is an open repository for geospatial machine learning training data. Radiant MLHub hosts open training datasets generated by the Radiant Earth Foundation team as well as other training data catalogs contributed by Radiant Earth\u2019s partners. All the data are hosted in a cloud-friendly format, and the API for allows easy discovery and download. Radiant MLHub is open to anyone to access existing training data and/or share their training data for broader impact.\nData Structure\nThe Radiant MLHub API is a STAC compliant API that serves metadata about label items and source imagery and links to download these items.\nA SpatioTemporal Asset Catalog (STAC) is a standardized specification for organizing metadata, making it easy to search for images or labels that match spatial, temporal, or other criteria. At the root level of the STAC API is a list of collections of items. In the Radiant MLHub API, each collection contains items for either source imagery or labels for a dataset. These items are descriptions of source imagery or labels and links to download assets related to these items. Properties found in these item descriptions include spatial extent, temporal extent, band descriptions in the case of optical imagery, label types and label properties in the case of labels, and other information like Digital Object Identifiers (DOIs) and citation examples to reference.\nTo learn more about Radiant MLHub API, check out this blogpost on Accessing and Downloading Training Data on the Radiant MLHub API.\nAuthenticating with the API\nTo access the Radiant MLHub API, you must be authenticated with an API key. Requests made to the API must contain a query parameter where the key is \u201ckey\u201d and the value is your API key. For example, a request made to the /collections endpoint would look similar to this:\nhttps://api.radiant.earth/mlhub/v1/collections?key=your_api_key_here\nYou can obtain an API key by creating a free account on Radiant Earth Foundation's dashboard and navigating to the \u201cAPI Keys\u201d tab.\nAccessing the data\nThere are three collections that contain data for this competition:\nnasa_tropical_storm_competition_train_source contains the train images (jpegs) and metadata (jsons, one per image)\nnasa_tropical_storm_competition_train_labels contains the train labels (jsons, one per image)\nnasa_tropical_storm_competition_test_source contains the test images (jpegs) and metadata (jsons, one per image)\nTo download the tropical storm images, metadata, and labels, you should first crawl the nasa_tropical_storm_competition_train_labels collection and download the labels file located within the label item. Then, to download the associated image, loop through the links array and navigate to the source imagery items. Source imagery items will have the \u201crel\u201d type of \u201csource\u201d. Once you navigate to a source imagery item you can find a link to the image (image) as well as a link to the metadata json (features) within the assets dictionary.\nAn example notebook which implements these steps can be found here.",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/72/predict-wind-speeds/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Open Data\nGood news! The data partner from this competition has made the challenge data available under the CC-BY-4.0 license for ongoing use, practice and learning. This data will be maintained by the partner and released at their discretion.\nFor additional information about the data, see the Problem description page.\nPlease note that we won't be able to field questions about the data, but we wanted to share it here for the benefit of the community. If you have a question, feel free to post it to the forum.\nAccess the data:\nBioMassters: A Benchmark Dataset for Forest Biomass Estimation using Multi-modal Satellite Time-series",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/99/biomass-estimation/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/82/competition-wildlife-video-depth-estimation/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/65/clog-loss-alzheimers-research/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/58/disaster-response-roof-type/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/42/senior-data-science-safe-aging-with-sphere/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/4/box-plots-for-education/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/8/naive-bees-classifier/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "DrivenDatistas - Here are the datasets used in this competition, for your practice and enjoyment! For more details on the results of the competition, check out our blog post.\nUnfortunately we can't field questions about the data, but we wanted to release it openly so that people everywhere could use it. If you have a question, feel free to run it by the community on the forum. Enjoy!\nData for download\nAll Historical Violations: All of the historical violations for Boston (includes Test + Train from Phase I and more recent violations).\nRestaurant ID Mapping: Matches restaurant_id in the violations data to business_id in the Yelp data.\nYelp Restaurant Reviews: This dataset is maintained by Yelp, released at its discretion, and is subject to its data use agreement. At this link, you can download restaurant reviews from Yelp.",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/5/keeping-it-fresh-predict-restaurant-inspections/"
    },
    {
        "competition_overview": "Recent literature suggests that the demand for women\u2019s health care will grow over 6% by 2020. Given how rapidly the health landscape has been changing over the last 15 years, it\u2019s increasingly important that we understand how these changes affect what care people receive, where they go for it, and how they pay. Through the National Survey of Family Growth, the CDC provides one of the few nationally representative datasets that dives deep into the questions that women face when thinking about their health.\nCan you predict what drives women\u2019s health care decisions in America?\nCompetition End Date:\nApril 14, 2015, 11:59 p.m. UTC\nPlace Prize Amount\n1st $3,000\n2nd $1,500\n3rd $500",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/6/countable-care-modeling-womens-health-care-decisions/"
    },
    {
        "competition_overview": "US presidential elections come but once every 4 years, and this one's a big one. The new president will help shape policies on education, healthcare, energy, the environment, international relations, aid, and more. There are lots of people trying to predict what will happen. Can you top them?\nIn this challenge, you'll predict the percent of each state that will vote for each candidate. You can use any data you can get your hands on. Come election night, we'll see who's model had the best vision for the country!\n2016 Predictions Due By:\nNov. 8, 2016, 10 a.m. UTC\nPlace Prize Amount\n1st $750\n2nd $250\nPrize generously supplied by our friends at Civis Analytics.",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/43/americas-next-top-statistical-model/"
    },
    {
        "competition_overview": "Welcome! We're happy you're here!\nCan you write abstracts for academic social science papers using a large language model that runs on your computer? OK, but can you do it well?\nScientific papers describing research methodology and findings are the gold standard for sharing scientific work. These papers typically start with an abstract that summarizes the entire paper in a single paragraph. Your goal in this practice competition is to generate these abstracts from SocArXiv papers using large language models, or LLMs.\nWhile this task isn't easy to do well, it's easy to start! This is a practice competition designed to be accessible to participants at all levels. That makes it a great place to dive into the world of data science competitions or explore LLMs for the first time.\n\nCompetition End Date:\nApril 1, 2026, 11:59 p.m. UTC\nThis competition is for learning and exploring, so the deadline may be extended in the future.",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/297/whats-up-docs/"
    },
    {
        "competition_overview": "Can you predict whether people got H1N1 and seasonal flu vaccines using information they shared about their backgrounds, opinions, and health behaviors?\nIn this challenge, we will take a look at vaccination, a key public health measure used to fight infectious diseases. Vaccines provide immunization for individuals, and enough immunization in a community can further reduce the spread of diseases through \"herd immunity.\"\nAs of the launch of this competition, vaccines for the COVID-19 virus are still under development and not yet available. The competition will instead revisit the public health response to a different recent major respiratory disease pandemic. Beginning in spring 2009, a pandemic caused by the H1N1 influenza virus, colloquially named \"swine flu,\" swept across the world. Researchers estimate that in the first year, it was responsible for between 151,000 to 575,000 deaths globally.\nA vaccine for the H1N1 flu virus became publicly available in October 2009. In late 2009 and early 2010, the United States conducted the National 2009 H1N1 Flu Survey. This phone survey asked respondents whether they had received the H1N1 and seasonal flu vaccines, in conjunction with questions about themselves. These additional questions covered their social, economic, and demographic background, opinions on risks of illness and vaccine effectiveness, and behaviors towards mitigating transmission. A better understanding of how these characteristics are associated with personal vaccination patterns can provide guidance for future public health efforts.\nThis is a practice competition designed to be accessible to participants at all levels. That makes it a great place to dive into the world of data science competitions. Come on in from the waiting room and try your (hopefully steady) hand at predicting vaccinations.\n\nCompetition End Date:\nJuly 30, 2026, 11:59 p.m. UTC\nThis competition is for learning and exploring, so the deadline may be extended in the future.",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/66/flu-shot-learning/"
    },
    {
        "competition_overview": "Can you predict which water pumps are faulty?\nUsing data from Taarifa and the Tanzanian Ministry of Water, can you predict which pumps are functional, which need some repairs, and which don't work at all? This is an intermediate-level practice competition. Predict one of these three classes based on a number of variables about what kind of pump is operating, when it was installed, and how it is managed. A smart understanding of which waterpoints will fail can improve maintenance operations and ensure that clean, potable water is available to communities across Tanzania.\nCompetition End Date:\nOct. 5, 2026, 11:59 p.m.\nThis competition is for learning and exploring, so the deadline may be extended in the future.",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/"
    },
    {
        "competition_overview": "Can you predict local epidemics of dengue fever?\nDengue fever is a mosquito-borne disease that occurs in tropical and sub-tropical parts of the world. In mild cases, symptoms are similar to the flu: fever, rash, and muscle and joint pain. In severe cases, dengue fever can cause severe bleeding, low blood pressure, and even death.\nBecause it is carried by mosquitoes, the transmission dynamics of dengue are related to climate variables such as temperature and precipitation. Although the relationship to climate is complex, a growing number of scientists argue that climate change is likely to produce distributional shifts that will have significant public health implications worldwide.\nIn recent years dengue fever has been spreading. Historically, the disease has been most prevalent in Southeast Asia and the Pacific islands. These days many of the nearly half billion cases per year are occurring in Latin America:\n\nUsing environmental data collected by various U.S. Federal Government agencies\u2014from the Centers for Disease Control and Prevention to the National Oceanic and Atmospheric Administration in the U.S. Department of Commerce\u2014can you predict the number of dengue fever cases reported each week in San Juan, Puerto Rico and Iquitos, Peru?\nThis is an intermediate-level practice competition. Your task is to predict the number of dengue cases each week (in each location) based on environmental variables describing changes in temperature, precipitation, vegetation, and more.\nAn understanding of the relationship between climate and dengue dynamics can improve research initiatives and resource allocation to help fight life-threatening pandemics.\nCompetition End Date:\nOct. 5, 2026, 11:59 p.m.\nThis competition is for learning and exploring, so the deadline may be extended in the future.",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/44/dengai-predicting-disease-spread/"
    },
    {
        "competition_overview": "Based on aspects of building location and construction, your goal is to predict the level of damage to buildings caused by the 2015 Gorkha earthquake in Nepal.\n\nThis is an intermediate-level practice competition.\nThe data was collected through surveys by Kathmandu Living Labs and the Central Bureau of Statistics, which works under the National Planning Commission Secretariat of Nepal. This survey is one of the largest post-disaster datasets ever collected, containing valuable information on earthquake impacts, household conditions, and socio-economic-demographic statistics.\n\nCompetition End Date:\nOct. 5, 2026, 11:59 p.m.\nThis competition is for learning and exploring, so the deadline may be extended in the future.",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/57/nepal-earthquake/"
    },
    {
        "competition_overview": "Can you classify the wildlife species that appear in camera trap images collected by conservation researchers?\nWelcome to the African jungle! In recent years, automated surveillance systems called camera traps have helped conservationists study and monitor a wide range of ecologies while limiting human interference. Camera traps are triggered by motion or heat, and passively record the behavior of species in the area without significantly disturbing their natural tendencies.\nHowever, camera traps also generate a vast amount of data that quickly exceeds the capacity of humans to sift through. That's where machine learning can help! Advances in computer vision can help automate tasks like species detection and classification, localization, depth estimation, and individual identification so humans can more effectively learn from and protect these ecologies.\nIn this challenge, we will take a look at object classification for wildlife species. Classifying wildlife is an important step to sort through images, quantify observations, and quickly find those with individual species.\nThis is a practice competition designed to be accessible to participants at all levels. That makes it a great place to dive into the world of data science competitions and computer vision. Try your hand at image classification and see what animals your model can find!\n\nCompetition End Date:\nOct. 5, 2026, 11:59 p.m. UTC\nThis competition is for learning and exploring, so the deadline may be extended in the future.",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/87/competition-image-classification-wildlife-conservation/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Open data\nIn this challenge, solvers found and used datasets that were made publicly available by the U.S. government. Below, you can see data access resources that were provided to solvers or learn about the data used by challenge winners.\nPlease note that we won't be able to field questions about the data, but we wanted to share it here for the benefit of the community. If you have a question, feel free to post it to the forum.\nLearn about the open data used in this challenge: Open earth observation data for the pale blue dot: Visualization challenge\nSee the solutions and data from challenge winners: Pale Blue Dot: Visualization Challenge Winner's Repository",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/256/pale-blue-dot/"
    },
    {
        "competition_overview": "Budgets for schools and school districts are huge, complex, and unwieldy. It's no easy task to digest where and how schools are using their resources. Education Resource Strategies is a non-profit that tackles just this task with the goal of letting districts be smarter, more strategic, and more effective in their spending.\nYour task is a multi-class-multi-label classification problem with the goal of attaching canonical labels to the freeform text in budget line items. These labels let ERS understand how schools are spending money and tailor their strategy recommendations to improve outcomes for students, teachers, and administrators.",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/46/box-plots-for-education-reboot/"
    },
    {
        "competition_overview": "In the year 2000, the member states of the United Nations agreed to a set of goals to measure the progress of global development. The aim of these goals was to increase standards of living around the world by emphasizing human capital, infrastructure, and human rights.\n\n\nThe eight goals are:\nTo eradicate extreme poverty and hunger\nTo achieve universal primary education\nTo promote gender equality and empower women\nTo reduce child mortality\nTo improve maternal health\nTo combat HIV/AIDS, malaria, and other diseases\nTo ensure environmental sustainability\nTo develop a global partnership for development\nCompetition End Date:\nFeb. 5, 2021, noon\nThis competition is for learning and exploring, so the deadline may be extended in the future.",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/1/united-nations-millennium-development-goals/"
    },
    {
        "competition_overview": "US presidential elections come but once every 4 years, and this one's a big one. The new president will help shape policies on the pandemic response, healthcare, the environment, the economy, and more. There are lots of people trying to predict what will happen. Can you top them?\nIn this challenge, you are asked to predict the fraction of each state that will vote for each major candidate. You can use any data that is freely available to the public. Come election night (or election week... or election month), we'll see who's model had the most accurate vision for the country!\nPhase I: The Buildup\nThrough Election Day (November 3rd 12:00 PM UTC)\nAssemble your data and build your model! Submit your final predictions by the start of Election Day. During this time you can test how well your models score against the past election.\nPhase II: Election Day\nNovember 3rd through finalized returns\nWatch with bated breath as the results roll in. We'll update scores throughout the counting period to show the current leaders. When all votes are counted, we'll see whose projections came closest to what really happened.",
        "dataset_overview": "The 5 competitors with the top scores may receive swag for an official DrivenData 2020 Zoom Uniform\u2014DrivenData sweatpants, shirt, and beanie!",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/71/election-competition-2020/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/54/machine-learning-with-a-heart/"
    },
    {
        "competition_overview": "Competition overview not found",
        "dataset_overview": "Dataset overview not found",
        "evaluation_overview": "Evaluation overview not found",
        "url": "https://www.drivendata.org/competitions/2/warm-up-predict-blood-donations/"
    }
]