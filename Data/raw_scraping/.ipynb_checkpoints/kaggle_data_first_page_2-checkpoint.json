[
    {
        "name": "CPROD1: Consumer PRODucts contest #1",
        "url": "https://www.kaggle.com/competitions/cprod1",
        "overview_text": "Overview text not found",
        "description_text": "A significant proportion of web usage relates to discussions, research, and purchase of consumer products. Currently, hundreds of thousands of blogs, forums, product review sites, and e-commerce merchants currently exist, in part, to service consumer's need to access product related information and demand to share experiences with products. The goal of this competition is to determine the state-of-the-art methods to automatically recognize product mentions in such textual content and to also disambiguate which product(s) in product catalogs are being referenced. Specifically, the task is to automatically identify all mentions of consumer products in a largely user generated collection of web-content, and to correctly identify the product(s) that each product mention refers to from a large catalog of products. The datasets provided includes hundreds of thousands of text items, a product catalog with over fifteen million products, and hundreds of manually annotated product mentions to support data-driven approaches. The prize pool for the contest is $10,000 and is divided as follows: $6,000 for first, $3,000 for second and $1,000 for third place submissions. Note that the contest is colocated with the ICDM-2012 conference. There will be a workshop on the contest results on December 10th.",
        "dataset_text": "The CPROD1 competition involves the release of six data files to contestants. Five of the files are provided immediately, while the model evalulation set of text items will be released later to determine the contest winners. Files are in two formats: JSON format and .CSV format The six files are as follows: Sample Perl Code is provided in the file CPROD1_baseline.120707.pl, which also produces a simple benchmark. The files to be used for model training have been bundled together in TrainingSet.zip/.7z: products.json, training-annotated-text.json, training-disambiguated-text.json, training-non-annotated-text.json. The file PublicLeaderboardSet.zip/.7z contains leaderboard-text.json for determining the leaderboard rankings. Below we describe the key data entities involved (text items, products, and disambiguated product mentions), along with the process used to generate the data. For the contest, a \u201ctext item\u201d stands for a tokenized representation of a portion or entirety of a web page or a web-forum postings page. We processed each web item to create text items as follows: Here is an example of a text_item: \"TextItem\": {\"0c1edc5b2ed5abb25e25b966ccdb01d2\": [\"Here\",\"'s\",\"an\",\"example\",\"of\",\"a\",\"(\",\"pre-tokenized\",\")\",\"text\u201d, \u201citem\",\".\",\"\", \" \", \"Check\",\"out\",\"the\",\"new\",\"iPhone\",\"4s\",\"!\"]}  Notice: 1) how the word \"Here's\" has been divided into two tokens: \"Here\" and \"\u2019s\"; 2) how the end-of-sentence punctuation have been placed into their own tokens; and 3) how sentences have been seperated by both a \"\u201d token representing an end-of-sentence and a \" \" token representing an end of paragraph. For the contest a \u201cproduct item\u201d is a semi-structured record that represents some purchasable consumer product from either the consumer electronics (CE) or automotive (AU) verticals. Each record has: 1) a unique string-based identifier, and 2) an array composed of a string-based \u201cname\u201d, a two character-based product category, and a two digit-based price.  A sample of some of the products records is presented below (in tabular format): {\"1QlV3Pe6T0W\":[\"iphone 3\",\"CE\",399.95],\"op6rsUEYjWc\":[\"ifone case\",\"CE\",21.25],\"P7Ntsvaer1Y\":[\"hawk break pads for SUVs\",\"AU\",110.00]}   For the contest a disambiguated product mention is a structured record composed of two fields: a product mention identifier, and a space-separated set of product item identifiers. The identiffier represents some specific product mention within some specific text item, for example: 0c1edc5b2ed5abb25e25b966ccdb01d2:0-2 represents the product mention that begins on the first token and ends on the third token in textitem 0c1edc5b2ed5abb25e25b966ccdb01d2. (product mentions are always complete substrings of one or more tokens). Finally, the set of space-separated product identifiers represents the products within the product catalog that have been deemed to refer to the same product as the mention.   We used the following process to annotate text items. The annotation task involved two phases: 1) the identification of product mention\nwithin text items, and 2) the labeling of product records for each annotated product mention with True match (or False match). During the first phase a set of text items were randomly selected. Each text item was reviewed by at least two different annotators. In cases where there was disagreement about mentions a third annotator broke ties. During the second phase the human annotators were asked to classify which products were legitimate references for each of the product mentions. This phase was significantly more time consuming so only a small portion of product candidates were reviewed by two or more annotators.   We randomly separated the annotated text items into training set, leaderboard set and model evaluation set using 50%, 25% and 25% proportions."
    },
    {
        "name": "EMC Israel Data Science Challenge",
        "url": "https://www.kaggle.com/competitions/emc-data-science",
        "overview_text": "Overview text not found",
        "description_text": "The EMC source code classification challenge requires you to classify source code files according to the projects they belong to. Given a set of source code files collected from various open source projects, how well can unseen source code files from the same set of open source projects can be classified? Possible real-world applications:",
        "dataset_text": "Dataset description not found"
    },
    {
        "name": "Practice Fusion Diabetes Classification",
        "url": "https://www.kaggle.com/competitions/pf2012-diabetes",
        "overview_text": "Overview text not found",
        "description_text": "In the first phase of this prediction challenge Practice Fusion invited anyone with an interest in using electronic medical record data to improve public health to submit and vote on ideas for prediction problems based on a new dataset of 10,000 de-identified medical records. The votes are in and Shea Parkes' top voted submission has won. Practice Fusion is now sponsoring the second and final phase of the challenge inspired by the winning problem: Identify patients diagnosed with Type 2 Diabetes Mellitus. Over 25 million people, or nearly 8.3% of the entire United States population, have diabetes. Diabetes is also associated with a wide range of complications from heart disease and stroke to blindness and kidney disease. Predicting who has diabetes will lead to a better understanding of these complications and the common comorbidities that diabetics suffer. The Challenge: Given a de-identified data set of patient electronic health records, build a model to determine who has a diabetes diagnosis, as defined by ICD9 codes 250, 250.0, 250.*0 or 250.*2 (e.g., 250, 250.0, 250.00, 250.10, 250.52, etc).",
        "dataset_text": "Update: this dataset has been removed at the request of the host. The goal of this competition is to build a model that identifies who in the test set has a diagnosis of Type 2 diabetes mellitus (T2DM). Diagnosis of T2DM is defined by a set of ICD9 codes: {'250', '250.0', 250.*0, and 250.*2} where 250.*0 means '250.00', '250.10', '250.20', ... '250.90' and 250.*2 means '250.02', '250.12', ... '250.92'. Note that ICD9 codes 250.*1 and 250.*3 are for Type I diabetes mellitus and are not to be classified. ICD9 codes are found in the table SyncDiagnosis. The trainingSet and testSet each consist of 17 different files, 2 common files and 15 data set-specific files. They are in comma separated value (csv) format. Please refer to the data set dictionary for a description of the table elements and for a chart showing how the tables are connected. There are a total of 9,948 patients in the training set and 4,979 patients in the test set. In the training set file training_SyncPatient.csv, an indicator column has been added to show who has a diagnosis of Type 2 diabetes mellitus. Also provided are the data tables in a SQLite database along with the script used to create the database. These are found in the file compDataAsSQLiteDB. Starter code (in R) that works with the SQLite database and performs a simple data flattening tranformation is provided in the files sample_code.R and sample_code_library.R. This code was used to generate the Random Forest Benchmark (also provided, randomForest-Benchmark.csv ). This code is by no means complete, but is provided to help you get started analyzing the data and creating models."
    },
    {
        "name": "Detecting Insults in Social Commentary",
        "url": "https://www.kaggle.com/competitions/detecting-insults-in-social-commentary",
        "overview_text": "Overview text not found",
        "description_text": "  The challenge is to detect when a comment from a conversation would be considered insulting to another participant in the conversation. Samples could be drawn from conversation streams like news commenting sites, magazine comments, message boards, blogs, text messages, etc. The idea is to create a generalizable single-class classifier which could operate in a near real-time mode, scrubbing the filth of the internet away in one pass. Besides the prize money, eternal fame and glory, monuments in your honor, and the admiration of friend and foe alike, you get a chance for an interview at Impermium for the Principal Data Engineer role. In addition, there will be a Visualization prospect attached to the contest.  Slice and dice the data to show us the most amazing, informative and thought-provoking infographics, diagrams or plots! Submission open 1 week before the contest ends.  (Please note that referenced fame, glory, monuments and admiration are to be bestowed only in the imagination of the contestants)",
        "dataset_text": "The data consists of a label column followed by two attribute fields.  This is a single-class classification problem. The label is either 0 meaning a neutral comment, or 1 meaning an insulting comment (neutral can be considered as not belonging to the insult class.  Your predictions must be a real number in the range [0,1] where 1 indicates 100% confident prediction that comment is an insult. The first attribute is the time at which the comment was made. It is sometimes blank, meaning an accurate timestamp is not possible. It is in the form \"YYYYMMDDhhmmss\" and then the Z character. It is on a 24 hour clock and corresponds to the localtime at which the comment was originally made. The second attribute is the unicode-escaped text of the content, surrounded by double-quotes. The content is mostly english language comments, with some occasional formatting."
    },
    {
        "name": "Cause-effect pairs",
        "url": "https://www.kaggle.com/competitions/cause-effect-pairs",
        "overview_text": "Overview text not found",
        "description_text": "Come to our NIPS workshop (dec 9 or 10 in Tahoe).                                The problem of attributing causes to effects is pervasive in science, medicine, economy and almost every aspects of our everyday life involving human reasoning and decision making. What affects your health? the economy? climate changes? The gold standard to establish causal relationships is to perform randomized controlled experiments. However, experiments are costly while non-experimental \"observational\" data collected routinely around the world are readily available. Unraveling potential cause-effect relationships from such observational data could save a lot of time and effort. Consider for instance a target variable B, like occurence of \"lung cancer\" in patients. The goal would be to find whether a factor A, like \"smoking\", might cause B. The objective of the challenge is to rank pairs of variables {A, B} to prioritize experimental verifications of the conjecture that A causes B. As is known, \"correlation does not mean causation\". More generally, observing a statistical dependency between A and B does not imply that A causes B or that B causes A;  A and B could be consequences of a common cause. But, is it possible to determine from the joint observation of samples of two variables A and B that A should be a cause of B? There are new algorithms that have appeared in the literature in the past few years that tackle this problem. This challenge is an opportunity to evaluate them and propose new techniques to improve on them. We provide hundreds of pairs of real variables with known causal relationships from domains as diverse as chemistry, climatology, ecology, economy, engineering, epidemiology, genomics, medicine, physics. and sociology. Those are intermixed with controls (pairs of independent variables and pairs of variables that are dependent but not causally related) and semi-artificial cause-effect pairs (real variables mixed in various ways to produce a given outcome). This challenge is limited to pairs of variables deprived of their context. Thus constraint-based methods relying on conditional independence tests and/or graphical models are not applicable. The goal is to push the state-of-the art in complementary methods, which can eventually disambiguate Markov equivalence classes. If you are skeptical that this is possible, try this quiz: Examine the plot below of values of variable B plotted as a function of values of variable A. Can you guess which one is a cause of the other? Hint: Some non-linear functions are non-invertible.   July 1: A new data release was made to address a normalization problem and the deadline was extended. Scores on the public leaderboard prior to July 1 were decreased by 0.5. Please make new submissions with the new validation set. The competition is open to new teams.",
        "dataset_text": "This is the July 1, 2013 final data release.  The data provided on this page is in csv format, suitable to be read by: Archived data and data in the split format (one pair per file) are also available. The file CEfinal_basic_python_benchmark.csv provides a sample submission. We released the final test data and an equivalent amount of training and validation data distributed similarly. The test data is encrypted, the decryption key will be revealed at the end of the development phase. The new validation set is replacing the old validation set on the leaderboard and all the scores are reset to 0.5, please re-submit results on the new validation set. The new data include pairs of variables generated in a similar way as those of SUP2data and pairs of real variables from various sources. The final data is different from the original training and validation data with respect to normalization and quantization of variables to address a problem of bias in the original data. NEW: May-June 2013 supplementary data release: We provide three additional training datasets artificially generated: SUP1data, SUP2data, and SUP3data. Those training datasets have normalized numerical variables and have balanced number of unique values across all classes. SUP1data includes ~6000 pairs of numerical variables. SUP2 includes ~6000 pairs of mixed variables (numerical, categorical, binary). SUP3 data includes 81 pairs of real cause-effect pairs and 81 control pairs A|B and A-B generated from the real pairs. March 2013 data release: archived. In CEfinal_xxx_text.zip, you will find the following files:"
    },
    {
        "name": "Multi-modal Gesture Recognition",
        "url": "https://www.kaggle.com/competitions/multi-modal-gesture-recognition",
        "overview_text": "Overview text not found",
        "description_text": "The Multi-modal gesture recognition challenge, focused on gesture recognition from 2D and 3D video data using Kinect, is organized by ChaLearn in conjunction with ICMI 2013. Kinect is revolutionizing the field of gesture recognition given the set of input data modalities it provides, including RGB image, depth image (using an infrared sensor), and audio. Gesture recognition is genuinely important in many multi-modal interaction and computer vision applications, including image/video indexing, video surveillance, computer interfaces, and gaming. It also provides excellent benchmarks for algorithms. The recognition of continuous, natural signing is very challenging due to the multimodal nature of the visual cues (e.g., movements of fingers and lips, facial expressions, body pose), as well as technical limitations such as spatial and temporal resolution and unreliable depth cues.  The Multi-modal Challenge workshop will be devoted to the presentation of most recent and challenging techniques from multi-modal gesture recognition. The committee encourages paper submissions in the following topics (but not limited to): The results of the challenge will be discussed at the workshop. It features a quantitative evaluation of automatic gesture recognition from a multi-modal dataset recorded with Kinect (providing RGB images of face and body, depth images of face and body, skeleton information, joint orientation and audio sources), including about 15,000 Italian gestures from several users. The emphasis of this edition of the competition will be on multi-modal automatic learning of a vocabulary of 20 types of Italian gestures performed by several different users while explaining a history, with the aim of performing user independent continuous gesture recognition combined with audio information.  Additionally, the challenge includes a live competition of demos/systems of applications based on multi-modal gesture recognition techniques. Demos using data from different modalities and different kind of devices are welcome. The demos will be evaluated in terms of multi-modality, technical quality, and applicability. Best workshop papers and top three ranked participants of the quantitative evaluation will be invited to present their work at ICMI 2013 and their papers will be published in the ACM proceedings. Additionally, there will be travel grants (based on availability) and the possibility to be invited to present extended versions of their works to a special issue in a high impact factor journal. Moreover, all three top ranking participants in both, quantitative and qualitative challenges will be awarded with a ChaLearn winner certificate and an economic prize (based on availability). We will also announce a best paper and best student paper awards among the workshop contributions.",
        "dataset_text": "The data is also available here. The focus of the challenge is on \u201cmultiple instance, user independent learning\u201d of gestures, which means learning to recognize gestures from several instances for each category performed by different users, drawn from a gesture vocabulary of 20 categories. A gesture vocabulary is a set of unique gestures, generally related to a particular task. In this challenge we will focus on the recognition of a vocabulary of 20 Italian cultural/anthropological signs. Development Phase: Create a learning system capable of learning from several training examples a gesture classification problem. Practice with training data (a large database of 7,754 manually labeled gestures is available) and submit predictions on-line on validation data (3,362 labelled gestures) to get immediate feed-back on the leaderboard. Final Evaluation Phase: Make predictions on the new final evaluation data (around 3,000 gestures) revealed at the end of the development phase. The participants will have few days to train their systems and upload their predictions. Both for the development and final evaluation phase, the data will have the same format. We provide several ZIP files for each dataset, each file containing all the files for one sequence. The name of the ZIP file is assumed as the sequence identifier (eg. Sample00001, Sample00002, ...), and all their related files start with this SessionID: The development data contains the recording of multi-modal RGB-Depth-Audio data and user mask and skeleton information of 7,754 gesture instances from a vocabulary of 20 gesture categories of Italian signs. For each sequence, it is expected to recognize each gesture of interest (that means, gestures from the list of 20 Italian gestures) and generate the list of seen gestures. For instance: where Session00001 is the sequence id (name of the ZIP file) and we predict that it contains first the gesture 2 (vieniqui), then the gesture 12 (cosatifarei) and finally the gesture 3 (perfetto). Be aware that sequence can contain non interest gestures (gestures that people perform but are not in the list of those 20 gestures), and those gestures can not be in the prediction list. Next we list the types of gestures, represented by a numeric label (from 1 to 20), together with the number of training performances recorded for each gesture in brackets: In the scripts section, we provide a Matlab GUI application that allows to view and listen the data and to export it to make easy to work. It exports the multimodal data in the following four Matlab structures: After exportation, an individual mat (Sample00001_X.mat, where X indicates the number of the frame) file for each frame is generated containing the following structures: The detailed descriptions about each one of these structures are explained in the following section. A generated MAT file stores the selected visual data in three structures named \u2018RGB\u2019, \u2018Depth\u2019, \u2018UserIndex\u2019, and \u2018Skeleton\u2019. The data of each structure is aligned regarding RGB data. The following subsections will describe each of these structures."
    },
    {
        "name": "Loan Default Prediction - Imperial College London",
        "url": "https://www.kaggle.com/competitions/loan-default-prediction",
        "overview_text": "Overview text not found",
        "description_text": "This competition asks you to determine whether a loan will default, as well as the loss incurred if it does default. Unlike traditional finance-based approaches to this problem, where one distinguishes between good or bad counterparties in a binary way, we seek to anticipate and incorporate both the default and the severity of the losses that result. In doing so, we are building a bridge between traditional banking, where we are looking at reducing the consumption of economic capital, to an asset-management perspective, where we optimize on the risk to the financial investor. This competition is sponsored by researchers at Imperial College London. ",
        "dataset_text": "This data corresponds to a set of financial transactions associated with individuals. The data has been standardized, de-trended, and anonymized. You are provided with over two hundred thousand observations and nearly 800 features.  Each observation is independent from the previous.  For each observation, it was recorded whether a default was triggered. In case of a default, the loss was measured. This quantity lies between 0 and 100. It has been normalised, considering that the notional of each transaction at inception is 100. For example, a loss of 60 means that only 40 is reimbursed. If the loan did not default, the loss was 0. You are asked to predict the losses for each observation in the test set. Missing feature values have been kept as is, so that the competing teams can really use the maximum data available, implementing a strategy to fill the gaps if desired. Note that some variables may be categorical (e.g. f776 and f777). The competition sponsor has worked to remove time-dimensionality from the data. However, the observations are still listed in order from old to new in the training set. In the test set they are in random order."
    },
    {
        "name": "Grasp-and-Lift EEG Detection",
        "url": "https://www.kaggle.com/competitions/grasp-and-lift-eeg-detection",
        "overview_text": "Overview text not found",
        "description_text": "Think back to this morning: turning off the alarm, getting dressed, brushing your teeth, making coffee, drinking coffee, and locking the door as you left for work. Now imagine doing all those things again, without the use of your hands.  Patients who have lost hand function due to amputation or neurological disabilities wake up to this reality everyday. Restoring a patient's ability to perform these basic activities of daily life with a brain-computer interface (BCI) prosthetic device would greatly increase their independence and quality of life. Currently, there are no realistic, affordable, or low-risk options for neurologically disabled patients to directly control external prosthetics with their brain activity.  Recorded from the human scalp, EEG signals are evoked by brain activity. The relationship between brain activity and EEG signals is complex and poorly understood outside of specific laboratory tests. Providing affordable, low-risk, non-invasive BCI devices is dependent on further advancements in interpreting EEG signals.  This competition challenges you to identify when a hand is grasping, lifting, and replacing an object using EEG data that was taken from healthy subjects as they performed these activities. Better understanding the relationship between EEG signals and hand movements is critical to developing a BCI device that would give patients with neurological disabilities the ability to move through the world with greater autonomy.  This competition is sponsored by the WAY Consortium (Wearable interfaces for hAnd function recoverY; FP7-ICT-288551). ",
        "dataset_text": "This data contains EEG recordings of subjects performing grasp-and-lift (GAL) trials. The following video shows an example of a trial:  A detailed account of the data can be found in  There are 12 subjects in total, 10 series of trials for each subject, and approximately 30 trials within each series. The number of trials varies for each series. The training set contains the first 8 series for each subject. The test set contains the 9th and 10th series. For each GAL, you are tasked to detect 6 events: These events always occur in the same order. In the training set, there are two files for each subject + series combination: The events files for the test set are not provided and must be predicted. Each timeframe is given a unique id column according to the subject, series, and frame to which it belongs. The six label columns are either zero or one, depending on whether the corresponding event has occurred within \u00b1150ms (\u00b175frames). A perfect submission will predict a probability of one for this entire window. The columns in the data files are labeled according to their associated electrode channels. You may make use of the spatial relationship between the electrode locations, as shown in this diagram: "
    },
    {
        "name": "Right Whale Recognition",
        "url": "https://www.kaggle.com/competitions/noaa-right-whale-recognition",
        "overview_text": "Overview text not found",
        "description_text": "With fewer than 500 North Atlantic right whales left in the world's oceans, knowing the health and status of each whale is integral to the efforts of researchers working to protect the species from extinction. Currently, only a handful of very experienced researchers can identify individual whales on sight while out on the water. For the majority of researchers, identifying individual whales takes time, making it difficult to effectively target whales for biological samples, acoustic recordings, and necessary health assessments.  To track and monitor the population, right whales are photographed during aerial surveys and then manually matched to an online photo-identification catalog. Customized software has been developed to aid in this process (DIGITS), but this still relies on a manual inspection of the potential comparisons, and there is a lag time for those images to be incorporated into the database. The current identification process is extremely time consuming and requires special training. This constrains marine biologists, who work under tight deadlines with limited budgets. This competition challenges you to automate the right whale recognition process using a dataset of aerial photographs of individual whales. Automating the identification of right whales would allow researchers to better focus on their conservation efforts. Recognizing a whale in real-time would also give researchers on the water access to potentially life-saving historical health and entanglement records as they struggle to free a whale that has been accidentally caught up in fishing gear. MathWorks is sponsoring the competition prize pool. If your team is participating in this competition MathWorks is also providing complimentary software. Click here for more details on how to request your copy.  Thanks to Christin Khan and Leah Crowe from NOAA for hand labeling the images to create this one of a kind dataset and to the right whale research team at New England Aquarium for maintaining the photo-identification catalog. Without their continued efforts, none of this would be possible.  ",
        "dataset_text": "In this competition you are given aerial images, each containing a single Right whale. These images were taken over the course of 10 years and hundreds of helicopter trips, and have been selected and labeled by NOAA scientists with their whale IDs. To ensure that this is a computer vision problem, we have removed metadata such as creation dates and geo tags from the photos. To discourage hand labeling, we have supplemented the test dataset with some images that are resized, cropped, or flipped. These processed images are ignored and don't count towards your score.   Your goal is to build a \"face recognition system\" for whales. "
    },
    {
        "name": "Bengali.AI Handwritten Grapheme Classification",
        "url": "https://www.kaggle.com/competitions/bengaliai-cv19",
        "overview_text": "Overview text not found",
        "description_text": "Challenge and dataset summary paper available at https://arxiv.org/abs/2010.00170 Bengali is the 5th most spoken language in the world with hundreds of million of speakers. It\u2019s the official language of Bangladesh and the second most spoken language in India. Considering its reach, there\u2019s significant business and educational interest in developing AI that can optically recognize images of the language handwritten. This challenge hopes to improve on approaches to Bengali recognition.  Optical character recognition is particularly challenging for Bengali. While Bengali has 49 letters (to be more specific 11 vowels and 38 consonants) in its alphabet, there are also 18 potential diacritics, or accents. This means that there are many more graphemes, or the smallest units in a written language. The added complexity results in ~13,000 different grapheme variations (compared to English\u2019s 250 graphemic units). Bangladesh-based non-profit Bengali.AI is focused on helping to solve this problem. They build and release crowdsourced, metadata-rich datasets and open source them through research competitions. Through this work, Bengali.AI hopes to democratize and accelerate research in Bengali language technologies and to promote machine learning education. For this competition, you\u2019re given the image of a handwritten Bengali grapheme and are challenged to separately classify three constituent elements in the image: grapheme root, vowel diacritics, and consonant diacritics. By participating in the competition, you\u2019ll hopefully accelerate Bengali handwritten optical character recognition research and help enable the digitalization of educational resources. Moreover, the methods introduced in the competition will also empower cousin languages in the Indian subcontinent. Acknowledgements: If you use this dataset in your research, please cite this paper\n@inproceedings{alam2021large,\ntitle={A Large Multi-target Dataset of Common Bengali Handwritten Graphemes},\nauthor={Alam, Samiul and Reasat, Tahsin and Sushmit, Asif Shahriyar and Siddique, Sadi Mohammad and Rahman, Fuad and Hasan, Mahady and Humayun, Ahmed Imtiaz},\nbooktitle={International Conference on Document Analysis and Recognition},\npages={383--398},\nyear={2021},\norganization={Springer}\n}",
        "dataset_text": "Challenge and dataset summary available at https://arxiv.org/abs/2010.00170 Published @ ICDAR21 Citation: @inproceedings{alam2021large, title={A Large Multi-target Dataset of Common Bengali Handwritten Graphemes}, author={Alam, Samiul and Reasat, Tahsin and Sushmit, Asif Shahriyar and Siddique, Sadi Mohammad and Rahman, Fuad and Hasan, Mahady and Humayun, Ahmed Imtiaz}, booktitle={International Conference on Document Analysis and Recognition}, pages={383--398}, year={2021}, organization={Springer} } This dataset contains images of individual hand-written Bengali characters.\nBengali characters (graphemes) are written by combining three components: a grapheme_root, vowel_diacritic, and consonant_diacritic. Your challenge is to classify the components of the grapheme in each image. There are roughly 10,000 possible graphemes, of which roughly 1,000 are represented in the training set. The test set includes some graphemes that do not exist in train but has no new grapheme components. It takes a lot of volunteers filling out sheets like this to generate a useful amount of real data; focusing the problem on the grapheme components rather than on recognizing whole graphemes should make it possible to assemble a Bengali OCR system without handwriting samples for all 10,000 graphemes. Every image in the test set will require three rows of predictions, one for each component. This csv specifies the exact order for you to provide your labels. Each parquet file contains tens of thousands of 137x236 grayscale images. The images have been provided in the parquet format for I/O and space efficiency. Each row in the parquet files contains an image_id column, and the flattened image. Maps the class labels to the actual Bengali grapheme components. This is a synchronous rerun code competition, you can assume that the complete test set will contain essentially the same size and number of images as the training set. Consider performing inference on just one batch at a time to avoid memory errors. Only the first few rows/images in the test set and sample submission files can be downloaded. These samples provided so you can review the basic structure of the files and to ensure consistency between the publicly available set of file names and those your code will have access to while it is being rerun for scoring. The parquet files were written with pyarrow v 0.10.0 for compatibility with notebooks; you might not be able to read them with other versions of arrow/parquet. Update: March 2020 Two of the consonant diacritics \u09b0\u09cd (class 2) and \u09cd\u09b0 ( class 5) can coexist in the same grapheme.\nThe original labeling scheme did not account for this possibility so these cases are labeled as class (2) for the purpose of this competition. class_map_corrected.csv is an updated class map that adds this special case as class 7, train_multi_diacritics.csv is a list of the affected rows in the training set. Approximately 450 rows were affected in each of the train and test sets."
    },
    {
        "name": "Indoor Location & Navigation",
        "url": "https://www.kaggle.com/competitions/indoor-location-navigation",
        "overview_text": "Overview text not found",
        "description_text": "Your smartphone goes everywhere with you\u2014whether driving to the grocery store or shopping for holiday gifts. With your permission, apps can use your location to provide contextual information. You might get driving directions, find a store, or receive alerts for nearby promotions. These handy features are enabled by GPS, which requires outdoor exposure for the best accuracy. Yet, there are many times when you\u2019re inside large structures, such as a shopping mall or event center. Accurate indoor positioning, based on public sensors and user permission, allows for a great location-based experience even when you aren\u2019t outside. Current positioning solutions have poor accuracy, particularly in multi-level buildings, or generalize poorly to small datasets. Additionally, GPS was built for a time before smartphones. Today\u2019s use cases often require more granularity than is typically available indoors. In this competition, your task is to predict the indoor position of smartphones based on real-time sensor data, provided by indoor positioning technology company XYZ10 in partnership with Microsoft Research. You'll locate devices using \u201cactive\u201d localization data, which is made available with the cooperation of the user. Unlike passive localization methods (e.g. radar, camera), the data provided for this competition requires explicit user permission. You'll work with a dataset of nearly 30,000 traces from over 200 buildings. If successful, you\u2019ll contribute to research with broad-reaching possibilities, including industries like manufacturing, retail, and autonomous devices. With more accurate positioning, existing location-based apps could even be improved. Perhaps you\u2019ll even see the benefits yourself the next time you hit the mall. XYZ10 is a rising indoor positioning technology company in China. Since 2017, XYZ10 has been accumulating a privacy-sensitive indoor location dataset of WiFi, geomagnetic, and Bluetooth signatures with ground truths from nearly 1,000 buildings. Microsoft Research is the research subsidiary of Microsoft. Its goal is to advance state-of-the-art computing and solve difficult world research-motivated competition problems through technological innovation in collaboration with academic, government, and industry researchers.",
        "dataset_text": "The dataset for this competition consists of dense indoor signatures of WiFi, geomagnetic field, iBeacons etc., as well as ground truth (waypoint) (locations) collected from hundreds of buildings in Chinese cities. The data found in path trace files (*.txt) corresponds to an indoor path between position p_1 and p_2 walked by a site-surveyor. During the walk, an Android smartphone is held flat in front of the surveyors body, and a sensor data recording app is running on the device to collect IMU (accelerometer, gyroscope) and geomagnetic field (magnetometer) readings, as well as WiFi and Bluetooth iBeacon scanning results. A detailed description of the format of trace file is shown, along with other details and processing scripts, at this github link. In addition to raw trace files, floor plan metadata (e.g., raster image, size, GeoJSON) are also included for each floor. A note on data quality: In the training files, you may find occasionally that a line is missing the ending newline character, causing it to run on to the next line. It is up to you how you want to handle this issue. This issue is not found in the test data."
    },
    {
        "name": "Google Smartphone Decimeter Challenge",
        "url": "https://www.kaggle.com/competitions/google-smartphone-decimeter-challenge",
        "overview_text": "Overview text not found",
        "description_text": "Have you ever hit a surprise pothole or other road obstruction? Do you wish your navigation app could provide more precise location or lane-level accuracy? These and other novel features are powered by smartphone positioning services. Machine learning and precision GNSS algorithms are expected to improve this accuracy and provide billions of Android phone users with a more fine-tuned positioning experience.  Global Navigation Satellite System (GNSS) provides raw signals, which the GPS chipset uses to compute a position. Current mobile phones only offer 3-5 meters of positioning accuracy. While useful in many cases, it can create a \u201cjumpy\u201d experience. For many use cases the results are not fine nor stable enough to be reliable. This competition, hosted by the Android GPS team, is being presented at the ION GNSS+ 2021 Conference. They seek to advance research in smartphone GNSS positioning accuracy and help people better navigate the world around them. In this competition, you'll use data collected from the host team\u2019s own Android phones to compute location down to decimeter or even centimeter resolution, if possible. You'll have access to precise ground truth, raw GPS measurements, and assistance data from nearby GPS stations, in order to train and test your submissions. If successful, you'll help produce more accurate positions, bridging the connection between the geospatial information of finer human behavior and mobile internet with much finer granularity. Mobile users could gain better lane-level coordinates, enhanced experience in location-based gaming, and greater specificity in the location of road safety issues. You may even notice it's easier to get you where you need to go. The Android GPS team would like to show its appreciation to Verizon Hyper Precise Location Service and Swift Navigation Skylark Correction Service who provided assistance data for datasets in the challenge.",
        "dataset_text": "This challenge provides data from a variety of instruments useful for determining a phone's position: signals from GPS satellites, accelerometer readings, gyroscope readings, and more. As this challenge\u2019s design is focused on post-processing applications such as lane-level mapping, future data along a route will be available to generate positions as precisely as possible. You may also make use of information from neighboring phones to aid your estimation, as many routes may be represented by multiple phones. In order to encourage the development of a general GNSS positioning algorithm, in-phone GPS chipset locations will not be provided, as they are derived from a manufacturer proprietary algorithm that varies by phone model and other factors. A detailed explanation of the data collection process can be found in this paper. If publishing work based on this dataset/challenge, please ensure proper citation per the Competition Rules. [train]/[drive_id]/[phone_name]/ground_truth.csv - Only provided for the training set. Reference locations at expected timestamps. [train/test]/[drive_id]/[phone_name]/supplemental/[phone_name][.20o/.21o/.nmea] - Equivalent data to the gnss logs in other formats used by the GPS community. baseline_locations_[train/test].csv - Estimated coordinates generated using a simple approach. ground_truth.csv - Reference locations at expected timestamps. [train/test]/[drive_id]/[phone_name]/[phone_name]_derived.csv - GNSS intermediate values derived from raw GNSS measurements, provided for convenience. With these derived values, a corrected pseudorange (i.e. a closer approximation to the geometric range from the phone to the satellite)\ncan be computed as: correctedPrM = rawPrM + satClkBiasM - isrbM - ionoDelayM - tropoDelayM. The baseline locations\nare computed using correctedPrM and the satellite positions, using a standard Weighted Least Squares (WLS) solver, with the phone's position\n(x, y, z), clock bias (t), and isrbM for each unique signal type as states for each epoch. [train/test]/[drive_id]/[phone_name]/[phone_name]_GnssLog.txt - The phone's logs as generated by the GnssLogger App. This notebook demonstrates how to parse the logs.\nEach gnss file contains several sub-datasets, each of which is detailed below: Raw - The raw GNSS measurements of one GNSS signal (each satellite may have 1-2 signals for L5-enabled smartphones), collected\nfrom the Android API GnssMeasurement. Status - The status of a GNSS signal, as collected from the Android API GnssStatus. UncalAccel - Readings from the uncalibrated accelerometer, as collected from the Android API Sensor#TYPE_ACCELEROMETER_UNCALIBRATED. UncalGyro - Readings from the uncalibrated gyroscope, as collected from the Android API\nSensor#TYPE_GYROSCOPE_UNCALIBRATED. UncalMag - Readings from the uncalibrated magnetometer as collected from the Android API Sensor#STRING_TYPE_MAGNETIC_FIELD_UNCALIBRATED. OrientationDeg - Each row represents an estimated device orientation, collected from Android API SensorManager#getOrientation.\nThis message is only available in logs collected since March 2021."
    },
    {
        "name": "chaii - Hindi and Tamil Question Answering",
        "url": "https://www.kaggle.com/competitions/chaii-hindi-and-tamil-question-answering",
        "overview_text": "Overview text not found",
        "description_text": "With nearly 1.4 billion people, India is the second-most populated country in the world. Yet Indian languages, like Hindi and Tamil, are underrepresented on the web. Popular Natural Language Understanding (NLU) models perform worse with Indian languages compared to English, the effects of which lead to subpar experiences in downstream web applications for Indian users. With more attention from the Kaggle community and your novel machine learning solutions, we can help Indian users make the most of the web. Predicting answers to questions is a common NLU task, but not for Hindi and Tamil. Current progress on multilingual modeling requires a concentrated effort to generate high-quality datasets and modelling improvements. Additionally, for languages that are typically underrepresented in public datasets, it can be difficult to build trustworthy evaluations. We hope the dataset provided for this competition\u2014and additional datasets generated by participants\u2014will enable future machine learning for Indian languages. In this competition, your goal is to predict answers to real questions about Wikipedia articles. You will use chaii-1, a new question answering dataset with question-answer pairs. The dataset covers Hindi and Tamil, collected without the use of translation. It provides a realistic information-seeking task with questions written by native-speaking expert data annotators. You will be provided with a baseline model and inference code to build upon. If successful, you'll improve upon the baseline performance of NLU models in Indian languages. The results could improve the web experience for many of the nearly 1.4 billion people of India. Additionally, you\u2019ll contribute to multilingual NLP, which could be applied beyond the languages in this competition. Google Research India contributes fundamental advances in computer science and applies their research to big problems impacting India, Google, and communities around the world. The Natural Language Understanding group at Google Research India works specifically with ML to address the unique challenges in the Indian context (such as code mixing in Search, diversity of languages, dialects and accents in Assistant), learning from limited resources and advancing multilingual models. chaii (Challenge in AI for India) is a Google Research India initiative created with the purpose of sparking AI applications to address some of the pressing problems in India and to find unique ways to address them. Starting with a focus on NLU, chaii hopes to make progress towards multilingual modelling, as language diversity is significantly underserved on the web. Google Research India is working on transformational approaches to healthcare, agriculture and education, and also improving apps and services such as search, assistant and payments, e.g., to deal with challenges arising out of the diversity of languages in India. We also acknowledge the support from the AI4Bharat Team at the Indian Institute of Technology Madras.",
        "dataset_text": "In this competition, you will be predicting the answers to questions in Hindi and Tamil. The answers are drawn directly (see the Evaluation page for details) from a limited context. We have provided a small number of samples to check your code with. There is also a hidden test set. All files should be encoded as UTF-8. chaii 2021 dataset was prepared following the two step process as in TydiQA."
    },
    {
        "name": "BirdCLEF 2022",
        "url": "https://www.kaggle.com/competitions/birdclef-2022",
        "overview_text": "Overview text not found",
        "description_text": "As the \u201cextinction capital of the world,\u201d Hawai'i has lost 68% of its bird species, the consequences of which can harm entire food chains. Researchers use population monitoring to understand how native birds react to changes in the environment and conservation efforts. But many of the remaining birds across the islands are isolated in difficult-to-access, high-elevation habitats. With physical monitoring difficult, scientists have turned to sound recordings. Known as bioacoustic monitoring, this approach could provide a passive, low labor, and cost-effective strategy for studying endangered bird populations.  Current methods for processing large bioacoustic datasets involve manual annotation of each recording. This requires specialized training and prohibitively large amounts of time. Thankfully, recent advances in machine learning have made it possible to automatically identify bird songs for common species with ample training data. However, it remains challenging to develop such tools for rare and endangered species, such as those in Hawai'i. The Cornell Lab of Ornithology's K. Lisa Yang Center for Conservation Bioacoustics (KLY-CCB) develops and applies innovative conservation technologies across multiple ecological scales to inspire and inform the conservation of wildlife and habitats. KLY-CCB does this by collecting and interpreting sounds in nature and they've joined forces with Google Bioacoustics Group, LifeCLEF, Listening Observatory for Hawaiian Ecosystems (LOHE) Bioacoustics Lab at the University of Hawai'i at Hilo, and Xeno-Canto for this competition. In this competition, you\u2019ll use your machine learning skills to identify bird species by sound. Specifically, you'll develop a model that can process continuous audio data and then acoustically recognize the species. The best entries will be able to train reliable classifiers with limited training data. If successful, you'll help advance the science of bioacoustics and support ongoing research to protect endangered Hawaiian birds. Thanks to your innovations, it will be easier for researchers and conservation practitioners to accurately survey population trends. They'll be able to regularly and more effectively evaluate threats and adjust their conservation actions.",
        "dataset_text": "Your challenge in this competition is to identify which birds are calling in long recordings given quite limited training data. This is the exact challenge faced by scientists trying to monitor rare birds in Hawaii. For example, there are only a few thousand individual Nene geese left in the world, which makes it difficult to acquire recordings of their calls. This competition uses a hidden test. When your submitted notebook is scored, the actual test data (including a sample submission) will be made available to your notebook. train_metadata.csv - A wide range of metadata is provided for the training data. The most directly relevant fields are: train_audio/ - The bulk of the training data consists of short recordings of individual bird calls generously uploaded by users of xenocanto.org. These files have been downsampled to 32 kHz where applicable to match the test set audio and converted to the ogg format. test_soundscapes/ - When you submit a notebook, the test_soundscapes directory will be populated with approximately 5,500 recordings to be used for scoring. These are each within a few milliseconds of 1 minute long and in the ogg audio format. Only one soundscape is available for download. test.csv - Metadata for the test set. Only the first three rows are available for download; the full test.csv is provided in the hidden test set. sample_submission.csv - A valid sample submission. Only the first three rows are available for download; the full submission.csv is provided in the hidden test set. scored_birds.json - The subset of the species in the dataset that are scored. eBird_Taxonomy_v2021.csv - Data on the relationships between different species."
    },
    {
        "name": "Image Matching Challenge 2022",
        "url": "https://www.kaggle.com/competitions/image-matching-challenge-2022",
        "overview_text": "Overview text not found",
        "description_text": "2024 Update: you may want to check Image Matching Challenge 2024\n\n\n2023 Update: you may want to check Image Matching Challenge 2023\n\n\nFor most of us, our best camera is part of the phone in our pocket. We may take a snap of a landmark, like the Trevi Fountain in Rome, and share it with friends. By itself, that photo is two-dimensional and only includes the perspective of our shooting location. Of course, a lot of people have taken photos of that fountain. Together, we may be able to create a more complete, three-dimensional view. What if machine learning could help better capture the richness of the world using the vast amounts of unstructured image collections freely available on the internet? The process to reconstruct 3D objects and buildings from images is called Structure-from-Motion (SfM). Typically, these images are captured by skilled operators under controlled conditions, ensuring homogeneous, high-quality data. It is much more difficult to build 3D models from assorted images, given a wide variety of viewpoints, lighting and weather conditions, occlusions from people and vehicles, and even user-applied filters.  The first part of the problem is to identify which parts of two images capture the same physical points of a scene, such as the corners of a window. This is typically achieved with local features (key locations in an image that can be reliably identified across different views). Local features contain short description vectors that capture the appearance around the point of interest. By comparing these descriptors, likely correspondences can be established between the pixel coordinates of image locations across two or more images. This \u201cimage registration\u201d makes it possible to recover the 3D location of the point by triangulation.  Google employs Structure-from-Motion techniques across many Google Maps services, such as the 3D models created from StreetView and aerial imagery. In order to accelerate research into this topic, and better leverage the volume of data already publicly available, Google presents this competition in collaboration with the University of British Columbia and Czech Technical University. In this code competition, you\u2019ll create a machine learning algorithm that registers two images from different viewpoints. With access to a dataset of thousands of images to train and test your model, top-scoring notebooks will do so with the most accuracy. If successful, you'll help solve this well-known problem in computer vision, making it possible to map the world with unstructured image collections. Your solutions will have applications in photography and cultural heritage preservation, along with Google Maps. Winners will also be invited to give a presentation as part of the Image Matching: Local Features and Beyond workshop at the Conference on Computer Vision and Pattern Recognition (CVPR) in June. Eduard Trulls (Google), Yuhe Jin & Kwang Moo Yi (University of British Columbia, Vancouver, Canada), Dmytro Mishkin & Jiri Matas (Czech Technical University, Prague, Czech Republic) The organizers would like to thank the Machine Learning Lab at the Faculty of Applied Sciences, Ukrainian Catholic University (Lviv, Ukraine) for their help with dataset creation. Banner photo by Taneli Lahtinen on Unsplash. Trevi Fountain photos, left to right, then top to bottom: sarah|rose, kmaschke, jamingray, deglispiriti, Lucas Uyezu, justinknabb, Bogdan Migulski, S outH CheN, Melirius, 2bethere, Steve AM, L'amande.",
        "dataset_text": "Aligning photographs of the same scene is a problem of longstanding interest to computer vision researchers. Your challenge in this competition is to generate mappings between pairs of photos from various cities. This competition uses a hidden test. When your submitted notebook is scored, the actual test data (including a sample submission) will be made available to your notebook. train/*/calibration.csv train/*/pair_covisibility.csv train/scaling_factors.csv The poses for each scene where reconstructed via Structure-from-Motion, and are only accurate up to a scaling factor. This file contains a scalar for each scene which can be used to convert them to meters. For code examples, please refer to this notebook. train/*/images/ A batch of images all taken near the same location. train/LICENSE.txt Records of the specific source of and license for each image. sample_submission.csv A valid sample submission. test.csv Expect to see roughly 10,000 pairs of images in the hidden test set. test_images The test set. The test data comes from a different source than the train data and contains photos of mostly urban scenes with variable degrees of overlap. The two images forming a pair may have been collected months or years apart, but never less than 24 hours. Bridging this domain gap is part of the competition. The images have been resized so that the longest edge is around 800 pixels, may have different aspect ratios (including portrait and landscape), and are upright."
    },
    {
        "name": "Google Smartphone Decimeter Challenge 2022",
        "url": "https://www.kaggle.com/competitions/smartphone-decimeter-2022",
        "overview_text": "Overview text not found",
        "description_text": "The goal of this competition is to compute smartphones location down to the decimeter or even centimeter resolution which could enable services that require lane-level accuracy such as HOV lane ETA estimation. You'll develop a model based on raw location measurements from Android smartphones collected in opensky and light urban roads using datasets collected by the host.  Your work will help produce more accurate positions, bridging the connection between the geospatial information of finer human behavior and mobile internet with improved granularity. As a result, new navigation methods could be built upon the more precise data. Have you ever missed the lane change before a highway exit? Do you want to know the estimated time of arrival (ETA) of a carpool lane rather than other lanes? These and other useful features require precise smartphone positioning services. Machine learning models can improve the accuracy of Global Navigation Satellite System (GNSS) data. With more refined data, billions of Android phone users could have a more fine-tuned positioning experience. GNSS chipsets provide raw measurements, which can be used to compute the smartphone\u2019s position. Current mobile phones only offer 3-5 meters of positioning accuracy. For advanced use cases, the results are not fine enough nor reliable. Urban obstructions create the largest barriers to GPS accuracy. The data in this challenge includes only traces collected on opensky and light urban roads. These highways and main streets are the most widely used roads and will test the limits of smartphone positioning.  The Android GPS team in Google hosted the Smartphone Decimeter Challenge in 2021. Works by the three winners were presented at the ION GNSS+ 2021 Conference. This year, co-sponsored by the Institute of Navigation, this competition continues to seek advanced research in smartphone GNSS positioning accuracy and help people better navigate the world around them. In order to build upon last year\u2019s progress, the data also includes traces from the 2021 competition. Future competitions could include traces collected in harsher environments, such as deep urban areas with obstacles to satellite signals. Your efforts in this competition could impact how this more difficult data is interpreted. With decimeter level position accuracy, mobile users could gain better lane-level navigation, AR walk/drive, precise agriculture via phones, and greater specificity in the location of road safety issues. It will also enable a more personalized fine tuned navigation experience. Photos by Jared Murray, Thaddaeus Lim and Tobias Rademacher on Unsplash.",
        "dataset_text": "This challenge provides data from a variety of instruments useful for determining a phone's position: signals from GPS satellites, accelerometer readings, gyroscope readings, and more. Compared to last year's competition you will see more data overall, a wider variety of routes, and only a single phone per drive in the test set. As this challenge\u2019s design is focused on post-processing applications such as lane-level mapping, future data along a route will be available to generate positions as precisely as possible. In order to encourage the development of a general GNSS positioning algorithm, in-phone GPS chipset locations will not be provided, as they are derived from a manufacturer proprietary algorithm that varies by phone model and other factors. The data collection process used the same core approach described in this paper. If publishing work based on this dataset/challenge, please ensure proper citation per the Competition Rules. [train/test]/[drive_id]/[phone_name]/supplemental/[phone_name][.20o/.21o/.22o/.nmea] - Equivalent data to the gnss logs in other formats used by the GPS community. train/[drive_id]/[phone_name]/ground_truth.csv - Reference locations at expected timestamps. [train/test]/[drive_id]/[phone_name]/device_gnss.csv - Each row contains raw GNSS measurements, derived values, and a baseline estimated location.. This baseline was computed using correctedPrM and the satellite positions, using a standard Weighted Least Squares (WLS) solver, with the phone's position (x, y, z), clock bias (t), and isrbM for each unique signal type as states for each epoch. Some of the raw measurement fields are not included in this file because they are deprecated or are not populated in the original gnss_log.txt. [train/test]/[drive_id]/[phone_name]/device_imu.csv - Readings the phone's accelerometer, gyroscope, and magnetometer. [train/test]/[drive_id]/[phone_name]/supplemental/rinex.o\nA text file of GNSS measurements on , collected from Android APIs (same as the \"Raw\" messages above), then converted to the RINEX v3.03 format. refers to the last two digits of the year. During the conversion, the following treatments are taken to comply with the RINEX format. Essentially, this file contains a subset of information in the _GnssLog.txt. [train/test]/[drive_id]/[phone_name]/supplemental/gnss_log.txt - The phone's logs as generated by the GnssLogger App. This notebook demonstrates how to parse the logs. Each gnss file contains several sub-datasets, each of which is detailed below: Raw - The raw GNSS measurements of one GNSS signal (each satellite may have 1-2 signals for L5-enabled smartphones), collected\nfrom the Android API GnssMeasurement. Status - The status of a GNSS signal, as collected from the Android API GnssStatus. OrientationDeg - Each row represents an estimated device orientation, collected from Android API SensorManager#getOrientation.\nThis message is only available in logs collected since March 2021."
    },
    {
        "name": "Mayo Clinic - STRIP AI",
        "url": "https://www.kaggle.com/competitions/mayo-clinic-strip-ai",
        "overview_text": "Overview text not found",
        "description_text": "The goal of this competition is to classify the blood clot origins in ischemic stroke. Using whole slide digital pathology images, you'll build a model that differentiates between the two major acute ischemic stroke (AIS) etiology subtypes: cardiac and large artery atherosclerosis. Your work will enable healthcare providers to better identify the origins of blood clots in deadly strokes, making it easier for physicians to prescribe the best post-stroke therapeutic management and reducing the likelihood of a second stroke. Stroke remains the second-leading cause of death worldwide. Each year in the United States, over 700,000 individuals experience an ischemic stroke caused by a blood clot blocking an artery to the brain. A second stroke (23% of total events are recurrent) worsens the chances of the patient\u2019s survival. However, subsequent strokes may be mitigated if physicians can determine stroke etiology, which influences the therapeutic management following stroke events. During the last decade, mechanical thrombectomy has become the standard of care treatment for acute ischemic stroke from large vessel occlusion. As a result, retrieved clots became amenable to analysis. Healthcare professionals are currently attempting to apply deep learning-based methods to predict ischemic stroke etiology and clot origin. However, unique data formats, image file sizes, as well as the number of available pathology slides create challenges you could lend a hand in solving. The Mayo Clinic is a nonprofit American academic medical center focused on integrated health care, education, and research. Stroke Thromboembolism Registry of Imaging and Pathology (STRIP) is a uniquely large multicenter project led by Mayo Clinic Neurovascular Lab with the aim of histopathologic characterization of thromboemboli of various etiologies and examining clot composition and its relation to mechanical thrombectomy revascularization. To decrease the chances of subsequent strokes, the Mayo Clinic Neurovascular Research Laboratory encourages data scientists to improve artificial intelligence-based etiology classification so that physicians are better equipped to prescribe the correct treatment. New computational and artificial intelligence approaches could help save the lives of stroke survivors and help us better understand the world's second-leading cause of death.",
        "dataset_text": "The dataset for this competition comprises over a thousand high-resolution whole-slide digital pathology images. Each slide depicts a blood clot from a patient that had experienced an acute ischemic stroke. The slides comprising the training and test sets depict clots with an etiology (that is, origin) known to be either CE (Cardioembolic) or LAA (Large Artery Atherosclerosis). We include a set of supplemental slides with a either an unknown etiology or an etiology other than CE or LAA. Your task is to classify the etiology (CE or LAA) of the slides in the test set for each patient. Please note that this is a Code Competition, which means you will submit a notebook that makes predictions against a hidden test set. The test set visible here is only an example to help you author submission code. When you submit your notebook for scoring, this example data will be replaced by the actual test data."
    },
    {
        "name": "EMI Music Data Science Hackathon - July 21st - 24 hours",
        "url": "https://www.kaggle.com/competitions/MusicHackathon",
        "overview_text": "Overview text not found",
        "description_text": "Competition closes at 1pm London ( UTC+1) July 22nd \u201cSoulful\u201d ... \u201cCatchy\u201d ... \u201cCool\" ... \"Cheesy\" ... \"Edgy\u201d How do people connect to and describe the music they have just heard? EMI Insight performs extensive market research about their artists by interviewing thousands of people around the world. This research has produced EMI One Million Interview Dataset; one of the largest music preference datasets in the world today, that connects data about people--who they are, where they live, how they engage with music in their daily lives-- with their opinions about EMI\u2019s artists.\n\nThis Data Science London hackathon will focus on one key subset of this data: understanding what it is about people and artists that predicts how much people are going to like a particular track. We have taken a sample of the data from the United Kingdom that provides a granular mixture of profile, word-association, and rating data. The goal of this weekend hackathon is to design an algorithm that combines users\u2019 (a) demographics, (b) artist and track ratings, (c) answers to questions about their preferences for music, and (d) words that they use to describe EMI artists in order to predict how much they like tracks they have just heard. There is also a Visualization thread where you can submit your most amazing Music-Data Viz and view and vote on other contestants' entries.  Go to 'Prospect' at the top of this page.  Submissision will open at the same time as the competition. (Data will be made available 24 hours prior to the start of the contest) For more info http://musicdatascience.com/ hashtag #musicdata #ds_ldn #DSGhack   Proudly brought to you by   ",
        "dataset_text": ""
    },
    {
        "name": "Predicting Parkinson's Disease Progression with Smartphone Data",
        "url": "https://www.kaggle.com/competitions/predicting-parkinson-s-disease-progression-with-smartphone-data",
        "overview_text": "Overview text not found",
        "description_text": "There are many symptoms and features of Parkinson\u2019s disease which can be objectively measured and monitored using simple technology devices we carry every day. Mobile phones are some of the most pervasive forms of monitoring devices, with many smartphones carrying basic sensors that can be used to give a window into a patient\u2019s life. We have taken the initial steps with such a device, developing a basic collection application, and collecting data from a group of Parkinson\u2019s patients and control subjects. Now the challenge is on you to determine the best way to use it! The challenge is to develop a way to help patients and clinicians using objective, passively collected data points. The goal is to use the provided data to distinguish PD patients from control subjects and/or to quantify PD symptoms in a way that could enable the measurement of disease progression. You are invited to submit an entry showing the way you would use the data to describe a solution that addresses the objectives of the contest, including next steps to be done if more data were available.  ENTRY DEADLINE: March 26, 2013, 11:59 PM EST Share your participation and progress online: #PDdata ",
        "dataset_text": "The data is available as binary files or as text file .csvs (where the binary data has already been converted for you). You do not need both versions. If you don't want to write your own binary readers, download the text version. If you are unsure, download the text version. We strongly encourage looking at the data samples to get an idea of how the data is formatted.  If using text files, you'll want to download: If using binary files, you'll want to download: If a file is missing from a folder, it indicates data was not recorded for that particular time period and collection stream. The data files for this contest are BIG (up to 11GB compressed). You may have trouble downloading them on a slow connection. Fill out this form if you encounter download timeouts. A note on download managers: for security, download links are only active for a short period of time (to prevent sharing of data to people who have not accepted the rules).  Once started, the download can take as long as it takes, but it can't be resumed once stopped. Over a period ranging roughly December 2011 \u2013 March 2012, data was collected from 9 PD patients, at varying stages of the disease, and 7 healthy controls (not manifesting PD at the moment of recruitment), roughly matched for age and gender.   Subjects were asked to do the following: Data Description: The data contains the following streams: All streams contain data recorded at most once per second. Aggregate of over 6,000 hours of data has been collected to date (over 18,000 hours of all individual streams). All this data currently sits in raw data form, in hour long zip packets"
    },
    {
        "name": "Personalized Web Search Challenge",
        "url": "https://www.kaggle.com/competitions/yandex-personalized-web-search-challenge",
        "overview_text": "Overview text not found",
        "description_text": "The Personalized Web Search Challenge provides a unique opportunity to consolidate and scrutinize the work from industrial labs on personalizing web search using user-logged search behavior context. It provides a fully anonymized dataset shared by Yandex, which has anonymized user ids, queries, query terms, urls, url domains and clicks. This Challenge and the shared dataset will enable a whole new set of researchers to study the problem of personalizing web search experience. The Personalized Web Search Challenge is a part of series of contests organized by Yandex over many years. This year\u2019s event is the eighth since 2004. In previous years, participants tried to learn to rank documents, predict traffic jams, find similar images, predict relevance of documents using search logs and detect search engine switchings in search sessions. The Challenge is intended as a logical follow-up to the previous two challenges. We ask participants to re-rank URLs of each SERP returned by the search engine according to the personal preferences of the users. In other words, participants need to personalize search using the long-term (user history based) and short-term (session-based) user context. The evaluation relies on a variant of a dwell-time based model of personal relevance and is data-driven, as it is presently accepted in the state-of the-art research on personalized search. The Challenge is a part of the Web Search Click Data workshop (WSCD 2014) and the reports of the best teams are welcome to be presented at this workshop, to be held at WSDM 2014 conference, on 28th February in New York, USA. The workshop is organized by Pavel Serdyukov (Yandex), Georges Dupret (Yahoo!) and Nick Craswell (Microsoft Research/Bing). ",
        "dataset_text": "The dataset includes user sessions extracted from Yandex logs, with user ids, queries, query terms, URLs, their domains, URL rankings and clicks. Warning: the training set is ~16GB when uncompressed. To allay privacy concerns the user data is fully anonymized. Only meaningless numeric IDs of users, queries, query terms, sessions, URLs and their domains are released. The queries are grouped by sessions. Noteworthy characteristics of the dataset: Preprocessing done with the raw dataset before release: The training period shared with participants corresponds to 27 days of search activity. The next 3 days correspond to the test period. For each user from the test period, we take all her queries from the test period with at least one click with the dwell time not less than 50 time units (so, the clicked document is relevant or highly relevant according to our definition of personal relevance, see Evaluation). From this set of queries we filter out all queries with two or more clicks performed at the same unit of time. Finally, from the resulting set of queries we uniformly sample only one query and consider it to be a test query.  If the sampled query does not have any short-term context (it is the first one in the session) and the user that asked this query has no search sessions in the training period, we remove this query from the test set (since, it has neither short nor long-term context useful for personalization). We do not disclose any user actions made after the test query. However, the user's actions performed in the same session before the test query are provided (if any). We use 50% of test queries for Public Leaderboard and 50% for Private Leaderboard. "
    },
    {
        "name": "PAKDD 2014 - ASUS Malfunctional Components Prediction",
        "url": "https://www.kaggle.com/competitions/pakdd-cup-2014",
        "overview_text": "Overview text not found",
        "description_text": " The goal of PAKDD 2014 competition is to predict future malfunctional components of ASUS notebooks from historical data. This will help estimate how many products will require maintenance or repair services. ASUS has provided information on its laptop shipments as well as the laptops requiring maintenance or repair services. Participants will use this information to estimate how many of each module of a specific model will require maintenance or repair services. The organizers of PAKDD would like to thank ASUS for sponsorship of this competition.",
        "dataset_text": "In this page, we describe the format of the historical data and the desired output prediction. Two kinds of historical information are given: sale log and repair log. The time period of the sale log is from January/2005 to February/2008; while the time period of the repair log is from February/2005 to December/2009. Details of these two files are described in the File description section. Participants should exploit the sale and repair log to predict the the monthly repair amount for each module-component from January/2010 to July/2011. In other words, the model should output a series (nineteen elements, one element for one month) of predicted real-value (amount of repair) for each module-component. The desired output format is specified in the File description section. All the information provided by ASUS omits customers\u2019 personal information. The numbers in the data are transformed from the original values."
    },
    {
        "name": "KDD Cup 2012, Track 1",
        "url": "https://www.kaggle.com/competitions/kddcup2012-track1",
        "overview_text": "Overview text not found",
        "description_text": "Online social networking services have become tremendously popular in recent years, with popular social networking sites like Facebook, Twitter, and Tencent Weibo adding thousands of enthusiastic new users each day to their existing billions of actively engaged users. Since its launch in April 2010, Tencent Weibo, one of the largest micro-blogging websites in China, has become a major platform for building friendship and sharing interests online. Currently, there are more than 200 million registered users on Tencent Weibo, generating over 40 million messages each day. This scale benefits the Tencent Weibo users but it can also flood users with huge volumes of information and hence puts them at risk of information overload. Reducing the risk of information overload is a priority for improving the user experience and it also presents opportunities for novel data mining solutions. Thus, capturing users\u2019 interests and accordingly serving them with potentially interesting items (e.g. news, games, advertisements, products), is a fundamental and crucial feature social networking websites like Tencent Weibo.  The prediction task involves predicting whether or not a user will follow an item that has been recommended to the user. Items can be persons, organizations, or groups and will be defined more thoroughly below.  First, we define some notations as follows: \u201cItem\u201d: An item is a specific user in Tencent Weibo, which can be a person, an organization, or a group, that was selected and recommended to other users. Typically, celebrities, famous organizations, or some well-known groups were selected to form the \u2018items set\u2019 for recommendation. The size of this is about 6K items in the dataset.  Items are organized in categories; each category belongs to another category, and all together they form a hierarchy. For example, an item, a vip user Dr. Kaifu LEE, vip user: http://t.qq.com/kaifulee (wikipedia: http://en.wikipedia.org/wiki/Kai-Fu_Lee) represented as We can see that categories in different levels are separated by a dot \u2018.\u2019, and the category information about an item can help enhance your model prediction. For example, if a user Peter follows kaifulee, he may be interested in the other items of the category that kaifulee belongs to, and might also be interested in the items of the parent category of kaifulee\u2019s category. \u201cTweet\u201d: a \u201ctweet\u201d is the action of a user posting a message to the microblog system, or the posted message itself. So when one user is \u201ctweeting\u201c, his/her followers will see the \u201ctweet\u201d. \u201cRetweet\u201d: a user can repost a tweet and append some comments (or do nothing), to share it with more people (my followers). \u201cComment\u201d: a user can add some comments to a tweet. The contents of the comments  will not be automatically pushed to his/her followers as \u2018tweeting\u2019 or \u2018retweeting\u2019,but will appear at the \u2018comment history\u2019 of the commented tweet. \u201cFollowee/follower\u201d: If User B is followed by User A, B is a followee to A, and A is a follower to B. We describe the datasets as follows: The dataset represents a sampled snapshot of Tencent Weibo users\u2019 preferences for various items \u2013\u2013 the recommendation of items to users and the history of users\u2019 \u2018following\u2019 history. It is of a larger scale compared to other publicly available datasets ever released. Also it provides richer information in multiple domains such as user profiles, social graph, item category, which may hopefully evoke deeply thoughtful ideas and methodology. The users in the dataset, numbered in millions, are provided with rich information (demographics, profile keywords, follow history, etc.) for generating a good prediction model. To protect the privacy of the users, the IDs of both the users and the recommended items are anonymized as random numbers such that no identification is revealed. Furthermore, their information, when in Chinese, will be encoded as random strings or numbers, thus no contestant who understands Chinese would get advantages. Timestamps for recommendation are given for performing session analysis. Two datasets in 7 text files, downloadable: a) Training dataset : some fields are in the file rec_log_train.txt  b) Testing dataset: some fields are in the file rec_log_test.txt Format of the above 2 files: (UserId)\\t(ItemId)\\t(Result)\\t(Unix-timestamp) Result: values are 1 or -1, where 1 represents the user UserId accepts the recommendation of item ItemId and follows it (i.e., adds it to his/her social network), and -1 represents the user rejects the recommended item. We provide the true values of the \u2018Result\u2019 field in rec_log_train.txt, whereas in  rec_log_test.txt, the true values of the \u2018Result\u2019 field are withheld (for simplicity, in the file they are always 0). Another difference from rec_log_test.txt to rec_log_train.txt is that repeated recommended (UserId,ItemId) pairs were removed. c)      More fields of the training and the testing datasets about the user and the item are in the following 5 files:           i.              User profile data: user_profile.txt Each line contains the following information of a user: the year of birth, the gender, the number of tweets and the tag-Ids. It is important to note that information about the users to be recommended is also in this file. Format: (UserId)\\t(Year-of-birth)\\t(Gender)\\t(Number-of-tweet)\\t(Tag-Ids) Year of birth is selected by user when he/she registered. Gender has an integer value of 0, 1, or 2, which represents \u201cunknown\u201d, \u201cmale\u201d, or \u201cfemale\u201d, respectively. Number-of-tweet is an integer that represents the amount of tweets the user has posted. Tags are selected by users to represent their interests. If a user likes mountain climbing and swimming, he/she may select \"mountain climbing\" or \"swimming\" to be his/her tag. There are some users who select nothing. The original tags in natural languages are not used here, each unique tag is encoded as an unique integer. Tag-Ids are in the form \u201ctag-id1;tag-id2;...;tag-idN\u201d. If a user doesn\u2019t have tags, Tag-Ids will be \"0\".         ii.              Item data: item.txt Each line contains the following information of an item: its category and keywords. Format: (ItemId)\\t(Item-Category)\\t(Item-Keyword) Item-Category is a string \u201ca.b.c.d\u201d, where the categories in the hierarchy are delimited by the character \u201c.\u201d, ordered in top-down fashion (i.e., category \u2018a\u2019 is a parent category of \u2018b\u2019, and category \u2018b\u2019 is a parent category of \u2018c\u2019, and so on. Item-Keyword contains the keywords extracted from the corresponding Weibo profile of the person, organization, or group. The format is a string \u201cid1;id2;\u2026;idN\u201d, where each unique keyword is encoded as an unique integer such that no real term is revealed.       iii.              User action data: user_action.txt The file user_action.txt contains the statistics about the \u2018at\u2019 (@) actions between the users in a certain number of recent days. Format: (UserId)\\t(Action-Destination-UserId)\\t(Number-of-at-action)\\t(Number-of-retweet )\\t(Number-of-comment) If user A wants to notify another user about his/her tweet/retweet/comment, he/she would use an \u2018at\u2019 (@) action to notify the other user, such as \u2018@tiger\u2019 (here the user to be notified is \u2018tiger\u2019).. For example, user A has retweeted user B 5 times, has \u201cat\u201d B 3 times, and has commented user B 6 times, then there is one line \u201cA   B     3     5     6\u201d in user_action.txt.        iv.              User sns data: user_sns.txt The file user_sns.txt contains each user\u2019s follow history (i.e., the history of following another user). Note that the following relationship can be reciprocal. Format: (Follower-userid)\\t(Followee-userid)          v.              User key word data: user_key_word.txt The file user_key_word.txt contains the keywords extracted from the tweet/retweet/comment by each user. Format: (UserId)\\t(Keywords) Keywords is in the form \u201ckw1:weight1;kw2:weight2;\u2026kw3:weight3\u201d. Keywords are extracted from the tweet/retweet/comment of a user, and can be used as features to better represent the user in your prediction model. The greater the weight, the more interested the user is with regards to the keyword. Every keyword is encoded as a unique integer, and the keywords of the users are from the same vocabulary as the Item-Keyword.  Teams\u2019 scores and ranks on the leaderboard are based on a metric calculated from the predicted results in submitted result file and the held out ground truth of a validation dataset whose instances were a fixed set sampled from the testing dataset in the beginning and, until the last day of the competition (June 1, 2012) by then the scores and associated ranks on leaderboard are based on the predicted results and that of the rest of the testing dataset. This entails that the top-3 ranked teams at the time when the competition ends are the winners. The log for forming the training dataset corresponds to earlier time than that of the testing dataset. The evaluation metric is average precision. For a detailed definition of the metric, please refer to the tab \u2018Evaluation\u2019.  The prizes for the 1st, 2nd and 3rd winners for task 1 are US Dollars $5000, $2000, and $1000, respectively.  ",
        "dataset_text": "Dataset description not found"
    },
    {
        "name": "KDD Cup 2012, Track 2",
        "url": "https://www.kaggle.com/competitions/kddcup2012-track2",
        "overview_text": "Overview text not found",
        "description_text": "TASK 2 DESCRIPTION Search advertising has been one of the major revenue sources of the Internet industry for years. A key technology behind search advertising is to predict the click-through rate (pCTR) of ads, as the economic model behind search advertising requires pCTR values to rank ads and to price clicks. In this task, given the training instances derived from session logs of the Tencent proprietary search engine, soso.com, participants are expected to accurately predict the pCTR of ads in the testing instances. TRAINING DATA FILE    The training data file is a text file, where each line is a training instance derived from search session log messages. To understand the training data, let us begin with a description of search sessions.    A search session refers to an interaction between a user and the search engine. It contains the following ingredients: the user, the query issued by the user, some ads returned by the search engine and thus impressed (displayed) to the user, and zero or more ads that were clicked by the user. For clarity, we introduce a terminology here. The number of ads impressed in a session is known as the \u2019depth\u2019. The order of an ad in the impression list is known as the \u2018position\u2019 of that ad. An Ad, when impressed, would be displayed as a short text known as \u2019title\u2019, followed by a slightly longer text known as the \u2019description\u2019, and a URL (usually shortened to save screen space) known as \u2019display URL\u2019.    We divide each session into multiple instances, where each instance describes an impressed ad under a certain setting  (i.e., with certain depth and position values).  We aggregate instances with the same user id, ad id, query, and setting in order to reduce the dataset size. Therefore, schematically, each instance contains at least the following information: the number of search sessions in which the ad (AdID) was impressed by the user (UserID) who issued the query (Query). the number of times, among the above impressions, the user (UserID) clicked the ad (AdID).    Moreover, the training, validation and testing data contain more information than the above list, because each ad and each user have some additional properties. We include some of these properties into the training, validation  and the testing instances, and put other properties in separate data files that can be indexed using ids in the instances. For more information about these data files, please refer to the section ADDITIONAL DATA FILES.  Finally, after including additional features, each training instance is a line consisting of fields delimited by the TAB character:  1. Click: as described in the above list.  2. Impression: as described in the above list.  3. DisplayURL: a property of the ad.  The URL is shown together with the title and description of an ad. It is usually the shortened landing page URL of the ad, but not always. In the data file,  this URL is hashed for anonymity.  4. AdID: as described in the above list.  5. AdvertiserID: a property of the ad.  Some advertisers consistently optimize their ads, so the title and description of their ads are more attractive than those of others\u2019 ads.  6. Depth: a property of the session, as described above.    7. Position: a property of an ad in a session, as described above.  8. QueryID:  id of the query.  This id is a zero\u2010based integer value. It is the key of the data file 'queryid_tokensid.txt'. 9. KeywordID: a property of ads.  This is the key of  'purchasedkeyword_tokensid.txt'.  10. TitleID: a property of ads.  This is the key of 'titleid_tokensid.txt'.  11. DescriptionID: a property of ads.   This is the key of 'descriptionid_tokensid.txt'.  12. UserID  This is the key of 'userid_profile.txt'.  When we cannot identify the user, this field has a special value of 0.  ADDITIONAL DATA FILES There are five additional data files, as mentioned in the above section:  1. queryid_tokensid.txt  2. purchasedkeywordid_tokensid.txt  3. titleid_tokensid.txt  4. descriptionid_tokensid.txt  5. userid_profile.txt  Each line of the first four files maps an id to a list of tokens, corresponding to the query, keyword, ad title, and ad description, respectively. In each line, a TAB character separates the id and the token set.  A token can basically be a word in a natural language. For anonymity, each token is represented by its hash value.  Tokens are delimited by the character \u2018|\u2019.  Each line of \u2018userid_profile.txt\u2019 is composed of UserID, Gender, and Age, delimited by the TAB character. Note that not every UserID in the training and the testing set will be present in \u2018userid_profile.txt\u2019. Each field is described below:  1. Gender:  '1'  for male, '2' for female,  and '0'  for unknown.  2. Age:  '1'  for (0, 12],  '2' for (12, 18], '3' for (18, 24], '4'  for  (24, 30], '5' for (30,  40], and '6' for greater than 40.  TESTING DATASET The testing dataset shares the same format as the training dataset, except for the counts of ad impressions and ad clicks that are needed for computing the empirical CTR. A subset of the testing dataset is used to consistently rank submitted/updated results on the leaderboard. The testing dataset is used for picking the final winners. The log for forming the training dataset corresponds to earlier time than that of the testing dataset. EVALUATION Teams are expected to submit their result file in text format, in which each line corresponds to a line in the downloaded file with the same order, and there is only one field in each line: the predicted CTR. In the result file, the lines corresponding to the lines from validation dataset will be used to score for the ranking on the leaderboard during the competition except the last day (June 1, 2012), and the lines corresponding to the lines from testing dataset will be used for the ranking on the leaderboard on the day of June 1, 2012, and for picking the final winners. The performance of the prediction will be scored in terms of the AUC (for more details about AUC, please see \u2018ROC graphs: Notes and practical considerations for researchers\u2018 by Tom Fawcett). For a detailed definition of the metric, please refer to the tab \u2018Evalaution\u2019. PRIZES Teams with the best performance scores will be the winners. The prizes for the 1st, 2nd and 3rd winners for task 2 are US Dollars $5000, $2000, and $1000, respectively.  ",
        "dataset_text": "Dataset description not found"
    },
    {
        "name": "UPenn and Mayo Clinic's Seizure Detection Challenge",
        "url": "https://www.kaggle.com/competitions/seizure-detection",
        "overview_text": "Overview text not found",
        "description_text": "For individuals with drug-resistant epilepsy, responsive neurostimulation systems hold promise for augmenting current therapies and transforming epilepsy care. Of the more than two million Americans who suffer from recurrent, spontaneous epileptic seizures, 500,000 continue to experience seizures despite multiple attempts to control the seizures with medication. For these patients responsive neurostimulation represents a possible therapy capable of aborting seizures before they affect a patient's normal activities.   In order for a responsive neurostimulation device to successfully stop seizures, a seizure must be detected and electrical stimulation applied as early as possible. A seizure that builds and generalizes beyond its area of origin will be very difficult to abort via neurostimulation. Current seizure detection algorithms in commercial responsive neurostimulation devices are tuned to be hypersensitive, and their high false positive rate results in unnecessary stimulation. In addition, physicians and researchers working in epilepsy must often review large quantities of continuous EEG data to identify seizures, which in some patients may be quite subtle. Automated algorithms to detect seizures in large EEG datasets with low false positive and false negative rates would greatly assist clinical care and basic research. Intracranial EEG was recorded from dogs with naturally occurring epilepsy using an ambulatory monitoring system. EEG was sampled from 16 electrodes at 400 Hz, and recorded voltages were referenced to the group average.   In addition, datasets from patients with epilepsy undergoing intracranial EEG monitoring to identify a region of brain that can be resected to prevent future seizures are included in the contest. These datasets have varying numbers of electrodes and are sampled at 500 Hz or 5000 Hz, with recorded voltages referenced to an electrode outside the brain.  Seizure detection algorithms are relevant to two specific applications: 1) High sensitivity & specificity applications. Potential applications of this algorithm would be for automated seizure diaries, where latency to seizure onset is not critical. Here the goal is to optimize the accuracy of detection. 2) For responsive stimulation application the latency of onset is of particular importance. The key to successful therapy is the ability to rapidly detect the onset of seizures. Often a highly sensitive detector is created, and high false positive rates are tolerated as the stimulation is below patient perception. This competition is sponsored by the National Institues of Health (NINDS), and the American Epilepsy Society. ",
        "dataset_text": "Data are organized in folders containing training and testing data for each human or canine subject. The training data is organized into 1-second EEG clips labeled \"Ictal\" for seizure data segments, or \"Interictal\" for non-seizure data segments. Training data are arranged sequentially while testing data are in random order. Ictal training and testing data segments are provided covering the entire seizure, while interictal data segments are provided covering approximately the mean seizure duration for each subject. Starting points for the interictal data segments were chosen randomly from the full data record, with the restriction that no interictal segment be less than one hour before or after a seizure. Within folders data segments are stored in matlab .mat files, arranged in a data structure with fields as follow: The human data are from patients with temporal and extratemporal lobe epilepsy undergoing evaluation for epilepsy surgery. The iEEG recordings are from depth electrodes implanted along anterior-posterior axis of hippocampus, and from subdural electrode grids in various locations. Data sampling rates vary from 500 Hz to 5,000 Hz. The canine data are from an implanted device acquiring data from 16 subdural electrodes. Two 4-contact strips are implanted over each hemisphere in an antero-posterior orientation. Data are recorded continuously at a sampling frequency of 400 Hz and referenced to the group average. Additional annotated intracranial EEG data is freely available at the International Epilepsy Electrophysiology Portal, jointly developed by the University of Pennsylvania and the Mayo Clinic."
    },
    {
        "name": "Africa Soil Property Prediction Challenge",
        "url": "https://www.kaggle.com/competitions/afsis-soil-properties",
        "overview_text": "Overview text not found",
        "description_text": "Advances in rapid, low cost analysis of soil samples using infrared spectroscopy, georeferencing of soil samples, and greater availability of earth remote sensing data provide new opportunities for predicting soil functional properties at unsampled locations. Soil functional properties are those properties related to a soil\u2019s capacity to support essential ecosystem services such as primary productivity, nutrient and water retention, and resistance to soil erosion. Digital mapping of soil functional properties, especially in data sparse regions such as Africa, is important for planning sustainable agricultural intensification and natural resources management.  Diffuse reflectance infrared spectroscopy has shown potential in numerous studies to provide a highly repeatable, rapid and low cost measurement of many soil functional properties. The amount of light absorbed by a soil sample is measured, with minimal sample preparation, at hundreds of specific wavebands across a range of wavelengths to provide an infrared spectrum (Fig. 1). The measurement can be typically performed in about 30 seconds, in contrast to conventional reference tests, which are slow and expensive and use chemicals. Conventional reference soil tests are calibrated to the infrared spectra on a subset of samples selected to span the diversity in soils in a given target geographical area. The calibration models are then used to predict the soil test values for the whole sample set. The predicted soil test values from georeferenced soil samples can in turn be calibrated to remote sensing covariates, which are recorded for every pixel at a fixed spatial resolution in an area, and the calibration model is then used to predict the soil test values for each pixel. The result is a digital map of the soil properties. This competition asks you to predict 5 target soil functional properties from diffuse reflectance infrared spectroscopy measurements. This competition is sponsored by the Africa Soil Information Service.",
        "dataset_text": "SOC, pH, Ca, P, Sand are the five target variables for predictions. The data have been monotonously transformed from the original measurements and thus include negative values.  We have also included some potential spatial predictors from remote sensing data sources. Short variable descriptions are provided below and additional descriptions can be found at AfSIS data. The data have been mean centered and scaled. An example model using Bayesian Additive Regression Trees can be found here. Why not more data? We will not introduce additional data (e.g. georeference) at this stage of the competition. We think that would be confusing (to us), as we would really like to find out how predictive the spectral methods are/would be when they are applied in new places and/or at different points in time by data science experts such as yourselves. Subsequent Kaggle competitions may focus on explicitly spatial and or space-time predictions. Background on data set creation There have been a number of questions regarding why and how the data were ordered in the training and test sets. As some of you have surmised there is certainly geographical clustering in this dataset. This is due to the spatially stratified multilevel sampling design that was used to assemble the data. The following is an abbreviated version of how this came about. When the Africa Soil Information Service (AfSIS) project started in 2009, we were faced with the enormous logistical task of obtaining a representative sample covering ~18.1 million km2 of the non-desert portion of Africa, including Madagascar, that could be used as a baseline for monitoring soil and other ecosystem properties. The way we chose to go about this was to select 60, 10 \u00d7 10 km sized \u201cSentinel Landscapes\u201d, stratified by the major Koeppen-Geiger climate zones of Africa, excluding the true deserts and some of the African countries which we were not allowed to work in at the time, due to security reasons. Within each of the 60 Sentinel Landscapes AfSIS field teams sampled 16, 1 km2 \u201cSampling Clusters\u201d (1 km2 circular areas) with 10, 1000 m2, randomly located circular \u201cSampling Plots\u201d. Topsoil (0-20 cm) and subsoil (20-50 cm) samples were subsequently recovered by physically mixing core subsamples from 4 locations within each Sampling Plot. Hence the intent was to obtain a representative multilevel/multistage sample consisting of: Multiply those numbers and you obtain the intended number of composite soil samples (19,200) that were to be collected in the field over a 4-year period between 2009-2012. To achieve this target, we pre-generated appropriately randomized GPS coordinates for every Sampling Plot and, AfSIS field teams then navigated to (most) of those spots on the map to collect samples (insert n/N Sampling Plots). As might be expected with an exercise of this magnitude, the actual total number of soil samples in this dataset is somewhat smaller than intended, as some locations were either completely inaccessible by 4WD vehicle and/or on foot or that had soil depth restrictions that prevented the field teams from recovering physical samples. All physically recovered samples went into our lab (in Nairobi) to be characterized with the MIR spectral measurements that you are currently using. The potential spatial predictors, which cover the entire African continent (and beyond), were derived from NASA remote sensing data missions. A 10% subsample of all the soils that were measured with the MIR method, subsequently went on to be characterized with more reference measurements. \u201cReference measurements\u201d are much more expensive (potentially hundreds of U$ per sample) and time-consuming. The other 90% of samples that were not characterized with \u201creference\u201d methods have been physically archived, so that we can potentially retrieve those for calibrating new analytical methods and/or validating old methods. What is posted for this Kaggle is the complete spectral + reference dataset that we have currently, subject to the sampling procedures described above. The training and test data have been split along Sentinel Landscape levels because we are primarily interested in predicting soil properties at new Sentinel Landscapes."
    },
    {
        "name": "Global Energy Forecasting Competition 2012 - Load Forecasting",
        "url": "https://www.kaggle.com/competitions/global-energy-forecasting-competition-2012-load-forecasting",
        "overview_text": "Overview text not found",
        "description_text": "This is the Load Forecasting track of Global Energy Forecasting Competition 2012 (GEFCom2012).This competition will bring together the state-of-the-art techniques for energy forecasting, serve as the bridge to connect academic research and industry practice, promote analytics in power engineering education, and prepare the industry to overcome forecasting challenges in the smart grid world. The prize pool for the load forecasting track is $7,500. GEFCom is not a paper contest. Instead, this is a competition that requires participants to develop models and submit forecasts based on a given data set. Accuracy of the forecasts will be one evaluation criteria. In addition to accuracy, the participants are also required to submit a report describing the methodology, findings and models. Selected entries will be invited to IEEE PES General Meeting 2013 at Vancouver, Canada to present their methodologies and results. The team that finishes top of the leaderboard will win a cash prize. However an overall winner of the competition will be determined by the GEFCom Award Committee after the presentations based on forecasting accuracy, clarity of documentation, rigors of the approach, interpretability of the models and practicality to the industry. A few winning entries will be invited to submit the report in scientific paper format to prestigious scholarly journals, such as International Journal of Forecasting and IEEE Transactions on Smart Grid. The topic for the load forecasting track is a hierarchical load forecasting problem: backcasting and forecasting hourly loads (in kW) for a US utility with 20 zones. The participants are required to backcast and forecast at both zonal level (20 series) and system (sum of the 20 zonal level series) level, totally 21 series. Data (loads of 20 zones and temperature of 11 stations) history ranges from the 1st hour of 2004/1/1 to the 6th hour of 2008/6/30. Given actual temperature history, the 8 weeks below in the load history are set to be missing and are required to be backcasted. It's OK to use the entire history to backcast these 8 weeks. 2005/3/6 - 2005/3/12; 2005/6/20 - 2005/6/26; 2005/9/10 - 2005/9/16; 2005/12/25 - 2005/12/31; 2006/2/13 - 2006/2/19; 2006/5/25 - 2006/5/31; 2006/8/2 - 2006/8/8; 2006/11/22 - 2006/11/28; In addition, the particpants need to forecast hourly loads from 2008/7/1 to 2008/7/7. No actual temperatures are given for this week. ",
        "dataset_text": "In each of the 5 data files, there is a header row. Three columns of calendar variables: year, month of the year and day of the month. The last 24 columns are the 24 hours of the day. In \"Load_history.csv\", Column A is zone_id ranging from 1 to 20.  In \"Temperature_history.csv\", Column A is station_id ranging from 1 to 11. In \"submission_template.csv\", \"weights.csv\", and \"Benchmark.csv\", Column A is id, the identifier for each row; Column B is zone_id ranging from 1 to 21, where the 21st \"zone\" represents system level, which is the sum of the other 20 zones. \"Benchmark.csv\" shows the results from a benchmark model. Please make sure the submission strictly follow the format as indicated in \"submission_template.csv\", where the year was sorted in smallest to largest order first, then month, then day, and then zone_id."
    },
    {
        "name": "Global Energy Forecasting Competition 2012 - Wind Forecasting",
        "url": "https://www.kaggle.com/competitions/GEF2012-wind-forecasting",
        "overview_text": "Overview text not found",
        "description_text": "This is the Wind Forecasting track of Global Energy Forecasting Competition 2012 (GEFCom2012).This competition will bring together state-of-the-art techniques for energy forecasting, serve as the bridge to connect academic research and industry practice, promote analytics in power engineering education, and prepare the industry to overcome forecasting challenges in the smart grid world. The total prize pool for the wind forecasting track is $7,500. GEFCom is not a paper contest. Instead, this is a competition that requires participants to develop models and submit forecasts based on a given data set. Accuracy of the forecasts will be one of the evaluation criteria. In addition to accuracy, the participants are also required to submit a report describing the methodology, findings and models. Selected entries will be invited to IEEE PES General Meeting 2013 in Vancouver, Canada to present their methodologies and results. The team that finishes at the top of the leaderboard will win a cash prize. However overall winners of the competition will be determined by the GEFCom Award Committee after the presentations based on forecasting accuracy, clarity of documentation, rigors of the approach, interpretability of the models and practicality to the industry. A few winning entries will be invited to submit the report in scientific paper format to prestigious scholarly journals, such as International Journal of Forecasting and IEEE Transactions on Smart Grid. The topic for the wind forecasting track is focused on mimicking the operation 48-hour ahead prediction of hourly power generation at 7 wind farms, based on historical measurements and additional wind forecast information (48-hour ahead predictions of wind speed and direction at the sites). The data is available for period ranging from the 1st hour of 2009/7/1 to the 12th hour of 2012/6/28. The period between 2009/7/1 and 2010/12/31 is a model identification and training period, while the remainder of the dataset, that is, from 2011/1/1 to 2012/6/28, is there for the evaluation. The training period is there to be used for designing and estimating models permiting to predicting wind power generation at lead times from 1 to 48 hours ahead, based on past power observations and/or available meteorological wind forecasts for that period. Over the evaluation part, it is aimed at mimicking real operational conditions. For that, a number of 48-hour periods with missing power observations where defined. All these power observations are to be predicted. These periods are defined as following. The first period with missing observations is that from 2011/1/1 at 01:00 until 2011/1/3 at 00:00. The second period with missing observations is that from 2011/1/4 at 13:00 until 2011/1/6 at 12:00. Note that to be consistent, only the meteorological forecasts for that period that would actually be available in practice are given. These two periods then repeats every 7 days until the end of the dataset. Inbetween periods with missing data, power observations are available for updating the models.  ",
        "dataset_text": "\"train.csv\" contains the training data: - the first column (\"date\") is a timestamp giving date and time of the hourly wind power measurements in following columns. For instance \"2009070812\" is for the 8th of July 2009 at 12:00; - the following 7 columns (\"wp1\" to \"wp7\") gather the normalized wind power measurements for the 7 wind farms. They are normalized so as to take values between 0 and 1 in order for the wind farms not to be recognizable. In parallel, files with explanatory variables (wind forecasts) are also provided for those who may want to use them. For example, the file \"windforecasts_wf1\" contains the wind forecasts for the wind farm 1. In these files: - the first column (\"date\") is a timestamp giving date and time at which the forecasts are issued. For instance \"2009070812\" is for the 8th of July 2009 at 12:00; - the second column (\"hors\") is for the lead time of the forecast. For instance if \"date\" = 2009070812 and \"hors\" = 1, the forecast is for the 8th of July 2009 at 13:00 - the following 4 columns (\"u\", \"v\", \"ws\" and \"wd\") are the forecasts themselves, the first two being the zonal and meridional wind components, while the following two are corresponding wind speed and direction. Finally, the file \"benchmark.csv\" provide example forecast results from the persistence forecast method (\"what you see is what you get\"). This file also gives a template for submission of results that should be strictly followed. Please notice that the first column of \"benchmark.csv\" is called \"id\" and contains unique identifier for each row. The other 8 columns are the same as \"train.csv\". When submitting the results, please make sure that your file includes totally 9 columns in the same format as \"benchmark.csv\"."
    },
    {
        "name": "KDD Cup 2013 - Author-Paper Identification Challenge (Track 1)",
        "url": "https://www.kaggle.com/competitions/kdd-cup-2013-author-paper-identification-challenge",
        "overview_text": "Overview text not found",
        "description_text": "THIS COMPETITION IS COMPLETE. CONGRATULATIONS TO THE PRELIMINARY WINNERS!  The ability to search literature and collect/aggregate metrics around publications is a central tool for modern research. Both academic and industry researchers across hundreds of scientific disciplines, from astronomy to zoology, increasingly rely on search to understand what has been published and by whom. Microsoft Academic Search is an open platform that provides a variety of metrics and experiences for the research community, in addition to literature search. It covers more than 50 million publications and over 19 million authors across a variety of domains, with updates added each week. One of the main challenges of providing this service is caused by author-name ambiguity. On one hand, there are many authors who publish under several variations of their own name.  On the other hand, different authors might share a similar or even the same name. As a result, the profile of an author with an ambiguous name tends to contain noise, resulting in papers that are incorrectly assigned to him or her. This KDD Cup task challenges participants to determine which papers in an author profile were truly written by a given author. ",
        "dataset_text": "Code to create the benchmarks is available on Github. The files are available in two formats: as a zip archive containing CSV files (data.zip), and as a PostgreSQL relational database backup (data.postgres) that can be restored to an empty database. Instructions to restore the PostgreSQL datbase are on Github. The dataset(s) for the challenge are provided by Microsoft Corporation and come from their Microsoft Academic Search (MAS) database. MAS is a free academic search engine that was developed by Microsoft Research, and covers more than 50 million publications and over 19 million authors across a variety of domains. The provided datasets are based on a snapshot taken in Jan 2013 and contain: An Author dataset (Author.csv) with profile information about 250K authors, such as author name and affiliation. The same author can appear more than once in this dataset, for instance because he/she publishes under different versions of his/her name, such as J. Doe, Jane Doe, and J. A. Doe. A Paper dataset (Paper.csv) with data about 2.5M papers, such as paper title, conference/journal information, and keywords. The same paper may have been obtained through different data sources and hence have multiple copies in the dataset. A corresponding Paper-Author dataset (PaperAuthor.csv) with (paper ID, author ID) pairs. The Paper-Author dataset is noisy, containing possibly incorrect paper-author assignments that are due to author name ambiguity and variations of author names. Since each paper is either a conference or a journal, additional metadata about conferences and journals is provided where available (Conference.csv, Journal.csv). Co-authorship can be derived from the Paper-Author dataset.   Papers that authors have \"confirmed\" (acknowledging they were the author) or deleted (meaning they were not the author) have been split into Train, Validation, and Test sets based on the author's Id. The Train.csv and Valid.csv sets are provided now, and the Test.csv set will be released later in the competition.   "
    },
    {
        "name": "KDD Cup 2013 - Author Disambiguation Challenge (Track 2)",
        "url": "https://www.kaggle.com/competitions/kdd-cup-2013-author-disambiguation",
        "overview_text": "Overview text not found",
        "description_text": " The ability to search literature and collect/aggregate metrics around publications is a central tool for modern research. Both academic and industry researchers across hundreds of scientific disciplines, from astronomy to zoology, increasingly rely on search to understand what has been published and by whom. Microsoft Academic Search is an open platform that provides a variety of metrics and experiences for the research community, in addition to literature search. It covers more than 50 million publications and over 19 million authors across a variety of domains, with updates added each week. One of the main challenges of providing this service is caused by author-name ambiguity. This KDD Cup task challenges participants to determine which authors in a given data set are duplicates.",
        "dataset_text": "The data for the Author-Disambiguation is identical to the data for the Author-Paper Identification Challenge. You do not need to re-download if you already have the former. The files are available in two formats: as a zip archive containing CSV files (data.zip), and as a PostgreSQL relational database backup (data.postgres) that can be restored to an empty database.  The dataset(s) for the challenge are provided by Microsoft Corporation and come from their Microsoft Academic Search (MAS) database. MAS is a free academic search engine that was developed by Microsoft Research, and covers more than 50 million publications and over 19 million authors across a variety of domains. The provided datasets are based on a snapshot taken in Jan 2013 and contain: An Author dataset (Author.csv) with profile information about 250K authors, such as author name and affiliation. The same author can appear more than once in this dataset, for instance because he/she publishes under different versions of his/her name, such as J. Doe, Jane Doe, and J. A. Doe. A Paper dataset (Paper.csv) with data about 2.5M papers, such as paper title, conference/journal information, and keywords. The same paper may have been obtained through different data sources and hence have multiple copies in the dataset. A corresponding Paper-Author dataset (PaperAuthor.csv) with (paper ID, author ID) pairs. The Paper-Author dataset is noisy, containing possibly incorrect paper-author assignments that are due to author name ambiguity and variations of author names. Since each paper is either a conference or a journal, additional metadata about conferences and journals is provided where available (Conference.csv, Journal.csv). Co-authorship can be derived from the Paper-Author dataset.   "
    },
    {
        "name": "Google Brain - Ventilator Pressure Prediction",
        "url": "https://www.kaggle.com/competitions/ventilator-pressure-prediction",
        "overview_text": "Overview text not found",
        "description_text": "What do doctors do when a patient has trouble breathing? They use a ventilator to pump oxygen into a sedated patient's lungs via a tube in the windpipe. But mechanical ventilation is a clinician-intensive procedure, a limitation that was prominently on display during the early days of the COVID-19 pandemic. At the same time, developing new methods for controlling mechanical ventilators is prohibitively expensive, even before reaching clinical trials. High-quality simulators could reduce this barrier. Current simulators are trained as an ensemble, where each model simulates a single lung setting. However, lungs and their attributes form a continuous space, so a parametric approach must be explored that would consider the differences in patient lungs. Partnering with Princeton University, the team at Google Brain aims to grow the community around machine learning for mechanical ventilation control. They believe that neural networks and deep learning can better generalize across lungs with varying characteristics than the current industry standard of PID controllers. In this competition, you\u2019ll simulate a ventilator connected to a sedated patient's lung. The best submissions will take lung attributes compliance and resistance into account. If successful, you'll help overcome the cost barrier of developing new methods for controlling mechanical ventilators. This will pave the way for algorithms that adapt to patients and reduce the burden on clinicians during these novel times and beyond. As a result, ventilator treatments may become more widely available to help patients breathe. Photo by Nino Liverani on Unsplash",
        "dataset_text": "The ventilator data used in this competition was produced using a modified open-source ventilator connected to an artificial bellows test lung via a respiratory circuit. The diagram below illustrates the setup, with the two control inputs highlighted in green and the state variable (airway pressure) to predict in blue. The first control input is a continuous variable from 0 to 100 representing the percentage the inspiratory solenoid valve is open to let air into the lung (i.e., 0 is completely closed and no air is let in and 100 is completely open). The second control input is a binary variable representing whether the exploratory valve is open (1) or closed (0) to let air out. In this competition, participants are given numerous time series of breaths and will learn to predict the airway pressure in the respiratory circuit during the breath, given the time series of control inputs.  Each time series represents an approximately 3-second breath. The files are organized such that each row is a time step in a breath and gives the two control signals, the resulting airway pressure, and relevant attributes of the lung, described below."
    },
    {
        "name": "EMC Data Science Global Hackathon (Air Quality Prediction)",
        "url": "https://www.kaggle.com/competitions/dsg-hackathon",
        "overview_text": "Overview text not found",
        "description_text": "Hosted by Data Science London and Data Science Global as part of a Big Data Week event, and organised by Kaggle, the first ever global data science hackathon will take place at the same time in several cities around the world spanning a 24-hour period. During this time the data scientists will compete with each other for cash prizes using a large dataset provided by the Cook County, Illinois, local government. The challenge for the hackathon is to build with better more accurate predictive models of metropolitan air pollution. The EPA\u2019s Air Quality Index is used daily by people suffering from asthma and other respiratory diseases to avoid dangerous levels of outdoor air pollutants, which can trigger attacks. According to the World Health Organisation there are now estimated to be 235 million people suffering from asthma. Globally, it is now the most common chronic disease among children, with incidence in the US doubling since 1980.  The model we build could be used as the basis for an early warning system that is capable of accurately predicting dangerous levels of air pollutants on an hourly basis. Data Science Global is a non-profit organization dedicated to bringing together the world\u2019s communities of data scientists, artists, technologists and visionaries.  For our inaugural event, we are hosting a global data science hackathon.   It will be taking place simultaneously in cities around the world: London, New York, Boston, Chicago, San Francisco, Melbourne, Canberra, Sydney and Turku, Finland, as well as remote participants competing directly through Kaggle.  You can join in the live webcast from the participating venues at datascienceglobal.org",
        "dataset_text": "The data consist of hourly measurements of various quantities (mostly are pollutants), where each row contains the measurements for one hour. Time slices (\"chunks\") of 11 days have been created, with the first 8 days of each chunk available in the training data. You are asked to make pdictions about various points within the following 3 days (1, 2 ,3, 4, 5, 10, 17, 24, 48, and 72 hours after the end of the 8-day training data).\n\nWithin the training data, you are provided the following: The variables described above with \"_(site_number)\" are available for various sites, and similarly, \"_(target_number)\" will vary across several targets. You are provided with the lat/long of each sample site in a separate file. During the 3-day pdiction periods, you are provided only: You should get these from the sample submission file provided.\n\nYour submission will include: All of the \"target\" variables have been transformed to be approximately on the same scale (each with mean approximately 0 [CORRECTION: only the variance was normalized, not the mean] and variance approximately 1). We will reveal what quantity each target variable is a measurement of after the competition ends.\n\nThere are many rows for which some of the measurements are missing. This occurs in both the training and evaluation data. Our intention is to ignore these in calculating your score (MAE). To do achieve that effect, we have (in the solution file) transformed all NA's to \"-1,000,000\" and shown you where these occur by providing you with a sample submission that has these values in all of the correct places, and 0's everywhere else. You should make sure your solution has \"-1,000,000\" in the appropriate places. We apologize for the inconvenience."
    },
    {
        "name": "Job Salary Prediction",
        "url": "https://www.kaggle.com/competitions/job-salary-prediction",
        "overview_text": "Overview text not found",
        "description_text": "This competition was launched under the Kaggle Startup Program. If you're a startup with a predictive modelling challenge, please apply! Adzuna wants to build a prediction engine for the salary of any UK job ad, so they can make huge improvements in the experience of users searching for jobs, and help employers and jobseekers figure out the market worth of different positions. At the moment, approximately half of the UK job ads they index have a salary publicly displayed.  They need your help to bring more transparency to this important market.   Adzuna has a large dataset (hundreds of thousands of records), which is mostly unstructured text, with a few structured data fields. These can be in a number of different formats because of the hundreds of different sources of records.  Adzuna needs the help of the Kaggle community to figure out the best techniques to apply to this data set to build a highly accurate predictive model for new ads. You will build, train and test your salary prediction engines against a wide field of competitors.   As an added perk, Adzuna intends to implement their chosen model on their website, both in the UK and worldwide. You will have the satisfaction of seeing your work implemented in production and change the way people search for jobs in the future. Successful models will incorporate some analysis of the impact of including different keywords or phrases, as well as making use of the structured data fields like location, hours or company.  Some of the structured data shown (such as category) is 'inferred' by Adzuna's own processes, based on where an ad came from or its contents, and may not be \"correct\" but is representative of the real data. You will be provided with a training data set on which to build your model, which will include all variables including salary.  A second data set will be used to provide feedback on the public leaderboard.  After approximately 6 weeks, Kaggle will release a final data set that does not include the salary field to participants, who will then be required to submit their salary predictions against each job for evaluation.",
        "dataset_text": "  The main dataset consists of a large number of rows representing individual job ads, and a series of fields about each job ad.  These fields are as follows:\n\nId - A unique identifier for each job ad Title - A freetext field supplied to us by the job advertiser as the Title of the job ad.  Normally this is a summary of the job title or role. FullDescription - The full text of the job ad as provided by the job advertiser.  Where you see ***s, we have stripped values from the description in order to ensure that no salary information appears within the descriptions.  There may be some collateral damage here where we have also removed other numerics.\n\nLocationRaw - The freetext location as provided by the job advertiser.\n\nLocationNormalized - Adzuna's normalised location from within our own location tree, interpreted by us based on the raw location.  Our normaliser is not perfect! ContractType - full_time or part_time, interpreted by Adzuna from description or a specific additional field we received from the advertiser. ContractTime - permanent or contract, interpreted by Adzuna from description or a specific additional field we received from the advertiser. Company - the name of the employer as supplied to us by the job advertiser. Category - which of 30 standard job categories this ad fits into, inferred in a very messy way based on the source the ad came from.  We know there is a lot of noise and error in this field. SalaryRaw - the freetext salary field we received in the job advert from the advertiser. SalaryNormalised - the annualised salary interpreted by Adzuna from the raw salary.  Note that this is always a single value based on the midpoint of any range found in the raw salary.  This is the value we are trying to predict. SourceName - the name of the website or advertiser from whom we received the job advert.  All of the data is real, live data used in job ads so is clearly subject to lots of real world noise, including but not limited to: ads that are not UK based, salaries that are incorrectly stated, fields that are incorrectly normalised and duplicate adverts.   This is a supplemental data set that describes the hierarchical relationship between the different Normalised Locations shown in the job data.  It it is likely that there are meaningful relationships between the salaries of jobs in a similar geographical area, for example average salaries in London and the South East are higher than in the rest of the UK.      "
    },
    {
        "name": "Google Research Football with Manchester City F.C.",
        "url": "https://www.kaggle.com/competitions/google-football",
        "overview_text": "Overview text not found",
        "description_text": "Manchester City F.C. and Google Research are proud to present AI football competition using the Google Research Football Environment.  Brian Prestidge, Director of Data Insights & Decision Technology at City Football Group, the owners of Manchester City F.C., sets out the challenge. \u201cFootball is a tough environment to perform in and an even tougher environment to learn in. Learning is all about harnessing failure, but failure in football is seldom accepted. Working with Google Research\u2019s physics based football environment provides us with a new place to learn through simulation and offers us the capabilities to test tactical concepts and refine principles so that they are strong enough for a coach to stake their career on.\u201d \u201cWe are therefore very pleased to be working with Google\u2019s research team in creating this competition and are looking forward to the opportunity to support some of the most creative and successful competitors through funding and exclusive prizes. We hope to establish ongoing collaboration with the winners beyond this competition, and that it will provide us all with the platform to explore and establish fundamental principles of football tactics, thus improving our ability to perform and be successful on the pitch.\u201d Greg Swimer, Chief Technology Officer at City Football Group added \"Technologies such as Machine Learning and Artificial Intelligence have huge future potential to enhance the understanding and enjoyment of football for players, coaches and fans. We are delighted to be collaborating with Google's research team to help broaden the knowledge, talent, and innovation working in this exciting and transformational area\". The world gets a kick out of football (soccer in the United States). As the most popular sport on the planet, millions of fans enjoy watching Sergio Ag\u00fcero, Raheem Sterling, and Kevin de Bruyne on the field. Football video games are less lively, but still immensely popular, and we wonder if AI agents would be able to play those properly. Researchers want to explore AI agents' ability to play in complex settings like football. The sport requires a balance of short-term control, learned concepts such as passing, and high-level strategy, which can be difficult to teach agents. A current environment exists to train and test agents, but other solutions may offer better results. The teams at Google Research aspire to make discoveries that impact everyone. Essential to their approach is sharing research and tools to fuel progress in the field. Together with Manchester City F.C., Google Research has put forth this competition to get help in reaching their goal.  In this competition, you\u2019ll create AI agents that can play football. Teams compete in \u201csteps,\u201d where agents react to a game state. Each agent in an 11 vs 11 game controls a single active player and takes actions to improve their team\u2019s situation. As with a typical football game, you want your team to score more than the other side. You can optionally see your efforts rendered in a physics-based 3D football simulation. If controlling 11 football players with code sounds difficult, don't be discouraged! You only need to control one player at a time (the one with the ball on offense, or the one closest to the ball on defense) and your code gets to pick from 1 of 19 possible actions. We have prepared a getting started example to show you how simple a basic strategy can be. Before implementing your own strategy, however, you might want to learn more about the Google Research football environment, especially observations provided to you by the environment and available actions. You can also play the game yourself on your computer locally to get better understanding of the environment's dynamics and explore different scenarios. If successful, you'll help researchers explore the ability of AI agents to play in complex settings. This could offer new insights into the strategies of the world's most-watched sport. Additionally, this research could pave the way for a new generation of AI agents that can be trained to learn complex skills.",
        "dataset_text": "This page shows up underneath the data files. Here you describe what files you have provided and the format of each. There is no single format for this page that is appropriate for all competitons, but you should strive to describe as much as you can here. A little time spent describing the data here can save a lot of time answering questions later. Participants should be able to answer questions like these after reading the data description: What files do I need? What should I expect the data format to be? What am I predicting? What acroynms will I encounter?"
    },
    {
        "name": "Give Me Some Credit",
        "url": "https://www.kaggle.com/competitions/GiveMeSomeCredit",
        "overview_text": "Overview text not found",
        "description_text": "Banks play a crucial role in market economies. They decide who can get finance and on what terms and can make or break investment decisions. For markets and society to function, individuals and companies need access to credit.  Credit scoring algorithms, which make a guess at the probability of default, are the method banks use to determine whether or not a loan should be granted. This competition requires participants to improve on the state of the art in credit scoring, by predicting the probability that somebody will experience financial distress in the next two years. The goal of this competition is to build a model that borrowers can use to help make the best financial decisions. Historical data are provided on 250,000 borrowers and the prize pool is $5,000 ($3,000 for first, $1,500 for second and $500 for third).",
        "dataset_text": "Training, Test, Sample Entry and Submission Files are provided. Please check the format of the submission file."
    },
    {
        "name": "What Do You Know?",
        "url": "https://www.kaggle.com/competitions/WhatDoYouKnow",
        "overview_text": "Overview text not found",
        "description_text": "When studying for a test, you want to know how well you're going to do.  More specifically, you want to know what areas you need to study more.  In order to help students answer this question, we are attempting to predict their probability of answering questions correctly.  The data in this competition comes from students studying for three tests: the GMAT, SAT, and ACT. You are attempting to predict, for each question attempted in the test set, whether the student will answer the question correctly.  To succeed, you will need to improve on the state-of-the-art in student evaluation.  While the questions included labels indicating their specified test area, there may be structure which helps better organize the areas of knowledge involved in each question.  In the short term, this will help students figure out what areas they are weak in; but ultimately, this will help create tests to better measure what a student actually knows. The prize pool is $5,000 ($3,000 for first, $1,500 for second and $500 for third), with entries judged using Capped Binomial Deviance.",
        "dataset_text": "Data description The fields of the data are as follows (the mappings from category fields to numeric values are in category_labels.csv): Test data does not include answer_id or outcome (from which correctness could be determined). Test/training generation The data used in this competition is a sample of Grockit students (from the past three years) answering questions to prepare for the sat, gmat, or act.  The test/training split is derived by finding users who answered at least 6 questions, taking one of their answers (uniformly random, from their 6th question to their last), and inserting it into the test set.  Any later answers by this user are removed, and any earlier answers are included in the training set.  All answers from users not in the test set are also used for the training set (as they may be useful in estimating question parameters or baseline ability distributions). The test data distribution is thus different from training data in ways that may be significant.  First, it does not include 'timeout' or 'skipped' outcomes: all test results are from the student actually answering the question.  Second, it is biased towards users with more questions in the training set and biased towards their later answers.  Third, it is one entry per user, so the distribution of various aspects of the data (such as correct/incorrect) is over users, not over all answered questions. We have attempted to provide a reasonable validation split on the training data by taking the previous correct/incorrect answer for each of the students in the test set, for those users who had at least one previous answer.  The results are in the additional files valid_training.csv and valid_test.csv (in the zip/7z file).  Participants may find it helpful to compare potential algorithms by training on the valid_training.csv file and computing their performance on the valid_test.csv files.  However, there is no guarantee that this is an optimal validation set."
    },
    {
        "name": "Photo Quality Prediction",
        "url": "https://www.kaggle.com/competitions/PhotoQualityPrediction",
        "overview_text": "Overview text not found",
        "description_text": "Background We have a large collection of user-generated photos. We want to automatically pick out particularly enjoyable or impressive ones to highlight, especially travel-related, using only the meta-data associated with the images such as caption text, image dimensions and approximate location in the world. We know from our preliminary experiments and intuition that certain words and places are correlated with good photos, and others are indicators of less enjoyable pictures, but we would like to develop an algorithm to tie together these multiple signals. Objective Given anonymized information on thousands of photo albums, predict whether a human evaluator would mark them as 'good'.",
        "dataset_text": "Data format For anonymity reasons, the caption, title and description texts have been broken down into word tokens, and the most common (excluding stop words) have been encoded as numbers. The fields are: Notes From our own experiments, we know there's obvious correlations that make sense, such as areas in Africa rich in wildlife having a high proportion of good photos, and certain words that are associated with poor albums. Our goal is to turn some of those informal correlations into a more rigorous model we can reuse on much larger data sets. To encode the text I'm creating a whitelist of words, consisting of non-stop-words that occur more than twenty times in different albums. For each word in that list, I assign a numerical value, and write out the numbers of all the words that appear in each album's title, description and photo captions. The main goal of this exercise is to ensure that there's not enough information to reconstruct the text, whilst still having enough content for a prediction algorithm to sink its teeth into. I've done some preliminary work to make sure there are promising signals in the data that's remaining, and there do seem to be strong correlations, so I'm hopeful it's a good compromise. Why don't we make the image data available? Our application needs to make quality decisions on very large volumes of externally-hosted images in a short amount of time, so image processing solutions aren't feasible for us. We're hopeful that the meta-data will provide a good enough approximation to be useful"
    },
    {
        "name": "Event Recommendation Engine Challenge",
        "url": "https://www.kaggle.com/competitions/event-recommendation-engine-challenge",
        "overview_text": "Overview text not found",
        "description_text": "We (the competition hosts) are excited to sponsor the Event Recommendation Engine Challenge, which asks you to predict what events our users will be interested in based on events they\u2019ve responded to in the past, user demographic information, and what events they\u2019ve seen and clicked on in our app. The insights you discover from this data, and the algorithms the winners create, will allow us to improve our event recommendation algorithm, a core part of our applications and a key element in improving user experience. This is the first competition launching under the Kaggle Startup Program!",
        "dataset_text": "  Benchmark Code There are six files in all: train.csv, test.csv, users.csv, user_friends.csv, events.csv, and event_attendees.csv. train.csv has six columns:  user, event, invited, timestamp, interested, and not_interested.  Test.csv contains the same columns as train.csv, except for interested and not_interested. Each row corresponds to an event that was shown to a user in our application.  event is an id identifying an event in a our system.  user is an id representing a user in our system.  invited is a binary variable indicated whether the user has been invited to the event. timestamp is a ISO-8601 UTC time string representing the approximate time (+/- 2 hours) when the user saw the event in our application. interested is a binary variable indicating whether a user clicked on the \"Interested\" button for this event; it is 1 if the user clicked Interested and 0 if the user did not click the button.  Similarly, not_interested is a binary variable indicating whether a user clicked on the \"Not Interested\" button for this event; it is 1 if the user clicked the button and 0 if not.  It is possible that the user saw an event and clicked neither Interested nor Not Interested, and hence there are rows that contain 0,0 as values for interested,not_interested. users.csv contains demographic data about our some of our users (including all of the users appearing in the train and test files), and it has the following columns: user_id, locale, birthyear, gender, joinedAt, location, and timezone. user_id is the id of the user in our system.  locale is a string representing the user's locale, which should be of the form language_territory. birthyear is a 4-digit integer representing the year when the user was born. gender is either male or female, depending on the user's gender.  joinedAt is an ISO-8601 UTC time string representing when the user first used our application.  location is a string representing the user's location (if known).  timezone is a signed integer representing the user's UTC offset (in minutes). user_friends.csv contains social data about this user, and contains two columns:  user and friends.  user is the user's id in our system, and friends is a space-delimited list of the user's friends' ids. events.csv contains data about events in our system, and has 110 columns.  The first nine columns are event_id, user_id, start_time, city, state, zip, country, lat, and lng.  event_id is the id of the event, and user_id is the id of the user who created the event.  city, state, zip, and country represent more details about the location of the venue (if known).  lat and lng are floats representing the latitude and longitude coordinates of the venue, rounded to three decimal places.  start_time is the ISO-8601 UTC time string representing when the event is scheduled to begin.  The last 101 columns require a bit more explanation; first, we determined the 100 most common word stems (obtained via Porter Stemming) occuring in the name or description of a large random subset of our events.  The last 101 columns are count_1, count_2, ..., count_100, count_other, where count_N is an integer representing the number of times the Nth most common word stem appears in the name or description of this event.  count_other is a count of the rest of the words whose stem wasn't one of the 100 most common stems. event_attendees.csv contains information about which users attended various events, and has the following columns: event_id, yes, maybe, invited, and no. event_id identifies the event. yes, maybe, invited, and no are space-delimited lists of user id's representing users who indicated that they were going, maybe going, invited to, or not going to the event."
    },
    {
        "name": "Amazon.com - Employee Access Challenge",
        "url": "https://www.kaggle.com/competitions/amazon-employee-access-challenge",
        "overview_text": "Overview text not found",
        "description_text": " When an employee at any company starts work, they first need to obtain the computer access necessary to fulfill their role. This access may allow an employee to read/manipulate resources through various applications or web portals. It is assumed that employees fulfilling the functions of a given role will access the same or similar resources. It is often the case that employees figure out the access they need as they encounter roadblocks during their daily work (e.g. not able to log into a reporting portal). A knowledgeable supervisor then takes time to manually grant the needed access in order to overcome access obstacles. As employees move throughout a company, this access discovery/recovery cycle wastes a nontrivial amount of time and money. There is a considerable amount of data regarding an employee\u2019s role within an organization and the resources to which they have access. Given the data related to current employees and their provisioned access, models can be built that automatically determine access privileges as employees enter and leave roles within a company. These auto-access models seek to minimize the human involvement required to grant or revoke employee access. The objective of this competition is to build a model, learned using historical data, that will determine an employee's access needs, such that manual access transactions (grants and revokes) are minimized as the employee's attributes change over time. The model will take an employee's role information and a resource code and will return whether or not access should be granted. This competition is hosted in collaboration with the IEEE International Workshop on Machine Learning for Signal Processing (MLSP 2013)",
        "dataset_text": "The data consists of real historical data collected from 2010 & 2011.  Employees are manually allowed or denied access to resources over time. You must create an algorithm capable of learning from this historical data to predict approval/denial for an unseen set of employees.  train.csv - The training set. Each row has the ACTION (ground truth), RESOURCE, and information about the employee's role at the time of approval test.csv - The test set for which predictions should be made.  Each row asks whether an employee having the listed characteristics should have access to the listed resource.   "
    },
    {
        "name": "StumbleUpon Evergreen Classification Challenge",
        "url": "https://www.kaggle.com/competitions/stumbleupon",
        "overview_text": "Overview text not found",
        "description_text": "StumbleUpon is a user-curated web content discovery engine that recommends relevant, high quality pages and media to its users, based on their interests. While some pages we recommend, such as news articles or seasonal recipes, are only relevant for a short period of time, others maintain a timeless quality and can be recommended to users long after they are discovered. In other words, pages can either be classified as \"ephemeral\" or \"evergreen\". The ratings we get from our community give us strong signals that a page may no longer be relevant - but what if we could make this distinction ahead of time? A high quality prediction of \"ephemeral\" or \"evergreen\" would greatly improve a recommendation system like ours. Many people know evergreen content when they see it, but can an algorithm make the same determination without human intuition? Your mission is to build a classifier which will evaluate a large set of URLs and label them as either evergreen or ephemeral. Can you out-class(ify) StumbleUpon? As an added incentive to the prize, a strong performance in this competition may lead to a career-launching internship at one of the best places to work in San Francisco.",
        "dataset_text": "Note: researchers who wish to use this data outside the competition should download and read the data access agreement. There are two components to the data provided for this challenge: The first component is two files: train.tsv and test.tsv. Each is a tab-delimited text file containing the fields outlined below for 10,566 urls total. Fields for which no data is available are indicated with a question mark. The second component is raw_content.zip, a zip file containing the raw content for each url, as seen by StumbleUpon's crawler. Each url's raw content is stored in a tab-delimited text file, named with the urlid as indicated in train.tsv and test.tsv. The following table includes field descriptions for train.tsv and test.tsv:"
    },
    {
        "name": "Instant Gratification",
        "url": "https://www.kaggle.com/competitions/instant-gratification",
        "overview_text": "Overview text not found",
        "description_text": "Welcome to Instant (well, almost) Gratification! In 2015, Kaggle introduced Kernels as a resource to competition participants. It was a controversial decision to add a code-sharing tool to a competitive coding space. We thought it was important to make Kaggle more than a place where competitions are solved behind closed digital doors. Since then, Kernels has grown from its infancy--essentially a blinking cursor in a docker container--into its teenage years. We now have more compute, longer runtimes, better datasets, GPUs, and an improved interface. We have iterated and tested several Kernels-only (KO) competition formats with a true holdout test set, in particular deploying them when we would have otherwise substituted a two-stage competition. However, the experience of submitting to a Kernels-only competition has typically been asynchronous and imperfect; participants wait many days after a competition has concluded for their selected Kernels to be rerun on the holdout test dataset, the leaderboard updated, and the winners announced. This flow causes heartbreak to participants whose Kernels fail on the unseen test set, leaving them with no way to correct tiny errors that spoil months of hard work. We're now pleased to announce general support for a synchronous Kernels-only format. When you submit from a Kernel, Kaggle will run the code against both the public test set and private test set in real time. This small-but-substantial tweak improves the experience for participants, the host, and Kaggle: This competition is a low-stakes, trial-run introduction to our new synchronous KO implementation. We want to test that the process goes smoothly and gather feedback on your experiences. While it may feel like a normal KO competition, there are complicated new mechanics in play, such as the selection logic of Kernels that are still running when the deadline passes. Since the competition also presents an authentic machine learning problem, it will also award Kaggle medals and points. Have fun, good luck, and welcome to the world of synchronous Kernels competitions!",
        "dataset_text": "This is an anonymized, binary classification dataset found on a USB stick that washed ashore in a bottle. There was no data dictionary with the dataset, but this poem was handwritten on an accompanying scrap of paper: In a synchronous Kernels-only competition, the files you can observe and download will be different than the private test set and sample submission. The files may have different ids, may be a different size, and may vary in other ways, depending on the problem. You should structure your code so that it predicts on the public test.csv in the format specified by the public sample_submission.csv, but does not hard code aspects like the id or number of rows. When Kaggle runs your Kernel privately, it substitutes the private test set and sample submission in place of the public ones. "
    },
    {
        "name": "Predict Grant Applications",
        "url": "https://www.kaggle.com/competitions/unimelb",
        "overview_text": "Overview text not found",
        "description_text": "Description text not found",
        "dataset_text": "This dataset includes 249 features (or predictors). Participants should use these variables to predict the target variable (or outcome), \"Grant Status\". A grant status of 1 represents a successful grant application, while a grant status of 0 represents an unsuccessful application.\n\nThe training dataset, which participants use to build their models, is unimelb_training.csv. It contains 8,707 grant applications from late 2005 to 2008. The test dataset, unimelb_test.csv, contains 2,176 grant applications from 2009 to mid 2010. The grant status variable is withheld from the test dataset.\n\nPredictions should take the same format as unimelb_example.csv (a CSV file with 2,176 rows, a grant application ID in the first column and a probability of success - between 0 and 1 - in the second column). \n\nThe university has provided the following features:\nSponsor Code: an ID used to represent different sponsors\nGrant Category Code: categorization of the sponsor (e.g. Australian competitive grants, cooperative research centre, industry)\nContract Value Band: the grant's value (see key below)\nStart Date: the date the grant application was submitted\nRFCD Code: research fields, courses and disciplines classification (see definitions)\nRFCD Percentage: if there are several RFCD codes that are relevant to a project\nSEO Code: socio economic objective classification (see definitions)\nSEO Percentage: if there are several SEO codes that are relevant to a project\nPerson ID: the investigator's unique ID\nRole: the investigator's role in the study\nYear of Birth: the investigator's year of birth (rounded to the nearst five year interval)\nCountry of birth: the investigator's country of birth (often aggregated to by-continent)\nHome Language: the investigator's native language (classified into English and Other)\nDept No: the investigator's department\nFaculty No: the investigator's faculty\nGrade Level: the investigator's level of seniority\nNo. of years in Uni at time of grant: the number of years the investigator had been at the University of Melbourne when the grant application was made\nNumber of Successful Grant: the number of successful grant applications the investigator had made\nNumber of Unsuccessful Grant: the number of unsuccessful grant applications the investigator had made\nA: number of A journal articles\nA: number of A journal articles\nB: number of B journal articles\nC: number of C journal articles\n\nContract value band key:"
    },
    {
        "name": "Visualize the State of Public Education in Colorado",
        "url": "https://www.kaggle.com/competitions/visualize-the-state-of-education-in-colorado",
        "overview_text": "Overview text not found",
        "description_text": "About Us Colorado School Grades was created by a coalition of non-profit, community organizations that believe all children deserve access to a high-performing school. Our mission is to provide community members, parents, students, and educators with school performance information that is both accessible and easy-to-understand. Our hope is that this will help families and students make more informed decisions about the school they choose based in some part on academic performance information. We also aim to inspire and equip community members, parents, students, and educators with the information and resources they need to effectively engage in local school improvement efforts.  We provide resources for community stakeholders to improve their chosen schools. Colorado School Grades receives over 300,000 parents annually. Please use the site as a reference for any additional questions that you may have. And please be sure to include the Colorado School Grades link and logo in your visualization. Competition is organized and administered by Ryan Wilson at FiveFifty.     Data Visualization Interests We believe that information is power and our work translates the state of Colorado\u2019s school performance labels into easier-to-understand letter grades. We make these grades public on an intuitive, user-friendly platform at www.ColoradoSchoolGrades.com. Now that we have three years of letter grades for the schools, we are interested in seeing what trends and insights visualisation experts can identify and explain in compelling data visualizations. Here is a list of some the questions we find intriguing:",
        "dataset_text": "Summary of Grading Methodology While most other rating systems are based solely on student academic achievement, or one snapshot in time of student performance, Colorado School Grades believes in the importance of using student academic growth to calculate overall school performance. For this reason, Colorado School Grades worked with the Center for Education Policy Analysis at the University of Colorado at Denver to calculate the grades using the exact same variables and weights as the Colorado Department of Education\u2019s School Performance Framework. The input data for calculating the overall grades includes:     The key difference between Colorado School Grades and the Colorado Department of Education\u2019s School Performance Framework are the labels and the cut scores that are used. The Colorado Department of Education uses the following four labels and the forced grading curve:     While Colorado has been a pioneer in developing an academic growth model and school performance framework, Colorado School Grades believes that these statutorily defined labels do not provide parents and community members with an accurate or intuitive portrayal of their school\u2019s performance.  When parents and community members do not understand how their schools are performing, it is challenging to engage them in school improvement efforts. It is also difficult for parents to select schools based on academic performance and growth. Therefore, only the savviest parents are able to navigate choice in this complicated system.   For these reasons, Colorado School Grades replaces fuzzy descriptors such as \u201cperformance\u201d and \u201cpriority improvement\u201d with universally understood letter grades (A-F). Our theory is that community members, parents, students, and educators can much more easily understand grades, which convey a ranking scale in a way that a collection of descriptors do not.   Additionally, Colorado School Grades does not believe that the state\u2019s forced curve is rigorous enough. Under the state\u2019s existing system, schools in the 41st percentile and schools in the 99th percentile are both listed in the state\u2019s top category. Therefore, Colorado School Grades assigns letter grades based on the following more nuanced, rigorous ratings model.     Additional information about the methodology can be found in the University of Colorado \u2013 Denver\u2019s Technical Notes \u2013 attached. Spreadsheet Descriptions Each year of data (2010, 2011, and 2012 has its own spreadsheet, which contains all of the data provided on ColoradoSchoolGrades.com. 2010 refers to the 2009-10 school year 2011 refers to the 2010-11 school year 2012 refers to the 2011-12 school year.   Below is a description of the various spreadsheets. Each spreadsheet name is underlined.   Data map \u2013 This worksheet is a table of contents used by the researcher and the website developer to organize the data and the various files that are included in the spreadsheet. Final Grades \u2013 This is the most important worksheet in the file. ***If the column is not described in the tables below, please ignore them as they are not germane to the competition.   Grades Logic - The grades are actually displayed as a number 1-13. Here\u2019s how that breaks down.   1&3year change \u2013 this worksheet identifies the trend arrows for the school that are listed on the report card. Trend arrows are either going up, down, or flat. There are trend arrows for the overall grade, the academic achievement grade, and the academic growth grade. New columns in this tab include:   Trend Arrow Logic Enrl_Working This worksheet identifies the race/ethnicity characteristics of the students enrolled at each school. New columns in this tab include:   COACT \u2013 This tab identifies whether or not the students at a particular school are ready for college or career based on their ACT scores. The answer is a simple yes or no   Yes/No Logic   Remediation_HS_With_Data - This worksheet identifies whether or not the students graduating from that school require remediation. Remediation means that those students need to re-take high school level courses while they are in college for which they do not earn college credit. The column entitled \u201cRemediation_at_leastone_pct2010\u201d identifies the percentage of students who go on to colleges in Colorado and have to re-take high school level courses (the remediation rate). K-12_FRL-Working \u2013 This worksheet identifies the percentage of students enrolled in the school who qualify for free or reduced price lunches. This is a proxy for low income students. 2011 School Coordinates \u2013 This worksheet displays the schools locations in a different format, using GPS coordinates, instead of physical locations. If you do not use the GPS coordinates, please use the physical location as opposed to the mailing address. Frequently Asked Questions Q: Who created the letter grades? A: The Colorado School Grades coalition worked with an independent, third-party organization - the Center for Education Policy Analysis at the University of Colorado at Denver - to translate the Colorado Department of Education\u2019s School Performance data into letter grades.  When necessary, the coalition made key decisions regarding the forced grading curve and the criteria for which schools received grades.   Q: How are the overall letter grades calculated? A: The Center for Education Policy Analysis at the University of Colorado at Denver calculated the grades using the same variables and weights as the Colorado Department of Education\u2019s School Performance Framework. The input data for calculating the overall grades includes:     Using the same criteria as the Colorado Department of Education, the measures and metrics of each of key performance indicator are combined using the exact same weights as the state and letter grades are assigned based on the following forced curve.       Q: Why do you grade on a curve? A: Colorado School Grades did not plan to grade on a forced curve that grades schools relative to other schools in the state. Our intent was to create absolute cut scores that were determined by defensible, non-arbitrary rationale. As such, Colorado School Grades would have preferred to use a standards-based model that set absolute cut scores, which would allow all schools to achieve an \u201cA\u201d grade if they reached a certain level of performance.  There were two challenges to this goal. The Colorado Growth Model is built on relative measures that compare the academic growth of students across the state.  Thus, the primary input performance data to the system rated schools relative to other schools in the state.  Second, we found that other jurisdictions had to regularly adjust their standards-based cut scores as input measures to the system changed (such as new assessment tools).  Because of the comparative nature of state\u2019s model, and lessons learned from other jurisdictions, Colorado School Grades was unable to develop absolute cut scores. This allowed us to keep indicators and weights consistent and develop a cut score methodology that can be maintained as the state develops new assessments or different input measures.    Q: What is academic achievement? A:  According to the Colorado Department of Education, academic achievement reflects how a school\u2019s students are doing at meeting the Colorado\u2019s model content standards. It is measured by the percentage of students scoring proficient or advanced on Colorado\u2019s standards-based assessments. Read more   Q: How is the academic achievement sub-grade calculated? A: The academic achievement grade is calculated by ranking all schools from highest performing to lowest performing based on the percentage of students in the school who scored proficient or advanced on the statewide standardized assessment. Elementary and middle schools are compared relative to other elementary and middle schools. However, high schools are only compared to other high schools because the states system uses different weights and criteria than in elementary and middle schools.  Letter grades are assigned based on the same curve as the overall grades.   Q: What is academic growth? A: According to the Colorado Department of Education, academic growth measures academic progress using the Colorado Growth Model. For an individual student, growth is a measure of progress in academic achievement in comparison to other similar students. For some states, this measure might simply be a change (a gain or a loss) in test scores from one year to the next. For Colorado, growth is not expressed in test score point gains or losses, but in student academic growth percentiles. Read more   Q: How is the academic growth sub-grade calculated? A: The academic growth grade is calculated by ranking all schools from highest performing to lowest performing based on the school\u2019s overall median growth percentile. Elementary and middle schools are compared relative to other elementary and middle schools. However, high schools are only compared to other high schools because the states system uses different weights and criteria than in elementary and middle schools.  Letter grades are assigned based on the same curve as the overall grades.   Q: What are academic growth gaps? A: According to the Colorado Department of Education, academic growth gaps measure the academic progress of historically disadvantaged student subgroups and students needing to catch up. It disaggregates academic growth into student subgroups including: students eligible for Free or Reduced Lunch (low-income), minority students, students with disabilities (IEP status), English Language Learners, and students needing to catch up.   Q: Why are academic growth gaps not shown on the school\u2019s report card? A: Academic growth gaps are very complex indicators. Because the growth gap data are disaggregated into subgroups, there is no one measure that can be solely relied on to rank and compare schools. Although, academic growth gaps are not displayed on the school\u2019s report card, the data for each school is embedded in the overall school\u2019s grade, just as it is in the Colorado Department of Education\u2019s School Performance Framework.   Q: What is college and career readiness? A: According to the Colorado Department of Education, college and career readiness measures the preparedness of students for post-secondary education or the workforce upon completing high school. The indicator reflects student graduation rates, dropout rates, and school averages of the Colorado ACT composite scores. While these indicators are included in the calculation of the school\u2019s overall grade, they are not displayed on the school\u2019s report card page. Instead, we chose to display the school\u2019s graduation rate, remediation rate, and subject-level ACT data on the report card.   Q: Why is there not a sub-grade for college and career readiness? A: Similar to academic growth gaps, the college and career readiness indicator is actually based on a series of different data points. Because the data is organized this way, there is no one measure that can be solely relied on to rank and compare schools. Although a sub-grade is not provided for this category, the data for each high school is embedded in the overall school\u2019s grade, just as it is in the Colorado Department of Education\u2019s School Performance Framework. Additionally, Colorado School Grades decided to display more detailed information for this indicator than is used by the state\u2019s system.   Q: What data is used to determine if a high school\u2019s average student is college or career ready in each subject? A: The ACT is the measurement instrument and the indicator of whether or not the average student is college or career ready in a particular subject. The ACT has set benchmarks scores that represent the level of achievement required for students to have a 50% chance of obtaining a B or higher, or about a 75% chance of obtaining a C or higher in corresponding credit-bearing first-year college courses. The ACT College Readiness Benchmarks are:     Q: How is the school\u2019s ranking determined? A: Using the same criteria as the Colorado Department of Education, schools are assigned a ranking based on the total number of percentage points earned on the SPF out of a possible 100. The ranking is based on the number of schools that receive a score higher than the school in question. Because the indicators that are used are different depending on the type of school, elementary and middle schools are compared to all other elementary and middle schools. However, high schools are only compared to other high schools. To illustrate, if School A receives a ranking of 5, that means that four schools received a higher number of total percentage points on the SPF, so School A is in fifth place. The schools are then ranked from highest performing to lowest performing based on their total score. In some cases, ties occur because the total scores are identical.   Q: How is the 3 year trend calculated? A: For each school that data is available, the school\u2019s performance in the current year is compared to a blended average of the previous three years. For example, the 3 year trend for a school\u2019s overall grade is calculated by comparing the school\u2019s total score in 2011 to a blended average of its total score between 2009-2011.   Q: Why don't private schools have letter grades? A: Private schools are not required to publicly report test results, therefore we do not have comparative data necessary to calculate a letter grade.   Q: Why do some public schools not have a letter grade? A: Colorado School Grades does not grade schools if they do not have 100% of the input data used to calculate a total score.  As a reminder, that input data includes academic proficiency, academic growth, academic growth gaps for all schools, as well as college and career readiness for high schools. There are 1,950 schools in Colorado and grades were provided for 1,811 of them. Of the 139 schools without letter grades, most fall into this category for one of three reasons. First, they are a new school and have not been open long enough for the Colorado Department of Education to calculate academic growth or growth gaps data. Second, they are a small school and the Colorado Department of Education does not publically report data points when there are between 16 and 20 students (depending on the measure) to protect student privacy.   Third, they are an alternative school and the Colorado Department of Education opts not to provide them a label and instead defers to the local school district. Colorado School Grades provides as much data as the Colorado Department of Education reports.  If your school does not have a grade and you are interested in learning more, we encourage you to visit the Colorado Department of Education\u2019s webpage and contact your school directly to discuss its performance with the principal.   Q: How can a school's overall grade be high when the academic achievement grades are low, or vice versa? A: The reason for either of these instances has to do with the grading formula. In elementary and middle schools, academic growth is weighted twice as much as academic achievement. In fact, 75% of elementary and middle schools\u2019 total score is comprised of academic growth and academic gaps.  Similarly, in high schools, academic growth is weighted more than twice as much as academic achievement. Academic growth and academic growth gaps account for 50% of the total score, while academic achievement only accounts for 15%.  "
    },
    {
        "name": "Accelerometer Biometric Competition",
        "url": "https://www.kaggle.com/competitions/accelerometer-biometric-competition",
        "overview_text": "Overview text not found",
        "description_text": "Since everyone moves differently and accelerometers are fast becoming ubiquitous, this competition is designed to investigate the feasibility of using accelerometer data as a biometric for identifying users of mobile devices. Seal has collected accelerometer data from several hundred users over a period of several months during normal device usage. To collect the data, we published an app on Googles\u2019s Android PlayStore that samples accelerometer data in the background and posts it to a central database for analysis. We have uploaded approximately 60 million unique samples of accelerometer data collected from 387 different devices. These are split into equal sets for training and test. Samples in the training set are labeled with the unique device from which the data was collected. The test set is demarcated into 90k sequences of consecutive samples from one device.  A file of test questions is provided in which you are asked to determine whether the accelerometer data came from the proposed device.",
        "dataset_text": "Your task is to determine whether the accelerometer recordings in the test set (test.csv)  belong to the proposed devices in the question set (questions.csv). "
    },
    {
        "name": "DecMeg2014 - Decoding the Human Brain",
        "url": "https://www.kaggle.com/competitions/decoding-the-human-brain",
        "overview_text": "Overview text not found",
        "description_text": "Understanding how the human brain works is a primary goal in neuroscience research. Non-invasive functional neuroimaging techniques, such as magnetoencephalography (MEG), are able to capture the brain activity as multiple timeseries. When a subject is presented a stimulus and the concurrent brain activity is recorded, the relation between the pattern of recorded signal and the category of the stimulus may provide insights on the underlying mental process. Among the approaches to analyse the relation between brain activity and stimuli, the one based on predicting the stimulus from the concurrent brain recording is called brain decoding. The goal of this competition is to predict the category of a visual stimulus presented to a subject from the concurrent brain activity. The brain activity is captured with an MEG device which records 306 timeseries at 1KHz of the magnetic field associated with the brain currents. The categories of the visual stimulus for this competition are two: face and scrambled face. A stimulus and the concurrent MEG recording is called trial and thousands of randomized trials were recorded from multiple subjects. The trials of some of the subjects, i.e. the train set, are provided to create prediction models. The remaining trials, i.e. the test set, belong to different subjects and they will be used to score the prediction models. Because of the variability across subjects in brain anatomy and in the patterns of brain activity, a certain degree of difference is expected between the data of different subjects and thus between the train set and the test set.  Full details of the neuroscientific experiment in which the data were collected are described in: A brief survey of the scientific literature on the problem of decoding across subjects, together with the description of the train set of this competition and a preliminary solution in terms of transfer learning, are described in: This competition is associated with the the 19th International Conference on Biomagnetism, Biomag 2014. The Biomag conference will be held in Halifax, Canada, August 24-28, 2014. This competition is organized by Emanuele Olivetti, Mostafa Kia and Paolo Avesani (NeuroInformatics Lab, Fondazione Bruno Kessler and Universit\u00e0 di Trento, IT). The awards of this competition are funded by Elekta Oy, MEG International Services Ltd (MISL), Fondazione Bruno Kessler, and Besa. We would also like to thank Daniel Wakeman (Martinos Center, MGH, USA), Richard Henson (MRC/CBU, Cambridge, UK), Ole Jensen (Donders Institute, NL), Nathan Weisz (University of Trento, IT) and Alexandre Gramfort (Telecom ParisTech, CNRS, CEA / Neurospin) for their contributions in preparing this competition.                    ",
        "dataset_text": "The training data consist of 9414 trials, i.e. MEG recordings and the class labels (Face/Scramble), from 16 subjects (subject01 to subject16). The test set comprises 4058 MEG recordings from 7 subjects (subject17 to subject23), without class labels. For each subject approximately 580-590 trials are available. Each trial consists of 1.5 seconds of MEG recording (starting 0.5sec before the stimulus starts) and the related class label, Face (class 1) or Scramble (class 0) . The data were down-sampled to 250Hz and high-pass filtered at 1Hz. 306 timeseries were recorded, one for each of the 306 channels, for each trial. All the pre-processing steps were carried out with mne-python. The trials of each subject are arranged into a 3D data matrix (trial x channel x time) of size 580 x 306 x 375. For each subject in the train set, the 3D data matrix and the associated vector of 580 class labels are saved into a file named train_subjectXX.mat. The data of the subjects in the test set are saved in files named test_subjectXX.mat. These files are in Matlab v5.0 format and can be easily read from Octave/Matlab, Python (see scipy.io.loadmat()), R (see the R.matlab package), Java (see JMatIO) and from other programming languages. Notice that the all train and test files are conveniently grouped into 4 large zip files: train_01_06.zip, train_07_12.zip, train_13_16.zip and test_17_23.zip. The MEG sensors are spatially arranged as described in the file Vectorview-mag.lout, which is a 2D approximation of the actual layout (see the picture in the description page). The 306 sensors are grouped, three at a time, in 102 locations. At each location two orthogonal gradiometers and one magnetometer record the magnetic field induced by the brain currents. The magnetometer measures the z (radial) component of the magnetic field, while the gradiometers measure the x and y spatial derivative of the magnetic field. Notice that if the number in the sensor name ends with \"1\" then it is a magnetometer. If it ends with \"2\" or \"3\", then it is a gradiometer. For example, \"MEG 0113\" is a gradiometer and \"MEG 0111\" is a magnetometer. Simple Python and Matlab code examples to load the data and to create a submission file, are available on GitHub at this link. Examples of code for other programming languages may be available in future. Contributions are welcome. Each file train_subjectXX.mat has the following fields: Each file test_subjectXX.mat has the same fields as above with the exception of the absence of field y and the addition of this field:"
    },
    {
        "name": "Tradeshift Text Classification",
        "url": "https://www.kaggle.com/competitions/tradeshift-text-classification",
        "overview_text": "Overview text not found",
        "description_text": "In the late 90's, Yann LeCun's team pioneered the successful application of machine learning to optical character recognition. 25 years later, machine learning continues to be an invaluable tool for text processing downstream from the OCR process.  Tradeshift has created a dataset with thousands of documents, representing millions of words. In each document, several bounding boxes containing text are selected. For each piece of text, many features are extracted and certain labels are assigned.  In this competition, participants are asked to create and open source an algorithm that correctly predicts the probability that a piece of text belongs to a given class.",
        "dataset_text": "For all the documents, words are detected and combined to form text blocks that may overlap to each other. Each text block is enclosed within a spatial box, which is depicted by a red line in the sketch below. The text blocks from all documents are aggregated in a data set where each text block corresponds to one sample (row).  For example, if we have 3 documents with 34, 62 and 53 text blocks, respectively, the data set will have 149 samples. For each sample, several features are extracted that are stored in the train.csv and test.csv. The features include content, parsing, spatial and relational information. The feature values can be: The number of samples is \\\\(N\\\\), the number of features is \\\\(M\\\\) and the number of labels is \\\\(K\\\\). One sample may belong to one or more labels, i.e. multi-label problem. The values in the trainLabels.csv are in the range [0,1], where 0 implies false and 1 implies true. Thus, the ij-element (ith row, jth column) indicates if the i-sample belongs to the j-label, where \\\\(i \\in \\left \\{ 1, .. , N \\right \\}\\\\) and \\\\(j \\in \\left \\{ 1, .. , K \\right \\}\\\\). As a sample may belong to several labels, the sum per row is not always one. In addition, the sum per column does not also add up to one. An example of features and labels for the training data is presented below. The dimensions of the example are N=7, M=6 and K=4.  The test data is split into public (30%) and private (70%) sets, which are used for the public and private leaderboards. All the files follow a format of comma-separated values (csv) where the headers are 1-indexed. Each row in the files stores a different sample. "
    },
    {
        "name": "Freesound Audio Tagging 2019",
        "url": "https://www.kaggle.com/competitions/freesound-audio-tagging-2019",
        "overview_text": "Overview text not found",
        "description_text": "One year ago, Freesound and Google\u2019s Machine Perception hosted an audio tagging competition challenging Kagglers to build a general-purpose auto tagging system. This year they\u2019re back and taking the challenge to the next level with multi-label audio tagging, doubled number of audio categories, and a noisier than ever training set. If you like raising your ML game, this challenge is for you. ",
        "dataset_text": "If you are interested in the FSDKaggle2019 dataset used for this competition, please download it from Zenodo. The Zenodo version contains all the dataset files, including the full test set and labels as well as as updated csv files with metadata and licenses. Please download FSDKaggle2019 at https://doi.org/10.5281/zenodo.3612637 If you use the FSDKaggle2019 dataset or baseline code, please cite our DCASE 2019 paper: You can also consider citing our ISMIR 2017 paper, which describes how we gathered the manual annotations included in FSDKaggle2019. The following 5 audio files in the curated train set have a wrong label, due to a bug in the file renaming process:\nf76181c4.wav, 77b925c2.wav, 6a1f682a.wav, c7db12aa.wav, 7752cc8a.wav The audio file 1d44b0bd.wav in the curated train set was found to be corrupted (contains no signal) due to an error in format conversion. Current machine learning techniques require large and varied datasets in order to provide good performance and generalization. However, manually labelling a dataset is time-consuming, which limits its size. Websites like Freesound or Flickr host large volumes of user-contributed audio and metadata, and labels can be inferred automatically from the metadata and/or making predictions with pre-trained models. Nevertheless, these automatically inferred labels might include a substantial level of label noise. The main research question addressed in this competition is how to adequately exploit a small amount of reliable, manually-labeled data, and a larger quantity of noisy web audio data in a multi-label audio tagging task with a large vocabulary setting. In addition, since the data comes from different sources, the task encourages domain adaptation approaches to deal with a potential domain mismatch. The dataset used in this challenge is called FSDKaggle2019, and it employs audio clips from the following sources: The audio data is labeled using a vocabulary of 80 labels from Google\u2019s AudioSet Ontology [1], covering diverse topics: Guitar and other Musical instruments, Percussion, Water, Digestive, Respiratory sounds, Human voice, Human locomotion, Hands, Human group actions, Insect, Domestic animals, Glass, Liquid, Motor vehicle (road), Mechanisms, Doors, and a variety of Domestic sounds. The full list of categories can be inspected in the sample_submission.csv at the the bottom of this page. The ground truth labels are provided at the clip-level, and express the presence of a sound category in the audio clip, hence can be considered weak labels or tags. Audio clips have variable lengths (roughly from 0.3 to 30s, see more details below). The audio content from FSD has been manually labeled by humans following a data labeling process using the Freesound Annotator platform. Most labels have inter-annotator agreement but not all of them. More details about the data labeling process and the Freesound Annotator can be found in [2]. The YFCC soundtracks were labeled using automated heuristics applied to the audio content and metadata of the original Flickr clips. Hence, a substantial amount of label noise can be expected. The label noise can vary widely in amount and type depending on the category, including in- and out-of-vocabulary noises. More information about some of the types of label noise that can be encountered is available in [3]. All clips are provided as uncompressed PCM 16 bit, 44.1 kHz, mono audio files. All clips used in this competition are released under Creative Commons (CC) licenses, some of them requiring attribution to their original authors and some forbidding further commercial reuse. In order to be able to comply with the CC licenses terms, a full list of audio clips with their associated licenses and a reference to the original content (in Freesound or Flickr) will be published at the end of the competition. Until then, the provided audio files can only be used for the sole purpose of participating in the competition. The train set is meant to be for system development. The idea is to limit the supervision provided (i.e., the manually-labeled data), thus promoting approaches to deal with label noise. The train set is composed of two subsets as follows: The curated subset is a small set of manually-labeled data from FSD. The duration of the audio clips ranges from 0.3 to 30s due to the diversity of the sound categories and the preferences of Freesound users when recording/uploading sounds. It can happen that a few of these audio clips present additional acoustic material beyond the provided ground truth label(s). The noisy subset is a larger set of noisy web audio data from Flickr videos taken from the YFCC dataset [5]. The duration of the audio clips ranges from 1s to 15s, with the vast majority lasting 15s. Considering the numbers above, per-class data distribution available for training is, for most of the classes, 300 clips from the noisy subset and 75 clips from the curated subset, which means 80% noisy - 20% curated at the clip level (not at the audio duration level, considering the variable-length clips). The test set is used for system evaluation and consists of manually-labeled data from FSD. Since most of the train data come from YFCC, some acoustic domain mismatch between the train and test set can be expected. All the acoustic material present in the test set is labeled, except human error, considering the vocabulary of 80 classes used in the competition. The test set is split into two subsets, for the public and private leaderboards. In this competition, the submission is to be made through Kaggle Kernels. Only the test subset corresponding to the public leaderboard is provided (without ground truth). Submissions must be made with inference models running in Kaggle Kernels. However, participants can decide to train also in the Kaggle Kernels or offline (see Kernels Requirements for details). This is a kernels-only competition with two stages. The first stage comprehends the submission period until the deadline on June 10th. After the deadline, in the second stage, Kaggle will rerun your selected kernels on an unseen test set. The second-stage test set is approximately three times the size of the first. You should plan your kernel's memory, disk, and runtime footprint accordingly. Each row of the train_curated.csv and train_noisy.csv files contains the following information: [1] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal, and Marvin Ritter. \"Audio set: An ontology and human-labeled dataset for audio events.\" In Proceedings of the International Conference on Acoustics, Speech and Signal Processing, 2017. [PDF] [2] Eduardo Fonseca, Jordi Pons, Xavier Favory, Frederic Font, Dmitry Bogdanov, Andres Ferraro, Sergio Oramas, Alastair Porter, and Xavier Serra. \"Freesound Datasets: A Platform for the Creation of Open Audio Datasets.\" In Proceedings of the International Conference on Music Information Retrieval, 2017. [PDF] [3] Eduardo Fonseca, Manoj Plakal, Daniel P. W. Ellis, Frederic Font, Xavier Favory, and Xavier Serra. \"Learning Sound Event Classifiers from Web Audio with Noisy Labels.\" In Proceedings of the International Conference on Acoustics, Speech and Signal Processing, 2019. [PDF] [4] Frederic Font, Gerard Roma, and Xavier Serra. \"Freesound technical demo.\" Proceedings of the 21st ACM international conference on Multimedia, 2013. https://freesound.org [5] Bart Thomee, David A. Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li, YFCC100M: The New Data in Multimedia Research, Commun. ACM, 59(2):64\u201373, January 2016"
    },
    {
        "name": "BirdCLEF 2021 - Birdcall Identification",
        "url": "https://www.kaggle.com/competitions/birdclef-2021",
        "overview_text": "Overview text not found",
        "description_text": "Birds of a feather flock together. Thankfully, this makes it easier to hear them! There are over 10,000 bird species around the world. Identifying the red-winged blackbirds or Bewick\u2019s wrens in an area, for example, can provide important information about the habitat. As birds are high up in the food chain, they are excellent indicators of deteriorating environmental quality and pollution. Monitoring the status and trends of biodiversity in ecosystems is no small task. With proper sound detection and classification\u2014aided by machine learning\u2014researchers can improve their ability to track the status and trends of biodiversity in important ecosystems, enabling them to better support global conservation efforts.  Recent advances in machine listening have improved acoustic data collection. However, it remains a challenge to generate analysis outputs with high precision and recall. The majority of data is unexamined due to a lack of effective tools for efficient and reliable extraction of the signals of interests (e.g., bird calls). The Cornell Lab of Ornithology is dedicated to advancing the understanding and protection of birds and the natural world. The Lab joins with people from all walks of life to make new scientific discoveries, share insights, and galvanize conservation action. For this competition, they're collaborating with Google Research, LifeCLEF, and Xeno-canto. In this competition, you\u2019ll automate the acoustic identification of birds in soundscape recordings. You'll examine an acoustic dataset to build detectors and classifiers to extract the signals of interest (bird calls). Innovative solutions will be able to do so efficiently and reliably. The ornithology community is collecting many petabytes of acoustic data every year, but the majority of data remains unexamined. If successful, you'll help researchers properly detect and classify bird sounds, significantly improving their ability to monitor the status and trends of biodiversity in important ecosystems. Researchers will better be able to infer factors about an area\u2019s quality of life based on a changing bird population, which allows them to identify how they can best support global conservation efforts. The LifeCLEF Bird Recognition Challenge (BirdCLEF) focuses on developing machine learning algorithms to identify avian vocalizations in continuous soundscape data to aid conservation efforts worldwide. Launched in 2014, it has become one of the largest bird sound recognition competitions in terms of dataset size and species diversity.",
        "dataset_text": "Your challenge in this competition is to identify which birds are calling in long recordings, given training data generated in meaningfully different contexts. This is the exact problem facing scientists trying to automate the remote monitoring of bird populations. This competition builds on the previous one by adding soundscapes from new locations, more bird species, richer metadata about the test set recordings, and soundscapes to the train set. train_short_audio -\nThe bulk of the training data consists of short recordings of individual bird calls generously uploaded by users of xenocanto.org. These files have been downsampled to 32 kHz where applicable to match the test set audio and converted to the ogg format. The training data should have nearly all relevant files; we expect there is no benefit to looking for more on xenocanto.org. train_soundscapes -\nAudio files that are quite comparable to the test set. They are all roughly ten minutes long and in the ogg format. The test set also has soundscapes from the two recording locations represented here. test_soundscapes -\nWhen you submit a notebook, the test_soundscapes directory will be populated with approximately 80 recordings to be used for scoring. These will be roughly 10 minutes long and in ogg audio format. The file names include the date the recording was taken, which can be especially useful for identifying migratory birds. This folder also contains text files with the name and approximate coordinates of the recording location plus a csv with the set of dates the test set soundscapes were recorded. test.csv -\nOnly the first three rows are available for download; the full test.csv is in the hidden test set. train_metadata.csv -\nA wide range of metadata is provided for the training data. The most directly relevant fields are: train_soundscape_labels.csv - sample_submission.csv - A properly formed sample submission file. Only the first three rows are public, the remainder will be provided to your notebook as part of the hidden test set."
    },
    {
        "name": "dunnhumby & hack/reduce Product Launch Challenge",
        "url": "https://www.kaggle.com/competitions/hack-reduce-dunnhumby-hackathon",
        "overview_text": "Overview text not found",
        "description_text": "hack/reduce and dunnhumby announce the Product launch challenge, as part of a one day hackathon. This competition asks you to predict how successful each of a number of product launches will be 26 weeks after the launch, based only on information up to the 13th week after the launch.  The training set and question set each contain by week for each launch: Competition begins: Saturday, May 11, 9am EDT (1:00pm UTC)\nCompetition ends: Saturday, May 11, 7pm EDT (11:00pm UTC) This competition awards 25% the ranking points of a standard competition, but does not count towards tiers. The top remote participant in the Kaggle competition will receive a \"Prize Winner\" achievement on their profile, in addition to the three local winners.",
        "dataset_text": "Update - the data has been removed to comply with the client's data sharing policy. We apologize for any inconvenience. This ZIP files a training set of 18 columns and 71969 rows (including header).  These represent a historical sample of 2768 previous product launches with full information for all 26 weeks of launch that can be used to create your models. The ZIP file also contains a question set of 18 columns and 28315 rows (including header).  These represent the set of 1089 product launches that we want you to predict unit sales in week 26, and only contain sales information up to week 13.  (The stores selling information is for the full 26 weeks as store distribution would be known in advance, and a key factor in predicting future sales). Both of these files are in the same format, and contain the following columns: This competition now uses an improved parser. The new submission format has 2 columns:  Product_Launch_Id: Product Launch Id for given prediction Units_that_sold_that_week: Predicted number of units sold in week 26. Please download and refer the sample format."
    },
    {
        "name": "WSDM - KKBox's Music Recommendation Challenge",
        "url": "https://www.kaggle.com/competitions/kkbox-music-recommendation-challenge",
        "overview_text": "Overview text not found",
        "description_text": " The 11th ACM International Conference on Web Search and Data Mining (WSDM 2018) is challenging you to build a better music recommendation system using a donated dataset from KKBOX. WSDM (pronounced \"wisdom\") is one of the the premier conferences on web inspired research involving search and data mining. They're committed to publishing original, high quality papers and presentations, with an emphasis on practical but principled novel models. Not many years ago, it was inconceivable that the same person would listen to the Beatles, Vivaldi, and Lady Gaga on their morning commute. But, the glory days of Radio DJs have passed, and musical gatekeepers have been replaced with personalizing algorithms and unlimited streaming services. While the public\u2019s now listening to all kinds of music, algorithms still struggle in key areas. Without enough historical data, how would an algorithm know if listeners will like a new song or a new artist? And, how would it know what songs to recommend brand new users? WSDM has challenged the Kaggle ML community to help solve these problems and build a better music recommendation system. The dataset is from KKBOX, Asia\u2019s leading music streaming service, holding the world\u2019s most comprehensive Asia-Pop music library with over 30 million tracks. They currently use a collaborative filtering based algorithm with matrix factorization and word embedding in their recommendation system but believe new techniques could lead to better results. Winners will present their findings at the conference February 6-8, 2018 in Los Angeles, CA. For more information on the conference, click here, and don't forget to check out the other KKBox/WSDM competition: KKBox Music Churn Prediction Challenge",
        "dataset_text": "In this task, you will be asked to predict the chances of a user listening to a song repetitively after the first observable listening event within a time window was triggered. If there are recurring listening event(s) triggered within a month after the user\u2019s very first observable listening event, its target is marked 1, and 0 otherwise in the training set. The same rule applies to the testing set. KKBOX provides a training data set consists of information of the first observable listening event for each unique user-song pair within a specific time duration. Metadata of each unique user and song pair is also provided. The use of public data to increase the level of accuracy of your prediction is encouraged. The train and the test data are selected from users listening history in a given time period. Note that this time period is chosen to be before the WSDM-KKBox Churn Prediction time period. The train and test sets are split based on time, and the split of public/private are based on unique user/song pairs. sample submission file in the format that we expect you to submit The songs. Note that data is in unicode. user information."
    },
    {
        "name": "WSDM - KKBox's Churn Prediction Challenge",
        "url": "https://www.kaggle.com/competitions/kkbox-churn-prediction-challenge",
        "overview_text": "Overview text not found",
        "description_text": " The 11th ACM International Conference on Web Search and Data Mining (WSDM 2018) is challenging you to build an algorithm that predicts whether a subscription user will churn using a donated dataset from KKBOX. WSDM (pronounced \"wisdom\") is one of the the premier conferences on web inspired research involving search and data mining. They're committed to publishing original, high quality papers and presentations, with an emphasis on practical but principled novel models. For a subscription business, accurately predicting churn is critical to long-term success. Even slight variations in churn can drastically affect profits. KKBOX is Asia\u2019s leading music streaming service, holding the world\u2019s most comprehensive Asia-Pop music library with over 30 million tracks. They offer a generous, unlimited version of their service to millions of people, supported by advertising and paid subscriptions. This delicate model is dependent on accurately predicting churn of their paid users. In this competition you\u2019re tasked to build an algorithm that predicts whether a user will churn after their subscription expires. Currently, the company uses survival analysis techniques to determine the residual membership life time for each subscriber. By adopting different methods, KKBOX anticipates they\u2019ll discover new insights to why users leave so they can be proactive in keeping users dancing. Winners will present their findings at the WSDM conference February 6-8, 2018 in Los Angeles, CA. For more information on the conference, click here.",
        "dataset_text": "In this challenge, you are asked to predict whether a user will churn after his/her subscription expires. Specifically, we want to forecast if a user make a new service subscription transaction within 30 days after the current membership expiration date. KKBOX offers subscription based music streaming service. When users signs up for our service, users can choose to either manual renew or auto-renew the service. Users can actively cancel their membership at any time. The churn/renewal definition can be tricky due to KKBox's subscription model. Since the majority of KKBox's subscription length is 30 days, a lot of users re-subscribe every month. The key fields to determine churn/renewal are transaction date, membership expiration date, and is_cancel. Note that the is_cancel field indicates whether a user actively cancels a subscription. Subscription cancellation does not imply the user has churned. A user may cancel service subscription due to change of service plans or other reasons. The criteria of \"churn\" is no new valid service subscription within 30 days after the current membership expires. UPDATE: As of November 6, 2017, we have refreshed the test data to predict user churn in the month of April, 2017 The training and the test data are selected from users whose membership expire within a certain month. The train data consists of users whose subscription expires within the month of February 2017, and the test data is with users whose subscription expires within the month of March 2017. This means we are looking at user churn or renewal roughly in the month of March 2017 for train set, and the user churn or renewal roughly in the month of April 2017. Train and test sets are split by transaction date, as well as the public and private leaderboard data. In this dataset, KKBox has included more users behaviors than the ones in train and test datasets, in order to enable participants to explore different user behaviors outside of the train and test sets. For example, a user could actively cancel the subscription, but renew within 30 days. . the train set, containing the user ids and whether they have churned. same format as train.csv, refreshed 11/06/2017, contains the churn data for March, 2017. the test set, containing the user ids, in the format that we expect you to submit same format as sample_submission_zero.csv, refreshed 11/06/2017, contains the test data for April, 2017. transactions of users up until 2/28/2017. same format as transactions.csv, refreshed 11/06/2017, contains the transactions data until 3/31/2017. daily user logs describing listening behaviors of a user. Data collected until 2/28/2017. same format as user_logs.csv, refreshed 11/06/2017, contains the user logs data until 3/31/2017. user information. Note that not every user in the dataset is available. Refreshed 11/13/2017, replaces members.csv data with the expiration date data removed. We include the code \"WSDMChurnLabeller.scala\" for generating labels for the user of our interest.\nThe code provided is the one we used to generate the label for the test data set.\nNote that the date values in the script is modified so it is easier to run on personal laptops.\nOn our cluster, the log history starts from 2015-01-01 to 2017-03-31.\nWith the provision of the user label generator,\nwe encourage participants to generate training labels using data not included in our sample training labels. One important information in the data extraction process is the definition of membership expiration date. Suppose we have a sequence for a user with the tuple of (transaction date, membership expiration date, and is_cancel): (2017-01-01, 2017-02-28, false) (2017-02-25, 0217-03-15, false) (2017-04-30, 3017-05-20, false) (data used for demo only, not included in competition dataset) This user is included in the dataset since the expiration date falls within our time period. Since the subscription transaction is 30 days away from 2017-03-15, the previous expiration date, we will count this user as a churned user. Let's consider a more complex example derive the last one, suppose now a user has the following transaction sequence (2017-01-01, 2017-02-28, false) (2017-02-25, 2017-04-03, false) (2017-03-15, 2017-03-16, true) (2017-04-01, 3017-06-30, false) The above entries is quite typical for a user who changes his subscription plan. Entry 3 indicates that the membership expiration date is moved from 2017-04-03 back to 2017-03-16 due to the user making an active cancellation on the 15th. On April 1st, the user made a long term (two month subscription), which is 15 days after the \"current\" expiration date. So this user is not a churn user. Now let's consider the a sequence that indicate the user does not falls in our scope of prediction (2017-01-01, 2017-02-28, false) (2017-02-25, 2017-04-03, false) (2017-03-15, 2017-03-16, true) (2017-03-18, 2017-04-02, false) Note that even the 3rd entry has member ship expiration date falls in 2017-03-16, but the fourth entry extends\nthe membership expiration date to 2017-04-02, not between 2017-03-01 and 2017-03-31, so we will not make a prediction\nfor the user."
    },
    {
        "name": "Nomad2018 Predicting Transparent Conductors",
        "url": "https://www.kaggle.com/competitions/nomad2018-predict-transparent-conductors",
        "overview_text": "Overview text not found",
        "description_text": "Innovative materials design is needed to tackle some of the most important health, environmental, energy, social, and economic challenges of this century. In particular, improving the properties of materials that are intrinsically connected to the generation and utilization of energy is crucial if we are to mitigate environmental damage due to a growing global demand. Transparent conductors are an important class of compounds that are both electrically conductive and have a low absorption in the visible range, which are typically competing properties. A combination of both of these characteristics is key for the operation of a variety of technological devices such as photovoltaic cells, light-emitting diodes for flat-panel displays, transistors, sensors, touch screens, and lasers. However, only a small number of compounds are currently known to display both transparency and conductivity suitable enough to be used as transparent conducting materials. Aluminum (Al), gallium (Ga), indium (In) sesquioxides are some of the most promising transparent conductors because of a combination of both large bandgap energies, which leads to optical transparency over the visible range, and high conductivities. These materials are also chemically stable and relatively inexpensive to produce. Alloying of these binary compounds in ternary or quaternary mixtures could enable the design of a new material at a specific composition with improved properties over what is current possible. These alloys are described by the formula\n; where x, y, and z can vary but are limited by the constraint x+y+z = 1. The total number of atoms in the unit cell,\n(where N is an integer), is typically between 5 and 100. However, the main limitation in the design of compounds is that identification and discovery of novel materials for targeted applications requires an examination of enormous compositional and configurational degrees of freedom (i.e., many combinations of x, y, and z). To avoid costly and inefficient trial-and-error of synthetic routes, computational data-driven methods can be used to guide the discovery of potentially more efficient materials to aid in the development of advanced (or totally new) technologies. In computational material science, the standard tool for computing these properties is the quantum-mechanical method known as density-functional theory (DFT). However, DFT calculations are expensive, requiring hundreds or thousands of CPU hours on supercomputers for large systems, which prohibits the modeling of a sizable number of possible compositions and configurations. As a result, potential\nmaterials remain relatively unexplored. Data-driven models offer an alternative approach to efficiently search for new possible compounds in targeted applications but at a significantly reduced computational cost. This competition aims to accomplish this goal by asking participants to develop or apply data analytics/data mining/machine-learning models for the prediction of two target properties: the formation energy (which is an indication of the stability of a new material) and the bandgap energy (which is an indication of the potential for transparency over the visible range) to facilitate the discovery of new transparent conductors and allow for advancements in the above-mentioned technologies.",
        "dataset_text": "High-quality data are provided for 3,000 materials that show promise as transparent conductors. The following information has been included: A domain expert will understand the physical meaning of the above information but those with a data mining background may simply use the data as input for their models. The task for this competition is to predict two target properties: Note: For each line of the CSV file, the corresponding spatial positions of all of the atoms in the unit cell (expressed in Cartesian coordinates) are provided as a separate file. train.csv - contains a set of materials for which the bandgap and formation energies are provided test.csv - contains the set of materials for which you must predict the bandgap and formation energies /{train|test}/{id}/geometry.xyz - files with spatial information about the material. The file name corresponds to the id in the respective csv files."
    },
    {
        "name": "See Click Predict Fix",
        "url": "https://www.kaggle.com/competitions/see-click-predict-fix",
        "overview_text": "Overview text not found",
        "description_text": "This competition is the successor to the See Click Predict Fix Hackathon. The purpose of both competitions is to quantify and predict how people will react to a specific 311 issue. What makes an issue urgent? What do citizens really care about? How much does location matter? Being able to predict the most pressing 311 topics will allow governments to focus their efforts on fixing the most important problems. The data set for the competitions contains several hundred thousand 311 issues from four cities. For those who are more interested in using the data for visualization or \"non-predictive\" data mining, we have added a $500 visualization prize. You may submit as many entries as you wish via the Visualization page. If you're plotting issues on maps, displaying the text in some meaningful way, or making any other creative use of the data, save it and post it! 311 is a mechanism by which citizens can express their desire to solve a problem the city or government by submitting a description of what needs to be done, fixed, or changed. In effect, this provides a high degree of transparency between government and its constituents. Once an issue has been established, citizens can vote and make comments on the issue so that government officials have some degree of awareness about what is the most important issue to address.  The meeting space has been provided by Microsoft.  Prize money is graciously offered by our sponsors:  On the citizen side, SeeClickFix leverages crowdsourcing to help both maintain the flow of incoming requests but show the public how effective you can be. When anyone in the community can report or comment on any issue, the entire group has a better perspective on what's happening--and how to fix it effectively. For governments, SeeClickFix acts as a completely-customizable CRM that plugs into your existing request management tools. From types of service requests to managing different watch areas, SeeClickFix helps better maintain and fulfill 311 requests in your city.  A public policy entrepreneur and open innovation expert David advises numerous governments on open government and open data and works with leading non-profits and businesses on strategy, open innovation and community management. In addition to his work, David is an affiliate with the Berkman Centre for Internet and Society at Harvard where he is looking at issues surrounding the politics of data. You can find David's writing on open innovation, public policy, public sector renewal and open source systems at his blog, or at TechPresident. In addition to his writing, David is frequently invited to speak on open government, policy making, negotiation and strategy to executives, policymakers, and students. You can read a background on how this challenge came to be here.",
        "dataset_text": "You are provided with 311 issues from four cities covering the time period since 2012. The goal of the contest is to predict the number of views, votes, and comments that a given issue has received to date. The training set contains the 311 data with the three target variables. The test set contains just the 311 data. While we have done a small amount of data cleaning, this is largely raw data from SeeClickFix. It will contain noise! Expect to find repeated issues, completed descriptions, and any number of data quality hurdles. Among the unique challenges of this data set: id - a randomly assigned id\nlatitude - the lattitude of the issue\nlongitude - the longitude of the issue\nsummary - a short text title\ndescription - a longer text explanation\nnum_votes - the number of user-generated votes\nnum_comments - the number of user-generated comments\nnum_views - the number of views\nsource - a categorical variable indicating where the issue was created\ncreated_time - the time the issue originated\ntag_type - a categorical variable (assigned automatically) of the type of issue"
    },
    {
        "name": "Mapping Dark Matter",
        "url": "https://www.kaggle.com/competitions/mdm",
        "overview_text": "Overview text not found",
        "description_text": "   The universe isn't behaving. Or at least, that's the view of many of the world's leading scientists: the universe behaves as if there is far more matter than we can observe. And that's important, because it means either that vital scientific theories are wrong, or that there are whole new types of stuff that we haven't yet discovered. Mapping Dark Matter is a image analysis competition whose aim is to encourage the development of new algorithms that can be applied to challenge of measuring the tiny distortions in galaxy images caused by dark matter. The aim is to measure the shapes of galaxies to reconstruct the gravitational lensing signal in the presence of noise and a known Point Spread Function. The signal is a very small change in the galaxies\u2019 ellipticity, an exactly circular galaxy image would be changed into an ellipse; however real galaxies are not circular. The challenge is to measure the ellipticity of 100,000 simulated galaxies.  The data consists of : Galaxy images, that are very noisy images of elliptical objects with a simple brightness profile. The galaxy images are convolved or smoothed with a kernel that would act to turn a single point into a blurry image. Part of the challenge is to attempt to remove or account for that blurring effect.       To help account for the blurring effect each galaxy image has a star image where we provide a pixelised version of the kernel that with which the galaxy image was convolved. Participants are provided with 100,000 galaxy and star pairs. A participant should provide an estimate for the ellipticity for each galaxy.         * NASA Support : Jet Propulsion Laboratory, operated by the California Institute of Technology, under contract with the National Aeronautics and Space Administration (NASA).  * RAS Support : Through funding for the PI (Kitching)",
        "dataset_text": "The data consists of training data and test data. Galaxy images (galaxy+convolution kernel): 40,000 postage stamp png files, each file contains a simulated galaxy image. Star images (convolution kernel only): 40,000 postage stamp png files, each file contains a pixelised version of the convolution kernel. Galaxy images (galaxy+convolution kernel): 60,000 postage stamp png files, each file contains a simulated galaxy image. Star images (convolution kernel only): 60,000 postage stamp png files, each file contains a pixelised version of the convolution kernel. For each galaxy postage stamp numbered mdm_galaxy_nnnn.png the convolution kernel is provided n the star image mdm_star_nnnn.png . We also provide a short PDF showing the main effects that happen to astronomical images : blurring (convolution kernel), pixelisation and noise. The challenge is to measure the galaxy ellipticity in presence of these effects.  We provide the the solution to the training data and also the results of running the \"unweight quadrupole moment method\" (UWQM) describeded here https://www.kaggle.com/c/mdm/Details/Ellipticity on the data. Note that UWQM is the simplest method of measuring ellipticities, but is not a good method because it does not account for the convolution, and it does not account for the noise in the images. "
    },
    {
        "name": "Traveling Santa Problem",
        "url": "https://www.kaggle.com/competitions/traveling-santa-problem",
        "overview_text": "Overview text not found",
        "description_text": " This competition will launch at midnight UTC on Saturday, December 15. Santa Claus was excited to learn about the Kaggle competition platform, and wanted to use it for a slightly different purpose. Rather than a predictive modeling problem, he has an optimization problem for you: a very, very important optimization problem. Santa needs help choosing the route he takes when delivering presents around the globe. Every year, Santa has to visit every boy and girl on his list.  It's a tough challenge, and Santa admits he scored a B- on his combinatorical optimization final. He's hoping you can develop algorithms that will solve his problem year after year. Santa asked that we give you one particular instance of his TSP (Traveling Santa Problem). However, Santa's dilemma isn't quite the same as the Traveling Salesman Problem with which you may be familiar. Santa likes to see new terrain every year--don't ask, it's a reindeer thing--and doesn't want his route to be predictable.  You're looking for shortest-distance paths through a set of chimneys, but instead of providing one path, Santa asks you to provide two disjoint paths. If one of your paths contains an edge from A to B, your other path must not contain an edge from A to B or from B to A (either order still counts as using that edge). Your score is the larger of the two distances.  Santa asks competition winners to publish and open source the algorithms they use (for his future use, of course). Rudolph was very adament about minimizing his workload. Trust us, you don't want to be on Rudolph's bad side. Important note about prizes: We believe that Kaggle's public leaderboard is very important for both the fun of the competition and achieving great results, and we want to provide an incentive for everyone to submit to the public leaderboard all along the way (even though you can easily determine your submission's score all by yourself). So the competition will have two sets of prizes, one based on the scores at the end of the competition, and one based on the scores at the end of a randomly chosen day (UTC) between December 23 and January 17.  The day will not be revealed (or even chosen) until after the competition ends. (The competition will end at the end of the day UTC on January 18.)   Attributions: Data generation and lots of help framing the problem (including coming up with this TSP variant): Robert Bosch of Oberlin College Math Department Santa photo: Aur\u00e9lienS\nSleigh photo: Creative Tools\nGlobe: William Cook    ",
        "dataset_text": "The cities are specified in a CSV file with 3 columns: id, x, y."
    },
    {
        "name": "CONNECTOMICS",
        "url": "https://www.kaggle.com/competitions/connectomics",
        "overview_text": "Overview text not found",
        "description_text": "Understanding the brain structure and some of its disease alterations is key to research on the treatment of epilepsy, Alzheimer's disease, and other neuropathologies, as well as understanding the general function of the brain and its learning capabilities. The brain contains nearly 100 billion neurons with an average 7000 synaptic connections.  Recovering the exact wiring of the brain (connectome) at this neural level is therefore a daunting task. Traditional neuroanatomic methods of axonal tracing cannot scale up to very large networks. Could there be alternative methods to recovering neural network structures from patterns of neural activity? [Learn more ...]   Today's cutting edge optical imaging of neural activity (using fluorescent calcium indicators) provides a tool to monitor the activity of tens of thousands of neurons simultaneously. Mathematical algorithms capable of discovering network structures are faced with the challenge of solving a new inverse problem: recover the neural network structure of a living system given the observation of a very large population of neurons. A promising way to experimentally proceed is to use neuronal cultures. Such cultures consist in a number of individual cells (dissected and dissociated from actual brain tissue) that are plated on a cover glass and maintained for several weeks in vitro. These living neuronal networks typically contain on the order of few thousand cells. One can then monitor their activity by fluorescence imaging, reconstruct their connectivity from activity data and, finally, compare the reconstructed circuitry with the real one. However, to fully understand the degree of accuracy of the reconstruction one needs first to procure superior reconstruction algorithms: this is where you can help by entering this competition! Monitoring changes in effective connectivity patterns of a network during behavior promises to advance our understanding of learning and intelligence. This challenge will stimulate research on network-structure learning from neurophysiological data, including causal discovery methods. [Learn more...]  Brain of the zebrafish in action. Today's cutting edge neurophysiology multi-electrode recording tools are capable of  recording (and even stimulating) of the order of 100 neurons. Optical imaging of neural activity using fluorescent calcium indicator molecules (calcium imaging) increases the number of neurons recorded by three orders of magnitude. Recently, researchers have been able to record in vivo the activity of the brain of a zebrafish embryo in 80% of its 100,000 neurons. This video comes from the work of Arens et al. Nature  485,  471\u2013477 (May 2012). This competition is brought to you by ChaLearn. See our credits page. ",
        "dataset_text": "What files do I need? At least the \"valid.tgz\" and \"test.tgz\" data archives, containing the data you need to make your predictions. Your network structure predictions on validation data (valid for short) will be scored on the \"public leaderboard\". The predictions on test data will remain hidden on the \"private leaderboard\" until they are revealed at the end of the challenge. The final ranking and determination of the winners will be based on the \"private leaderboard\" test data results. The other archives contain examples of networks for which the connectivity between neurons is revealed. They may be used for \"training\" or experimenting with algorithms. Data mirrors of the competition data are available. What's in the data? Validation and test data include time series of activities of neurons \"extracted\" from simulated calcium fluorescence imaging data. The neurons are arranged on a flat surface simulating a neural culture and you get the coordinates of each neuron. The model takes into account light scattering effects. [Learn more...] For \"training\" data, the network connectivity is also provided. What am I predicting? The network connectivity for the validation (valid) and test data. How do I get started? We provide a sample submission corresponding to the Correlation benchmark, sample code and tutorial material. See also our frequently asked questions.  "
    },
    {
        "name": "Harvard Business Review 'Vision Statement' Prospect",
        "url": "https://www.kaggle.com/competitions/harvard-business-review-vision-statement-prospect",
        "overview_text": "Overview text not found",
        "description_text": "View the Winning Entry in the HBR >> Data-mine the progress of almost a century's worth of the most influential management concepts and ideas.  The Harvard Business Review is asking you to turn your data-vision on the archival history of the HBR. The goal of this prospect to to generate analysis and visualizations from the metadata and abstracts of every article they have published over the last 90 years. Winning entries will be featured in the Vision Statement feature of the upcoming 90th anniversary issue. What makes a great entry?  Check out the past 'Vision Statement' features scattered throughout the contest page, and available for download.  The HBR wants you to find the story behind the data. Don't just build a latent topic model... show how the important topics have trended over the last 90 years.  Once you quantify the impact of an article, can you pick out the most seminal case-studies of the 20th century? Entries must contain not just an idea, but actual analysis and visualization. You don't have to be a professional graphic designer, but you should keep in mind how your work will make its point to a professional, but possibly non-technical, audience.  This is a contest for every analyst who has struggled to explain the value of data to his or her boss.   Well, now is your chance to show what you can do to your boss's boss's boss. ",
        "dataset_text": "There are two files available for download.  The pdf contains more examples of past Vision Statements.  The csv is the data for the contest itself (the updated version which fixes a bug in the abstracts that was found by Kagglers on the forum )   Articles should have publication dates betwen 1922 and 2012 ( so if your date parser is giving you articles from the future, remember to subtract 100 years )  "
    },
    {
        "name": "Google Landmark Recognition Challenge",
        "url": "https://www.kaggle.com/competitions/landmark-recognition-challenge",
        "overview_text": "Overview text not found",
        "description_text": "[UPDATE] 2019 challenge launched: https://kaggle.com/c/landmark-recognition-2019 Did you ever go through your vacation photos and ask yourself: What is the name of this temple I visited in China? Who created this monument I saw in France? Landmark recognition can help! This technology can predict landmark labels directly from image pixels, to help people better understand and organize their photo collections. Today, a great obstacle to landmark recognition research is the lack of large annotated datasets. In this competition, we present the largest worldwide dataset to date, to foster progress in this problem. This competition challenges Kagglers to build models that recognize the correct landmark (if any) in a dataset of challenging test images. Many Kagglers are familiar with image classification challenges like the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which aims to recognize 1K general object categories. Landmark recognition is a little different from that: it contains a much larger number of classes (there are a total of 15K classes in this challenge), and the number of training examples per class may not be very large. Landmark recognition is challenging in its own way. This challenge is organized in conjunction with the Landmark Retrieval Challenge ( https://www.kaggle.com/c/landmark-retrieval-challenge ). In particular, note that the test set for both challenges is the same, to encourage participants to compete in both. We also encourage participants to use the training data from the recognition challenge to train models which could be useful for the retrieval challenge. Note, however, that there are no landmarks in common between the training/index sets of the two challenges.",
        "dataset_text": "Note: Since this challenge is finalized, the data files (test.csv and train.csv) were transferred to the Google-Landmarks dataset webpage. In this competition, you are asked to take test images and recognize which landmarks (if any) are depicted in them. The test images are listed in test.csv, while train.csv contains a large number of images labeled with their associated landmarks. Test images may depict no landmark, one landmark, or more than one landmark. The training set images each depict exactly one landmark. Each image has a unique id (a hash) and each landmark has a unique id (an integer). Due to restrictions on distributing the actual files, the dataset contains a url for each image (this Python script may be useful to download the images). Since these images are not directly in our control, a small percentage of them will be unavailable (and the availability may change over time). To account for this, the organizers will download the images near the competition end date and adjust the solution file to ignore any missing test images. You should expect this challenge to involve a small amount of leakage on account of image metadata. The hosts have intentionally selected images that do not contain geolocation information, but there may be predictive information present in the Exif data. Since Kaggle cannot detect whether this metadata is being used, consider its use to be permitted. The training set was constructed by clustering photos with respect to their geolocation and visual similarity using an algorithm similar to the one described in [1]. Matches between training images were established using local feature matching. Note that there may be multiple clusters per landmark, which typically correspond to different views or different parts of the landmark. To avoid bias, no computer vision algorithms were used for ground truth generation. Instead, we established ground truth correspondences between test images and landmarks using human annotators.  [1] Y.-T. Zheng, M. Zhao, Y. Song, H. Adam, U. Buddemeier, A. Bissacco, F. Brucher T.-S. Chua, and H. Neven, \u201cTour the World: Building a Web-Scale Landmark Recognition Engine,\u201d Proc. CVPR\u201909 If you make use of this dataset in your research, please consider citing:"
    },
    {
        "name": "Google Landmark Retrieval Challenge",
        "url": "https://www.kaggle.com/competitions/landmark-retrieval-challenge",
        "overview_text": "Overview text not found",
        "description_text": "[UPDATE] 2019 challenge launched: https://kaggle.com/c/landmark-retrieval-2019 Image retrieval is a fundamental problem in computer vision: given a query image, can you find similar images in a large database? This is especially important for query images containing landmarks, which accounts for a large portion of what people like to photograph. In this competition, Kagglers are given query images and, for each query, are expected to retrieve all database images containing the same landmarks (if any). The new dataset is the largest worldwide dataset for image retrieval research, comprising more than a million images of 15K unique landmarks. We hope that this release will accelerate progress in this important research problem. This challenge is organized in conjunction with the Landmark Recognition Challenge (https://www.kaggle.com/c/landmark-recognition-challenge). In particular, note that the test set for both challenges is the same, to encourage participants to compete in both. We also encourage participants to use the training data from the recognition challenge to train models which could be useful for the retrieval challenge. Note, however, that there are no landmarks in common between the training/index sets of the two challenges.",
        "dataset_text": "Note: Since this challenge is finalized, the data files (test.csv and index.csv) were transferred to the Google-Landmarks dataset webpage. In this competition, you are asked to take a query image and retrieve a set of images that depict a landmark contained in the query image. The query images are listed in test.csv, while the \"index\" images from which you are retrieving are listed in index.csv. Each image has a unique id. Due to restrictions on distributing the actual files, the dataset contains URLs that point to each image (this Python script may be useful to download the images). Since these images are not directly in our control, a small percentage of them will be unavailable (and the availability may change over time). To account for this, the organizers will download the images near the competition end date and adjust the solution file to ignore any missing query images. You should expect this challenge to involve a small amount of leakage on account of image metadata. The hosts have intentionally selected images that do not contain geolocation information, but there may be predictive information present in the Exif data. Since Kaggle can not detect whether this metadata is being used, consider its use to be permitted. The index set was constructed by clustering photos with respect to their geolocation and visual similarity using an algorithm similar to the one described in [1]. Matches between index images were established using local feature matching. Note that there may be multiple clusters per landmark, which typically correspond to different views or different parts of the landmark. To avoid bias, no computer vision algorithms were used for the ground truth generation. Instead, we established ground truth correspondences between query images and landmarks using human annotators. Caveat: In the ground truth, if a landmark was rated as relevant for a query image by human annotators, we mark all images in the corresponding landmark cluster as relevant for the query. Therefore, some index images may be marked as relevant for queries even though they do not precisely visually match. Moreover, due to matching errors in the index construction, there may be a small number of irrelevant images in the landmark clusters, which will be marked as relevant for some queries. These caveats were necessary to keep the human labeling effort reasonable and should have only a very minor effect on submission scores. [1] Y.-T. Zheng, M. Zhao, Y. Song, H. Adam, U. Buddemeier, A. Bissacco, F. Brucher T.-S. Chua, and H. Neven, \u201cTour the World: Building a Web-Scale Landmark Recognition Engine,\u201d Proc. CVPR\u201909 If you make use of this dataset in your research, please consider citing:"
    },
    {
        "name": "iMaterialist Challenge (Fashion) at FGVC5",
        "url": "https://www.kaggle.com/competitions/imaterialist-challenge-fashion-2018",
        "overview_text": "Overview text not found",
        "description_text": "As shoppers move online, it would be a dream come true to have products in photos classified automatically. But, automatic product recognition is tough because for the same product, a picture can be taken in different lighting, angles, backgrounds, and levels of occlusion. Meanwhile different fine-grained categories may look very similar, for example, royal blue vs turquoise in color. Many of today\u2019s general-purpose recognition machines simply cannot perceive such subtle differences between photos, yet these differences could be important for shopping decisions. Tackling issues like this is why the Conference on Computer Vision and Pattern Recognition (CVPR) has put together a workshop specifically for data scientists focused on fine-grained visual categorization called the FGVC5 workshop. As part of this workshop, CVPR is partnering with Google, Wish, and Malong Technologies to challenge the data science community to help push the state of the art in automatic image classification. In this competition, FGVC workshop organizers with Wish and Malong Technologies challenge you to develop algorithms that will help with an important step towards automatic product detection \u2013 to accurately assign attribute labels for fashion images. Individuals/Teams with top submissions will be invited to present their work live at the FGVC5 workshop. Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.",
        "dataset_text": "All the data described below are txt files in JSON format. Overview sample_submission_randomlabel.csv: example submission file with random predictions to illustrate the submission file format test.json: images of which the participants need to generate predictions. Only image URLs are provided. train.json: training data with image urls and labels validation.json: validation data with the same format as train.json Training Data The training dataset includes images from 228 fashion attribute classes with multiple ground truth labels for each image. It includes a total of 1,014,544 images for training and 10,586 images for validation and 42,590 images for testing. All train/validation/test sets have the same format as shown below: { \"images\" : [image], \"annotations\" : [annotation], } image{ \"image_id\" : int, \"url\": [string] } annotation{ \"image_id\" : int, \"label_id\" : [int] } Note that for each image, we only provide URLs instead of the image content. Users need to download the images by themselves. Note that the image urls are hosted by Wish so they are expected to be stable. This year, we omit the names of the labels to avoid hand labeling the test images. Testing data and submissions The testing data only has images as shown below: { \"images\" : [image], } { \"image_id\" : int, \"url\" : [string], } We also provide a sample submission csv file as an example. The evaluation section has a more detailed description of the submission format."
    },
    {
        "name": "iMaterialist Challenge (Furniture) at FGVC5",
        "url": "https://www.kaggle.com/competitions/imaterialist-challenge-furniture-2018",
        "overview_text": "Overview text not found",
        "description_text": " Tackling issues like this is why the Conference on Computer Vision and Pattern Recognition (CVPR) has put together a workshop specifically for data scientists focused on fine-grained visual categorization called the FGVC5 workshop. As part of this workshop, CVPR is partnering with Google, Malong Technologies and Wish to challenge the data science community to help push the state of the art in automatic image classification. In this competition, FGVC5 workshop organizers and Malong Technologies challenge you to develop algorithms that will help with an important step towards automatic product recognition \u2013 to accurately assign category labels for furniture and home goods images. Individuals/Teams with top submissions will be invited to present their work live at the FGVC5 workshop. Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.  ",
        "dataset_text": "All the data described below are txt files in JSON format. Overview train.json: training data with image urls and labels validation.json: validation data with the same format as train.json test.json: images of which the participants need to generate predictions. Only image URLs are provided. sample_submission_randomlabel.csv: example submission file with random predictions to illustrate the submission file format Training Data The training dataset includes images from 128 furniture and home goods classes with one ground truth label for each image. It includes a total of 194,828 images for training and 6,400 images for validation and 12,800 images for testing.\nTrain and validation sets have the same format as shown below: { \"images\" : [image], \"annotations\" : [annotation], } image{ \"image_id\" : int, \"url\": [string] } annotation{ \"image_id\" : int, \"label_id\" : int } Note that for each image, we only provide URL instead of the image content. Users need to download the images by themselves. Note that the image urls may become unavailable over time. Therefore we suggest that the participants start downloading the images as early as possible. We are considering image hosting service in order to handle unavailable URLs. We'll update here if that could be finalized. This year, we omit the names of the labels to avoid hand labeling the test images. Testing data and submissions The testing data only has images as shown below: { \"images\" : [image], } image { \"image_id\" : int, \"url\" : [string], } We also provide a sample submission csv file as an example. The evaluation section has a more detailed description of the submission format."
    },
    {
        "name": "CVPR 2018 WAD Video Segmentation Challenge",
        "url": "https://www.kaggle.com/competitions/cvpr-2018-autonomous-driving",
        "overview_text": "Overview text not found",
        "description_text": "When you're driving, how important is it to be able to quickly tell the difference between a person vs. a stop sign? It's a hugely important, but typically very simple, distinction that you would make reflexively. Autonomous vehicles are not able to do this quite as effortlessly. This challenge, hosted by the 2018 CVPR workshop on autonomous driving (WAD), asks you to help give autonomously driven vehicles the same edge. Using an unprecedented dataset, you're asked to segment movable objects, such as cars and pedestrians, at instance level within image frames.  By participating in this competition, you'll be helping to further our understand of the current status of computer vision algorithms in solving environmental perception problems for autonomous driving. This challenge is a truly unique opportunity to work on a tremendously high value and high profile problem. The dataset presented here contains over 10 times more fine-labeled images than the largest public dataset of its type. This competition is hosted by the 2018 CVPR workshop on autonomous driving (WAD), with dataset and evaluation metric contributed by Baidu Inc.",
        "dataset_text": "In this competition, you will predict segmentations of different movable objects appearing in the view of a car camera. This dataset contains a large number of segmented and original driving images. There are multiple labels: In this competition, we evaluate seven different instance-level annotations, which are car, motorcycle, bicycle, pedestrian, truck, bus, and tricycle. The corresponding groups, such as car group and bicycle group, are annotated when boundaries cannot be distinguished by labelers. These groups are not evaluated currently. The training images labels are encoded in a format mixing spatial and label/instance information: The submission should be in Run Length Encoding format. Each submission line should represent one object instance, with the following columns:\nImageId,LabelId,Confidence,PixelCount,EncodedPixels\nwhere"
    },
    {
        "name": "Influencers in Social Networks",
        "url": "https://www.kaggle.com/competitions/predict-who-is-more-influential-in-a-social-network",
        "overview_text": "Overview text not found",
        "description_text": " Data Science London and the UK Windows Azure Users Group in partnership with Microsoft and Peerindex, announce the Influencers in Social Networks competition as part of The Big Data Hackathon.  This competition asks you to predict human judgements about who is more influential on social media. The dataset, provided by Peerindex, comprises a standard, pair-wise preference learning task. Each datapoint describes two individuals, A and B. For each person, 11 pre-computed, non-negative numeric features based on twitter activity (such as volume of interactions, number of followers, etc) are provided. The binary label represents a human judgement about which one of the two individuals is more influential. A label '1' means A is more influential than B. 0 means B is more influential than A. The goal of the challenge is to train a machine learning model which, for pairs of individuals, predicts the human judgement on who is more influential with high accuracy. Labels for the dataset have been collected by PeerIndex using an application similar to the one described in this post. A python script computing a sample benchmark solution is available here: https://gist.github.com/fhuszar/5372873 Competition begins: Saturday, Apr 13, 1pm BST (12 noon UTC)\nCompetition ends: Sunday, Apr 14, 1pm BST (12 noon UTC) This competition awards 25% the ranking points of a standard competition, but does not count towards tiers. The top remote participant in the Kaggle competition will recieve a \"Prize Winner\" achievement on their profile, in addition to the three local winners.",
        "dataset_text": "The dataset, provided by Peerindex, comprises a standard, pair-wise preference learning task. Each datapoint describes two individuals. Pre-computed, standardised features based on twitter activity (such as volume of interactions, number of followers, etc) is provided for each individual. The discrete label represents a human judgement about which one of the two individuals is more influential. The goal of the challenge is to train a machine learning model which, for a pair of individuals, predicts the human judgement on who is more influential with high accuracy. Labels for the dataset have been collected by PeerIndex using an application similar to the one described in this post.  UPDATE: This competition now has an improved parser for scoring solutions, and the submission format has changed. Please check the sample_predictions.csv, generated using this, with only Id and Choice Columns. Id's are 1...n, in the order of test cases in tests.csv"
    },
    {
        "name": "KDD Cup 2014 - Predicting Excitement at DonorsChoose.org",
        "url": "https://www.kaggle.com/competitions/kdd-cup-2014-predicting-excitement-at-donors-choose",
        "overview_text": "Overview text not found",
        "description_text": " DonorsChoose.org is an online charity that makes it easy to help students in need through school donations. At any time, thousands of teachers in K-12 schools propose projects requesting materials to enhance the education of their students. When a project reaches its funding goal, they ship the materials to the school. The 2014 KDD Cup asks participants to help DonorsChoose.org identify projects that are exceptionally exciting to the business, at the time of posting. While all projects on the site fulfill some kind of need, certain projects have a quality above and beyond what is typical. By identifying and recommending such projects early, they will improve funding outcomes, better the user experience, and help more students receive the materials they need to learn. Successful predictions may require a broad range of analytical skills, from natural language processing on the need statements to data mining and classical supervised learning on the descriptive factors around each project. KDD 2014 is a premier interdisciplinary conference that brings together researchers and practitioners from all aspects of data science, data mining, knowledge discovery, large-scale data analytics, and big data. This year's KDD features 4 keynotes, 151 Research Track papers, 44 Industry & Government Track papers, 24 workshops, 12 tutorials, and more. Data and logistical support has been graciously provided by DonorsChoose.org. DonorsChoose.org is an online charity and 501(c)(3) nonprofit organization that makes it easy for anyone to help students in need. Public school teachers from every corner of America post classroom project requests, and donors can give any amount to the project that most inspires them. ",
        "dataset_text": "The data is provided in a relational format and split by dates. Any project posted prior to 2014-01-01 is in the training set (along with its funding outcomes). Any project posted after is in the test set. Some projects in the test set may still be live and are ignored in the scoring. We do not disclose which projects are still live to avoid leakage regarding the funding status.  Exciting projects meet a number of requirements specified by DonorsChoose.org. Note that the term \"exciting\" is meant as a business construct and does not imply that non-exciting projects are not compelling to teachers/students/donors! To be exciting, a project must meet all of the following five criteria. The name in parentheses indicates the field containing each feature in the data set. You will find this information summarized in outcomes.csv, including the boolean value for is_exciting. Below is a brief explanation of the provided data fields. Descriptions of self-explanatory names are omitted. outcomes.csv\nis_exciting - ground truth of whether a project is exciting from business perspective\nat_least_1_teacher_referred_donor - teacher referred = donor donated because teacher shared a link or publicized their page\nfully_funded - project was successfully completed\nat_least_1_green_donation - a green donation is a donation made with credit card, PayPal, Amazon or check\ngreat_chat - project has a comment thread with greater than average unique comments\nthree_or_more_non_teacher_referred_donors - non-teacher referred is a donor that landed on the site by means other than a teacher referral link/page\none_non_teacher_referred_donor_giving_100_plus - see above\ndonation_from_thoughtful_donor - a curated list of ~15 donors that are power donors and picky choosers (we trust them selecting great projects)\ngreat_messages_proportion -  how great_chat is calculated. proportion of comments on the project page that are unique. If > avg (currently 62%) then great_chat = True\nteacher_referred_count - number of donors that were teacher referred (see above)\nnon_teacher_referred_count - number of donors that were non-teacher referred (see above) projects.csv\nprojectid - project's unique identifier\nteacher_acctid - teacher's unique identifier (teacher that created a project)\nschoolid - school's unique identifier (school where teacher works)\nschool_ncesid - public National Center for Ed Statistics id\nschool_latitude\nschool_longitude\nschool_city\nschool_state\nschool_zip\nschool_metro\nschool_district\nschool_county\nschool_charter - whether a public charter school or not (no private schools in the dataset)\nschool_magnet - whether a public magnet school or not\nschool_year_round - whether a public year round school or not\nschool_nlns - whether a public nlns school or not\nschool_kipp - whether a public kipp school or not\nschool_charter_ready_promise - whether a public ready promise school or not\nteacher_prefix - teacher's gender\nteacher_teach_for_america - Teach for America or not\nteacher_ny_teaching_fellow - New York teaching fellow or not\nprimary_focus_subject - main subject for which project materials are intended\nprimary_focus_area - main subject area for which project materials are intended\nsecondary_focus_subject - secondary subject\nsecondary_focus_area - secondary subject area\nresource_type - main type of resources requested by a project\npoverty_level - school's poverty level.\nhighest: 65%+ free of reduced lunch\nhigh: 40-64%\nmoderate: 10-39%\nlow: 0-9%\ngrade_level - grade level for which project materials are intended\nfulfillment_labor_materials - cost of fulfillment\ntotal_price_excluding_optional_support - project cost excluding optional tip that donors give to DonorsChoose.org while funding a project\ntotal_price_including_optional_support - see above\nstudents_reached - number of students impacted by a project (if funded)\neligible_double_your_impact_match - project was eligible for a 50% off offer by a corporate partner (logo appears on a project, like Starbucks or Disney)\neligible_almost_home_match - project was eligible for a $100 boost offer by a corporate partner\ndate_posted - data a project went live on the site donations.csv\ndonationid - unique donation identifier\nprojectid - unique project identifier (project that received the donation)\ndonor_acctid - unique donor identifier (donor that made a donation)\ndonor_city\ndonor_state\ndonor_zip\nis_teacher_acct - donor is also a teacher\ndonation_timestamp\ndonation_to_project - amount to project, excluding optional support (tip)\ndonation_optional_support - amount of optional support\ndonation_total - donated amount\ndollar_amount - donated amount in US dollars\ndonation_included_optional_support - whether optional support (tip) was included for DonorsChoose.org\npayment_method - what card/payment option was used\npayment_included_acct_credit - whether a portion of a donation used account credits redemption\npayment_included_campaign_gift_card - whether a portion of a donation included corporate sponsored giftcard\npayment_included_web_purchased_gift_card - whether a portion of a donation included citizen purchased giftcard (ex: friend buy a giftcard for you)\npayment_was_promo_matched - whether a donation was matched 1-1 with corporate funds\nvia_giving_page - donation given via a giving / campaign page (example: Mustaches for Kids)\nfor_honoree - donation made for an honoree\ndonation_message - donation comment/message. Used to calcualte great_chat essays.csv\nprojectid - unique project identifier\nteacher_acctid - teacher id that created a project\ntitle - title of the project\nshort_description - description of a project\nneed_statement - need statement of a project\nessay - complete project essay resources.csv\nresourceid - unique resource id\nprojectid - project id that requested resources for a classroom\nvendorid - vendor id that supplies resources to a project\nvendor_name\nproject_resource_type - type of resource\nitem_name - resource name (ex: ipad 32 GB)\nitem_number - resource item identifier\nitem_unit_price - unit price of the resource\nitem_quantity - number of a specific item requested by a teacher"
    },
    {
        "name": "MLSP 2013 Bird Classification Challenge",
        "url": "https://www.kaggle.com/competitions/mlsp-2013-birds",
        "overview_text": "Overview text not found",
        "description_text": " It is important to gain a better understanding of bird behavior and population trends. Birds respond quickly to environmental change, and may also tell us about other organisms (e.g., insects they feed on), while being easier to detect. Traditional methods for collecting data about birds involve costly human effort. A promising alternative is acoustic monitoring. There are many advantages to recording audio of birds compared to human surveys, including increased temporal and spatial resolution and extent, applicability in remote sites, reduced observer bias, and potentially lower costs. However, it is an open problem for signal processing and machine learning to reliably identify bird sounds in real-world audio data collected in an acoustic monitoring scenario. Some of the major challenges include multiple simultaneously vocalizing birds, other sources of non-bird sound (e.g. buzzing insects), and background noise like wind, rain, and motor vehicles. The goal in this challenge is to predict the set of bird species that are present given a ten-second audio clip. This is a multi-label supervised classification problem. The training data consists of audio recordings paired with the set of species that are present. The audio dataset for this challenge was collected in the H. J. Andrews (HJA) Long-Term Experimental Research Forest, in the Cascade mountain range of Oregon. Since 2009, members of the OSU Bioacoustics group have collected over 10TB of audio data in HJA using Songmeter audio recording devices. A Songmeter has two omnidirectional microphones, and records audio in WAV format to flash memory. A Songmeter can be left in the field for several weeks at a time before either its batteries run out, or its memory is full. HJA has been the site of decades of experiments and data collection in ecology, geology and meteorology. This means, for example, that given an audio recording from a particular day and location in HJA, it is possible to look up the weather, vegetative composition, elevation, and much more. Such data enables unique discoveries through cross-examination, and long-term analysis.  Previous experiments on supervised classification using multi-instance and/or multi-label formulations have used audio data collected with song meters in HJA. The dataset for this competition is similar to, but perhaps more difficult than that dataset used in these prior works; in earlier work care was taken to avoid recordings with rain and loud wind, or no birds at all, and all of the recordings came from a single day. In this competition, you will consider a new dataset which includes rain and wind, and represents a sample from two years of audio recording at 13 different locations. To participate in the conference, participants should email the following information to catherine.huang {at} intel.com no later than August 19, 2013: (1) the names of the team members (each person may belong to at most one team), (2) the name(s) of the host institutions of the researchers, (3) a 1-3 paragraph description of the approach used, (4) their submission score.  Those planning to attend the conference should additionally upload their source code to reproduce results.  Submitted models should follow the model submission best practices as closely as possible.  You do not need to submit code/models before the deadline to participate in the Kaggle competition. Collection and preparation of this dataset was partially funded by NSF grant DGE 0333257, NSF-CDI grant 0941748, NSF grant 1055113, NSF grant CCF-1254218, and the College of Engineering, Oregon State University. We would also like to thank Sarah Hadley, Jed Irvine, and others for their contributions in data collection and labeling.",
        "dataset_text": "The dataset for this challenge consists of 645 ten-second audio recordings collected in HJA over a two-year period. In addition to the raw WAV audio files, we provide data from several stages of pre-processing, e.g. features that can be used directly for classification. The dataset is described in more detail in the included documentation, mlsp13birdchallenge_documentation.pdf and README.txt. Please note: rules/changes/modifications on Kaggle.com take precedence over those in the pdf documentation. *** Essential Files *** (see /essential_data) These are the most essential files- if you want to do everything from scratch, these are the only files you need. /src_wavs This folder contains the original wav files for the dataset (both training and test sets). These are 10-second mono recordings sampled at 16kHz, 16 bits per sample. rec_id2filename.txt Each audio file has a unique recording identifier (\"rec_id\"), ranging from 0 to 644. The file rec_id2filename.txt indicates which wav file is associated with each rec_id. species_list.txt There are 19 bird species in the dataset. species_list.txt gives each a number from 0 to 18. CVfolds_2.txt The dataset is split into training and test sets. CVfolds_2.txt gives the fold for each rec_id. 0 is the training set, and 1 is the test set. rec_labels_test_hidden.txt This is your main label training data. For each rec_id, a set of species is listed. The format is: rec_id,[labels] for example: 14,0,4 indicates that rec_id=14 has the label set {0,4} For recordings in the test set, a ? is listed instead of the label set. Your task is to make predictions for these ?s. sample_submission.csv This file is an example of the format you should submit results in. Each line gives 3 numbers: i,j,p (i) - the rec_id of a recording *in the test set*. ONLY INCLUDE PREDICTIONS FOR RECORDINGS IN THE TEST SET\n(j) - the species/class #. For each rec_id, there should be 19 lines for species 0 through 18.\n(p) - your classifier's prediction about the probability that species j is present in rec_id i. THIS MUST BE IN THE RANGE [0,1]. Your submission should have exactly 6138 lines (no blank line at the end), and should include the header as the first line (\"rec_id,species,probability\"). *** Supplementary Files *** (see /supplemental_data) There are a lot of steps to go from the raw WAV data to predictions. Some participants may wish to use some supplementary data we provide which gives one implementation of a sequence of processing steps. Participants may use any of this data to improve their classifier. /spectrograms This folder contains BMP image files of spectrograms corresponding to each WAV audio file in the dataset. These spectrograms are computed by dividing the WAV signal into overlapping frames, and applying the FFT with a Hamming window. The FFT returns complex Fourier coefficients. To enhance contrast, we first normalize the spectrogram so that the maximum coefficient magnitude is 1, then take the square root of the normalized magnitude as the pixel value for an image. The spectrogram has time on the x-axis (from 0 to the duration of the sound), and frequency on the y-axis. The maximum frequency in the spectrogram is half the sampling frequency (16kHz/2 = 8kHz). /filtered_spectrograms This folder contains modified versions of the spectrograms, which have had a stationary noise filter applied. Roughly speaking, it estimates the frequency profile of noise from low-energy frames, then modifies the spectrogram to suppress noise. See \"Acoustic classification of multiple simultaneous bird species: A multi-instance multi-label approach\" for more details on the noise reduction. /segmentation_examples For a few recordings in the training set (20 of them), we provide additional annotation of the spectrogram at the pixel level (coarsely drawn). Red pixels (R=255,G=0,B=0) indicate bird sound, and blue pixels (R=0,G=0,B=255) indicate rain or loud wind. These segmentation examples were used to train the baseline method's segmentation system. /supervised_segmentation This folder contains spectrograms with the outlines of segments drawn on top of them. These segments are obtained automatically in the baseline method, using a segmentation algorithm that is trained on the contents of /segmentation_examples. You are not required to use this segmentation, but you can if you want to!!! This segmentation is used in several other data files mentioned below. For example- segment_mosaic.bmp -- this is a visualization of all of the segments in /supervised_segmentation. Looking at this can give you some idea of the variety of bird sounds present in the dataset. segment_features.txt This text file contains a 38-dimensional feature vector describing each segment in the segmentation shown in /supervised_segmentation. The file is formatted so each line provides the feature vector for one segment. The format is: rec_id,segment_id,[feature vector] The first column is the rec_id, the second is an index for the segment within the recording (starting at 0, and going up to whatever number of segments are in that recording). There might be 0 segments in a recording (that doesn't necessarily mean it has nothing in it, just that the baseline segmentation algorithm didn't find anything). So not every rec_id appears in segment_features.txt Note that segment_features can be thought of as a \"multi-instance\" representation of the data:\n- each \"bag\" is a recording\n- each \"instance\" is a segment described by a 38-d feature vector Combined with bag label sets, this give a multi-instance multi-label (MIML) representation, which has been used in prior work on similar datasets. segment_rectangles.txt You might want to compute your own different features based on rectangles around calls/syllables/segments (rather than irregular blobs), but not worry about doing segmentation from scratch. Good news: we provide some data that can help with this. Bad news: your results might depend on imperfect/bad baseline segmentation. segment_rectangles.txt contains a bounding box for each segment in the baseline segmentation method. The bounding box is specified by the min/max x/y coordinates for pixels in the spectrogram BMP images. histogram_of_segments.txt Some participants may prefer not to worry about the \"multi-instance\" structure in the data, and instead focus on a standard multi-label classification scenario, where each recording is described by a fixed-length feature vector. The baseline method uses this approach, and we provide the feature vector that it computes. The feature vector for each recording is obtained based on the 38-d segment features described above. All segments from both training and test datasets are clustered using k-means++ with k=100. This clustering forms a \"codebook.\" For each recording, we find the cluster center that is closest in L2 distance to each segment, and count the number of times each cluster is selected. The vector of counts, normalized to sum to 1, is the \"histogram of segments\" feature (used in \"Multi-Label Classifier Chains for Bird Sound,\" http://arxiv.org/abs/1304.5862). A visualization of the clustering is shown in segment_clusters.bmp."
    },
    {
        "name": "Forecast Eurovision Voting",
        "url": "https://www.kaggle.com/competitions/Eurovision2010",
        "overview_text": "Overview text not found",
        "description_text": "Description text not found",
        "dataset_text": "Data provided\n\nHistorical data for 'Finals' is provided from 1998, when mass televoting was first implemented. \nHistorical data for 'Semi-finals' is provided from 2004, when semi-finals were introduced.  Please note that from 2004 to 2007 there was one semi-final; from 2008 onwards there have been two semi-finals.\n\nThe dataset incorporates 3 sheets.  These are provided in a single Microsoft Excel file, or alternatively the identical sheets are available in a different format as 3 separate csv files.\n\nThe 3 sheets are as follows:\n1)  'Final data by year'\n2)  'Semi-final data by year'\n3)  '2010 Data'\n\nFinal data by year The data in this sheet can be used to calibrate your model. Column A - Year of competition, from 1998 - 2009\nColumn B - Competitor country\nColumn C - Region of competitor country (nb: please see 'Region Classifications' below)\nColumn D - Language of song entered by competitor country\nColumn E - Name of performing artist\nColumn F - Name of song\nColumn G - Name of song, English translation\nColumn H - Gender of artist (nb: this is designated as \"male\" or \"female\" or \"both\" if a mixed-gender group)\nColumn I - Group or solo performer\nColumn J - Place achieved in final (nb: the # of competitors and thus places has changed on a yearly basis)\nColumn K - Total points received by competing country, an aggregate of votes\nColumn L - Host country\nColumn M - Region of host country (nb: please see 'Region Classifications' below)\nColumn N - Whether the Contest was held in the competitor's own country (\"Home\") or another country (\"Away\")\nColumn O - Whether the Contest was held in the competitor's own region (\"Home\") or another region (\"Away\")\nColumn P - Approximate betting prices (nb: please see 'Approximate Betting Prices data' below)\nColumns Q-BK - Voting countries and their allocation of scores to competitor countries listed in Column A (nb: these columns contain the list of all countries which have competed in the Eurovision Song Contest Final from 1998-2009, however not every country voted/entered the Contest every year)\n\nSemi-final data by year The data in this sheet can also be used to calibrate your model. Column A - Year of competition, from 2004 - 2009, and designation as \"SF1\" (semi-final 1) or \"SF2\" (semi-final 2).  (nb: for years 2004-2007, there was only one semi-final per year, and these have been designated as \"SF1\")\nColumn B - Competitor country\nColumn C - Region of competitor country (nb: please see 'Region Classifications' below)\nColumn D - Language of song entered by competitor country\nColumn E - Name of performing artist\nColumn F - Name of song\nColumn G - Name of song, English translation\nColumn H - Place achieved in final (nb: the # of competitors and thus places has changed on a yearly basis)\nColumn I - Total points received by competing country, an aggregate of votes\nColumn J - Host country\nColumn K - Region of host country (nb: please see 'Region Classifications' below)\nColumn L - Whether the Contest was held in the competitor's own country (\"Home\") or another country (\"Away\")\nColumn M - Whether the Contest was held in the competitor's own region (\"Home\") or another region (\"Away\")\nColumn N - Approximate betting prices (nb: please see 'Approximate Betting Prices data' below)\nColumns O-BI - Voting countries and their allocation of scores to competitor countries listed in Column A (nb: these columns contain the list of all countries which have competed in the Eurovision Song Contest semi-finals from 2004-2009, however not every country voted/entered the Contest every year)\n\n2010 Data The data in this sheet can be used to generate your entry.  Column A - Year of competition, ie: 2010\nColumn B - Competitor country (nb: this contains list of all 2010 Contest contestants, of which only 25 will compete in the final.  Germany, United Kingdom, Spain, France and Norway will definitely compete in the Final)\nColumn C - Region of competitor country (nb: please see 'Region Classifications' below)\nColumn D - Language of song entered by competitor country\nColumn E - Name of performing artist\nColumn F - Name of song\nColumn G - Name of song, English translation\nColumn H - Gender of artist (nb: this is designated as \"male\" or \"female\" or \"both\" if a mixed-gender group)\nColumn I - Group or solo performer\nColumn J - Place achieved in final\nColumn K - Total points received by competing country, an aggregate of votes\nColumn L - Host country\nColumn M - Region of host country (nb: please see 'Region Classifications' below)\nColumn N - Whether the Contest is being held in the competitor's own country (\"Home\") or another country (\"Away\") (nb: this will read \"Home\" for Norway and \"Away\" for every other country)\nColumn O - Whether the Contest is being held in the competitor's own region (\"Home\") or another region (\"Away\") (nb: this will read \"Home\" for Scandinavian countries and \"Away\" for all other countries)\nColumn P - Approximate betting prices (nb: please see 'Approximate Betting Prices data' below)\n\nThis year's participants\n\n39 countries have confirmed their participation in this year's Eurovision Song Contest. \nSemi-final 1 - Moldova, Russia, Estonia, Slovakia, Finland, Latvia, Serbia, Bosnia and Herzegovina, Poland, Belgium, Malta, Albania, Greece, Portugal, Macedonia, Belarus, Iceland\nSemi-final 2 - Lithuania, Armenia, Israel, Denmark, Switzerland, Sweden, Azerbaijan, Ukraine, Netherlands, Romania, Slovenia, Ireland, Bulgaria, Cyprus, Croatia, Georgia, Turkey\nStraight to final - Spain, Norway, United Kingdom, France, Germany\n\nVoting procedure in semi-finals\n\nSemi-finals were introduced in 2004.  There was one semi-final every year from 2004-2007, and two semi-finals per year for 2008 onwards (including this year).  \n\nAs explained in the 'Background' sheet, a 'positional voting' system is used, whereby each voting country must allocate scores of 1, 2, 3, 4, 5, 6, 7, 8, 10, and 12 to competitor countries.  Only the countries competing in that semi-final vote, the exception being the 5 countries which automatically qualify for the final.  In 2008, Germany and Spain voted in semi-final 1; France, United Kingdom and Serbia voted in semi-final 2.  In 2009, Germany and United Kingdom voted in semi-final 1; France, Spain and Russia voted in semi-final 2.  This year, France, Germany and Spain will vote in semi-final 1; Norway and United Kingdom will vote in semi-final 2.\n\nVoting procedure for 2004-2007 - the top 10 from the previous year's final and this year's semi-final would qualify for this year's final.  (eg: in 2004, the final contestants were the top 10 from 2004's semi-final plus the top 10 from 2003's final.)\nVoting procedure for 2008-2009 - the top 9 from each semi-final, as determined by televote, would proceed to the final.  Additionally, the top country in each semi-final as voted by the back-up juries, but which was not in the top 9 televotes, would also proceed to the final.  In total, this meant 9 televotes + 1 jury wild-card per semi-final, ie: 20 final contestants.\nVoting procedure for 2010 - replacing the system from the previous 2 years, the top 10 from each semi-final, determined by 50% jury vote and 50% televote, will proceed to the final.\n\nVoting procedure in the final\n\nAs explained in the 'Background' sheet, a 'positional voting' system is used, whereby each voting country must allocate scores of 1, 2, 3, 4, 5, 6, 7, 8, 10, and 12 to competitor countries.  All countries involved in the Contest, whether knocked out after the semi-finals or competing in the final, must vote.  Thus, in this year's final, there will be 25 competing countries, and 39 voting countries.\n\nRegion classifications\n\nWe have classified the countries by region as follows:\n\nScandinavia - Denmark, Finland, Iceland, Norway, Sweden\nWestern Europe - Andorra, Austria, Belgium, France, Germany, Greece, Italy, Luxembourg, Malta, Monaco, Netherlands, Portugal, San Marino, Spain, Switzerland\nIndependent - Cyprus, Ireland, Israel, Turkey, United Kingdom\nFormer Socialist Bloc - Albania, Armenia, Azerbaijan, Belarus, Bulgaria, Czech Republic, Estonia, Georgia, Hungary, Latvia, Lithuania, Moldova, Poland, Romania, Russia, Slovakia, Ukraine\nFormer Yugoslavia - Bosnia and Herzegovina, Croatia, Kosovo, Macedonia, Montenegro / Serbia and Montenegro / Serbia, Slovenia\n\nApproximate Betting Prices data\n\nYou will notice that Column P 'Approximate Betting Prices' in the '2010 Data' sheet has been filled however this data is obviously constantly changing.  For the most up-to-date price data, we suggest using the following website:\nhttp://www.oddschecker.com/specials/tv/eurovision/win-market\n\nThe historical 'Approximate Betting Prices' data (Column P and Column N in the 'Final data by year' and 'Semi-final data by year' sheets respectively) is provided by Betfair, and is an average over time of betting prices for each competitor country from one particular betting website, Betfair Pty Ltd.  In contrast, the above link for oddschecker.com provides spot data which averages betting prices for each competitor country across a range of betting websites.  \n\nThe 'Approximate Betting Prices' data for semi-finals is the same as for finals.\n\nAnomalies by year including exceptions to televoting rule\n\n1998 - Hungary, Ireland, Romania and Turkey used back-up jury vote\n1999 - Lithuania, Turkey, Ireland, and Bosnia and Herzegovina used back-up jury vote\n2000 - Netherlands, Romania, Russia, Macedonia and Turkey used back-up jury vote\n2001 - Bosnia and Herzegovina, Russia and Turkey used back-up jury vote; Croatia, Malta and Greece used a 50/50 jury/televote system\n2002 - Russia, Bosnia and Herzegovina, Turkey and Romania used back-up jury vote; Cyprus, Greece, Croatia and Malta used a 50/50 jury/televote system\n2003 - Ireland, Bosnia and Herzegovina and Russia used back-up jury vote\n2004 - Turkey did not hear Slovenia's entry due to technical faults and therefore could not vote for it\n2005 - Monaco, Andorra and Moldova used back-up jury vote in the final.  (Andorra, Monaco and Albania used back-up jury vote in the semi-final)\n2006 - Monaco and Albania used back-up jury vote in the final.  (Andorra, Monaco and Albania used back-up jury vote in the semi-final)\n2007 - Albania and Andorra used back-up jury vote in the final.  (Albania and Andorra used back-up jury vote in the semi-final).  This was the first year that Serbia and Montenegro competed as separate entities\n2008 - Germany, Poland and United Kingdom all received a total of 14 points in the final however have been ranked in the provided data as 23rd, 24th and 25th respectively, as Germany received the most 12s, Poland the most 10s, and United Kingdom the fewest 2009 - Norway used back-up jury vote and Hungary used sms vote only in the final.  (Spain and Albania used back-up jury vote in the semi-final). Note: you can open CSV files with Microsoft Excel or OpenOffice for more human-friendly viewing."
    },
    {
        "name": "ICDAR 2011 - Arabic Writer Identification",
        "url": "https://www.kaggle.com/competitions/WIC2011",
        "overview_text": "Overview text not found",
        "description_text": "Description text not found",
        "dataset_text": "In this contest, more than 50 writers were asked to write three different paragraphs in Arabic language. The first two paragraphs are used for training and the third one for testing. (For some writers, the first two paragraphs have been removed from the training set to test the ability of systems to detect unknown writers.) "
    },
    {
        "name": "ICFHR 2012 - Arabic Writer Identification",
        "url": "https://www.kaggle.com/competitions/awic2012",
        "overview_text": "Overview text not found",
        "description_text": "Writer identification is a very active research field. It is of a primordial importance in forensic document examination when it helps experts in delibirating on the authenticity of a certain document. This is a follow-up contest of the last year' Arabic Writer Identification Contest. As we mentioned in the last edition, writer identification generally requires two steps. The first one is an image-processing step, where features are extracted from the images. The second step is a classification step, where the document is assigned to the \u201cclosest\u201d document in the dataset according to the \u201cdifference\u201d between their features. With regards to the last edition, this contest has several improvments: This competition is organized in conjunction with the International Conference of Frontiers in Handwriting Recognition ICFHR2012 which will be held in Bari, Italy in September 18-20.",
        "dataset_text": "Additional 1-Nearest-Neighbor benchmark in Python In this contest, more than 200 writers were asked to write three different paragraphs in Arabic language. The first two paragraphs are used for training and the third one for testing. For some writers, the first two paragraphs have been removed from the training set to test the ability of systems to detect unknown writers. Also, some writers have written the third paragraph more than once and some other participants did not writer the third paragraph at all."
    },
    {
        "name": "Psychopathy Prediction Based on Twitter Usage",
        "url": "https://www.kaggle.com/competitions/twitter-psychopathy-prediction",
        "overview_text": "Overview text not found",
        "description_text": "The  aim of the competition is to determine to what degree it's possible to predict people with a sufficiently high degree of Psychopathy based on Twitter usage and Linguistic Inquiry. The organizers provide all interested participants an anonymised dataset of users self assessed psychopathy scores together with 337 variables derived from functions of Twitter information, useage and lingusitc analysis. Psychopathy scores are based on a checklist developed by Professor Del Paulhus at the University of British Columbia. The model should aim to identify people scoring high in Psychopathy, for the purpose of this competition, defined as 2 SD's above a mean of 1.98. This accounts for roughly 3% of the entire sample and therefore the challenge with this dataset is developing a model to work with a highly imbalanced dataset.\n\nThe best performing model(s) will be formally cited in a future paper/papers. The authors of the winning model may also be invited to attend future conferences to discuss their model.   The intention of this research is to seperate fact from fiction and examine just what can be predicted by social media use and how this information might be used, both for good and bad. As an organization, the Online Privacy Foundation works to raise awareness of online privacy issues and empower people to make informed choices about what they do online. We hope you'll support our mission and take part in this competition.   ",
        "dataset_text": "Three files are provided 1. DescriptiveStats.pdf - Descriptive Statistics of each personality dimension 2. Pyschopath_Trainingset_v1.csv - Training set providing personality self assessment results(cols 2 - 8)  together with 337 variables derived from a users Twitter activity (as follows) 3. Pyschopath_Testset_v1.csv - Accompanying test set"
    },
    {
        "name": "Data Mining Hackathon on BIG DATA (7GB) Best Buy mobile web site",
        "url": "https://www.kaggle.com/competitions/acm-sf-chapter-hackathon-big",
        "overview_text": "Overview text not found",
        "description_text": "3 months of real-world mobile behavior, 1.8 million clicks, 1.2 million users. You can enter one or both tracks of the competition: There will also be a Visualization Contest that can be entered from either track. http://www.sfbayacm.org/DM-Hackathon-2012-10  (links to presentations were added 8/19/2012) Data Provided by:   Cloud Compute Sponsors:                         ",
        "dataset_text": "The main data for this competition is in the train.csv and test.csv files. These files contain information on what items users clicked on after making a search. Each line of train.csv describes a user's click on a single item. It contains the following fields: test.csv contains all of the same fields as train.csv except for sku. It is your job to estimate which sku's were clicked on in these test queries. Due to the internal structure of BestBuy's databases, there is no guarantee that the user clicks resulted from a search with the given query. What we do know is that the user made a query at query_time, and then, at click_time, they clicked on the sku, but we don't know that the click came from the search results. The click_time is never more than five minutes after the query_time. In addition, there is information about products, product categories, and product reviews in product_data.tar.gz. We have also provided a sample benchmark submission and the code that produces it. popular_skus.py is a simple python script that finds the most popular skus in each product category, and then estimates that a user clicked on one of the five most popular skus in their product category. This script produces the benchmark in popular_skus.csv. The syntax of a submission should be the same as that in popular_skus.csv: A file with the header \"sku\", and each of the following lines containing the space-delimited estimates of the clicked sku that resulted after the queries in test.csv, and in the same order. Product Data Dictionary\nhttps://bbyopen.com/documentation/products-api/product-attributes#TableProdRefInfo For more BestBuy Data and APIs, check out https://bbyopen.com/ and more BigDataR tools https://github.com/koooee/BigDataR_Examples/tree/master/ACM_comp"
    },
    {
        "name": "Follow the Money: Investigative Reporting Prospect",
        "url": "https://www.kaggle.com/competitions/cir-prospect",
        "overview_text": "Overview text not found",
        "description_text": "The most expensive political campaign season in US history is well underway, with experts predicting that spending from both the Republican and Democratic camps will exceed $5.8 billion before races for president and Congress are decided in November. Needless to say, campaign finance is a big business. And where big money leads the way, big data is never too far behind. That's why the Center for Investigative Reporting, the nation's largest non-profit investigative reporting organization, is teaming up with Investigative Reporters and Editors, Inc. -- the world's leading investigative and data journalism trade group -- to offer this Prospect challenge in the hopes of answering one simple question: What can some of the world's most brilliant data scientists teach us about finding hidden patterns, interesting connections and ultimately compelling stories in a treasure trove of data about federal campaign contributions? Data journalists have been examining federal campaign finance records for decades, finding patterns and trends that have forced resignations and reforms. But despite many noteworthy successes, journalists' imaginations are limited by our skills. We're not mathematicians and machine learning experts! We might learn from a source, for instance, that a particular congressman has begun receiving campaign contributions from a new and unusual donor. But a carefully tuned anomaly detection system might reveal those patterns before our sources would ever notice them. What kinds of ideas are we looking for with this Prospect challenge? We want to see how sophisticated clustering algorithms can help spot donors that coordinate their operations, giving to the same candidates at the same times. We want to see how classification or anomaly detection systems can find donations that are particularly interesting or unusual. We want to know how you would mix up campaign contribution data with other sources -- lobbying records, congressional votes, federal contracts -- to find patterns that journalists are missing. Novel approaches to analysis, ideas for useful tools, and data visualizations will all be considered. What we're looking for is new ideas and approaches. To give you a sense of what journalists have done so far, we've included several examples of some of the most interesting campaign finance reporting around, along with some tipsheets that explain how journalists approach campaign finance data and what to look for.",
        "dataset_text": "FEC Itemized campaign finance records for the 2012 election cycle Fetched on Monday, September 3rd, 2012 at approximately 02:45:25 AM The Federal Elections Commission itemized contributions and transactions database has been an invaluable tool for reporters wanting to \"follow the money\" in federal elections campaigns. The data we've included here covers every campaign contribution given to federal candidates so far in the 2012 election cycle (January 2011 through early September 2012). It includes data about contributions given to every federal candidate nationwide -- for president, House and Senate -- by political action committees, individuals and Super PACs. It also contains information about contributors to those committees, and assorted details about each contribution: amount, date, location and occupation of the donor, etc. Use of outside data is allowed in this competition. An important disclaimer: This dataset does not include the electronic disclosure data, which is available on the FEC's Web site, and comes directly from campaigns. Disclosure data is much more complexed than the itemized set, but contains more information.   Tables comm2012.csv: List of all FEC licensed campaign committees for the election cycle. This table can be used as a reference when using the tables describing money changing hands. From the FEC Web site: \"The committee master file contains one record for each committee registered with the Federal Election Commission. This includes federal political action committees and party committees, campaign committees for presidential, house and senate candidates, as well as groups or organizations who are spending money for or against candidates for federal office. The file contains basic information about the committees. The ID number the Commission assigned to the committee is first, along with the name of the committee, the sponsor, where appropriate, the treasurer's name and the committee's address. The file also includes information about what type of committee is being described, along with the candidate's ID number if it is a campaign committee.\" Contains 13,065 records. Download record layout. cand2012.csv: Master list of candidates for the election cycle, providing information on the candidate, his or her campaign office, political party and office sought. In addition, the candidate's principle committee ID is listed with this record. From the FEC Web site: \"The candidate master file contains one record for each candidate who has either registered with the Federal Election Commission or appeared on a ballot list prepared by a state elections office. The file contains basic information about the candidate, including name, party, whether the candidate is an incumbent, challenger, or involved in an open seat, address, state and district in which the candidate is running and the year of the election for which the candidate is registered. (Note that incumbent/challenger status is dynamic in the current election cycle and there may be delays in identifying districts that will involve open seats. The file also includes the ID number assigned to the candidate by the FEC which is used in tracking campaign finance information about the campaign, as well as the ID number of the candidate's principal campaign committee.\" Contains 5,343 records. Download record layout. indiv2012.csv: Each contribution from an individual to a campaign committee, if the donation is more than $200. This is almost always the largest table in the dataset. Contains 1,988,139 records. Download record layout. pas2012.csv: The itemized committee contributions table contains each contribution from a committee to a candidate's campaign committee. From the FEC site: \"You will need to use the committee master and candidate master files in conjunction with this file to set up a relational database to analyze these data\". Contains 208,151 records. Download record layout. oth2012.csv: Miscellaneous transactions by committees. According to the FEC, it contains all the information in the pas table, plus transactions with committees that are not necessarily principle campaign committees for a candidate. Example include money transfers from state committees and PAC contributions to party committees. Contains 457,782 records. Download record layout. ccl2012.csv: All links between candidates and committees that are registered with the FEC. Contains 5,948 records. For each table, CIR and IRE have included a record layout file with the table's partial name on it (commlay.xls is the record layout for the committee table, for example). "
    },
    {
        "name": "AMS 2013-2014 Solar Energy Prediction Contest",
        "url": "https://www.kaggle.com/competitions/ams-2014-solar-energy-prediction-contest",
        "overview_text": "Overview text not found",
        "description_text": "Welcome to the American Meteorological Society 2013-2014 Solar Energy Prediction Contest! This contest is organized by the American Meteorological Society Committees on Artificial Intelligence Applications to Environmental Science, Probability and Statistics, and Earth and Energy. Prizes are sponsored by EarthRisk Technologies, Inc. Renewable energy sources, such as solar and wind, offer many environmental advantages over fossil fuels for electricity generation, but the energy produced by them fluctuates with changing weather conditions. Electric utility companies need accurate forecasts of energy production in order to have the right balance of renewable and fossil fuels available. Errors in the forecast could lead to large expenses for the utility from excess fuel consumption or emergency purchases of electricity from neighboring utilities. Power forecasts typically are derived from numerical weather prediction models, but statistical and machine learning techniques are increasingly being used in conjunction with the numerical models to produce more accurate forecasts.  The goal of this contest is to discover which statistical and machine learning techniques provide the best short term predictions of solar energy production. Contestants will predict the total daily incoming solar energy at 98 Oklahoma Mesonet sites, which will serve as \"solar farms\" for the contest. Input numerical weather prediction data for the contest comes from the NOAA/ESRL Global Ensemble Forecast System (GEFS) Reforecast Version 2. Data include all 11 ensemble members and the forecast timesteps 12, 15, 18, 21, and 24. Locations of the Mesonet sites relative to the GEFS data are shown in the above figure. Training data will come from 1994-2007. Public testing data will be from 2008-2009. Private testing data for a more recent period will be used for the final evaluation. Daily solar energy data were provided by the Oklahoma Mesonet with the assistance of Dr. Jeffrey Basara. The GEFS Reforecast Version 2 data were developed and provided by Dr. Thomas Hamill. The contest is being administered by David John Gagne and Dr. Amy McGovern of the University of Oklahoma. EarthRisk Technologies creates a market advantage for its clients by uniquely quantifying weather data.  Our company is a research pioneer that analyzes extreme weather risk at lead times longer than one week.  Our techniques enhance competitive business decisions. TempRisk, the company\u2019s first product suite, is a web-based platform that utilizes historical data, machine learning and predictive analytics to project risk for extreme winter cold and summer heat up to 40 days before it occurs. These patent-pending algorithms were developed in conjunction with Scripps Institution of Oceanography at the University of California San Diego.  Energy producers and commodity investment firms currently employ TempRisk in their daily operations. Our customers require a uniquely objective quantitative methods for extreme event prediction.  Our products are continuously developing thanks to ongoing support from customer-partners including large energy companies, investment firms, and reinsurance advisors.\n\nEarthRisk's leadership team is excited to be deeply engaged with the American Meteorological Society.  In addition to our engagement with the Committee on Artificial Intelligence Applications to Environmental Science, we're also active on the AMS Energy Committee, the Board on Private Sector Meteorology, the Financial Weather/Climate Risk Management Committee, and the Weather Enterprise Economic Evaluation Team.  We have a true passion for advancing meteorological methods through the intelligent application of technology and are proud to be part of the Solar Energy Prediction Contest!",
        "dataset_text": "The contest training data are separated into 3 files. gefs_train.tar.gz and gefs_train.zip contain all of the GEFS training data. The data are in netCDF4 files with each file holding the grids for each ensemble member at every time step for a particular variable. Each netCDF file contains the latitude-longitude grid and timestep values as well as metadata listing the full names of each variable and the associated units. More infromation about the netCDF format and links to open libraries for reading the files can be found here. NetCDF libraries are known to be available for C, Java, Python, R, and MATLAB. Each netCDF4 file contains the total data for one of the model variables and are stored in a multidimensional array. The first dimension is the date of the model run and will correspond directly with a row in either the train.csv or sampleSubmission.csv files. The second dimension is the ensemble member that the forecast comes from. The GEFS has 11 ensemble members with perturbed initial conditions. The third dimension is the forecast hour, which runs from 12 to 24 hours in 3 hour increments. All model runs start at 00 UTC, so they will always correspond to the same universal time although local solar time will vary over each year.  The fourth and fifth dimensions are the latitude and longitude uniform spatial grid. The longitudes in the file are in positive degrees from the Prime Meridian, so subtracting 360 from them will translate them to a similar range of values as in station_info.csv. A visualization of the grid can be seen on the main page. train.csv contains the total daily incoming solar energy in (J m-2) at 98 Oklahoma Mesonet sites that have been in continuous operation since January 1, 1994. The solar energy was directly measured by a pyranometer at each Mesonet site every 5 minutes and summed from sunrise to 23:55 UTC of the date listed in each column. station_info.csv contains the latitude, longitudes, and elevation (meters) of each Mesonet station. gefs_elevations.nc is a netCDF4 file that contains the model elevations of the GEFS grid points. Since the model terrain is smoothed compared to the real-world, the true elevation at a particular lat-lon point will likely not match the elevation in the model. The file contains two elevation variables. The elevation_control variable contains the elevations for the GEFS control run, which is the first ensemble member. The elevation_perturbation variable contains the elevations for the GEFS perturbations, which are the other ensemble members. There are up to 300 m differences in the elevations, so using one instead of the other could have an impact on your model. NOTE: If you do include elevation-based interpolation into any submission since September 10 and in your final submission, it must use the elevations from gefs_elevations.nc and not from any outside source."
    },
    {
        "name": "ICDAR2013 - Gender Prediction from Handwriting",
        "url": "https://www.kaggle.com/competitions/icdar2013-gender-prediction-from-handwriting",
        "overview_text": "Overview text not found",
        "description_text": "The prediction of gender from handwriting is a very interesting research field. It has many applications including the forensic application where it can help investigators focusing more on a certain category of suspects.  There are a few studies regarding the automatic detection of the gender of a handwritten document [1-3]. The aim of this competition is to attract the interest of the document analysis community to this research area and to measure the performance of recent advances in this field. The dataset used in this study has been described in this paper [4]. A total of 475 writers produced 4 handwritten documents: The training set consists of the first 282 writers for which the genders are provided. Participants are asked to predict the gender of the remaining 193 writers. For participants who are not familiar with digital image-processing, a set of features extracted from all the images will be provided. Those features are similiar to that of the previous 2011 and 2012 Arabic Writer Identification Contests. Those features are described in [5]. This competition is organized in the scope of the Twelfth International Conference on Document Analysis and Recognition ICDAR2013 that will be held in Washington, DC. Writer demographic identification using bagging and boosting [2] Liwicki, M., Schlapbach, A., Loretan, P., Bunke, H., Automatic detection of gender and handedness from online handwriting. In: Proc. 13th Conference of the International Graphonomics Society. pp. 179\u2013183 (2007). [3] Liwicki, M., Schlapbach, A., Bunke, H., Automatic gender detection using on-line and off-line information. Pattern Analysis and Applications 14, 87\u201392 (2011). [4] Al-Ma\u2019adeed, S., Ayouby, W., Hassaine, A., Aljaam, J., QUWI: An Arabic and English Handwriting Dataset for Offline Writer Identification. In: Frontiers in Handwriting Recognition, International Conference on. Bari, Italy (September 2012). [5] Hassa\u00efne, A., Al-Maadeed, S. and Bouridane, A., A Set of Geometrical Features for Writer Identification. Neural Information Processing. Springer Berlin/Heidelberg, 2012.",
        "dataset_text": "images_gender.zip contains all the images in 600dpi. We had some requests that images_gender.zip is damaged. While we look into this, please try this alternative download link which we have checked. You could also download this 300dpi version of images (which is generally enough for such tasks). As per some requests, we have splitted the 300dpi images (files from 1_50.zip to 451_475.zip). images_subset.zip contains a subset of 5 writers allowing you to have an idea about the dataset before downloading it. train_answers.csv contains two columns the first one being the ID of each writer and the second one indicating whether or not this writer is male. train.csv and test.csv contain the following columns: Submissions must have two columns the first one being the writer ID and the second being a probability value indicating how probable it is that this writer is male. R codes for several benchmarks are provided including: Those benchmarks are taken from this page."
    },
    {
        "name": "ICDAR2013 - Handwriting Stroke Recovery from Offline Data",
        "url": "https://www.kaggle.com/competitions/icdar2013-stroke-recovery-from-offline-data",
        "overview_text": "Overview text not found",
        "description_text": "There are two ways of acquiring signatures (or handwritings). The first one being the offline acquisition in which images of the signatures are acquired using an image scanner. The second one being the online acquisition in which x and y coordinates as well as the pressure are acquired with respect to time. Further details about online acquisition can be found here.  The detection of the online trajectory (or stroke recovery) of offline handwritings has many applications including the forensic application where it can help investigators converting an offline signature into its online equivalent in order to perform the verification at the online mode. It can also be used in a similar way in handwriting recognition as online handwriting recognition reaches higher recognition rates than offline recognition. There are several studies regarding the detection of trajectories of handwritings. A survey of such methods is given in [1]. The aim of this competition is to attract the interest of document image analysis researchers as well as data scientists to this research area and to measure the performance of recent advances in this field. The dataset used in this study consists of 1081 signatures of 200 writers [2]. The signatures have been acquired using a Wacom Intuos4 Large digitizing tablet and a Wacom Inking pen. A blank paper has been placed on this tablet in order to acquire in a subsequent stage the offline signature using a scanner. Offline signatures consists of jpg images scanned using an appropriate HP scanner. Online signatures are provided in a single sequential csv file containing x and y coordinates for each time interval. Pressure is not considered in this competition. In order to ease the comparison, each signature is normalized such that its x and y values will be in (0,1). The online data is provided for the first 605 signatures. Participants are to predict the online signatures of the other 476 signatures. This competition is organized in the scope of the Twelfth International Conference on Document Analysis and Recognition ICDAR2013 that will be held in Washington, DC. [1] Nguyen, Vu, and Michael Blumenstein. Techniques for static handwriting trajectory recovery: a survey. Proceedings of the 9th IAPR International Workshop on Document Analysis Systems. ACM, 2010. [2] S Al-Maadeed, W Ayouby, A Hassaine, A Al-Mejali, A Al-Yazeedi. Arabic Signature Verification Datasets. In: The International Arab Conference on Information Technology 2012.",
        "dataset_text": "The data consists of 1081 signatures of 200 writers. The offline data is provided for all the signatures (images_gender.zip). The online data is provided for the first 605 signatures in train.csv. This file has 7 olumns: As per some requests, we added the unnormalized training data (unnormalized_data.csv). The same data is provided for the remaining signatures in test.csv where x and y columns are to be determined. stroke_matlab.zip contains the code of the provided benchmarks. Important: the predicted x and y coordinates are to be normalized into (0,1) for each signature. Little reminder: as the submission sizes for this competition are a bit large, it is adviced to compress submissions before sending them."
    },
    {
        "name": "BCI Challenge @ NER 2015",
        "url": "https://www.kaggle.com/competitions/inria-bci-challenge",
        "overview_text": "Overview text not found",
        "description_text": "As humans think, we produce brain waves. These brain waves can be mapped to actual intentions. In this competition, you are given the brain wave data of people with the goal of spelling a word by only paying attention to visual stimuli. The goal of the competition is to detect errors during the spelling task, given the subject's brain waves.  The \u201cP300-Speller\u201d is a well-known brain-computer interface (BCI) paradigm which uses Electroencephalography (EEG) and the so-called P300 response evoked by rare and attended stimuli in order to select items displayed on a computer screen. In this experiment, each subject was presented with letters and numbers (36 possible items displayed on a matrix) to spell words. Each item of a word is selected one at a time, by flashing screen items in group and in random order. The selected item is the one for which the online algorithm could most likely recognize the typical target response. The goal of this challenge is to determine when the selected item is not the correct one by analyzing the brain signals after the subject received feedback. For each participant, a prototypical target response was learned from a short calibration session prior to the test sessions. In test sessions, the spelling performance is highly dependent upon the subject\u2019s attentional effort towards the target item and his/her simultaneous effort to ignore the flashes of the irrelevant items. Since subjects' attention might fluctuate, performance does too (e.g. over time, with fatigue). Two copy-spelling conditions were used, corresponding to short and long trials, respectively: At each trial, after the last flash, the subject was instructed to keep looking at the screen and wait for the feedback. The feedback consisted in the selected item, displayed in the middle of the screen in large font. Even if the feedback was incorrect, the subject was asked to then look at the next target.  Download sample video Twenty-six healthy subjects took part in this study (13 male, mean age = 28.8\u00b15.4 (SD), range 20-37). All subjects reported normal or corrected-to-normal vision and had no previous experience with the P300-Speller paradigm or any other BCI application. Subject\u2019s brain activity was recorded with 56 passive Ag/AgCl EEG sensors (VSM-CTF compatible system) whose placement followed the extended 10-20 system. Their signals were sampled at 600 Hz and were all referenced to the nose. The ground electrode was placed on the shoulder and impedances were kept below 10 k\u03a9. The subjects had to go through five copy spelling sessions. Each session consisted of twelve 5-letter words, except the fifth which consisted of twenty 5-letter words. In this paradigm and BCI in general, at least in situations where a discrete feedback can be presented to the user, the EEG evoked response to the feedback can be recorded and processed online in order to evaluate whether the item selection was correct or not. This decision, if reliable, could then be used to improve the BCI performance by implementing some error correction strategy. One possible strategy for online error detection and correction has been proposed in Perrin et al. 2012. Most of the data for this competition come from this study and this paper should be cited whenever the competition data will be used and results reported. In this competition, participants are asked to submit an Error Potential detection algorithm, capable of detecting the erroneous feedbacks online and to generalize across subjects (transfer learning). Perrin, M., Maby, E., Daligault, S., Bertrand, O., & Mattout, J. Objective and subjective evaluation of online error correction during P300-based spelling. Advances in Human-Computer Interaction, 2012, 4. (link)",
        "dataset_text": "Data are downsampled at 200 Hz. Subject\u2019s brain activity was recorded with 56 passive EEG sensors whose placement followed the extended 10-20 system. Eye movements are detected by EOG derivation. One ChannelsLocation.csv file whose informations are the same for all recordings. This file contains the following fields: One Data_S*_Session*.csv file per subject and per session.\nEach Data_S*_Session*.csv file contains the following fields: One TrainLabels.csv file contains the following fields:"
    },
    {
        "name": "Competition name not found",
        "url": "https://www.kaggle.com/competitions/the-seeclickfix-311-challenge",
        "overview_text": "Overview text not found",
        "description_text": "Description text not found",
        "dataset_text": "Decryption key for data.7z - 2p6yvksurc4wtw7srmf7 You are provided with 311 issues from four cities (Oakland, Richmond, New Haven, Chicago) covering the time period since 2012. The goal of the contest is to predict the number of views, votes, and comments that a given issue has received to date. The training set contains the 311 data with the three target variables. The test set contains just the 311 data. While we have done a small amount of data cleaning, this is largely raw data from SeeClickFix. It will contain noise! Expect to find repeated issues, completed descriptions, and any number of data quality hurdles. Among the unique challenges of this data set: id - a randomly assigned id\nlatitude - the lattitude of the issue\nlongitude - the longitude of the issue\nsummary - a short text title\ndescription - a longer text explanation\nnum_votes - the number of user-generated votes\nnum_comments - the number of user-generated comments\nnum_views - the number of views\nsource - a categorical variable indicating where the issue was created\ncreated_time - the time the issue originated\ntag_type - a categorical variable (assigned automatically) of the type of issue"
    },
    {
        "name": "Boston Data Festival Hackathon",
        "url": "https://www.kaggle.com/competitions/boston-data-festival-hackathon",
        "overview_text": "Overview text not found",
        "description_text": "Boston Data Festival is hosting a Hackathon on Sunday 11/10/13 from 10 am to 6 pm. The event will take place at Hack/Reduce (275 Third Street, Cambridge, MA). The goal of the Hackathon is to predict the directional accuracy of a stock prices. The following cash prices will be awarded! During the Hackathon every participant can use a free Matlab license. We thank MathWorks for providing free licences for competitors during the competition. ",
        "dataset_text": "Data consists of two files: training.csv - time series for 94 stocks (94 rows). First number in each row is the stock ID. Then data for 500 days. Data for each day contain - day opening price, day maximum price, day minimum price, day closing price, trading volume for the day. Price data normalised to the first day opening price. test.csv - data to create prediction. Data provided for 25 time segments. Each segment contains data for the same 94 stocks. Each segment has opening, max, min, closing, volume data for 9 days and opening for day #10. Each line of the file starts with segment number following by stock ID and then price and volume data organized by day the same way as training set.  Price data normalised to the first day opening price. Each line in train.csv and test.csv contains consecutive trading days. Days when market was closed were excluded. Thus day N may be Friday and day N+1 may be Monday or even Tuesday if Monday was a holiday.      Value to predict - probability of stock moving up from  opening of day 10 to closing of day 10. Prediction should be in 0-1 range, where 1 - \"stock surely will go up\", 0- \"stock surely will go down\". Test set is randomly sampled without overlapping from year following training data time period."
    },
    {
        "name": "The Random Number Grand Challenge",
        "url": "https://www.kaggle.com/competitions/random-number-grand-challenge",
        "overview_text": "Overview text not found",
        "description_text": "Quality pseudorandom number generation forms the bedrock on which all of computing is built. From cryptography to financial markets to particle physics, it is our trust in random numbers that props up the modern economy and allows technology to march forth, unabated.  Machine learning is incredibly powerful at recognizing statistical patterns. For this competition, we challenge you to apply your machine learning skills to predict a set of random numbers. To date, there are no known methods for predicting this set of numbers. Some experts have said it is too random, that it can't be done within current limitations of computing power. We believe the creativity of the Kaggle community will triumph over the cynical doubts of the naysayers. If there is one thing the working data scientist can do, it is extract insights from a sea of randomness. Before asking questions in the forums, we recommend reading the authoritative source in this field: A Million Random Digits with 100,000 Normal Deviates, RAND et. al. We also provide this helpful widget to cross validate your submissions: The numbers generated by this widget come from RANDOM.ORG's true random number generator.",
        "dataset_text": "The data is comprised of random numbers, which may or may not be integers. None of the numbers has an imaginary component."
    },
    {
        "name": "IJCNN Social Network Challenge",
        "url": "https://www.kaggle.com/competitions/socialNetwork",
        "overview_text": "Overview text not found",
        "description_text": "Description text not found",
        "dataset_text": "The data has been downloaded using the API of a social network. There are 7.2m contacts/edges of 38k users/nodes. These have been drawn randomly ensuring a certain level of closedness.\n\nYou are given 7,237,983 contacts/edges from a social network (social_train.zip). The first column is the outbound node and the second column is the inbound node. The ids have been encoded so that the users are anonymous. Ids reach from 1 to 1,133,547.\n\nThere are 37,689 outbound nodes and 1,133,518 inbound nodes. Most outbound nodes are also inbound nodes so that the total number of unique nodes is 1,133,547.\n\nThe way the contacts were sampled makes sure that the universe is roughly closed. Note that not every relationship is mutual.\n\nThe test dataset contains 8,960 edges from 8,960 unique outbound nodes (social_test.csv). Of those 4,480 are true and 4,480 are false edges. You are tasked to predict which are true (1) and which are false (0). You need to supply back a file with outbound node id,inbound node id,[0,1] in each row. This means you can assign a probability of being true to an edge. You are being scored on the AUC. A random model will have an AUC of 0.5, so you need to try to do better than that (ie have a higher AUC). Your entry should conform to the format in sample_submission.csv.\n\nYou are encouraged to explore techniques which explain the social network/graph. The best entrant should try to explain his approach/method to other users.\n\nDon\u2019t despair if your first couple of solutions score low, this is an explorative process."
    },
    {
        "name": "Stay Alert! The Ford Challenge",
        "url": "https://www.kaggle.com/competitions/stayalert",
        "overview_text": "Overview text not found",
        "description_text": "Description text not found",
        "dataset_text": ""
    },
    {
        "name": "Leaping Leaderboard Leapfrogs",
        "url": "https://www.kaggle.com/competitions/leapfrogging-leaderboards",
        "overview_text": "Overview text not found",
        "description_text": "The leaderboard is a central fixture of the Kaggle experience. It provides context to the incredible work accomplished by the Kaggle data science community. To a competitor, the leaderboard is a dynamic, living, action-filled battle. Tactics come to life. Individuals leapfrog over each other.  Teams merge and blend submissions.  Some submit early and often, attempting to build up insurmountable leads. Others bide time, waiting to pounce minutes before the buzzer with their finest of forests.  We see the joys of regularization and the agony of overfitting.  It's raw. It's beautiful. It's thousands of hours of collective human toil. It's boring. To an observer, the leaderboard is a spreadsheet.  They see funny team names, numbers with too many decimals, strange column titles, and none of the history behind the battle. We run a veritable nerd olympics, but instead of smashing the 100m world record, we're elbowing for a few decimal places of some esoteric quantity called a capped binomial deviance. It's faceless. It's cold. It fails to tell the story of the battle. And you know what that means? This means war. We're calling on you to bring the leaderboard to life.  Break out the D3. Sacrifice an old PC to the javascript gods. Abandon all text, ye who enter here.  We're bootstrapping our own community to do what they do best, and that is doing things better. What kinds of submissions do we hope result from this competition? Maybe you know an API or two and can create a motion chart?\nMaybe you know the hot, new HTML5 canvas tricks?\nMaybe you know of an R package that styles plots like The Economist or XKCD?\nMaybe you know Edward Tufte and can call in a favor? Be creative. Scrape profile photos. Examine team formation. Examine relative scores. Watch for edge cases, cluttered text, and all the gotchas that crop up when you juggle a leaderboard of 10 vs. 1000 teams.  We're looking for entries that convey the storyline behind the leaderboard.  Style and substance counts, as does reproducibility (sorry to the Bob Rosses of the world who want to hand draw their submission).  Web-readiness is appreciated, but we know better than to put such constraints on the Kaggle community.  Use whatever brush you wish to paint this masterpiece. We'd like to acknowledge Chris Mulligan at Columbia University for providing the impetus that put this prospect in motion. You can see his blog post or even check out a git repository of the code he used to do it. Image: Grimaldi's Leap Frog in the Comic Pantomime of the Golden Fish, 1812 (coloured engraving), Heath, William (1795-1840) / Victoria & Albert Museum, London, UK / The Bridgeman Art Library",
        "dataset_text": "This data has always been publicly available. We've taken the liberty to scrape the public leaderboards from our contests and provide them here.  We have also scraped all our user profile photos (at least, those which aren't the default goose) and provided them. The leaderboard files contain a TeamId, TeamName, SubmissionDate, and the best public score at the time of submission.  We can not expose every submission score because we want to restrict this to public information (which is the number of submissions and the best public score). Your visualization method should assume this file format as its input. Each TeamId may contain one or many Kaggle users.  Each Kaggle user has a unique UserId (this is the number in the URL when you visit the user pages at http://www.kaggle.com/users/UserId/).  We have provided a file teams.csv which lists the UserId's that belong to a given TeamId.  For example, to find the members of TeamID 12341, search for all the rows in teams.csv with a TeamId matching 12341. The RequestDate field contains the date and time the merger was requested. How do team mergers work?  The oldest TeamId persists throughout the merger.  This can make it a bit tricky to reconstruct whom is on a given team at a given time, and is the reason we have provided timestamps in teams.csv.  Let's walk through an example.  User 1 may be competing alone and have TeamId 1 at time 1.  (S)he decides to merge with User 2, who signed up after User 1.  At this point, both User 1 and User 2 will be assigned TeamId 1.  Team splits are not allowed.  Note that the merge times are not fully availble prior to TeamId 5008 and have NULL values in teams.csv."
    },
    {
        "name": "Greek Media Monitoring Multilabel Classification (WISE 2014)",
        "url": "https://www.kaggle.com/competitions/wise-2014",
        "overview_text": "Overview text not found",
        "description_text": "In the past, gathering information was paramount only for top-tier companies. In the information age, mining and categorization of relevant information is necessary for all companies. Media monitoring - the activity of monitoring the output of the print, online and broadcast media - allows every company to search a wide range of media, from printed media to internet publications, and be informed on their area of expertise and remain competitive.  This is a multi-label classification competition for articles coming from Greek printed media. Raw data comes from the scanning of print media, article segmentation, and optical character segmentation, and therefore is quite noisy. Each article is examined by a human annotator and categorized to one or more of the topics being monitored. Topics range from specific persons, products, and companies that can be easily categorized based on keywords, to more general semantic concepts, such as environment or economy. Building multi-label classifiers for the automated annotation of articles into topics can support the work of human annotators by suggesting a list of all topics by order of relevance, or even automate the annotation process for media and/or categories that are easier to predict. This saves valuable time and allows a media monitoring company to expand the portfolio of media being monitored.   The competition is organized by media monitoring solutions company DataScouting, media monitoring services company ENIMEROSI and the Deparment of Informatics of the Aristotle University of Thessaloniki. It is the challenge accompanying the 15th International Conference on Web Information System Engineering (WISE 2014) that will be held in Thessaloniki, Greece on 12-14 October 2014.      ARISTOTLE\n   UNIVERSITY OF\n   THESSALONIKI ",
        "dataset_text": "Data was collected by scanning a number of Greek print media from May 2013 to September 2013. Articles were manually segmented and their text extracted throuch OCR (optical character recognition) software. The text of the articles is represented using the bag-of-words model and for each token encountered inside the text of all articles, the tf-idf statistic is computed and unit normalization is applied to the tf-idf values of each article. There are therefore 301561 numerical attributes corresponding to the tokens encountered inside the text of the collected articles. Articles were manually annotated with one or more out of 203 labels. 99780 articles were collected. The chronologically first 64857 form the training set, and the following 34923 form the test set. The goal is to predict the relevant labels in the test set, where the labels of the articles are withheld.  Data are provided in two different formats: The ARFF format is mainly supported by Weka. You can work directly with this format by using the open-source Weka libraries Mulan and Meka. LIBSVM supports multi-label classification through LIBSVM tools. Matlab software for multi-label classification can also be found here.   "
    },
    {
        "name": "Chess ratings - Elo versus the Rest of the World",
        "url": "https://www.kaggle.com/competitions/chess",
        "overview_text": "Overview text not found",
        "description_text": "Description text not found",
        "dataset_text": "The dataset of chess results represents 105 months' worth of actual game-by-game results among 8,631 of the world's top 13,000 chess players, from sometime in the last 12 years. Out of the 8,631 players included in the dataset, only 70% of the games among those players have been included. Therefore, these players actually play with approximately twice the frequency that you see in this dataset.\n\nThe players are uniquely identified by ID #\u2019s ranging from 1 to 8,631.  The dataset is divided into a training dataset, representing a consecutive stretch of 100 months of game-by-game results among those top players, and a test dataset, representing the next 5 months of games played among those players (obviously the actual game-by-game results on the test dataset have been withheld). \n\nYou should use training_dataset.csv to train your models. It includes 65,053 rows of data, representing 65,053 distinct games played from months 1 through 100, with the following columns:\n\nMonth # (from 1 to 100)\n\nWhite Player # (from 1 to 8,631)\n\nBlack Player # (from 1 to 8,631)\n\nScore (either 0, 0.5, or 1)\n\n\u201cWhite Player\u201d represents the ID # of the player who had the white pieces, and \u201cBlack Player\u201d represents the ID # of the player who had the black pieces.  The possible values for Score represent the three possible outcomes of a chess game (1=White wins, 0.5=draw, 0=Black wins). \n\nIn chess, the player with the white pieces gets to move first and therefore has a slight advantage.  For instance, in the 65,053 games listed in the training dataset, White won 32.5% of the games, Black won 23.4% of the games, and 44.1% of the games were drawn (draws are very common among top players)\n\nThe test_dataset.csv should be used to frame submissions. It includes 7,809 rows of data, representing 7,809 distinct games played from months 101 through 105, with the following columns:\n\nMonth # (from 101 to 105)\n\nWhite Player # (from 1 to 8,631)\n\nBlack Player # (from 1 to 8,631)\n\nScore (either 0, 0.5, or 1)\n\nThe format of this dataset is the same as training_dataset.csv, except that the results for the Score column are not provided.  Competitors should calculate the white player's expected score (between 0 and 1) for each row. Once they have filled in the score column for test_dataset.csv, then can enter their submission. example_submission.csv gives an example entry.\n\nNote: the public leaderboard is calculated based on 781 matches in the test dataset (these 781 matches are not used in the calculation of the final standings). The rest of the test dataset is used in the calculation of the final standings.\n\nTo enter your submission, visit the Make a Submission page.\n\nUpdate: Several participants have noticed that the five-month test set and the final five months of the training set have some different characteristics.  This is because some filtering was done to the test set.  The filtering was done for good reasons (see the forum topic Cross Validation Dataset for more details) but it can make cross-validation challenging.  Therefore we have decided to release a cross validation dataset (cross_validation_data.csv - downloadable below), a subset of the month 96-100 training games that should more closely resemble the characteristics of the test set.  The cross validation dataset has been created, from months 96-100 of the training dataset, by excluding any games where one or both players had played fewer than 12 \"fully-rated\" games across months 48-95 of the training dataset, where a \"fully-rated game\" is one where both players already had a FIDE rating at the time the game was played.  We expect that this file can be productively used for cross validation where months 96-100 are treated like the test set."
    },
    {
        "name": "Data Mining Hackathon on (20 mb) Best Buy mobile web site - ACM SF Bay Area Chapter",
        "url": "https://www.kaggle.com/competitions/acm-sf-chapter-hackathon-small",
        "overview_text": "Overview text not found",
        "description_text": "Two years of mobile behavior, 67 million clicks, 27 million searches, 8 million users, 1 million products You can enter one or both tracks of the competition: There will also be a Visualization Contest that can be entered from either track. http://www.sfbayacm.org/DM-Hackathon-2012-10 Data Provided by:   Cloud Compute Sponsors:                         ",
        "dataset_text": "The main data for this competition is in the train.csv and test.csv files. These files contain information on what items users clicked on after making a search. Each line of train.csv describes a user's click on a single item. It contains the following fields: test.csv contains all of the same fields as train.csv except for sku. It is your job to estimate which sku's were clicked on in these test queries. Due to the internal structure of BestBuy's databases, there is no guarantee that the user clicks resulted from a search with the given query. What we do know is that the user made a query at query_time, and then, at click_time, they clicked on the sku, but we don't know that the click came from the search results. The click_time is never more than five minutes after the query_time. In addition, there is information about these xbox products in small_product_data.xml. We have also provided a sample benchmark submission and the code that produces it. popular_skus.py is a simple python script that predicts that each user clicked on one of the five most popular xbox skus. This script produces the benchmark in popular_skus.csv. Note that all of the predictions in popular_skus.csv are the same. The syntax of a submission should be the same as that in popular_skus.csv: A file with the header \"sku\", and each of the following lines containing the space-delimited estimates of the clicked sku that resulted after the queries in test.csv, and in the same order. Small Product Data Dictionary\nhttps://bbyopen.com/documentation/products-api/product-attributes#TableProdRefInfo For more BestBuy Data and APIs, check out https://bbyopen.com/   and for more BigDataR tools https://github.com/koooee/BigDataR_Examples/tree/master/ACM_comp"
    },
    {
        "name": "Predict HIV Progression",
        "url": "https://www.kaggle.com/competitions/hivprogression",
        "overview_text": "Overview text not found",
        "description_text": "Description text not found",
        "dataset_text": "Dataset description not found"
    },
    {
        "name": "Semi-Supervised Feature Learning",
        "url": "https://www.kaggle.com/competitions/SemiSupervisedFeatureLearning",
        "overview_text": "Overview text not found",
        "description_text": "There's recently been a lot of work done in unsupervised feature learning for classification, with great advances made by approaches such as deep belief nets, graphical models, and transfer learning. Meanwhile, there are a ton of older methods that also work well, including matrix factorization, random projections, and clustering methods. The purpose of this competition is to find out which of these methods work the best, on relatively large-scale high dimensional learning tasks. The Short Version In this task, you'll do the following: We have public data to be used for the leaderboard evaluations, and a separate private data set used for final evaluation through a special submission process.  We have scripts to simplify the training and evaluation; how you do the feature transformation is up to you.   The Long Version The task that we're evaluating on is a binary-class classification problem, drawn from web classification.  (The data has been cleaned heavily and anonymized.)  The data itself is sparse, high dimensional data with about a million features.  A few features have non-zero values in many or even all examples; most features have non-zero values in very few examples. Your task is to transform the data from a high dimensional space to a lower dimensional space of at most 100 features.  The goal is to make this new feature space so rich and informative that it allows a new classifier to be trained with the best possible predictive performance.  Any method of producing a condensed representation is fair game: deep learning graphical models, transfer learning, supervised learning, semi-supervised learning, matrix factorization, random projection, clustering, feature selection, or anything else you can invent. In addition to a small amount of labeled data, you will also be given a large amount of unlabeled data.  Both the labeled and the unlabeled data can be used to learn good ways to transform the feature space. The final evaluation will be done by using your method to transform training and test sets whose labels are not known.  These transformed data sets will be sent to tne organizers, who will apply the hidden labels and use these new data sets to train and test a standard supervised classifier.  The data set that produces the best classification performance on test data (using AUC as the evaluation measure) will be declared the winner.  We also provide versions of our evaluation scripts to be used on public versions of our private evaluation data sets, and these results can be used to update the leaderboard.  Please see the \"Evaluation\" page for full details. Although there is a modest cash prize, the main goal of this competition is to encourage research and share ideas.  The results of this competition will be included in a paper submitted to the 2011 NIPS workshop on deep learning and unsupervised feature learning.  Contestants will be acknowledged by name in this paper for noteworthy performance, including results that do especially well or which are especially interesting.",
        "dataset_text": "Dataset description not found"
    },
    {
        "name": "Personality Prediction Based on Twitter Stream",
        "url": "https://www.kaggle.com/competitions/twitter-personality-prediction",
        "overview_text": "Overview text not found",
        "description_text": "The aim of this competition is to determine the best models to predict the personality traits of Machiavellianism, Narcissism, Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism based on Twitter usage and linguistic inquiry. The organizers provide all interested participants an anonymised dataset of users self assessed personality scores (based checklists developed by Prof Del Paulhus at the University of British Columbia and Prof Sam Gosling and the University of Texas) together with 337 variables derived from functions of Twitter information and lingusitc analysis. The best performing model(s) will be formally cited in a future paper and any presentations.   The intention of this research is to seperate fact from fiction and examine just what can be predicted by social media use and how this information might be used, both for good and bad. As an organization, the Online Privacy Foundation works to raise awareness of online privacy issues and empower people to make informed choices about what they do online. We hope you'll support our mission and take part in this competition.    (Twitter is a trademark of Twitter, Inc.)",
        "dataset_text": "Three files are provided 1. DescriptiveStats.pdf - Descriptive Statistics of each personality dimension 2. Personality_Traits_Trainingset_v1.csv - Training set providing personality self assessment results(cols 2 - 8)  together with 337 variables derived from a users Twitter activity (as follows) 3. Personality_Traits_Testset_v1.csv - Accompanying test set      "
    },
    {
        "name": "Challenges in Representation Learning: Facial Expression Recognition Challenge",
        "url": "https://www.kaggle.com/competitions/challenges-in-representation-learning-facial-expression-recognition-challenge",
        "overview_text": "Overview text not found",
        "description_text": "One motivation for representation learning is that learning algorithms can design features better and faster than humans can. To this end, we hold this challenge that does not explicitly require that entries use representation learning. Rather, we introduce an entirely new dataset and invite competitors from all related communities to solve it. The dataset for this challenge is a facial expression classification dataset that we have assembled from the internet. Because this is a newly introduced dataset, this contest will see which methods are the easiest to get quickly working on new data. Example baseline submissions are available as part of the pylearn2 python package available at https://github.com/lisa-lab/pylearn2 The baseline submissions for this contest are in pylearn2/scripts/icml_2013_wrepl/emotions Because this task is very easy for humans to do, we will not provide the final test inputs until one week before the contest closes. Preliminary winners will need to release their winning code and demonstrate that they did not manually label the test set. We reserve the right to disqualify entries that may involve any manually labeling of the test set. Preliminary winners will need to release their winning code and demonstrate that they did not manually label the test set. We reserve the right to disqualify entries that may involve any manually labeling of the test set.",
        "dataset_text": "The data consists of 48x48 pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centered and occupies about the same amount of space in each image. The task is to categorize each face based on the emotion shown in the facial expression in to one of seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral). train.csv contains two columns, \"emotion\" and \"pixels\". The \"emotion\" column contains a numeric code ranging from 0 to 6, inclusive, for the emotion that is present in the image. The \"pixels\" column contains a string surrounded in quotes for each image. The contents of this string a space-separated pixel values in row major order. test.csv contains only the \"pixels\" column and your task is to predict the emotion column. The training set consists of 28,709 examples. The public test set used for the leaderboard consists of 3,589 examples. The final test set, which was used to determine the winner of the competition, consists of another 3,589 examples. This dataset was prepared by Pierre-Luc Carrier and Aaron Courville, as part of an ongoing research project. They have graciously provided the workshop organizers with a preliminary version of their dataset to use for this contest."
    },
    {
        "name": "Challenges in Representation Learning: The Black Box Learning Challenge",
        "url": "https://www.kaggle.com/competitions/challenges-in-representation-learning-the-black-box-learning-challenge",
        "overview_text": "Overview text not found",
        "description_text": "This is a black-box learning challenge: Competitors train a classifier on a dataset that is not human readable, without knowledge of what the data consists of. They are scored based on classification accuracy on a private test set. This challenge is designed to reduce the usefulness of having a human researcher working in the loop with the training algorithm. We are also providing a dataset of approx. 130,000 unsupervised examples that contestants can use to improve their models. The unsupervised data is a CSV file in the same format as the private test set (i.e. without the labels). The extra data comes from a distribution that is very similar to the training/test set distribution. We provide example code for this contest as part of the pylearn2 package at https://github.com/lisa-lab/pylearn2 For this contest, look at the pylearn2/scripts/icml_2013_wrepl/black_box directory.",
        "dataset_text": "  The data consists of 1,875 input features for each example. Each example is assigned to one of 9 classes. The training set consists of 1,000 labeled examples. The test set consists of 10,000 examples, split into 5,000 public test and 5,000 private test. The extra data download (available as a .csv file compressed into a .tgz archive) provides an additional 135,735 unlabeled examples that your training algorithm may exploit. Note that to be scored correctly your submission file should refer to the classes as \"1.0\" through \"9.0\". If you enter a different string such as \"0.0\" or \"2\", you will receive zero points for that example. We have migrated the competition to the new parser. We now expect a csv file instead of a single column txt file with columns Id and Class. Id is a sequence of numbers from 1 to 10000. And we now expect classes without the decimal part \"1\" through \"9\"."
    },
    {
        "name": "Challenges in Representation Learning: Multi-modal Learning",
        "url": "https://www.kaggle.com/competitions/challenges-in-representation-learning-multi-modal-learning",
        "overview_text": "Overview text not found",
        "description_text": "In this contest, competitors will design systems to learn about two modalities of data: images and text. The provided training data is Louis von Ahn's Small ESP Game Dataset, containing images and word tags for these images. Competitors should train their system to associate images to sets of word tags. At test time, the system is presented with two possible sets of word tags for an image, and must determine which is the correct set of word tags. Because this task is very easy for humans to do, we will not provide the final test inputs until one week before the contest closes. Preliminary winners will need to release their winning code and demonstrate that they did not manually label the test set. We reserve the right to disqualify entries that may involve any manual labeling of the test set.",
        "dataset_text": "The test examples come in triplets: an image, and two annotations. The test images are located in public_test_images.tgz.  The file public_test_options.tgz contains two files per test image: image_id.option_0.desc and image_id.option_1.desc. The models should predict which of the options fits the image better. If option 0 fits better, the prediction forimage_id should be 0. If option 1 fits better, the prediction for image_id should be 1. This test data was manually labeled by Ian Goodfellow, and designed to resemble the Small ESP Game dataset created by Luis von Ahn. You are therefore recommended to train your system on the Small ESP Game dataset, though you are allowed to use any publicly available training set. Please download the Small ESP Game dataset from this contest website, to avoid putting undue strain on Louis von Ahn's personal site. The ESP Game was an online game that awarded players points if they could label an image with the same word as another unknown player logged in from a different location. The fun game format encouraged a large number of people to participate in a data crowdsourcing effort. Keep in mind that the game format influences the kind of labels that were provided. The test data has some statistical differences from the ESP Game dataset. Most notably, test images are 300 pixels long on the larger dimension, while the training data comes in a variety of sizes. It is probably important to design your learning system to handle multiple scales of images. The ESP Game Dataset consists of 100,000 labeled images. The test data consists of 1000 images, split into public test and private test. The ESP Game Dataset has a single correct description for each image, while the test dataset provides a correct and an incorrect description for each image. The incorrect description is always the correct description of one other test image. An example script for preparing a submission is provided as part of the pylearn2 package: http://github.com/lisa-lab/pylearn2 The relevant script is at pylearn2/scripts/icml_2013_wrepl/multimodal"
    },
    {
        "name": "RecSys2013: Yelp Business Rating Prediction",
        "url": "https://www.kaggle.com/competitions/yelp-recsys-2013",
        "overview_text": "Overview text not found",
        "description_text": "  Looking for last year's challenge?  Check out RecSys Challenge 2012.",
        "dataset_text": "The data is a detailed dump of Yelp reviews, businesses, users, and checkins for the Phoenix, AZ metropolitan area. yelp_training_set.zip and yelp_training_set_mac.zip have the same files in them. You only need to download one; both are provided for compatibility. In the training set: In the testing set: Training set = yelp_training_set.zip\nTesting set = yelp_test_set.zip\nSample submission format= sample_submission.csv Each object type is in a separate file, one JSON object per line. The checkin format is the same as in the training set."
    },
    {
        "name": "The ICML 2013 Bird Challenge",
        "url": "https://www.kaggle.com/competitions/the-icml-2013-bird-challenge",
        "overview_text": "Overview text not found",
        "description_text": "This competition asks participants to identify which of 35 species of birds are present in continuous recordings taken at three different locations. The data is provided by the Mus\u00e9um national d'Histoire naturelle, one of the most respected bird survey institutions in the world.  We're aware the competition deadline is tight, but wanted to give Kagglers the chance to work on this interesting problem. The results will be presented at the Workshop on Machine Learning for Bioacoustics at ICML 2013. The competition test data was graciously provided by J\u00e9r\u00f4me Sueur. The competition is coorganized by Pr. Herv\u00e9 Glotin of the Institut Universitaire de France. This project is part of the Scaled Acoustic Biodiversity project - CNRS interdisciplinary mission (http://sabiod.org). Singing species were identified by\nFr\u00e9d\u00e9ric Jiguet (Mus\u00e9um national d'Histoire naturelle, France).",
        "dataset_text": "You are given recordings of 35 species of birds.  The task is to assign a probability that a given species of bird sings at any point in a continuous, 150 second recording.  This is a challenging task because of background noise, variability in the bird sounds, and the fact that the songs overlap. Training set info:\n16bit, frequency sampling = 44.1kHz\n35 recordings, one bird species per file, 30 sec by file.\nTotal train duration = 18 minutes.\nEach of these 35 species appears at least one time in the test set. Testing set info:\nData recorded by 3 microphones in the same area\n3 different forest states (A,B,C : mature, young, open)\n16bit, frequency sample = 44.1kHz.\nThese wav files were recorded the same day/hour 30 minutes before sunrise in Vall\u00e9e Chevreuse (Paris). The geography of A, B, C sites are (from west to east) given on this map: http://sabiod.univ-tln.fr/icml2013/map.html To assist with your development, the ground truth for the first day at all three locations has been released in README_TEST.  Additional information is provided within the respective README files. More details on the train data are given in : Deroussen, F., 2001. Oiseaux des jardins de France. Nashvert Production, Charenton, France; Deroussen, F., Jiguet, F., 2006. La sonotheque du Museum: Oiseaux de\nFrance, les passereaux. Nashvert production, Charenton, France; http://naturophonia.fr."
    },
    {
        "name": "The ICML 2013 Whale Challenge - Right Whale Redux",
        "url": "https://www.kaggle.com/competitions/the-icml-2013-whale-challenge-right-whale-redux",
        "overview_text": "Overview text not found",
        "description_text": "(right whale illustration courtesy of Pieter Folkens, \u00a92011) This competition complements the previously held Marinexplore Whale Detection Challenge, in which Cornell University provided data from a ship monitoring application termed \"Auto Buoy\", or AB Monitoring System. In the Marinexplore challenge we received solutions that exceeded 98% accuracy and will ultimately advance the process of automatically classifying North Atlantic Right Whales using the AB Monitoring Platform. Since the results from the previous challenge proved so successful, we decided to extend the goals and consider applications that involve running algorithms on archival data recorded using portable hydrophone assemblies, otherwise referred to as Marine Autonomous Recording Unit (or MARU\u2019s). Since Cornell and its partners have been using the MARU for over a decade, a sizable collection of data has been accumulated. This data spans several ocean basins and covers a variety of marine mammal species. Solutions to this challenge will be ported to a High Performance Computing (HPC) platform, being developed in part through funding provided by the Office of Naval Research (ONR grant N000141210585, Dugan, Clark, LeCun and Van Parijs). Together, Cornell will combine algorithms, HPC technologies and its data archives to explore data using highly accurate measuring tools. We encourage participants who developed prior solutions (through the collaboration with Marinexplore) to test them on this data. The results will be presented at the Workshop on Machine Learning for Bioacoustics at ICML 2013.",
        "dataset_text": "You are given four days of training data and three days of testing data.  The task is to assign a probability that each recording in the test set contains a right whale call (1) or noise (0). To run within hardware constraints, solutions would ideally function with limited training history, meaning the algorithm would page in no more than five previous minutes of data at a time. Since we cannot enforce this requirement in this particular competition, we provide this info for those who want to optimize on \"real world utility,\" as opposed to AUC."
    },
    {
        "name": "Partly Sunny with a Chance of Hashtags",
        "url": "https://www.kaggle.com/competitions/crowdflower-weather-twitter",
        "overview_text": "Overview text not found",
        "description_text": " In this competition you are provided a set of tweets related to the weather. The challenge is to analyze the tweet and determine whether it has a positive, negative, or neutral sentiment, whether the weather occurred in the past, present, or future, and what sort of weather the tweet references. It's a lot to mine from so few characters, but if the going gets tough you can always blame the weather... \"Please knock out the power giant storm that is passing thru....please.\" -Tweet #74096 We are excited to team up with CrowdFlower on the first of what we hope will be many fun machine learning projects. CrowdFlower is debuting a new open data library and we're always looking for an excuse to have a competition. Why is this exciting? Sweet, sweet Labels. Data repositories sometimes have more in common with a landfill than a library. They're home to tattered piles of spreadsheets in odd formats with nary a shred of documentation to tell the GDP of Chile from the migratory patterns of North American goldfinches. If creating value from this digital exhaust is a defining theme of the big data explosion, most repositories leave you choking on the diesel fumes of data disappointment. Such data is great if you are doing a report on the GDP of Chile, but not so useful if you are doing machine learning, or its red-headed step child, data science. Crowdflower's data sets provide the thing that makes so many repositories fall short - data paired with labels. One can decide whether two English sentences are related, make judgments about yogurt chatter, or rank emotions on tweets about nuclear energy. It's all about the (wo)manpower to label what these bytes actually mean. CrowdFlower Open Data Library is a repository of real data set samples that developers, researchers and data scientists can download and use to test and improve algorithms. Our mission is to encourage users to explore the possibilities and power of crowdsourcing. Open Data is free, available to anyone, and ready-to-use with CrowdFlower\u2019s Platform. New data sets are continuously added to CrowdFlower Open Data Library as users of the CrowdFlower Platform opt-in to share their data with the crowdsourcing community. Sample data sets currently available include tweets for sentiment and topic analysis, word combinations to test similarities, sentence combinations to test related topics, and more. Learn more at www.crowdflower.com.",
        "dataset_text": "The training set contains tweets, locations, and a confidence score for each of 24 possible labels.  The 24 labels come from three categories: sentiment, when, and kind. Human raters can choose only one label from the \"sentiment\" and \"when\" categories, but are allowed multiple choices for the \"kind\". Your goal is to predict a confidence score of all 24 labels for each tweet in the test set. For example, a tweet \"The hot and humid weather yesterday was awesome!\" could have s4=1, w4=1, k4=1, k5=1, with the rest marked as zero. Each tweet is reviewed by multiple raters, and some amount of disagreement on the labels is expected.  The confidence score accounts for two factors - the mixture of labels that the raters gave a tweet and the individual trust of each rater.  Since some raters are more accurate than others (e.g. they pay closer attention, take the job more seriously, etc.), these raters count more in the confidence score.  You have more confidence that a tweet was referring to the past if you trust the person telling you. In this competition you do not have access to the individual ratings or the raters' trust. This \"unknown trust\" issue is therefore a source of noise in the problem. However, you do know that the raters can choose only one label from the \"sentiment\" and \"when\" categories, but multiple choices for the \"kind\". The result is that confidences for the \"sentiment\" and \"when\" categories sum to one. Conversely, the sum of the \"kind\" category will not always sum to one."
    },
    {
        "name": "How Much Did It Rain?",
        "url": "https://www.kaggle.com/competitions/how-much-did-it-rain",
        "overview_text": "Overview text not found",
        "description_text": "For agriculture, it is extremely important to know how much it rained on a particular field. However, rainfall is variable in space and time and it is impossible to have rain gauges everywhere. Therefore, remote sensing instruments such as radar are used to provide wide spatial coverage. Rainfall estimates drawn from remotely sensed observations will never exactly match the measurements that are carried out using rain gauges, due to the inherent characteristics of both sensors. Currently, radar observations are \"corrected\" using nearby gauges and a single estimate of rainfall is provided to users who need to know how much it rained. This competition will explore how to address this problem in a probabilistic manner.  Knowing the full probabilistic spread of rainfall amounts can be very useful to drive hydrological and agronomic models -- much more than a single estimate of rainfall.  Unlike a conventional Doppler radar, a polarimetric radar transmits radio wave pulses that have both horizontal and vertical orientations. Because rain drops become flatter as they increase in size and because ice crystals tend to be elongated vertically, whereas liquid droplets tend to be flattened, it is possible to infer the size of rain drops and the type of hydrometeor from the differential reflectivity of the two orientations. In this competition, you are given polarimetric radar values and derived quantities at a location over the period of one hour. You will need to produce a probabilistic distribution of the hourly rain gauge total. More details are on the data page. This competition is sponsored by the Artificial Intelligence Committee of the American Meteorological Society. The Climate Corporation has kindly agreed to sponsor the prizes.",
        "dataset_text": "In this competition, you are given polarimetric radar values and derived quantities at a location over the period of one hour. You will need to produce a probabilistic distribution of the hourly rain gauge total, i.e., produce\n(\n)\nwhere y is the rain accumulation and Y lies between 0 and 69 mm (both inclusive) in increments of 1 mm. For every row in the dataset, submission files should contain 71 columns: Id and 70 numbers.  The training data consists of NEXRAD and MADIS data collected the first 8 days of Apr to Nov 2013 over midwestern corn-growing states. Time and location information have been censored, and the data have been shuffled so that they are not ordered by time or place. The test data consists of data from the same radars and gauges over the same months but in 2014. Please see this page to understand more about polarimetric radar measurements. The columns in the datasets are: TimeToEnd:  How many minutes before the end of the hour was this radar observation? DistanceToRadar:  Distance between radar and gauge.  This value is scaled and rounded to prevent reverse engineering gauge location Composite:  Maximum reflectivity in vertical volume above gauge HybridScan: Reflectivity in elevation scan closest to ground HydrometeorType:  One of nine categories in NSSL HCA. See presentation for details. Kdp:  Differential phase RR1:  Rain rate from HCA-based algorithm RR2:  Rain rate from Zdr-based algorithm RR3:  Rain rate from Kdp-based algorithm RadarQualityIndex:  A value from 0 (bad data) to 1 (good data) Reflectivity:  In dBZ ReflectivityQC:  Quality-controlled reflectivity RhoHV:  Correlation coefficient Velocity:  (aliased) Doppler velocity Zdr:  Differential reflectivity in dB LogWaterVolume:  How much of radar pixel is filled with water droplets? MassWeightedMean:  Mean drop size in mm MassWeightedSD:  Standard deviation of drop size Expected: the actual amount of rain reported by the rain gauge for that hour. When the reflectivity composite is provided, you get multiple values in one column (note that they are not comma-separated): This is because there are multiple radar observations (one per radar volume scan) within an hour and because there are two radars that observe the atmosphere above this rain gauge.   To determine there are two radars, look at the \"TimeToEnd\" column and notice that there are two sequences that count down to zero: You could also use the \"DistanceToRadar\" column, but theoretically, there could be a rain gauge that is equidistant from two radars.   Each column in a row will have the same number of values that is reflected by the \"TimeToEnd\" column. Hydrometeor types: There are five types of \"missing data\" codes in the dataset: We have anonymized and shuffled the data to remove time and location information. However, just to cover our bases, we will state that you are not allowed to infer the rain gauge corresponding to the input data and use the actual reported value from that rain gauge -- any entry that uses rain gauge data beyond that supplied in the problem will be disqualified. To reference this dataset in scientific publications, please use the following citation:  Lakshmanan, V, A. Kleeman, J. Boshard, R. Minkowsky, A. Pasch, 2015. The AMS-AI 2015-2016 Contest: Probabilistic estimate of hourly rainfall from radar. 13th Conference on Artificial Intelligence, American Meteorological Society, Phoenix, AZ"
    },
    {
        "name": "How Much Did It Rain? II",
        "url": "https://www.kaggle.com/competitions/how-much-did-it-rain-ii",
        "overview_text": "Overview text not found",
        "description_text": "After incorporating feedback from the Kaggle community, as well as scientific and educational partners, the Artificial Intelligence Committee of the American Meteorological Society is excited to be running a second iteration of the How Much Did It Rain? competition. How Much Did It Rain? II is focused on solving the same core rain measurement prediction problem, but approaches it with a new and improved dataset and evaluation metric. This competition will go even further towards building a useful educational tool for universities, as well as making a meaningful contribution to continued meteorological research. Rainfall is highly variable across space and time, making it notoriously tricky to measure. Rain gauges can be an effective measurement tool for a specific location, but it is impossible to have them everywhere. In order to have widespread coverage, data from weather radars is used to estimate rainfall nationwide. Unfortunately, these predictions never exactly match the measurements taken using rain gauges. Recently, in an effort to improve their rainfall predictors, the U.S. National Weather Service upgraded their radar network to be polarimetric. These polarimetric radars are able to provide higher quality data than conventional Doppler radars because they transmit radio wave pulses with both horizontal and vertical orientations.   Dual pulses make it easier to infer the size and type of precipitation because rain drops become flatter as they increase in size, whereas ice crystals tend to be elongated vertically. In this competition, you are given snapshots of polarimetric radar values and asked to predict the hourly rain gauge total. A word of caution: many of the gauge values in the training dataset are implausible (gauges may get clogged, for example). More details are on the data page. This competition is sponsored by the Artificial Intelligence Committee of the American Meteorological Society. Climate Corporation is providing the prize pool. ",
        "dataset_text": "The training data consists of NEXRAD and MADIS data collected on 20 days between Apr and Aug 2014 over midwestern corn-growing states. Time and location information have been censored, and the data have been shuffled so that they are not ordered by time or place. The test data consists of data from the same radars and gauges over the remaining days in that month. Please see this page to understand more about polarimetric radar measurements. To understand the data, you have to realize that there are multiple radar observations over the course of an hour, and only one gauge observation (the 'Expected'). That is why there are multiple rows with the same 'Id'. The columns in the datasets are: To reference this dataset in scientific publications, please use the following citation: Lakshmanan, V, A. Kleeman, J. Boshard, R. Minkowsky, A. Pasch, 2015. The AMS-AI 2015-2016 Contest: Probabilistic estimate of hourly rainfall from radar. 13th Conference on Artificial Intelligence, American Meteorological Society, Phoenix, AZ"
    },
    {
        "name": "Tourism Forecasting Part One",
        "url": "https://www.kaggle.com/competitions/tourism1",
        "overview_text": "Overview text not found",
        "description_text": "Description text not found",
        "dataset_text": "tourism_data.csv contains 518 yearly time series. The first row gives a series identifier and subsequent rows give the observations.\n\ntemplate.csv is the submission template. example.csv is an example submission.\n\nUpdate: we have made a small change to the format of tourism_data.csv in an attempt to make it more user friendly."
    },
    {
        "name": "Tourism Forecasting Part Two",
        "url": "https://www.kaggle.com/competitions/tourism2",
        "overview_text": "Overview text not found",
        "description_text": "Description text not found",
        "dataset_text": "tourism_data2.csv contains 793 time series. The first 366 columns contain monthly time series. The next 427 time series contain quarterly time series. The first row is a series identifier and subsequent rows\ngive the observations.\n\n\nParticipants are expected to submit 24 forecasts on the first 366 series and 8 forecasts on the next 427 series. Submissions should be based on example_submission.csv.\n\nUpdate 22 October: a correction was made to the data file. Please ensure that you build your models using tourism_data2_revision2.csv."
    },
    {
        "name": "Don't Overfit!",
        "url": "https://www.kaggle.com/competitions/overfitting",
        "overview_text": "Overview text not found",
        "description_text": "Description text not found",
        "dataset_text": "The data file contains 200 randomly generated variables, var_1 to var_200.\n\nThere are 20,000 rows of data, of which you are only given the 'Target' for the first 250. The 'Target' is either 1 or 0, so this is a classification problem.\n\nThere are also 5 other fields,\n\ncase_id - 1 to 20,000, a unique identifier for each row\n\ntrain - 1/0, this is a flag for the first 250 rows which are the training dataset\n\nTarget_Practice - we have provided all 20,000 Targets for this model, so you can develop your method completely off line.\n\nTarget_Leaderboard - only 250 Targets are provided. You submit your predictions for the remaining 19,750 to the Kaggle leaderboard.\n\nTarget_Evaluate - again only 250 Targets are provided. Those competitors who beat the 'benchmark' on the Leaderboard will be asked to make one further submission for the Evaluation model.\n\nThe three models (Practice, Leaderboard & Evaluate) are all based on the same underlying data, but the generated 'equation' is different for each. The equations are of a similar form, but the underlying model parameters differ.\n\nThe values to be predicted are represented as '-99' in the downloaded data."
    },
    {
        "name": "ECML/PKDD 15: Taxi Trajectory Prediction (I)",
        "url": "https://www.kaggle.com/competitions/pkdd-15-predict-taxi-service-trajectory-i",
        "overview_text": "Overview text not found",
        "description_text": "The taxi industry is evolving rapidly. New competitors and technologies are changing the way traditional taxi services do business. While this evolution has created new efficiencies, it has also created new problems.  One major shift is the widespread adoption of electronic dispatch systems that have replaced the VHF-radio dispatch systems of times past. These mobile data terminals are installed in each vehicle and typically provide information on GPS localization and taximeter state. Electronic dispatch systems make it easy to see where a taxi has been, but not necessarily where it is going. In most cases, taxi drivers operating with an electronic dispatch system do not indicate the final destination of their current ride.  Another recent change is the switch from broadcast-based (one to many) radio messages for service dispatching to unicast-based (one to one) messages. With unicast-messages, the dispatcher needs to correctly identify which taxi they should dispatch to a pick up location. Since taxis using electronic dispatch systems do not usually enter their drop off location, it is extremely difficult for dispatchers to know which taxi to contact.  To improve the efficiency of electronic taxi dispatching systems it is important to be able to predict the final destination of a taxi while it is in service. Particularly during periods of high demand, there is often a taxi whose current ride will end near or exactly at a requested pick up location from a new rider. If a dispatcher knew approximately where their taxi drivers would be ending their current rides, they would be able to identify which taxi to assign to each pickup request. The spatial trajectory of an occupied taxi could provide some hints as to where it is going. Similarly, given the taxi id, it might be possible to predict its final destination based on the regularity of pre-hired services. In a significant number of taxi rides (approximately 25%), the taxi has been called through the taxi call-center, and the passenger\u2019s telephone id can be used to narrow the destination prediction based on historical ride data connected to their telephone id. In this challenge, we ask you to build a predictive framework that is able to infer the final destination of taxi rides in Porto, Portugal based on their (initial) partial trajectories. The output of such a framework must be the final trip's destination (WGS84 coordinates). This is the first of two data science challenges that share the same dataset. The Taxi Service Trip Time competition predicts the total time of taxi rides. This competition is affiliated with the organization of ECML/PKDD 2015. ",
        "dataset_text": "We have provided an accurate dataset describing a complete year (from 01/07/2013 to 30/06/2014) of the trajectories for all the 442 taxis running in the city of Porto, in Portugal (i.e. one CSV file named \"train.csv\"). These taxis operate through a taxi dispatch central, using mobile data terminals installed in the vehicles. We categorize each ride into three categories: A) taxi central based, B) stand-based or C) non-taxi central based. For the first, we provide an anonymized id, when such information is available from the telephone call. The last two categories refer to services that were demanded directly to the taxi drivers on a B) taxi stand or on a C) random street. Each data sample corresponds to one completed trip. It contains a total of\n9 (nine) features, described as follows: Five test sets will be available to evaluate your predictive framework (in one single CSV file named \"test.csv\"). Each one of these datasets refer to trips that occurred between 01/07/2014 and 31/12/2014. Each one of these data sets will provide a snapshot of the current network status on a given timestamp. It will provide partial trajectories for each one of the on-going trips during that specific moment. The five snapshots included on the test set refer to the following timestamps: 14/08/2014 18:00:00\n30/09/2014 08:30:00\n06/10/2014 17:45:00\n01/11/2014 04:00:00\n21/12/2014 14:30:00 File sampleSubmission.csv uses the location of Porto main Avenue, in downtown (i.e. Avenida dos Aliados).  Along with these two files, we have also provided two additional files. One contains meta data regarding the taxi stands metaData_taxistandsID_name_GPSlocation.csv including id and location. The second one includes an evaluation script for both problems developed in the R language (\"evaluation_script.r\")."
    },
    {
        "name": "ECML/PKDD 15: Taxi Trip Time Prediction (II)",
        "url": "https://www.kaggle.com/competitions/pkdd-15-taxi-trip-time-prediction-ii",
        "overview_text": "Overview text not found",
        "description_text": "This is the second of two data science challenges that share the same dataset. The Taxi Service Trajectory competition predicts the final destination of taxi trips.   To improve the efficiency of electronic taxi dispatching systems it is important to be able to predict how long a driver will have his taxi occupied. If a dispatcher knew approximately when a taxi driver would be ending their current ride, they would be better able to identify which driver to assign to each pickup request.  In this challenge, we ask you to build a predictive framework that is able to infer the trip time of taxi rides in Porto, Portugal based on their (initial) partial trajectories. The output of such a framework must be the travel time of a particular taxi trip. This competition is affiliated with the organization of ECML/PKDD 2015. ",
        "dataset_text": "We have provided an accurate dataset describing a complete year (from 01/07/2013 to 30/06/2014) of the trajectories for all the 442 taxis running in the city of Porto, in Portugal (i.e. one CSV file named \"train.csv\"). These taxis operate through a taxi dispatch central, using mobile data terminals installed in the vehicles. We categorize each ride into three categories: A) taxi central based, B) stand-based or C) non-taxi central based. For the first, we provide an anonymized id, when such information is available from the telephone call. The last two categories refer to services that were demanded directly to the taxi drivers on a B) taxi stand or on a C) random street. Each data sample corresponds to one completed trip. It contains a total of\n9 (nine) features, described as follows: The total travel time of the trip (the prediction target of this competition) is defined as the (number of points-1) x 15 seconds. For example, a trip with 101 data points in POLYLINE has a length of (101-1) * 15 = 1500 seconds. Some trips have missing data points in POLYLINE, indicated by MISSING_DATA column, and it is part of the challenge how you utilize this knowledge.  Five test sets will be available to evaluate your predictive framework (in one single CSV file named \"test.csv\"). Each one of these datasets refer to trips that occurred between 01/07/2014 and 31/12/2014. Each one of these data sets will provide a snapshot of the current network status on a given timestamp. It will provide partial trajectories for each one of the on-going trips during that specific moment. The five snapshots included on the test set refer to the following timestamps: 14/08/2014 18:00:00\n30/09/2014 08:30:00\n06/10/2014 17:45:00\n01/11/2014 04:00:00\n21/12/2014 14:30:00 File sampleSubmission.csv uses the average travel time of all trips in the training set. Along with these two files, we have also provided two additional files. One contains meta data regarding the taxi stands metaData_taxistandsID_name_GPSlocation.csv including id and location. The second one includes an evaluation script for both problems developed in the R language (\"evaluation_script.r\")."
    },
    {
        "name": "R Package Recommendation Engine",
        "url": "https://www.kaggle.com/competitions/R",
        "overview_text": "Overview text not found",
        "description_text": "Description text not found",
        "dataset_text": "Dataset description not found"
    },
    {
        "name": "World Cup 2010 - Take on the Quants",
        "url": "https://www.kaggle.com/competitions/worldcup2010",
        "overview_text": "Overview text not found",
        "description_text": "Can you outdo the quants?\n\nAs a break from projecting the strength of subprime mortgages, credit default swaps and other abstruse financial instruments, quantitative analysts at Goldman Sachs, JP Morgan, UBS and Danske Bank have modeled the 2010 soccer FIFA World Cup. We have set up a forecasting competition, allowing competitors to go head-to-head with these corporate giants.\n\nThe challenge is to correctly predict how far each country will progress in the tournament. There is a small dataset and some links to variables of interest on the data page. There are links to the investment banks' predictions and other World Cup modeling efforts on the hints page.\n\nWe are running two challenges side-by-side - a Take on the Quants Challenge and a Confidence Challenge. The Take on the Quants Challenge simply requires competitors to pick how far teams will progress in the tournament. Competitors' entries will be ranked against the predictions made by the investment banks. The Confidence Challenge requires competitors to assign a level of confidence to each prediction - a competitor's score is weighted by their level of confidence. Competitors enter both challenges with a single submission. There are more details on the submission instructions page and the evaluation page.\n\nThe competition closes just before the first game kicks off on June 11th.\n\nWhat is your incentive to enter?\n\nThe winner of each challenge wins $USD100 to bet on the winner of the FIFA Golden Ball award. However, far more enticing is the opportunity take on some of the best brains at the world's most venerable investment banks.",
        "dataset_text": "We have provided a small dataset including: FIFA rankings back to 1994, countries' historical records, continent and host country information.\n\nYou are welcome to augment the dataset. Here are some links to data that might be helpful:\nBetting market data - http://www.oddschecker.com/football/internationals/world-cup/world-cup/world-cup/win-market/\nElo ratings - http://www.eloratings.net/world.html\nSalary data - http://sports.yahoo.com/soccer/blog/sow_experts/post/The-top-50-footballer-salaries-this-season?urn=sow,220011\nHistorical World Cup data - http://www.fifa.com/worldcup/archive/germany2006/statistics/index.html\nSocioeconomic data - http://data.worldbank.org/data-catalog\n\nWe encourage you to share data on the competition's forum."
    },
    {
        "name": "World Cup 2010 - Confidence Challenge",
        "url": "https://www.kaggle.com/competitions/worldcupconf",
        "overview_text": "Overview text not found",
        "description_text": "We are also running a World Cup 2010 - Take on the Quants Challenge.\n\nThe Confidence Challenge requires competitors to predict how far each country will progress through the World Cup and then assign a level of confidence to each prediction. A competitor's score is weighted by their level of confidence. There are more details on the submission instructions page and the evaluation page.\n\nThe competition closes just before the first game kicks off on June 11th.\n\nWhat is your incentive to enter?\n\nThe winner of this challenge wins $USD100 to bet on the winner of the FIFA Golden Ball award.",
        "dataset_text": "We have provided a small dataset including: FIFA rankings back to 1994, countries' historical records, continent and host country information.\n\nYou are welcome to augment the dataset. Here are some links to data that might be helpful:\nBetting market data - http://www.oddschecker.com/football/internationals/world-cup/world-cup/world-cup/win-market/\nElo ratings - http://www.eloratings.net/world.html\nSalary data - http://sports.yahoo.com/soccer/blog/sow_experts/post/The-top-50-footballer-salaries-this-season?urn=sow,220011\nHistorical World Cup data - http://www.fifa.com/worldcup/archive/germany2006/statistics/index.html\nSocioeconomic data - http://data.worldbank.org/data-catalog\n\nWe encourage you to share data on the competition's forum."
    },
    {
        "name": "INFORMS Data Mining Contest 2010",
        "url": "https://www.kaggle.com/competitions/informs2010",
        "overview_text": "Overview text not found",
        "description_text": "The INFORMS Data Mining Section (in conjunction with Sinapse) is pleased to announce its third annual data mining contest. This contest requires participants to develop a model that predicts stock price movements at five minute intervals. Competitors will be provided with intraday trading data showing stock price movements at five minute intervals, sectoral data, economic data, experts' predictions and indices. (We don\u2019t reveal the underlying stock to prevent competitors from looking up the answers.) Being able to better predict short-term stock price movements would be a boon for high-frequency traders, so the methods developed in this contest could have a big impact on the finance industry. We have provided a training database to allow participants to build their predictive models. Participants will submit their predictions for the test database (which doesn't include the variable being predicted). The public leaderboard will be calculated based on 10 per cent of the test dataset. See methods/techniques used by the top three competitors here. The winners of this contest will be honoured at a session of the INFORMS Annual Meeting in Austin-Texas (November 7-10).",
        "dataset_text": "Dataset description not found"
    },
    {
        "name": "Eye Movements Verification and Identification Competition",
        "url": "https://www.kaggle.com/competitions/emvic",
        "overview_text": "Overview text not found",
        "description_text": "The aim of the contest is to determine how people may be identified based on their eye movement characteristic. The organizers provide all interested participants dataset of eye movements' recordings in CSV format. After downloading the training dataset, participants may analyze it to prepare their own classification models and try to classify sapmles in test dataset. This in an official competition for BTAS 2012 (The Fifth IEEE International Conference on Biometrics: Theory, Applications and Systems, September 23-27, Washington DC, USA) and all results will be published during that conference (and of course on this web page as well). To become a participant you don't need to have any special eye tracking equipment. All data needed is ready to download! You only need to have some experience in data classification and... take your chance.",
        "dataset_text": "Code for benchmarks Dataset is stored in simple CSV format where first column is classification (0 or 1) and all other columns are values obtained from eye tracker.  The dataset consists of 978 samples from 37 subjects. Every sample is labeled with 1 (it belongs to one chosen specific person) or 0 (it belongs to someone else). Samples were taken with 250Hz frequency using Ober2 eye tracker. As it was 2048 measures taken, the whole measurements lasted 8192 ms. There was a jumping point on 3x3 matrix used as stimulus. The stimulus consists of eleven point position changes giving twelve consecutive point positions. First point appears in the middle of the screen and the person should look at it with eyes positioned directly ahead. After 1600 ms the point in the middle disappears and for 20 ms a screen is blank. In that time eyes are in instable state waiting for another point of interest. Then the point appears in the upper right corner. The flashing point on the blank screen attracts eyes attention even without the person's will. The 'jumps' of the point continue until the last point position in the middle of the screen is reached.  Datasets downloadable for competition are available in CSV format. It is a text file with one line for every sample. Every line is a list of comma separated elements as follows: The values are 0 for point in the middle, positive for point on the right or upper side of the screen and negative for points on the left or lower side of the screen. http://www.kasprowski.pl/emvic/stimFile.txt The dataset was collected at Silesian University of Technology, Poland by Dr. Pawe\u0142 Kasprowski. All data is published for purpose of competition only. However, if you intend to use the data in your future research you may do it only if the databases are acknowledged with the following reference: KASPROWSKI, P., OBER, J. 2004. Eye Movement in Biometrics, In Proceedings of Biometric Authentication Workshop, European Conference on Computer Vision in Prague 2004, LNCS 3087, Springer-Verlag.the IEEE/IARP International Conference on Biometrics (ICB), pp. 1-8."
    },
    {
        "name": "Million Song Dataset Challenge",
        "url": "https://www.kaggle.com/competitions/msdchallenge",
        "overview_text": "Overview text not found",
        "description_text": "The Million Song Dataset Challenge aims at being the best possible offline evaluation of a music recommendation system.  Any type of algorithm can be used: collaborative filtering, content-based methods, web crawling, even human oracles!* By relying on the Million Song Dataset, the data for the competition is completely open: almost everything is known and possibly available. What is the task in a few words? You have: 1) the full listening history for 1M users, 2) half of the listening history for 110K users (10K validation set, 100K test set), and you must predict the missing half. How much easier can it get? The most straightforward approach to this task is pure collaborative filtering, but remember that there is a wealth of information available to you through the Million Song Dataset. Go ahead, explore!  If you have questions, we recommend that you consult the MSD Mailing List. Ready to start recommending?  Read through our Getting Started tutorial. You can also look at this open-source solution offered by a contestant. For a more technical introduction to the MSD Challenge, see our AdMIRe paper. (Please use this following citation when referring to the contest in an academic setting.) * This contest is for computer models, but if you manage to get recommendations from humans for 110K listeners, we'd like to know how!   The Million Song Dataset Challenge is a joint effort between the Computer Audition Lab at UC San Diego and LabROSA at Columbia University. The user data for the challenge, like much of the data in the Million Song Dataset, was generously donated by The Echo Nest, with additional data contributed by SecondHandSongs, musiXmatch, and Last.fm. Follow-up evaluations will be conducted by IMIRSEL at the Graduate School of Library Information Science at UIUC as part of the Music Information Retrieval Evaluation eXchange (MIREX).  ",
        "dataset_text": "The files above contain: The half listening histories provided here are enough to get you started, but to leverage all the data available (in particular full listening histories for 1M users), you need to visit the Million Song Dataset (MSD) website, details below. The core data is the Taste Profile Subset released by The Echo Nest as part of the Million Song Dataset. It consists of triplets (user ID, song ID, play count). The data is split in two: Needless to say, the test set and the train set users are not overlapping. The metadata and audio features (among other things) for all songs are available through the Million Song Dataset. It is difficult to summarize the amount of information accessible to you, but here are a few pointers: Mapping from song to tracks: most MSD data is indexed by track, but the Taste Profile data is based on songs. There is a difference in The Echo Nest world, but you can ignore it at first. To go from song IDs to track IDs, use the file 'taste_profile_song_to_tracks.txt'. CAREFUL! Some songs map to more than one track, and a few songs don't have a corresponding track in the MSD. If you're curious about matching issues, read this blog post."
    },
    {
        "name": "Large Scale Hierarchical Text Classification",
        "url": "https://www.kaggle.com/competitions/lshtc",
        "overview_text": "Overview text not found",
        "description_text": "We are pleased to announce the 4th edition of the Large Scale Hierarchical Text Classification (LSHTC) Challenge. The LSHTC Challenge is a hierarchical text classification competition, using very large datasets.  Hierarchies are becoming ever more popular for the organization of text documents, particularly on the Web. Web directories and Wikipedia are two examples of such hierarchies. Along with their widespread use comes the need for automated classification of new documents to the categories in the hierarchy. As the size of the hierarchy grows and the number of documents to be classified increases, a number of interesting machine learning problems arise. In particular, it is one of the rare situations where data sparsity remains an issue, despite the vastness of available data: as more documents become available, more classes are also added to the hierarchy, and there is a very high imbalance between the classes at different levels of the hierarchy. Additionally, the statistical dependence of the classes poses challenges and opportunities for new learning methods. The challenge is based on a large dataset created from Wikipedia. The dataset is multi-class, multi-label and hierarchical. The number of categories is roughly 325,000 and number of the documents is 2,400,000. This challenge builds upon a series of successful challenges on large-scale hierarchical text classification. More information can be found at http://lshtc.iit.demokritos.gr/ This track concerns multi-label classification based on the Wikipedia dataset. The hierarchy is a graph that can have cycles.  The number of categories is roughly 325,000 and the number of documents is 2,400,000. A document can appear in multiple classes. Ioannis Partalas, LIG, Grenoble, France Massih-Reza Amini, LIG, Grenoble, France Ion Androutsopoulos, AUEB, Athens, Greece Thierry Arti\u00e8res, LIP6, Paris, France Nicolas Baskiotis, LIP6, Paris, France Patrick Gallinari, LIP6, Paris, France Eric Gaussier, LIG, Grenoble, France Aris Kosmopoulos, NCSR \"Demokritos\" & AUEB, Athens, Greece George Paliouras, NCSR \"Demokritos\", Athens, Greece Class-Y ANR project, University of Grenoble, University of Pierre and Marie Curie, NCSR \"Demokritos\", and Athens University of Economics and Business. We would also like to thank the Kaggle team for their support.",
        "dataset_text": "The hierarchy file contains the information regarding the hierarchy of classes. Each line of this file is a relation between a parent and a child node. For example, the line: is to be read as node 897 is parent of node 67 The format of each data file follows the libSVM format. Each line corresponds to a sparse document vector and has the following format: label is an integer and corresponds to the category to which the document vector belongs. Each document vector may belong to more than one category. The pair feat:value corresponds to a non-zero feature with index feat and value value. feat is an integer representing a term and value is a double that corresponds to the weight (tf) of the term in the document. For example: corresponds to a document vector whose features are all zeros except feature number 8 (with value 1) and feature number 18 (with value 2). This document vector belongs to categories 545 and 32. Each feature number is associated to a stemmed word. The labels of the test document vectors are set to 0."
    },
    {
        "name": "Multi-label Bird Species Classification - NIPS 2013",
        "url": "https://www.kaggle.com/competitions/multilabel-bird-species-classification-nips2013",
        "overview_text": "Overview text not found",
        "description_text": "The Neural Information Processing Scaled for Bioacoustics (NIPS4B) bird song competition asks participants to identify which of 87 sound classes of birds and their ecosystem are present in 1000 continuous wild recordings from different places in Provence, France. The data is provided by the BIOTOPE society, which maintains the largest collection of wild recordings of birds in Europe. This challenge is a more complex task than the previous ICML4B challenge, in which 77 teams participated (see proceedings at sabiod.org). For more information about the Neural Information Processing Scaled for Bioacoustics workshop, please visit the official site. Pr. H. Glotin - Institut Universitaire de France, CNRS LSIS and USTV, glotin@univ-tln.fr O. Dufour - CNRS LSIS, FR Dr. Y. Bas - BIOTOPE, FR",
        "dataset_text": "The training set contains 687 files. Each species is represented by nearly 10 training files (within various context / other species). The files were recorded at a frequency sample of 44.1 kHz on an SM2 system. The test set is composed of 1000 files. All species in the test set are in the training set. The training set matches the test set conditions. Provided here are two sample clips: The histogram of train and test file durations is here:  For some species we discriminate the song from the call. We have also included species which are not birds: 7 insects and a batracian. Each of the 87 classes should be predicted in the 1000 test files. Some training files are empty (background noise only, called 'empty class') to tune your model. This class is not to be predicted. We have also provided baseline features on the train and test .wav files. These are the optimized MFCC features, as described in the ICML4B 2013 bird challenge. The format is a matrix 17xN: 17 cepstral coefficients x N frames (frame size 11.6 ms, frame shift 3.9 ms, one line per frame). You may compute their speed and acceleration by simple line differences. These suggested features minimize the signal reconstruction error on average in each bird species. The script which produced these MFCC is: MFCC SCRIPT for BIRD SOUND REPRESENTATION, as defined in (Dufour et al. ICML4B.pdf & .bib) in ICML4B2013 proceedings (please cite if you use these features)."
    },
    {
        "name": "MLSP 2014 Schizophrenia Classification Challenge",
        "url": "https://www.kaggle.com/competitions/mlsp-2014-mri",
        "overview_text": "Overview text not found",
        "description_text": "Schizophrenia is a severe and disabling mental illnesses which has no well-established, non-invasive diagnosis biomarker. Currently, due to its symptom overlap with other mental illnesses (like bipolar disorder) it can only be diagnosed subjectively, by process of elimination. This competition invites you to automatically diagnose subjects with schizophrenia based on multimodal features derived from their brain magnetic resonance imaging (MRI) scans. The features made available in this competition are a result from current state-of-the art developments in neuroimaging and MRI data processing. Two modalities of MRI scans are used to obtain these features: functional and structural MRI. One challenge in this competition is how to optimally combine this type of multimodal information and select features that enhance diagnosis. Optional additional information is provided that could be helpful with this particular aspect of the task. This is an official competition of the IEEE International Workshop on Machine Learning for Signal Processing (MLSP 2014) Collection of this dataset was made at the Mind Research Network under an NIH NIGMS Centers of Biomedical Research Excellence (COBRE) grant 5P20RR021938/P20GM103472 to Vince Calhoun (PI).",
        "dataset_text": "To discourage certain forms of cheating (such as hand labeling) we have inflated the number of rows in the test set to create a much larger data sample. The extra rows will be ignored in the scoring and their presence will not affect the scoring. Your submission file must provide a prediction for each subject in the test set. Functional Network Connectivity (FNC) are correlation values that summarize the overall connection between independent brain maps over time. Therefore, the FNC feature gives a picture of the connectivity pattern over time between independent networks (or brain maps). The provided FNC information was obtained from functional magnetic resonance imaging (fMRI) from a set of schizophrenic patients and healthy controls at rest, using group independent component analysis (GICA). The GICA decomposition of the fMRI data resulted in a set of brain maps, and corresponding timecourses. These timecourses indicated the activity level of the corresponding brain map at each point in time. The FNC feature are the correlations between these timecourses. In a way, FNC indicates a subject's overall level of 'synchronicity' between brain areas. Because this information is derived from functional MRI scans, FNCs are considered a functional modality feature (i.e., they describe patterns of the brain function). More about FNCs can be found here: FNC paper. Source-Based Morphometry (SBM) loadings correspond to the weights of brain maps obtained from the application of independent component analysis (ICA) on the gray-matter concentration maps of all subjects. Gray-matter corresponds to the outer-sheet of the brain; it is the brain region in which much of the brain signal processing actually occurs. In a way, the concentration of gray-matter is indicative of the \"computational power\" available in a certain region of the brain. Processing gray-matter concentration maps with ICA yields independent brain maps whose expression levels (i.e., loadings) vary across subjects. Simply put, a near-zero loading for a given ICA-derived brain map indicates that the brain regions outlined in that map are lowly present in the subject (i.e., the gray-matter concentration in those regions are very low in that subject). Because this information is derived from structural MRI scans, SBM loadings are considered a structural modality feature (i.e., they describe patterns of the brain structure). More about SBM loadings can be found here: SBM paper. In order to enable more principled multimodal feature selection and combination strategies, we provide files that contain the actual brain maps to which the FNC and SBM features refer to. MATLAB and R support functions have also been provided to help loading and handling the additional data. Below is a description of each additional, non-essential file:"
    },
    {
        "name": "NIPS 2017: Non-targeted Adversarial Attack",
        "url": "https://www.kaggle.com/competitions/nips-2017-non-targeted-adversarial-attack",
        "overview_text": "Overview text not found",
        "description_text": "This research competition doesn't follow Kaggle's normal submission process. See the Submission Format tab for more details. Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. To accelerate research on adversarial examples, Google Brain is organizing Competition on Adversarial Attacks and Defenses within the NIPS 2017 competition track. The competition on Adversarial Attacks and Defenses consist of three sub-competitions: In each of the sub-competitions you're invited to make and submit a program which solves the corresponding task. In the end of the competition we will run all attacks against all defenses to evaluate how each of the attacks performs against each of the defenses. Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, rules, quality, or topic will be addressed by them.",
        "dataset_text": "Two helpful datasets are available through Kaggle Datasets (link these datasets if you're using kernels): dev_toolkit.zip is a development toolkit for the competition, which includes: Development toolkit is also available on GitHub as a part of CleverHans library\nfor adversarial training. sample_targeted_attack_submission.zip is an example of a valid submission."
    },
    {
        "name": "NIPS 2017: Targeted Adversarial Attack",
        "url": "https://www.kaggle.com/competitions/nips-2017-targeted-adversarial-attack",
        "overview_text": "Overview text not found",
        "description_text": "This research competition doesn't follow Kaggle's normal submission process. See the Submission Format tab for more details. Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. To accelerate research on adversarial examples, Google Brain is organizing Competition on Adversarial Attacks and Defenses within the NIPS 2017 competition track. The competition on Adversarial Attacks and Defenses consist of three sub-competitions: In each of the sub-competitions you're invited to make and submit a program which solves the corresponding task. In the end of the competition we will run all attacks against all defenses to evaluate how each of the attacks performs against each of the defenses. Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, rules, quality, or topic will be addressed by them.",
        "dataset_text": "Two helpful datasets are available through Kaggle Datasets (link these datasets if you're using kernels): dev_toolkit.zip is a development toolkit for the competition, which includes: Development toolkit is also available on GitHub as a part of CleverHans library\nfor adversarial training. sample_targeted_attack_submission.zip is an example of a valid submission."
    },
    {
        "name": "NIPS 2017: Defense Against Adversarial Attack",
        "url": "https://www.kaggle.com/competitions/nips-2017-defense-against-adversarial-attack",
        "overview_text": "Overview text not found",
        "description_text": "This research competition doesn't follow Kaggle's normal submission process. See the Submission Format tab for more details. Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. To accelerate research on adversarial examples, Google Brain is organizing Competition on Adversarial Attacks and Defenses within the NIPS 2017 competition track. The competition on Adversarial Attacks and Defenses consist of three sub-competitions: In each of the sub-competitions you're invited to make and submit a program which solves the corresponding task. In the end of the competition we will run all attacks against all defenses to evaluate how each of the attacks performs against each of the defenses. Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, rules, quality, or topic will be addressed by them.",
        "dataset_text": "Two helpful datasets are available through Kaggle Datasets (link these datasets if you're using kernels): dev_toolkit.zip is a development toolkit for the competition, which includes: Development toolkit is also available on GitHub as a part of CleverHans library\nfor adversarial training. sample_defense_submission.zip is an example of a valid submission."
    },
    {
        "name": "iMaterialist (Fashion) 2019 at FGVC6",
        "url": "https://www.kaggle.com/competitions/imaterialist-fashion-2019-FGVC6",
        "overview_text": "Overview text not found",
        "description_text": "Designers know what they are creating, but what, and how, do people really wear their products? What combinations of products are people using? In this competition, we challenge you to develop algorithms that will help with an important step towards automatic product detection \u2013 to accurately assign segmentations and attribute labels for fashion images. Visual analysis of clothing is a topic that has received increasing attention in recent years. Being able to recognize apparel products and associated attributes from pictures could enhance the shopping experience for consumers, and increase work efficiency for fashion professionals. We present a new clothing dataset with the goal of introducing a novel fine-grained segmentation task by joining forces between the fashion and computer vision communities. The proposed task unifies both categorization and segmentation of rich and complete apparel attributes, an important step toward real-world applications.  While early work in computer vision addressed related clothing recognition tasks, these are not designed with fashion insiders\u2019 needs in mind, possibly due to the research gap in fashion design and computer vision. To address this, we first propose a fashion taxonomy built by fashion experts, informed by product description from the internet. To capture the complex structure of fashion objects and ambiguity in descriptions obtained from crawling the web, our standardized taxonomy contains 46 apparel objects (27 main apparel items and 19 apparel parts), and 92 related fine-grained attributes. Secondly, a total of 50K clothing images (10K with both segmentation and fine-grained attributes, 40K with apparel instance segmentation) in daily-life, celebrity events, and online shopping are labeled by both domain experts and crowd workers for fine-grained segmentation. Individuals/Teams with top submissions will be invited to present their work live at the FGVC6 workshop at the Conference on Computer Vision and Pattern Recognition (CVPR) 2019 Checkout the iMaterialist-Fashion Competition Github repo for the specifics of the dataset. The iMat-Fashion Challenge 2019 is sponsored by Google AI, CVDF, Samasource and Fashionpedia. ",
        "dataset_text": "In this dataset, you are provided a large number of images and corresponding fashion/apparel segmentations. Images are named with a unique ImageId. You must segment and classify the images in the test set. All of the test set images have fine-grained segmentations. The training set contains images both with and without fine-grained class segmentations. This dataset contains images of people wearing a variety of clothing types in a variety of poses. It may have content that some consider sensitive. Please exercise discretion and maturity when working on the competition."
    },
    {
        "name": "iMet Collection 2019 - FGVC6",
        "url": "https://www.kaggle.com/competitions/imet-2019-fgvc6",
        "overview_text": "Overview text not found",
        "description_text": "The Metropolitan Museum of Art in New York, also known as The Met, has a diverse collection of over 1.5M objects of which over 200K have been digitized with imagery. The online cataloguing information is generated by Subject Matter Experts (SME) and includes a wide range of data. These include, but are not limited to: multiple object classifications, artist, title, period, date, medium, culture, size, provenance, geographic location, and other related museum objects within The Met\u2019s collection. While the SME-generated annotations describe the object from an art history perspective, they can also be indirect in describing finer-grained attributes from the museum-goer\u2019s understanding. Adding fine-grained attributes to aid in the visual understanding of the museum objects will enable the ability to search for visually related objects. This is an FGVCx competition hosted as part of the FGVC6 workshop at CVPR 2019. View the github page for more details.",
        "dataset_text": "In this dataset, you are presented with a large number of artwork images and associated attributes of the art. Multiple modalities can be expected and the camera sources are unknown. The photographs are often centered for objects, and in the case where the museum artifact is an entire room, the images are scenic in nature. Each object is annotated by a single annotator without a verification step. Annotators were advised to add multiple labels from an ontology provided by The Met, and additionally are allowed to add free-form text when they see fit. They were able to view the museum's online collection pages and advised to avoid annotating labels already present. The attributes can relate to what one \"sees\" in the work or what one infers as the object's \"utility.\" While we have made efforts to make the attribute labels as high quality as possible, you should consider these annotations noisy. There may be a small number of attributes with similar meanings. The competition metric, F2 score, was intentionally chosen to provide some robustness against noisy labels, favoring recall over precision. This is a kernels-only competition with two stages. After the deadline, Kaggle will rerun your selected kernels on an unseen test set. The second-stage test set is approximately five times the size of the first. You should plan your kernel's memory, disk, and runtime footprint accordingly. The filename of each image is its id."
    },
    {
        "name": "Halite by Two Sigma",
        "url": "https://www.kaggle.com/competitions/halite",
        "overview_text": "Overview text not found",
        "description_text": "Ahoy there! There's halite to be had and ships to be deployed! Are you ready to navigate the skies and secure your territory? Halite by Two Sigma (\"Halite\") is a resource management game where you build and control a small armada of ships. Your algorithms determine their movements to collect halite, a luminous energy source. The most halite at the end of the match wins, but it's up to you to figure out how to make effective and efficient moves. You control your fleet, build new ships, create shipyards, and mine the regenerating halite on the game board.  Created by Two Sigma in 2016, more than 15,000 people around the world have participated in a Halite challenge. Players apply advanced algorithms in a dynamic, open source game setting. The strategic depth and immersive, interactive nature of Halite games make each challenge a unique learning environment. Halite IV builds on the core game design of Halite III with a number of key changes that shift the focus of the game towards tighter competition on a smaller board. New game features include regenerating halite, shipyard creation, no more ship movement costs, and stealing halite from other players! So dust off your halite meters and fasten your seatbelts. The fourth season of Halite is about to begin!",
        "dataset_text": "Dataset description not found"
    },
    {
        "name": "Google Cloud & NCAA\u00ae ML Competition 2020-NCAAM",
        "url": "https://www.kaggle.com/competitions/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament",
        "overview_text": "Overview text not found",
        "description_text": "As a result of the continued collaboration between Google Cloud and the NCAA\u00ae, the seventh annual Kaggle-backed March Madness\u00ae competition is underway! Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness during this year's NCAA Division I Men\u2019s and Women\u2019s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA\u2019s historical data and your computing power, while the ground truth unfolds on national television.  In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible matchups in the 2020 NCAA Division I Men\u2019s and Women\u2019s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2020 results. As the official public cloud provider of the NCAA, Google is proud to provide a competition to help participants strengthen their knowledge of basketball, statistics, data modeling, and cloud technology. As part of its journey to the cloud, the NCAA has migrated 80+ years of historical and play-by-play data, from 90 championships and 24 sports, to Google Cloud Platform (GCP). The NCAA has tapped into decades of historical basketball data using BigQuery, Cloud Spanner, Datalab, Cloud Machine Learning and Cloud Dataflow, to power the analysis of team and player performance. The mission of the NCAA has long been about serving the needs of schools, their teams and students. Google Cloud is proud to support that mission by helping the NCAA use data and machine learning to better engage with its millions of fans, 500,000 student-athletes, and more than 19,000 teams. Game on! This page is for the NCAA Division I Men's tournament. Check out the NCAA Division I Women's tournament here. If you want to extend your analysis then try out our Analytics Competition here ",
        "dataset_text": "Each season there are thousands of NCAA basketball games played between Division I men's teams, culminating in March Madness\u00ae, the 68-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness\u00ae game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past. If you are unfamiliar with the format and intricacies of the NCAA\u00ae tournament, we encourage reading the wikipedia page before diving into the data. The data description and schema may seem daunting at first, but is not as complicated as it appears. As a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The MTeamSpellings file, which is listed in the bottom section below, may help you map external team references into our own Team ID structure. You will probably also need to understand exactly how dates work in our data. Remember as well that you are required to disclose your external sources of data prior to the start of the tournament Please also note that we have standardized the spelling of column names and filenames, so if you are re-using code from previous instances of this contest, you may need to adjust for this. All of the files that are specific to the men\u2019s contest now have a filename prefix of M, so for instance this year the teams file is named MTeams rather than just Teams. We extend our gratitude to Kenneth Massey for providing much of the historical data. Special Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition. Stage 1 - You should submit predicted probabilities for every possible matchup in the past 5 NCAA\u00ae tournaments (seasons 2015-2019). Stage 2 - You should submit predicted probabilities for every possible matchup before the 2020 tournament begins. Refer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict. Below we describe the format and fields of the contest data files. The data will likely be refreshed once in early March while Stage 1 of the competition is running. Many of the files are only complete through the end of last season. At the start of Stage 2, we will provide updates to these files to incorporate data from the current season. Right now the MSeasons and MTeamConferences files are the only two files that reference the 2020 season, since those are the only files where the final 2020 data is already finalized; the other files will be updated later on when we provide preliminary data (in early March) and final Stage 2 data (after Selection Sunday) for the current season. This section provides everything you need to build a simple prediction model and submit predictions. Special note about \"Season\" numbers: the college basketball season lasts from early November until the national championship tournament that starts in the middle of March. For instance, this year the first men\u2019s Division I games were played on November 5th, 2019 and the men\u2019s national championship game will be played on April 6th, 2020. Because a basketball season spans two calendar years like this, it can be confusing to refer to the year of the season. By convention, when we identify a particular season, we will reference the year that the season ends in, not the year that it starts in. So for instance, the current season will be identified in our data as the 2020 season, not the 2019 season or the 2019-20 season or the 2019-2020 season, though you may see any of these in everyday use outside of our data. Data Section 1 file: MTeams.csv This file identifies the different college teams present in the dataset. Each school is uniquely identified by a 4 digit id number. You will not see games present for all teams in all seasons, because the games listing is only for matchups where both teams are Division-I teams. There are 353 teams currently in Division-I, and an overall total of 367 teams in our team listing. Each year, some teams might start being Division-I programs, and others might stop being Division-I programs, and in fact this year is no exception. Savannah State (TeamID=1366) is no longer part of Division I and so their data in our dataset stops after the 2019 season, and they are no longer part of the Mid-Eastern Athletic Conference. To balance this out, Merrimack (TeamID=1467) has just moved to Division I this year, so they show up as a new team in the data and they are now part of the America East Conference. Other than the teams file and team conferences file, you won't see any data for Merrimack until preliminary 2020 season data starts coming in, as we approach Stage 2 of the contest. Data Section 1 file: MSeasons.csv This file identifies the different seasons included in the historical data, along with certain season-level properties. Data Section 1 file: MNCAATourneySeeds.csv This file identifies the seeds for all teams in each NCAA\u00ae tournament, for all seasons of historical data. Thus, there are between 64-68 rows for each year, depending on whether there were any play-in games and how many there were. In recent years the structure has settled at 68 total teams, with four \"play-in\" games leading to the final field of 64 teams entering Round 1 on Thursday of the first week (by definition, that is DayNum=136 each season). We will not know the seeds of the respective tournament teams, or even exactly which 68 teams it will be, until Selection Sunday on March 15, 2020 (DayNum=132). Data Section 1 file: MRegularSeasonCompactResults.csv This file identifies the game-by-game results for many seasons of historical data, starting with the 1985 season (the first year the NCAA\u00ae had a 64-team tournament). For each season, the file includes all games played from DayNum 0 through 132. It is important to realize that the \"Regular Season\" games are simply defined to be all games played on DayNum=132 or earlier (DayNum=132 is Selection Sunday, and there are always a few conference tournament finals actually played early in the day on Selection Sunday itself). Thus a game played on or before Selection Sunday will show up here whether it was a pre-season tournament, a non-conference game, a regular conference game, a conference tournament game, or whatever. Data Section 1 file: MNCAATourneyCompactResults.csv This file identifies the game-by-game NCAA\u00ae tournament results for all seasons of historical data. The data is formatted exactly like the MRegularSeasonCompactResults data. All games will show up as neutral site (so WLoc is always N). Note that this tournament game data also includes the play-in games (which always occurred on day 134/135) for those years that had play-in games. Thus each season you will see between 63 and 67 games listed, depending on how many play-in games there were. Because of the consistent structure of the NCAA\u00ae tournament schedule, you can actually tell what round a game was, depending on the exact DayNum. Thus: Special note: Each year, there are also going to be other games that happened after Selection Sunday, which are not part of the NCAA\u00ae Tournament. This includes tournaments like the postseason NIT, the CBI, the CIT, and the Vegas 16. Such games are not listed in the Regular Season or the NCAA\u00ae Tourney files; they can be found in the \"Secondary Tourney\" data files within Data Section 6. Although they would not be games you would ever be predicting directly for the NCAA\u00ae tournament, and they would not be games you would have data from at the time of predicting NCAA\u00ae tournament outcomes, you may nevertheless wish to make use of these games for model optimization, depending on your methodology. The more games that you can test your predictions against, the better your optimized model might eventually become, depending on how applicable all those games are. A similar argument might be advanced in favor of optimizing your predictions against conference tournament games, which might be viewed as reasonable proxies for NCAA\u00ae tournament games.  Data Section 1 file: MSampleSubmissionStage1.csv This file illustrates the submission file format for Stage 1. It is the simplest possible submission: a 50% winning percentage is predicted for each possible matchup. A submission file lists every possible matchup between tournament teams for one or more years. During Stage 1, you are asked to make predictions for all possible matchups from the past five NCAA\u00ae tournaments (seasons 2015, 2016, 2017, 2018, 2019). In Stage 2, you will be asked to make predictions for all possible matchups from the current NCAA\u00ae tournament (season 2020). When there are 68 teams in the tournament, there are 68*67/2=2,278 predictions to make for that year, so a Stage 1 submission file will have 2,278*5=11,390 data rows. Example #1: You want to make a prediction for Duke (TeamID=1181) against Arizona (TeamID=1112) in the 2017 tournament, with Duke given a 53% chance to win and Arizona given a 47% chance to win. In this case, Arizona has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Arizona's perspective (47%): 2017_1112_1181,0.47 Example #2: You want to make a prediction for Duke (TeamID=1181) against North Carolina (TeamID=1314) in the 2018 tournament, with Duke given a 51.6% chance to win and North Carolina given a 48.4% chance to win. In this case, Duke has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Duke's perspective (51.6%): 2018_1181_1314,0.516 Also note that a single prediction row serves as a prediction for each of the two teams' winning chances. So for instance, in Example #1, the submission row of \"2017_1112_1181,0.47\" specifically gives a 47% chance for Arizona to win, and doesn't explicitly mention Duke's 53% chance to win. However, our evaluation utility will automatically infer the winning percentage in the other direction, so a 47% prediction for Arizona to win also means a 53% prediction for Duke to win. And similarly, because the submission row in Example #2 gives Duke a 51.6% chance to beat North Carolina, we will automatically figure out that this also means North Carolina has a 48.4% chance to beat Duke. This section provides game-by-game stats at a team level (free throws attempted, defensive rebounds, turnovers, etc.) for all regular season, conference tournament, and NCAA\u00ae tournament games since the 2002-03 season.  Team Box Scores are provided in \"Detailed Results\" files rather than \"Compact Results\" files. However, the two files are strongly related. In a Detailed Results file, the first eight columns (Season, DayNum, WTeamID, WScore, LTeamID, LScore, WLoc, and NumOT) are exactly the same as a Compact Results file. However, in a Detailed Results file, there are many additional columns. The column names should be self-explanatory to basketball fans (as above, \"W\" or \"L\" refers to the winning or losing team): (and then the same set of stats from the perspective of the losing team: LFGM is the number of field goals made by the losing team, and so on up to LPF). Note: by convention, \"field goals made\" (either WFGM or LFGM) refers to the total number of fields goals made by a team, a combination of both two-point field goals and three-point field goals. And \"three point field goals made\" (either WFGM3 or LFGM3) is just the three-point fields goals made, of course. So if you want to know specifically about two-point field goals, you have to subtract one from the other (e.g., WFGM - WFGM3). And the total number of points scored is most simply expressed as 2*FGM + FGM3 + FTM. Data Section 2 file: MRegularSeasonDetailedResults.csv This file provides team-level box scores for many regular seasons of historical data, starting with the 2003 season. All games listed in the MRegularSeasonCompactResults file since the 2003 season should exactly be present in the MRegularSeasonDetailedResults file. Data Section 2 file: MNCAATourneyDetailedResults.csv This file provides team-level box scores for many NCAA\u00ae tournaments, starting with the 2003 season. All games listed in the MNCAATourneyCompactResults file since the 2003 season should exactly be present in the MNCAATourneyDetailedResults file. This section provides city locations of all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season Data Section 3 file: Cities.csv This file provides a master list of cities that have been locations for games played. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two contests. Also note that if you created any supplemental data last year on cities (latitude/longitude, altitude, etc.), the CityID's match between last year and this year, so you should be able to re-use that information. Data Section 3 file: MGameCities.csv This file identifies all games, starting with the 2010 season, along with the city that the game was played in. Games from the regular season, the NCAA\u00ae tourney, and other post-season tournaments, are all listed together. There should be no games since the 2010 season where the CityID is not known. Games from the 2009 season and before are not listed in this file. This section provides weekly team rankings for dozens of top rating systems - Pomeroy, Sagarin, RPI, ESPN, etc., since the 2002-2003 season Data Section 4 file: MMasseyOrdinals.csv This file lists out rankings (e.g. #1, #2, #3, ..., #N) of teams going back to the 2002-2003 season, under a large number of different ranking system methodologies. The information was gathered by Kenneth Massey and provided on his College Basketball Ranking Composite page. Note that a rating system is more precise than a ranking system, because a rating system can provide insight about the strength gap between two adjacently-ranked teams. A ranking system will just tell you who is #1 or who is #2, but a rating system might tell you whether the gap between #1 and #2 is large or small. Nevertheless, it can be hard to compare two different rating systems that are expressed in different scales, so it can be very useful to express all the systems in terms of their ordinal ranking (1, 2, 3, ..., N) of teams. Disclaimer: you ought to be careful about your methodology when using or evaluating these ranking systems. They are presented on a weekly basis, and given a consistent date on the Massey Composite page that typically is a Sunday; that is how the ranking systems can be compared against each other on this page. However, these systems each follow their own timeline and some systems may be released on a Sunday and others on a Saturday or Monday or even Tuesday. You should remember that if a ranking is released on a Tuesday, and was calculated based on games played through Monday, it will make the system look unusually good at predicting if you use that system to forecast the very games played on Monday that already inform the rankings. To avoid this methodological trap, we have typically used a conservative RankingDayNum of Wednesday to represent the rankings that were released at approximately the end of the weekend, a few days before, even though those rankings are represented on the composite page as being on a Sunday. For some of the older years, a more precise timestamp was known for each ranking system that allowed a more precise assignment of a RankingDayNum. By convention, the final pre-tournament rankings are always expressed as RankingDayNum=133, even though sometimes the rankings for individual systems are not released until Tuesday (DayNum=134) or even Wednesday or Thursday. If you decide to use some rankings from these Massey Ordinals to inform your predictions, be forewarned that we have no control over when they are released, and not all systems may turn out to be available in time to make pre-tournament predictions by our submission deadline. In such a situation, you may wish to use the rankings from DayNum=128 or you may need to dig into the details of the actual source of the rankings, by following the respective links on the Massey Composite Page. We may also be able to provide partial releases of the final pre-tournament Massey Ordinals on the forums, so that as systems come in on Monday or Tuesday you can use them right away. This section provides play-by-play event logs for more than 99.5% of each year's regular season, NCAA\u00ae tournament, and secondary tournament games since the 2014-15 season - including plays by individual players. This year we are transitioning to a different play-by-play source, which includes data since the 2014-2015 season rather than since the 2009-2010 season (that's what we had previously for men's data). However, we are now able to provide play-by-play for both men's and women's data, and there is locational play-by-play detail starting with games from the 2018-2019 season. This includes an X/Y location (ranging from 0 to 100 in each dimension) on the court for each shot attempt, turnover, and foul for many games, as well as an overall categorization of the area on the court that the shot or turnover or foul occurred in (inside left wing, outside right wing, under the basket, etc.) Some games in these recent seasons still lack the locational detail. The data from last year (2019 season) matches what you can expect for the current year (2020 season) as we approach the postseason. Despite the 99.5% coverage, there are still a few games missing annually, and we will try to bring those in as well, if possible. Data Section 5 file: MEvents2015.csv, MEvents2016.csv, MEvent2017.csv, MEvents2018.csv, MEvents2019.csv Each MEvents file lists the play-by-play event logs for more than 99.5% of games from that season. Each event is assigned to either a team or a single one of the team's players. Thus if a basket is made by one player and an assist is credited to a second player, that would show up as two separate records. The players are listed by PlayerID within the MPlayers.csv file. Event Types and Subtypes:   This is the diagram provided by the play-by-play source:    Here is a diagram of these regions on a typical court:  And you may also have noticed that the three-point-arc is different this year in college basketball. The width of the court is still 15.2 m (50 ft) and the length of the court is still 28.7 m (94 feet), but the distance to the arc has changed from 6.32 m (20 feet 9 inches) to 6.75 m (22.15 feet). Here is a diagram of the old court, which would apply for play-by-play in seasons 2019 and before:  And here is a diagram of the new court, which only applies for season 2020:  Data Section 5 file: MPlayers.csv Note: there are data collection errors within the events, in that they don't necessarily add up to the final stats for the game. In addition, the player name spellings may vary over the course of a season or career for the same player, which would lead to the same player showing up with different PlayerID values. Nevertheless, this was the highest quality data we could manage for play-by-play with the near-complete set of games. This section contains additional supporting information, including coaches, conference affiliations, alternative team name spellings, bracket structure, and game results for NIT and other postseason tournaments. Data Section 6 file: MTeamCoaches.csv This file indicates the head coach for each team in each season, including a start/finish range of DayNum's to indicate a mid-season coaching change. For scenarios where a team had the same head coach the entire season, they will be listed with a DayNum range of 0 to 154 for that season. For head coaches whose term lasted many seasons, there will be many rows listed, most of which have a DayNum range of 0 to 154 for the corresponding season. Data Section 6 file: Conferences.csv This file indicates the Division I conferences that have existed over the years since 1985. Each conference is listed with an abbreviation and a longer name. There has been no attempt to link up conferences who merged with other conferences, or whose names changed over time. Thus you will see, for instance, a \"Pacific-10\" conference up through the 2011 season, and then a \"Pacific-12\" conference starting in the 2012 season, and these look like different conferences in the data, even though it was still mostly the same teams. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two contests. However, the Team Conferences data differs slightly between men's and women's, so those files do have the prefixes. Data Section 6 file: MTeamConferences.csv This file indicates the conference affiliations for each team during each season. Some conferences have changed their names from year to year, and/or changed which teams are part of the conference. This file tracks this information historically. Data Section 6 file: MConferenceTourneyGames.csv This file indicates which games were part of each year's post-season conference tournaments (all of which finished on Selection Sunday or earlier), starting from the 2001 season. Many of these conference tournament games are held on neutral sites, and many of the games are played by tournament-caliber teams just a few days before the NCAA\u00ae tournament. Thus these games could be considered as very similar to NCAA\u00ae tournament games, and (depending on your methodology) may be of use in optimizing your predictions. However, this is NOT a new listing of games; these games are already present within the MRegularSeasonCompactResults and MRegularSeasonDetailedResults files. So this file simply helps you to identify which of the \"regular season\" games since the 2001 season were actually conference tournament games, in case that is useful information. Data Section 6 file: MSecondaryTourneyTeams.csv This file identifies the teams that participated in post-season tournaments other than the NCAA\u00ae Tournament (such events would run in parallel with the NCAA\u00ae Tournament). These are teams that were not invited to the NCAA\u00ae Tournament and instead were invited to some other tournament, of which the NIT is the most prominent tournament, but there have also been the CBI, CIT, and Vegas 16 (V16) at various points in recent years. Depending on your methodology, you might find it useful to have these additional game results, above and beyond what is available from the NCAA\u00ae Tournament results. Many of these teams, especially in the NIT, were \"bubble\" teams of comparable strength to several NCAA\u00ae Tournament invitees, and so these games may be of use in model optimization for predicting NCAA\u00ae Tournament results. Also note that this information could be determined just from inspecting the MSecondaryTourneyCompactResults file, but is presented in this file as well, for your convenience. Data Section 6 file: MSecondaryTourneyCompactResults.csv This file indicates the final scores for the tournament games of \"secondary\" post-season tournaments: the NIT, CBI, CIT, and Vegas 16. The detailed results (team box scores) have not been assembled for these games. For the most part, this file is exactly like other Compact Results listings, although it also has a column for Secondary Tourney. Also note that because these games are played after DayNum=132, they are NOT listed in the MRegularSeasonCompactResults file. Data Section 6 file: MTeamSpellings.csv This file indicates alternative spellings of many team names. It is intended for use in associating external spellings against our own TeamID numbers, thereby helping to relate the external data properly with our datasets. Over the years we have identified various external spellings of different team names (as an example, for Ball State we have seen \"ball st\", and \"ball st.\", and \"ball state\", and \"ball-st\", and \"ball-state\"). Other teams have had more significant changes to their names over the years; for example, \"Texas Pan-American\" and \"Texas-Rio Grande Valley\" are actually the same school. The current list is obviously not exhaustive, and we encourage participants to identify additional mappings and upload extended versions of this file to the forums. Data Section 6 file: MNCAATourneySlots This file identifies the mechanism by which teams are paired against each other, depending upon their seeds, as the tournament proceeds through its rounds. It can be of use in identifying, for a given historical game, what round it occurred in, and what the seeds/slots were for the two teams (the meaning of \"slots\" is described below). Because of the existence of play-in games for particular seed numbers, the pairings have small differences from year to year. You may need to know these specifics if you are trying to represent/simulate the exact workings of the tournament bracket. Data Section 6 file: MNCAATourneySeedRoundSlots.csv This file helps to represent the bracket structure in any given year. No matter where the play-in seeds are located, we can always know, for a given tournament seed, exactly what bracket slot they would be playing in, on each possible game round, and what the possible DayNum values would be for that round. Thus, if we know when a historical game was played, and what the team's seed was, we can identify the slot for that game. This can be useful in representing or simulating the tournament bracket structure."
    },
    {
        "name": "Google Cloud & NCAA\u00ae ML Competition 2020-NCAAW",
        "url": "https://www.kaggle.com/competitions/google-cloud-ncaa-march-madness-2020-division-1-womens-tournament",
        "overview_text": "Overview text not found",
        "description_text": "As a result of the continued collaboration between Google Cloud and the NCAA\u00ae, the seventh annual Kaggle-backed March Madness\u00ae competition is underway! Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness during this year's NCAA Division I Men\u2019s and Women\u2019s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA\u2019s historical data and your computing power, while the ground truth unfolds on national television. In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible matchups in the 2020 NCAA Division I Men\u2019s and Women\u2019s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2020 results. As the official public cloud provider of the NCAA, Google Cloud is proud to provide a competition to help participants strengthen their knowledge of basketball, statistics, data modeling, and cloud technology. As part of its journey to the cloud, the NCAA has migrated 80+ years of historical and play-by-play data, from 90 championships and 24 sports, to Google Cloud Platform (GCP). The NCAA has tapped into decades of historical basketball data using BigQuery, Cloud Spanner, Datalab, Cloud Machine Learning and Cloud Dataflow, to power the analysis of team and player performance. The mission of the NCAA has long been about serving the needs of schools, their teams and students. Google Cloud is proud to support that mission by helping the NCAA use data and machine learning to better engage with its millions of fans, 500,000 student-athletes and more than 19,000 teams. Game on! This page is for the NCAA Division I Women's tournament. Check out the NCAA Division I Men's tournament here. If you want to extend your analysis then try out our Analytics Competition here  ",
        "dataset_text": "Each season there are thousands of NCAA\u00ae basketball games played between Division I women's teams, culminating in March Madness, the 64-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness\u00ae game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past. If you are unfamiliar with the format and intricacies of the tournament, we encourage reading the wikipedia page before diving into the data. The data description and schema may seem daunting at first, but is not as complicated as it appears. As a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The WTeamSpellings file, which is listed in the bottom section below, may help you map external team references into our own Team ID structure. You will probably also need to understand exactly how dates work in our data. Remember as well that you are required to disclose your external sources of data prior to the start of the tournament. We extend our gratitude to Kenneth Massey for providing much of the historical data. Special Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition. Stage 1 - You should submit predicted probabilities for every possible matchup in the past 5 NCAA\u00ae tournaments (seasons 2015-2019). Stage 2 - You should submit predicted probabilities for every possible matchup before the 2020 tournament begins. Refer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict. Below we describe the format and fields of the contest data files. The data will likely be refreshed once in early March while Stage 1 of the competition is running. Many of the files are only complete through the end of last season. At the start of Stage 2, we will provide updates to these files to incorporate data from the current season. Right now the WSeasons and WTeamConferences files are the only two files that reference the 2020 season, since those are the only files where the final 2020 data is already finalized; the other files will be updated later on when we provide preliminary data (in early March) and final Stage 2 data (after Selection Sunday) for the current season. This section provides everything you need to build a simple prediction model and submit predictions. Special note about \"Season\" numbers: the college basketball season lasts from early November until the national championship tournament that starts in the middle of March. For instance, this year the first women\u2019s Division I games were played on November 5th, 2019 and the women\u2019s national championship game will be played on April 5th, 2020. Because a basketball season spans two calendar years like this, it can be confusing to refer to the year of the season. By convention, when we identify a particular season, we will reference the year that the season ends in, not the year that it starts in. So for instance, the current season will be identified in our data as the 2020 season, not the 2019 season or the 2019-20 season or the 2019-2020 season, though you may see any of these in everyday use outside of our data. Data Section 1 file: WTeams.csv This file identifies the different college teams present in the dataset. Each school is uniquely identified by a 4 digit id number. You will not see games present for all teams in all seasons, because the games listing is only for matchups where both teams are Division-I teams. There are 351 teams currently in Division-I, and an overall total of 365 teams in our team listing. Each year, some teams might start being Division-I programs, and others might stop being Division-I programs, and in fact this year is no exception. Savannah State (TeamID=3366) is no longer part of Division I and so their data in our dataset stops after the 2019 season, and they are no longer part of the Mid-Eastern Athletic Conference. To balance this out, Merrimack (TeamID=3467) has just moved to Division I this year, so they show up as a new team in the data and they are now part of the America East Conference. Other than the teams file and team conferences file, you won't see any data for Merrimack until preliminary 2020 season data starts coming in, as we approach Stage 2 of the contest. Also please note that we recently determined that VMI and Citadel only have men's basketball teams, not women's teams, and so we shouldn't have those schools in the women's data at all. This means all references to TeamID 3154 and TeamID 3440 have been removed from the women's contest data this year, and there are two fewer Division 1 teams this year because of this. Data Section 1 file: WSeasons.csv This file identifies the different seasons included in the historical data, along with certain season-level properties. The game dates in this dataset are expressed in relative terms, as the number of days since the start of the regular season, and aligned for each season so that day number #133 is the Monday right before the tournament, when team selections are made. During any given season, day number zero is defined to be exactly 19 weeks earlier than Selection Monday, so Day #0 is a Monday in late October or early November such that Day #132 is Selection Sunday (for the men's tournament) and Day #133 is Selection Monday (for the women's tournament). This doesn't necessarily mean that the regular season will always start exactly on day #0 or day #1; in fact, during the past decade, regular season games typically start being played on a Friday that is either Day #4 or Day #11, but further back there was more variety. Data Section 1 file: WNCAATourneySeeds.csv This file identifies the seeds for all teams in each NCAA\u00ae tournament, for all seasons of historical data. Thus, there are exactly 64 rows for each year, since there are no play-in teams in the women's tournament. We will not know the seeds of the respective tournament teams, or even exactly which 64 teams it will be, until Selection Monday on March 16, 2020 (DayNum=133). Data Section 1 file: WRegularSeasonCompactResults.csv This file identifies the game-by-game results for many seasons of historical data, starting with the 1998 season. For each season, the file includes all games played from DayNum 0 through 132. It is important to realize that the \"Regular Season\" games are simply defined to be all games played on DayNum=132 or earlier (DayNum=133 is Selection Monday). Thus a game played before Selection Monday will show up here whether it was a pre-season tournament, a non-conference game, a regular conference game, a conference tournament game, or whatever. Data Section 1 file: WNCAATourneyCompactResults.csv This file identifies the game-by-game NCAA\u00ae tournament results for all seasons of historical data. The data is formatted exactly like the WRegularSeasonCompactResults data. Each season you will see 63 games listed, since there are no women's play-in games. Although the scheduling of the men's tournament rounds has been consistent for many years, there has been more variety in the scheduling of the women's rounds. There have been four different schedules over the course of the past 20+ years for the women's tournament, as follows: 2017 season through 2020 season: 2015 season and 2016 season: 2003 season through 2014 season: 1998 season through 2002 season: Data Section 1 file: WSampleSubmissionStage1.csv This file illustrates the submission file format for Stage 1. It is the simplest possible submission: a 50% winning percentage is predicted for each possible matchup. A submission file lists every possible matchup between tournament teams for one or more years. During Stage 1, you are asked to make predictions for all possible matchups from the past five NCAA\u00ae tournaments (seasons 2015, 2016, 2017, 2018, and 2019). In Stage 2, you will be asked to make predictions for all possible matchups from the current NCAA\u00ae tournament (season 2020). Since there are 64 teams in the tournament, there are 64*63/2=2,016 predictions to make for that year, so a Stage 1 submission file will have 2,016*5=10,080 data rows. Example #1: You want to make a prediction for Duke (TeamID=3181) against Arizona (TeamID=3112) in the 2005 tournament, with Duke given a 53% chance to win and Arizona given a 47% chance to win. In this case, Arizona has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Arizona's perspective (47%): 2005_3112_3181,0.47 Example #2: You want to make a prediction for Duke (TeamID=3181) against North Carolina (TeamID=3314) in the 2005 tournament, with Duke given a 51.6% chance to win and North Carolina given a 48.4% chance to win. In this case, Duke has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Duke's perspective (51.6%): 2005_3181_3314,0.516 Also note that a single prediction row serves as a prediction for each of the two teams' winning chances. So for instance, in Example #1, the submission row of \"2005_3112_3181,0.47\" specifically gives a 47% chance for Arizona to win, and doesn't explicitly mention Duke's 53% chance to win. However, our evaluation utility will automatically infer the winning percentage in the other direction, so a 47% prediction for Arizona to win also means a 53% prediction for Duke to win. And similarly, because the submission row in Example #2 gives Duke a 51.6% chance to beat North Carolina, we will automatically figure out that this also means North Carolina has a 48.4% chance to beat Duke. This section provides game-by-game stats at a team level (free throws attempted, defensive rebounds, turnovers, etc.) for all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season.  Team Box Scores are provided in \"Detailed Results\" files rather than \"Compact Results\" files. However, the two files are strongly related. In a Detailed Results file, the first eight columns (Season, DayNum, WTeamID, WScore, LTeamID, LScore, WLoc, and NumOT) are exactly the same as a Compact Results file. However, in a Detailed Results file, there are many additional columns. The column names should be self-explanatory to basketball fans (as above, \"W\" or \"L\" refers to the winning or losing team): (and then the same set of stats from the perspective of the losing team: LFGM is the number of field goals made by the losing team, and so on up to LPF). Note: by convention, \"field goals made\" (either WFGM or LFGM) refers to the total number of fields goals made by a team, a combination of both two-point field goals and three-point field goals. And \"three point field goals made\" (either WFGM3 or LFGM3) is just the three-point fields goals made, of course. So if you want to know specifically about two-point field goals, you have to subtract one from the other (e.g., WFGM - WFGM3). And the total number of points scored is most simply expressed as 2*FGM + FGM3 + FTM. Data Section 2 file: WRegularSeasonDetailedResults.csv This file provides team-level box scores for many regular seasons of historical data, starting with the 2010 season. All games listed in the WRegularSeasonCompactResults file since the 2010 season should exactly be present in the WRegularSeasonDetailedResults file. Data Section 2 file: WNCAATourneyDetailedResults.csv This file provides team-level box scores for many NCAA\u00ae tournaments, starting with the 2010 season. All games listed in the WNCAATourneyCompactResults file since the 2010 season should exactly be present in the WNCAATourneyDetailedResults file. This section provides city locations of all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season Data Section 3 file: Cities.csv This file provides a master list of cities that have been locations for games played. Please notice that the Cities and Conferences files are the only two that don't start with an W; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two contests. Data Section 3 file: WGameCities.csv This file identifies all games, starting with the 2010 season, along with the city that the game was played in. Games from the regular season and the NCAA\u00ae tourney are all listed together. The CityID is present in more than 98% of games since the 2010 season. Games from the 2009 season and before are not listed in this file. This section provides play-by-play event logs for more than 99% of each year's regular season and NCAA\u00ae tournament, and secondary tournament women's games since the 2014-15 season - including plays by individual players. This year we are transitioning to a different play-by-play source, which includes data since the 2014-2015 season rather than since the 2009-2010 season (that's what we had previously for men's data). However, we are now able to provide play-by-play for both men's and women's data, and there is locational play-by-play detail starting with games from the 2018-2019 season. This includes an X/Y location (ranging from 0 to 100 in each dimension) on the court for each shot attempt, turnover, and foul for many games, as well as an overall categorization of the area on the court that the shot or turnover or foul occurred in (inside left wing, outside right wing, under the basket, etc.) Some games in these recent seasons still lack the locational detail. The data from last year (2019 season) matches what you can expect for the current year (2020 season) as we approach the postseason. Despite the 99.5% coverage, there are still a few games missing annually, and we will try to bring those in as well, if possible. Data Section 4 file: WEvents2015.csv, WEvents2016.csv, WEvents2017.csv, WEvents2018.csv, WEvents2019.csv Each WEvents file lists the play-by-play event logs for more than 99% of games from that season. Each event is assigned to either a team or a single one of the team's players. Thus if a basket is made by one player and an assist is credited to a second player, that would show up as two separate records. The players are listed by PlayerID within the WPlayers.csv file. Event Types and Subtypes:   This is the diagram provided by the play-by-play source:    Here is a diagram of these regions on a typical court:  And you may also have noticed that the three-point-arc is different this year in college basketball. The width of the court is still 15.2 m (50 ft) and the length of the court is still 28.7 m (94 feet), but the distance to the arc has changed from 6.32 m (20 feet 9 inches) to 6.75 m (22.15 feet). Here is a diagram of the old court, which would apply for play-by-play in seasons 2019 and before:  And here is a diagram of the new court, which only applies for season 2020:  Data Section 4 file: WPlayers.csv Note: there are data collection errors within the events, in that they don't necessarily add up to the final stats for the game. In addition, the player name spellings may vary over the course of a season or career for the same player, which would lead to the same player showing up with different PlayerID values. Nevertheless, this was the highest quality data we could manage for play-by-play with the near-complete set of games. This section contains additional supporting information, including alternative team name spellings and representations of bracket structure Data Section 5 file: WTeamSpellings.csv This file indicates alternative spellings of many team names. It is intended for use in associating external spellings against our own TeamID numbers, thereby helping to relate the external data properly with our datasets. Over the years we have identified various external spellings of different team names (as an example, for Ball State we have seen \"ball st\", and \"ball st.\", and \"ball state\", and \"ball-st\", and \"ball-state\"). Other teams have had more significant changes to their names over the years; for example, \"Texas Pan-American\" and \"Texas-Rio Grande Valley\" are actually the same school. The current list is obviously not exhaustive, and we encourage participants to identify additional mappings and upload extended versions of this file to the forums. Data Section 5 file: WNCAATourneySlots This file identifies the mechanism by which teams are paired against each other, depending upon their seeds, as the tournament proceeds through its rounds. It can be of use in identifying, for a given historical game, what round it occurred in, and what the seeds/slots were for the two teams (the meaning of \"slots\" is described below). You may need to know these specifics if you are trying to represent/simulate the exact workings of the tournament bracket. Unlike the analogous file on the men's side, it is not necessary to provide a Season within this file, because the women's tournament has never had play-in-games and so the 64-team women's bracket has always had the same structure each season. Data Section 5 file: Conferences.csv This file indicates the Division I conferences that have existed over the years since 1985. Each conference is listed with an abbreviation and a longer name. There has been no attempt to link up conferences who merged with other conferences, or whose names changed over time. Thus you will see, for instance, a \"Pacific-10\" conference up through the 2011 season, and then a \"Pacific-12\" conference starting in the 2012 season, and these look like different conferences in the data, even though it was still mostly the same teams. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two contests. However, the Team Conferences data differs slightly between men's and women's, so those files do have the prefixes. Data Section 5 file: WTeamConferences.csv This file indicates the conference affiliations for each team during each season. Some conferences have changed their names from year to year, and/or changed which teams are part of the conference. This file tracks this information historically."
    },
    {
        "name": "COVID19 Global Forecasting (Week 4)",
        "url": "https://www.kaggle.com/competitions/covid19-global-forecasting-week-4",
        "overview_text": "Overview text not found",
        "description_text": "This is week 4 of Kaggle's COVID-19 forecasting series, following the Week 3 competition. This is the 4th competition we've launched in this series. All of the prior discussion forums have been migrated to this competition for continuity. The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle) to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from National Academies of Sciences, Engineering, and Medicine\u2019s (NASEM) and the World Health Organization (WHO). Kaggle is launching a companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves forecasting confirmed cases and fatalities between April 15 and May 14 by region, the primary goal isn't only to produce accurate forecasts. It\u2019s also to identify factors that appear to impact the transmission rate of COVID-19. You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook. As the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE). We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19. There is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle\u2019s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community. JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.",
        "dataset_text": "In this challenge, you will be predicting the cumulative number of confirmed COVID19 cases in various locations across the world, as well as the number of resulting fatalities, for future dates. We understand this is a serious situation, and in no way want to trivialize the human impact this crisis is causing by predicting fatalities. Our goal is to provide better methods for estimates that can assist medical and governmental institutions to prepare and adjust as pandemics unfold."
    },
    {
        "name": "COVID19 Global Forecasting (Week 5)",
        "url": "https://www.kaggle.com/competitions/covid19-global-forecasting-week-5",
        "overview_text": "Overview text not found",
        "description_text": "This is week 5 of Kaggle's COVID-19 forecasting series, following the Week 4 competition. This competition has some changes from prior weeks - be sure to check the Evaluation and Data pages for more details. All of the prior discussion forums have been migrated to this competition for continuity. The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle) to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from National Academies of Sciences, Engineering, and Medicine\u2019s (NASEM) and the World Health Organization (WHO). Kaggle is launching a companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves developing quantile estimates intervals for confirmed cases and fatalities between May 12 and June 7 by region, the primary goal isn't only to produce accurate forecasts. It\u2019s also to identify factors that appear to impact the transmission rate of COVID-19. You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook. As the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE). We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19. There is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle\u2019s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community. JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.",
        "dataset_text": "In this challenge, you will be predicting the daily number of confirmed COVID19 cases in various locations across the world, as well as the number of resulting fatalities, for future dates. This latest challenge includes US state county data."
    },
    {
        "name": "Santa 2020 - The Candy Cane Contest",
        "url": "https://www.kaggle.com/competitions/santa-2020",
        "overview_text": "Overview text not found",
        "description_text": " It's the most wonderful time of the year\nWith the elves eating candy\nThey\u2019ll feel super dandy and be of good cheer\nIt's the most wonderful time of the year It's the hap-happiest season of all\nWhen spirits are lifted the toys will be gifted\nAnd games to enthrall!\nIt's the hap-happiest season of all The party for throwing\nHas snow cones a\u2019glowing\nWith bragging rights out on display.\nSo now you must plan it,\nTo beat the armed bandits\nwho keep all the candy away. It's the most wonderful time of the year!  Morale has been low at the North Pole this year. But Santa really believes in \u201cmaking spirits bright!\u201d So he has planned a friendly competition among the elves to keep the Christmas cheer alive and make as many toys as possible! And the winning team gets a snow cone party! As one of the team leaders, you know that nothing keeps your fellow elves more productive and motivated than a steady supply of candy canes! But all seven levels of the Candy Cane Forest are closed for revegetation, so the only ones available are stuck in the break room vending machines. And even though you receive free snacks on the job, the vending machines are always broken and don\u2019t always give you what you want. Due to social distancing, only two elves can be in the break room at once. You and another team leader will take turns trying to get candy canes out of the 100 possible vending machines in the room, but each machine is unpredictable in how likely it is to work. You do know, however, that the more often you try to use a machine, the less likely it will give you a candy cane. Plus, you only have time to try 2000 times on the vending machines until you need to get back to the workshop! If you can collect more candy canes than the other team leaders, you\u2019ll surely be able to help your team win Santa's contest! Try your hand at this multi-armed candy cane challenge! Image Credit: Photos by Joanna Kosinska and Misty Ladd on Unsplash.",
        "dataset_text": "This page appears underneath the data files. It describes what files have been provided & the format of each. It also defines the correct format for submission files. Participants should be able to answer these types of questions after reading the data description: What files do I need?\nWhat should I expect the data format to be?\nWhat am I predicting?\nWhat acronyms will I encounter?"
    },
    {
        "name": "Facebook II - Mapping the Internet",
        "url": "https://www.kaggle.com/competitions/facebook-ii",
        "overview_text": "Overview text not found",
        "description_text": "For this challenge, potential Facebook recruits will be exploring the map of the entire internet. Unlike the map of a city, where best routes are relatively fixed except for the occasional construction or parade detour, the paths that information travels over the web are constantly changing. There is no centralized system of stop-lights or traffic cops.  Instead, there are tens of thousands of autonomous systems using a common protocol to advertise the next available hops, updated depending on service-agreements, capacity, and load. This will be a test of both the candidates engineering know-how and their ability to statistically learn on complex, dynamic graph structures. The Task: you will be given a path which, at one point in the training time period, was an optimal path from node A to B. The question is then to make a probalistic prediction, for each of the 5 test graphs, whether the given path is STILL an optimal path.  This is a much more difficult task than link prediction alone. The global structure of the graph may affect many optimal routes, paths can have varying lengths (and thus varying a priori probabilities of being optimal), and there may be multiple optimal routes for a given source & destination.  The Prize: Facebook is seeking data-savvy software engineers (Data Engineers) to build the next generation of systems that will transform the online experience of over a billion users. Appropriate candidates should have experience with multiple components across the big data stack (check out the Visualization track for another way to highlight your skills).  There are many teams that they could be a fit for depending on their backgrounds . Positions are available in Menlo Park, Seattle, New York City, and London; candidates must have, or be eligible to obtain, authorization to work in the US or UK.     An example visualization of Internet topology (round-trip times) produced by Walrus \n(courtesy of Young Hyun and inverted for display purposes)",
        "dataset_text": "There are 15 graphs in the training set and 5 in the test set. These are directed, dynamic, real-world Internet topology graphs with nodes that correspond to Autonomous Systems (AS).  Each graph is sampled at a fixed time interval, \\\\(\\Delta t\\\\), apart.  The 5 test graphs represent the status of the network at the 5 time points after the final training graph, but are not supplied to participants.  Edges may appear and disappear as routing patterns change over time.  Some nodes are peers (A->B means A can exchange traffic through B for free) and some are connected by regular edges (A->B means A pays B to route their traffic).  Peers have weights of zero in the training data and regular edges have weights of one.  We have shuffled the AS numbers and replaced them with distorted versions of the AS names. In other words, the names are arbitrary with respect to reality, but consistent within the data.  For example, if the original graph has the AS number/name 26 CORNELL - Cornell University, it would be remapped to a random new number and its corresponding name, say (hypothetically), 11 HARVARD - Harvard University, with the text altered in some fashion.  The AS remapping is done to discourage reverse engineering the graph.  The text alteration is done to test participants' ability to turn messy, real-world data in to a computable format.  Some AS names are not available. For these, we have left the AS number intact. For the prediction task, you are given a path which, at one point in the training time period, was an optimal path from node A to B. Here, \u201coptimal\u201d accounts for the peer relationships with zero-weight edges (i.e. it is the weighted shortest path).  Many paths can have the same best optimal weight and are all considered optimal, regardless of the path length. The training set graphs give links in the form: Messy AS Name 1 | Messy AS Name 2 | Edge weight  The test set paths have the form: Messy AS Name Start | Messy AS Name | ... | Messy AS Name | Messy AS Name End Example real data from the training set: You are supplied with 10000 paths (paths.txt).  The task is then to predict, for all 5 test graphs, the probability that the given path is STILL an optimal path. Compute your probabilities as a matrix of size 10000 x 5 (10000 path probabilities, predicted at each of the 5 test times), sorted according to the original order of the test set. Concatenate the five 10000 x 1 columns to get a 50000 x 1 column vector, and prepend a header row, \"Probability\", to create a 50001-row column vector. To be explicit, if your probabilities for path \\\\(i\\\\) at time \\\\(j\\\\) are \\\\( p_{i,j} \\\\), the format should look like: ("
    },
    {
        "name": "Yelp Recruiting Competition",
        "url": "https://www.kaggle.com/competitions/yelp-recruiting",
        "overview_text": "Overview text not found",
        "description_text": "Here at Yelp, we really love the quality of our data. We're grateful that so many of our users take the time to write such great reviews. We track 3 community-powered metrics of review quality: Useful, Funny, Cool. Over time, a good review will accumulate lots of votes in these categories from the community. However, another extremely important quality feature is the freshness of a review. What if we didn't have to wait for the community to vote on the best reviews to know which ones are high quality? The goal of this competition is to estimate the number of Useful votes a review will receive. Yelp isn't only looking for the answer to this question; we're looking for an engineer that can solve this problem and push their code to production. The prize is a fast track through the recruiting process -- straight to an interview and the opportunity to show Yelp Engineers just what you've got. For more information about the exciting opportunities at Yelp, check out http://www.yelp.com/careers!  This competition counts towards rankings & achievements.  If you wish to be considered for an interview at Yelp, check the box \"Allow host to contact me\" when you form your team.",
        "dataset_text": "In the training set: Each file is composed of a single object type, one JSON object per line. The training data was recorded on 2013-01-19. The testing data contains reviews, businesses, users, and checkins from the period between 2013-01-19 and 2013-03-12. Many user and business records referenced in the test set can be found in the training data. Training set = yelp_training_set.tgz OR yelp_training_set.zip\nTesting set = yelp_test_set.tgz OR yelp_test_set.zip\nSample submission format  = sample_submission.csv  {\n'type': 'business',\n'business_id': (encrypted business id),\n'name': (business name),\n'neighborhoods': [(hood names)],\n'full_address': (localized address),\n'city': (city),\n'state': (state),\n'latitude': latitude,\n'longitude': longitude,\n'stars': (star rating, rounded to half-stars),\n'review_count': review count,\n'categories': [(localized category names)]\n'open': True / False (corresponds to permanently closed, not business hours),\n} {\n'type': 'review',\n'business_id': (encrypted business id),\n'user_id': (encrypted user id),\n'stars': (star rating),\n'text': (review text),\n'date': (date, formatted like '2012-03-14'),\n'votes': {'useful': (count), 'funny': (count), 'cool': (count)}\n} {\n'type': 'user',\n'user_id': (encrypted user id),\n'name': (first name),\n'review_count': (review count),\n'average_stars': (floating point average, like 4.31),\n'votes': {'useful': (count), 'funny': (count), 'cool': (count)}\n} {\n'type': 'checkin',\n'business_id': (encrypted business id),\n'checkin_info': { '0-0': (number of checkins from 00:00 to 01:00 on all Sundays),\n'1-0': (number of checkins from 01:00 to 02:00 on all Sundays),\n...\n'14-4': (number of checkins from 14:00 to 15:00 on all Thursdays),\n...\n'23-6': (number of checkins from 23:00 to 00:00 on all Saturdays) } # if there was no checkin for a hour-day block it will not be in the dict\n}"
    },
    {
        "name": "Dogs vs. Cats",
        "url": "https://www.kaggle.com/competitions/dogs-vs-cats",
        "overview_text": "Overview text not found",
        "description_text": "In this competition, you'll write an algorithm to classify whether images contain either a dog or a cat.  This is easy for humans, dogs, and cats. Your computer will find it a bit more difficult.  Deep Blue beat Kasparov at chess in 1997.\nWatson beat the brightest trivia minds at Jeopardy in 2011.\nCan you tell Fido from Mittens in 2013? Web services are often protected with a challenge that's supposed to be easy for people to solve, but difficult for computers. Such a challenge is often called a CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) or HIP (Human Interactive Proof). HIPs are used for many purposes, such as to reduce email and blog spam and prevent brute-force attacks on web site passwords. Asirra (Animal Species Image Recognition for Restricting Access) is a HIP that works by asking users to identify photographs of cats and dogs. This task is difficult for computers, but studies have shown that people can accomplish it quickly and accurately. Many even think it's fun! Here is an example of the Asirra interface: Asirra is unique because of its partnership with Petfinder.com, the world's largest site devoted to finding homes for homeless pets. They've provided Microsoft Research with over three million images of cats and dogs, manually classified by people at thousands of animal shelters across the United States. Kaggle is fortunate to offer a subset of this data for fun and research.  While random guessing is the easiest form of attack, various forms of image recognition can allow an attacker to make guesses that are better than random. There is enormous diversity in the photo database (a wide variety of backgrounds, angles, poses, lighting, etc.), making accurate automatic classification difficult. In an informal poll conducted many years ago, computer vision experts posited that a classifier with better than 60% accuracy would be difficult without a major advance in the state of the art. For reference, a 60% classifier improves the guessing probability of a 12-image HIP from 1/4096 to 1/459. The current literature suggests machine classifiers can score above 80% accuracy on this task [1]. Therfore, Asirra is no longer considered safe from attack.  We have created this contest to benchmark the latest computer vision and deep learning approaches to this problem. Can you crack the CAPTCHA? Can you improve the state of the art? Can you create lasting peace between cats and dogs? Okay, we'll settle for the former.  We extend our thanks to Microsoft Research for providing the data for this competition.",
        "dataset_text": "The training archive contains 25,000 images of dogs and cats. Train your algorithm on these files and predict the labels for test1.zip (1 = dog, 0 = cat). Per the rules and spirit of this contest, please do not manually label your submissions. We work hard to fair and fun contests, and ask for the same respect in return.  \n\"I'm a good doggie who deserves a good algorithm.\nAnd chew bones. Please send chew bones.\" However, given that there is always one willing to cheat to rise to glory, we reserve the right to wield the following anti-doping measures:"
    },
    {
        "name": "Learning Social Circles in Networks",
        "url": "https://www.kaggle.com/competitions/learning-social-circles",
        "overview_text": "Overview text not found",
        "description_text": "Social Circles help users organize their personal social networks.  These are implemented as \"circles\" on Google+, and as \"lists\" on Facebook and Twitter. Each circle consists of a subset of a particular user's friends. Such circles may be disjoint, overlap, or be hierarchically nested. The goal of this competition is to automatically infer users' social circles. You are provided a set of users, each of whose circles must be inferred. To do this, participants have access to: To give you an idea of how to use this data, the problem of detecting social circles has been discussed (in an academic setting) in http://i.stanford.edu/~julian/pdfs/nips2012.pdf Those of you who've been around Kaggle for a while may remember that we called upon you to create this data set. We extend our thanks to those of you who helped out. As a reward, you might be able to find yourself in the data and gain a one-row advantage?!",
        "dataset_text": "We use the following nomenclature We include the following files:"
    },
    {
        "name": "Facebook Recruiting III - Keyword Extraction",
        "url": "https://www.kaggle.com/competitions/facebook-recruiting-iii-keyword-extraction",
        "overview_text": "Overview text not found",
        "description_text": "Looking for a data science position at Facebook?  After two successful prior Kaggle competitions, Facebook continues their mission to identify the best data scientists and software engineers that Kaggle has to offer. In this third installment, they seek candidates who have experience text mining large amounts of data.  This competition tests your text skills on a large dataset from the Stack Exchange sites.  The task is to predict the tags (a.k.a. keywords, topics, summaries), given only the question text and its title. The dataset contains content from disparate stack exchange sites, containing a mix of both technical and non-technical questions. Positions are available in Menlo Park, Seattle, New York City, and London; candidates must have, or be eligible to obtain, authorization to work in the US or UK. Please note: you must compete as an individual in recruiting competitions. You may only use the data provided to make your predictions. Crawling stack exchange sites to look up answers is not permitted. Facebook will review the code of the top participants before deciding whether to offer an interview.  This competition counts towards rankings & achievements.  If you wish to be considered for an interview at Facebook, check the box \"Allow host to contact me\" when you make your first entry. We thank Stack Exchange (and its users) for generously releasing the source dataset through its Creative Commons Data Dumps. All data is licensed under the cc-by-sa license.",
        "dataset_text": "All of the data is in 2 files: Train and Test. Train.csv contains 4 columns: Id,Title,Body,Tags Test.csv contains the same columns but without the Tags, which you are to predict. The questions are randomized and contains a mix of verbose text sites as well as sites related to math and programming. The number of questions from each site may vary, and no filtering has been performed on the questions (such as closed questions). We thank Stack Exchange (and its users) for generously releasing the source dataset through its Creative Commons Data Dumps. All data is licensed under the cc-by-sa license."
    },
    {
        "name": "Conway's Reverse Game of Life",
        "url": "https://www.kaggle.com/competitions/conway-s-reverse-game-of-life",
        "overview_text": "Overview text not found",
        "description_text": " The Game of Life is a cellular automaton created by mathematician John Conway in 1970. The game consists of a board of cells that are either on or off. One creates an initial configuration of these on/off states and observes how it evolves. There are four simple rules to determine the next state of the game board, given the current state: These simple rules result in many interesting behaviors and have been the focus of a large body of mathematics.  As Wikipedia tells it, The emergence of order from simple rules begs an interesting question--what happens if we set time backwards? This competition is an experiment to see if machine learning (or optimization, or any method) can predict the game of life in reverse.  Is the chaotic start of Life predictable from its orderly ends?  We have created many games, evolved them, and provided only the end boards. You are asked to predict the starting board that resulted in each end board. Although some people have examined this problem, it is unknown (at least, to us...) just how difficult this will be.",
        "dataset_text": "We have provided 50,000 training games and 50,000 test games, whose starting board you must predict.  Each board is 20x20, for a total of 400 cells per board. Values are listed in a column-wise order.  You are free to create more training games if you desire. The provided variables are: id - each game has an id\ndelta - the number of steps between the start and stop boards \nstart.1 - row 1, column 1 of the game's starting board\nstart.2 - row 2, column 1 of the game's starting board\n...\nstop.1 - row 1, column 1 of the game's stopping board\n... Your test-set predictions should be the starting board at delta steps before the stopping board. The games were created by the following procedure: Why the need for warmup steps? The transition from an initial random board to the second step can be quite \"nonlinear\" and dramatic. For example, if a board is mostly alive at the first step, it will be mostly dead on the second. We allow the game to warmup for five steps in order to let the cells calm down and settle into a more \"life-like\" state. Can I predict any valid state? Unfortunately, we are asking you to predict the single valid state the we started with. If this competition is popular and our server bill doesn't explode, it's possible we will code a game-of-life evaluation metric that would allow any valid start state to score well if it is similar to the end state. The Game of Life loses information over time. What gives? Correct, this is a many-to-one problem (many starting states can lead to the same stopping state). For example, many boards that are sparse at the start will end up in the same state (and you will see this in the data). However, over short time scales we expect this to be a minor issue.  The largest step back in time in this competition is 5, which we hope is not so far that the degeneracy becomes an issue."
    },
    {
        "name": "Walmart Recruiting - Store Sales Forecasting",
        "url": "https://www.kaggle.com/competitions/walmart-recruiting-store-sales-forecasting",
        "overview_text": "Overview text not found",
        "description_text": "One challenge of modeling retail data is the need to make decisions based on limited history. If Christmas comes but once a year, so does the chance to see how strategic decisions impacted the bottom line.  In this recruiting competition, job-seekers are provided with historical sales data for 45 Walmart stores located in different regions. Each store contains many departments, and participants must project the sales for each department in each store. To add to the challenge, selected holiday markdown events are included in the dataset. These markdowns are known to affect sales, but it is challenging to predict which departments are affected and the extent of the impact. Want to work in a great environment with some of the world's largest data sets? This is a chance to display your modeling mettle to the Walmart hiring teams. This competition counts towards rankings & achievements.  If you wish to be considered for an interview at Walmart, check the box \"Allow host to contact me\" when you make your first entry.  You must compete as an individual in recruiting competitions. You may only use the provided data to make your predictions.",
        "dataset_text": "You are provided with historical sales data for 45 Walmart stores located in different regions. Each store contains a number of departments, and you are tasked with predicting the department-wide sales for each store. In addition, Walmart runs several promotional markdown events throughout the year. These markdowns precede prominent holidays, the four largest of which are the Super Bowl, Labor Day, Thanksgiving, and Christmas. The weeks including these holidays are weighted five times higher in the evaluation than non-holiday weeks. Part of the challenge presented by this competition is modeling the effects of markdowns on these holiday weeks in the absence of complete/ideal historical data. stores.csv This file contains anonymized information about the 45 stores, indicating the type and size of store. train.csv This is the historical training data, which covers to 2010-02-05 to 2012-11-01. Within this file you will find the following fields: test.csv This file is identical to train.csv, except we have withheld the weekly sales. You must predict the sales for each triplet of store, department, and date in this file. features.csv This file contains additional data related to the store, department, and regional activity for the given dates. It contains the following fields: For convenience, the four holidays fall within the following weeks in the dataset (not all holidays are in the data): Super Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13\nLabor Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13\nThanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13\nChristmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13"
    },
    {
        "name": "Facebook Recruiting IV: Human or Robot?",
        "url": "https://www.kaggle.com/competitions/facebook-recruiting-iv-human-or-bot",
        "overview_text": "Overview text not found",
        "description_text": "Ever wonder what it's like to work at Facebook? Facebook and Kaggle are launching an Engineering competition for 2015. Trail blaze your way to the top of the leader board to earn an opportunity at interviewing for a role as a software engineer, working on world class Machine Learning problems.  In this competition, you'll be chasing down robots for an online auction site. Human bidders on the site are becoming increasingly frustrated with their inability to win auctions vs. their software-controlled counterparts. As a result, usage from the site's core customer base is plummeting.  In order to rebuild customer happiness, the site owners need to eliminate computer generated bidding from their auctions. Their attempt at building a model to identify these bids using behavioral data, including bid frequency over short periods of time, has proven insufficient.  The goal of this competition is to identify online auction bids that are placed by \"robots\", helping the site owners easily flag these users for removal from their site to prevent unfair auction activity.  The data in this competition comes from an online platform, not from Facebook. Please note: You must compete as an individual in recruiting competitions. You may only use the data provided to make your predictions. ",
        "dataset_text": "There are two datasets in this competition. One is a bidder dataset that includes a list of bidder information, including their id, payment account, and address. The other is a bid dataset that includes 7.6 million bids on different auctions. The bids in this dataset are all made by mobile devices. The online auction platform has a fixed increment of dollar amount for each bid, so it doesn't include an amount for each bid. You are welcome to learn the bidding behavior from the time of the bids, the auction, or the device.  The data in this competition comes from an online platform, not from Facebook. For the bidder dataset For the bid dataset"
    },
    {
        "name": "Walmart Recruiting II: Sales in Stormy Weather",
        "url": "https://www.kaggle.com/competitions/walmart-recruiting-sales-in-stormy-weather",
        "overview_text": "Overview text not found",
        "description_text": " Walmart operates 11,450 stores in 27 countries, managing inventory across varying climates and cultures. Extreme weather events, like hurricanes, blizzards, and floods, can have a huge impact on sales at the store and product level.  In their second Kaggle recruiting competition, Walmart challenges participants to accurately predict the sales of 111 potentially weather-sensitive products (like umbrellas, bread, and milk) around the time of major weather events at 45 of their retail locations.  Intuitively, we may expect an uptick in the sales of umbrellas before a big thunderstorm, but it's difficult for replenishment managers to correctly predict the level of inventory needed to avoid being out-of-stock or overstock during and after that storm. Walmart relies on a variety of vendor tools to predict sales around extreme weather events, but it's an ad-hoc and time-consuming process that lacks a systematic measure of effectiveness.  Helping Walmart better predict sales of weather-sensitive products will keep valued customers out of the rain. It could also earn you a position at one of the most data-driven retailers in the world!  Please note: You must compete as an individual in recruiting competitions. You may only use the data provided to make your predictions.",
        "dataset_text": "You have been provided with sales data for 111 products whose sales may be affected by the weather (such as milk, bread, umbrellas, etc.). These 111 products are sold in stores at 45 different Walmart locations. Some of the products may be a similar item (such as milk) but have a different id in different stores/regions/suppliers. The 45 locations are covered by 20 weather stations (i.e. some of the stores are nearby and share a weather station). The competition task is to predict the amount of each product sold around the time of major weather events. For the purposes of this competition, we have defined a weather event as any day in which more than an inch of rain or two inches of snow was observed. You are asked to predict the units sold for a window of \u00b13 days surrounding each storm. The following graphic shows the layout of the test windows. The green dots are the training set days, the red dots are the test set days, and the event=True are the days with storms. Note that this plot is for the 20 weather stations. All days prior to 2013-04-01 are given out as training data.  You are provided with the full observed weather covering the entire data set. You do not need to forecast weather in addition to sales (it's as though you have a perfect weather forecast at your disposal)."
    },
    {
        "name": "Telstra Network Disruptions",
        "url": "https://www.kaggle.com/competitions/telstra-recruiting-network",
        "overview_text": "Overview text not found",
        "description_text": "In their first recruiting competition, Telstra is\n challenging Kagglers to predict the severity of service disruptions on their network. Using a dataset of features from their service logs, you're tasked with predicting if a disruption is a momentary glitch or a total interruption of connectivity. Telstra is on a journey to enhance the customer experience - ensuring everyone in the company is putting customers first. In terms of its expansive network, this means continuously advancing how it predicts the scope and timing of service disruptions. Telstra wants to see how you would help it drive customer advocacy by developing a more advanced predictive model for service disruptions and to help it better serve its customers. This challenge was crafted as a simulation of the type of problem you might tackle as a member of the team at Telstra. Kagglers who stand out will be considered for data science roles in Telstra's Big Data team in Telstra\u2019s absolute discretion. Highly-ranked participants will combine technical expertise and intuition in data science problems with a keen business sense and an effortless ability to work with technical and non-technical staff to turn data into real changes that impact customers. Highly-ranked participants will be considered by Telstra for interviews for employment, based on their work in the Competition and ability to meet the selection criteria for any suitable open job vacancy in Melbourne and Sydney, Australia. Participation in this Competition is not a recruitment process and Kaggle does not provide Telstra with recruitment services.",
        "dataset_text": "The goal of the problem is to predict Telstra network's fault severity at a time at a particular location based on the log data available. Each row in the main dataset (train.csv, test.csv) represents a location and a time point. They are identified by the \"id\" column, which is the key \"id\" used in other data files.  Fault severity has 3 categories: 0,1,2 (0 meaning no fault, 1 meaning only a few, and 2 meaning many).  Different types of features are extracted from log files and other sources: event_type.csv, log_feature.csv, resource_type.csv, severity_type.csv.  Note: \u201cseverity_type\u201d is a feature extracted from the log files (in severity_type.csv). Often this is a severity type of a warning message coming from the log. \"severity_type\" is categorical. It does not have an ordering. \u201cfault_severity\u201d is a measurement of actual reported faults from users of the network and is the target variable (in train.csv)."
    },
    {
        "name": "Airbnb New User Bookings",
        "url": "https://www.kaggle.com/competitions/airbnb-recruiting-new-user-bookings",
        "overview_text": "Overview text not found",
        "description_text": "Instead of waking to overlooked \"Do not disturb\" signs, Airbnb travelers find themselves rising with the birds in a whimsical treehouse, having their morning coffee on the deck of a houseboat, or cooking a shared regional breakfast with their hosts. New users on Airbnb can book a place to stay in 34,000+ cities across 190+ countries. By accurately predicting where a new user will book their first travel experience, Airbnb can share more personalized content with their community, decrease the average time to first booking, and better forecast demand. In this recruiting competition, Airbnb challenges you to predict in which country a new user will make his or her first booking. Kagglers who impress with their answer (and an explanation of how they got there) will be considered for an interview for the opportunity to join Airbnb's Data Science and Analytics team.  Wondering if you're a good fit? Check out this article on how Airbnb scaled data science to all sides of their organization, and visit their careers page for more on Airbnb's mission to create a world that inspires human connection.",
        "dataset_text": "In this challenge, you are given a list of users along with their demographics, web session records, and some summary statistics. You are asked to predict which country a new user's first booking destination will be. All the users in this dataset are from the USA. There are 12 possible outcomes of the destination country: 'US', 'FR', 'CA', 'GB', 'ES', 'IT', 'PT', 'NL','DE', 'AU', 'NDF' (no destination found), and 'other'. Please note that 'NDF' is different from 'other' because 'other' means there was a booking, but is to a country not included in the list, while 'NDF' means there wasn't a booking. The training and test sets are split by dates. In the test set, you will predict all the new users with first activities after 7/1/2014 (note: this is updated on 12/5/15 when the competition restarted). In the sessions dataset, the data only dates back to 1/1/2014, while the users dataset dates back to 2010. "
    },
    {
        "name": "Walmart Recruiting: Trip Type Classification",
        "url": "https://www.kaggle.com/competitions/walmart-recruiting-trip-type-classification",
        "overview_text": "Overview text not found",
        "description_text": "Walmart uses both art and science to continually make progress on their core mission of better understanding and serving their customers. One way Walmart is able to improve customers' shopping experiences is by segmenting their store visits into different trip types.   Whether they're on a last minute run for new puppy supplies or leisurely making their way through a weekly grocery list, classifying trip types enables Walmart to create the best shopping experience for every customer. Currently, Walmart's trip types are created from a combination of existing customer insights (\"art\") and purchase history data (\"science\"). In their third recruiting competition, Walmart is challenging Kagglers to focus on the (data) science and classify customer trips using only a transactional dataset of the items they've purchased. Improving the science behind trip type classification will help Walmart refine their segmentation process. Walmart is hosting this competition to connect with data scientists who break the mold.",
        "dataset_text": "For this competition, you are tasked with categorizing shopping trip types based on the items that customers purchased. To give a few hypothetical examples of trip types: a customer may make a small daily dinner trip, a weekly large grocery trip, a trip to buy gifts for an upcoming holiday, or a seasonal trip to buy clothes. Walmart has categorized the trips contained in this data into 38 distinct types using a proprietary method applied to an extended set of data. You are challenged to recreate this categorization/clustering with a more limited set of features. This could provide new and more robust ways to categorize trips. The training set (train.csv) contains a large number of customer visits with the TripType included. You must predict the TripType for each customer visit in the test set (test.csv). Each visit may only have one TripType. You will not be provided with more information than what is given in the data (e.g. what the TripTypes represent or more product information). The test set file is encrypted. You must complete this brief survey to receive the password."
    },
    {
        "name": "Yelp Restaurant Photo Classification",
        "url": "https://www.kaggle.com/competitions/yelp-restaurant-photo-classification",
        "overview_text": "Overview text not found",
        "description_text": "Does your favorite Ethiopian restaurant take reservations? Will a first date at that authentic looking bistro break your wallet? Is the diner down the street a good call for breakfast? Restaurant labels help Yelp users quickly answer questions like these, narrowing down their results to only restaurants that fit their nuanced needs. In this competition, Yelp is challenging Kagglers to build a model that automatically tags restaurants with multiple labels using a dataset of user-submitted photos. Currently, restaurant labels are manually selected by Yelp users when they submit a review. Selecting the labels is optional, leaving some restaurants un- or only partially-categorized.  In an age of food selfies and photo-centric social storytelling, it may be no surprise to hear that Yelp's users upload an enormous amount of photos every day alongside their written reviews. Can you turn their pictures into (less than a thousand) words?  Yelp isn\u2019t only looking for your best model; we\u2019re looking for data mining engineers that can help us use our data in novel ways while pushing code to production. The prize for this competition is a fast track through the recruiting process and an opportunity to show our data mining teams just what you\u2019ve got! For more information about exciting opportunities at Yelp, check out the Jobs at Yelp competition page and Yelp's own careers page.",
        "dataset_text": "At Yelp, there are lots of photos and lots of users uploading photos. These photos provide rich local business information across categories. Teaching a computer to understand the context of these photos is not an easy task. Yelp engineers work on deep learning image classification projects in-house, and you can read about them here.  In this competition, you are given photos that belong to a business and asked to predict the business attributes. There are 9 different attributes in this problem: 0: good_for_lunch\n1: good_for_dinner\n2: takes_reservations\n3: outdoor_seating\n4: restaurant_is_expensive\n5: has_alcohol\n6: has_table_service\n7: ambience_is_classy\n8: good_for_kids These labels are annotated by the Yelp community. Your task is to predict these labels purely from the business photos uploaded by users.  Since Yelp is a community driven website, there are duplicated images in the dataset. They are mainly due to: Yelp is including these as part of the competition, since these are challenges Yelp researchers face every day.  To deter hand labeling, Kaggle has supplemented the test set with additional \"ignored\" businesses. These are not counted in the scoring. "
    },
    {
        "name": "Facebook V: Predicting Check Ins",
        "url": "https://www.kaggle.com/competitions/facebook-v-predicting-check-ins",
        "overview_text": "Overview text not found",
        "description_text": "Ever wonder what it's like to work at Facebook? Facebook and Kaggle are launching a machine learning engineering competition for 2016. Trail blaze your way to the top of the leaderboard to earn an opportunity at interviewing for one of the 10+ open roles as a software engineer, working on world class machine learning problems.  The goal of this competition is to predict which place a person would like to check in to. For the purposes of this competition, Facebook created an artificial world consisting of more than 100,000 places located in a 10 km by 10 km square. For a given set of coordinates, your task is to return a ranked list of the most likely places. Data was fabricated to resemble location signals coming from mobile devices, giving you a flavor of what it takes to work with real data complicated by inaccurate and noisy values. Inconsistent and erroneous location data can disrupt experience for services like Facebook Check In. We highly encourage competitors to be active on Kaggle Scripts. Your work there will be thoughtfully included in the decision making process. Please note: You must compete as an individual in recruiting competitions. You may only use the data provided to make your predictions.",
        "dataset_text": "In this competition, you are going to predict which business a user is checking into based on their location, accuracy, and timestamp.   The train and test dataset are split based on time, and the public/private leaderboard in the test data are split randomly. There is no concept of a person in this dataset. All the row_id's are events, not people.  Note: Some of the columns, such as time and accuracy, are intentionally left vague in their definitions. Please consider them as part of the challenge. "
    },
    {
        "name": "Allstate Claims Severity",
        "url": "https://www.kaggle.com/competitions/allstate-claims-severity",
        "overview_text": "Overview text not found",
        "description_text": "When you\u2019ve been devastated by a serious car accident, your focus is on the things that matter the most: family, friends, and other loved ones. Pushing paper with your insurance agent is the last place you want your time or mental energy spent. This is why Allstate, a personal insurer in the United States, is continually seeking fresh ideas to improve their claims service for the over 16 million households they protect.  Allstate is currently developing automated methods of predicting the cost, and hence severity, of claims. In this recruitment challenge, Kagglers are invited to show off their creativity and flex their technical chops by creating an algorithm which accurately predicts claims severity. Aspiring competitors will demonstrate insight into better ways to predict claims severity for the chance to be part of Allstate\u2019s efforts to ensure a worry-free customer experience. New to Kaggle? This competition is a recruiting competition, your chance to get a foot in the door with the hiring team at Allstate.",
        "dataset_text": "Each row in this dataset represents an insurance claim. You must predict the value for the 'loss' column. Variables prefaced with 'cat' are categorical, while those prefaced with 'cont' are continuous."
    },
    {
        "name": "Two Sigma Connect: Rental Listing Inquiries",
        "url": "https://www.kaggle.com/competitions/two-sigma-connect-rental-listing-inquiries",
        "overview_text": "Overview text not found",
        "description_text": "Finding the perfect place to call your new home should be more than browsing through endless listings. RentHop makes apartment search smarter by using data to sort rental listings by quality. But while looking for the perfect apartment is difficult enough, structuring and making sense of all available real estate data programmatically is even harder. Two Sigma and RentHop, a portfolio company of Two Sigma Ventures, invite Kagglers to unleash their creative engines to uncover business value in this unique recruiting competition.  Two Sigma invites you to apply your talents in this recruiting competition featuring rental listing data from RentHop. Kagglers will predict the number of inquiries a new listing receives based on the listing\u2019s creation date and other features. Doing so will help RentHop better handle fraud control, identify potential listing quality issues, and allow owners and agents to better understand renters\u2019 needs and preferences. Two Sigma has been at the forefront of applying technology and data science to financial forecasts. While their pioneering advances in big data, AI, and machine learning in the financial world have been pushing the industry forward, as with all other scientific progress, they are driven to make continual progress. This challenge is an opportunity for competitors to gain a sneak peek into Two Sigma's data science work outside of finance. This competition is co-hosted by Two Sigma and RentHop (a portfolio company of Two Sigma Ventures, which is a division of Two Sigma Investments) to encourage creativity in using real world data to solve everyday problems.",
        "dataset_text": "In this competition, you will predict how popular an apartment rental listing is based on the listing content like text description, photos, number of bedrooms, price, etc. The data comes from renthop.com, an apartment listing website. These apartments are located in New York City. The target variable, interest_level, is defined by the number of inquiries a listing has in the duration that the listing was live on the site. "
    },
    {
        "name": "March Machine Learning Mania 2021 - NCAAM",
        "url": "https://www.kaggle.com/competitions/ncaam-march-mania-2021",
        "overview_text": "Overview text not found",
        "description_text": "Back again after a pandemic-year hiatus, Kaggle's March Machine Learning Mania challenges data scientists to predict winners and losers of the men's 2021 NCAA basketball tournament. You're provided data of historical NCAA games and are encouraged to use other sources of publicly available data to gain a winning edge.  In stage one of this two-stage competition, participants will build and test their models against previous tournaments. In the second stage, participants will predict the outcome of the 2021 tournament. You don\u2019t need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2021 results. Note: From 2018-2020, Google Cloud, of which Kaggle is a part, served as official corporate sponsors of the NCAA basketball tournaments. This official sponsorship has concluded, but we at Kaggle are happy to bring back this tradition for its 8th year! Banner image by Ben Hershey on Unsplash",
        "dataset_text": "Each season there are thousands of NCAA basketball games played between Division I men's teams, culminating in March Madness\u00ae, the 68-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness\u00ae game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past. If you are unfamiliar with the format and intricacies of the NCAA\u00ae tournament, we encourage reading the wikipedia page before diving into the data.  The data description and schema may seem daunting at first, but is not as complicated as it appears. As a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The TeamSpellings file, which is listed in the bottom section below, may help you map external team references into our own Team ID structure, and you may also need to understand exactly how dates work in our data. Please also note that we have standardized the spelling of column names and some filenames, so if you are re-using code from previous instances of this contest, you may need to adjust for this. For example, we are universally referencing Team ID columns with a spelling of \"TeamID\" rather than \"team_id\". We extend our gratitude to Kenneth Massey for providing much of the historical data. Special Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition. Stage 1 - You should submit predicted probabilities for every possible matchup in the past 5 NCAA\u00ae tournaments (2015-2019). Stage 2 - You should submit predicted probabilities for every possible matchup before the 2021 tournament begins. Refer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict. Below we describe the format and fields of the contest data files. The data will likely be refreshed once in late February while Stage 1 of the competition is running. Many of the files are only complete through the end of last season. At the start of Stage 2, we will provide updates to these files to incorporate data from the current season. This section provides everything you need to build a simple prediction model and submit predictions. Special note about \"Season\" numbers: the college basketball season lasts from early November until the national championship tournament that starts in the middle of March. For instance, this year the first men\u2019s Division I games were played on November 25th, 2020 and the men\u2019s national championship game will be played on April 5th, 2021. Because a basketball season spans two calendar years like this, it can be confusing to refer to the year of the season. By convention, when we identify a particular season, we will reference the year that the season ends in, not the year that it starts in. So for instance, the current season will be identified in our data as the 2021 season, not the 2020 season or the 2020-21 season or the 2020-2021 season, though you may see any of these in everyday use outside of our data. Data Section 1 file: Teams.csv This file identifies the different college teams present in the dataset. Each school is uniquely identified by a 4 digit id number. You will not see games present for all teams in all seasons, because the games listing is only for matchups where both teams are Division-I teams. There are 357 teams currently in Division-I, and an overall total of 371 teams in our team listing (each year, some teams might start being Division-I programs, and others might stop being Division-I programs). This year there are four teams that are new to Division I: Bellarmine (TeamID=1469), North Alabama (TeamID=1469), Tarleton State (TeamID=1470), and UC_San Diego (TeamID=1471) and so you will not see any historical data for these teams prior to the current season. In addition, some teams opted not to play during the 2021 season due to the impact of COVID-19 and will not have any games listed.   Data Section 1 file: MSeasons.csv This file identifies the different seasons included in the historical data, along with certain season-level properties. Data Section 1 file: MNCAATourneySeeds.csv This file identifies the seeds for all teams in each NCAA\u00ae tournament, for all seasons of historical data. Thus, there are between 64-68 rows for each year, depending on whether there were any play-in games and how many there were. In recent years the structure has settled at 68 total teams, with four \"play-in\" games leading to the final field of 64 teams entering Round 1 on Thursday of the first week (by definition, that is DayNum=136 each season). We will not know the seeds of the respective tournament teams, or even exactly which 68 teams it will be, until Selection Sunday on March 14, 2021 (DayNum=132). Data Section 1 file: MRegularSeasonCompactResults.csv This file identifies the game-by-game results for many seasons of historical data, starting with the 1985 season (the first year the NCAA\u00ae had a 64-team tournament). For each season, the file includes all games played from DayNum 0 through 132. It is important to realize that the \"Regular Season\" games are simply defined to be all games played on DayNum=132 or earlier (DayNum=132 is Selection Sunday, and there are always a few conference tournament finals actually played early in the day on Selection Sunday itself). Thus a game played on or before Selection Sunday will show up here whether it was a pre-season tournament, a non-conference game, a regular conference game, a conference tournament game, or whatever. Data Section 1 file: MNCAATourneyCompactResults.csv This file identifies the game-by-game NCAA\u00ae tournament results for all seasons of historical data. The data is formatted exactly like the MRegularSeasonCompactResults data. All games will show up as neutral site (so WLoc is always N). Note that this tournament game data also includes the play-in games (which always occurred on day 134/135) for those years that had play-in games. Thus each season you will see between 63 and 67 games listed, depending on how many play-in games there were. Because of the consistent structure of the NCAA\u00ae tournament schedule, you can actually tell what round a game was, depending on the exact DayNum. Thus: Special note: Each year, there are also going to be other games that happened after Selection Sunday, which are not part of the NCAA\u00ae Tournament. This includes tournaments like the postseason NIT, the CBI, the CIT, and the Vegas 16. Such games are not listed in the Regular Season or the NCAA\u00ae Tourney files; they can be found in the \"Secondary Tourney\" data files within Data Section 6. Although they would not be games you would ever be predicting directly for the NCAA\u00ae tournament, and they would not be games you would have data from at the time of predicting NCAA\u00ae tournament outcomes, you may nevertheless wish to make use of these games for model optimization, depending on your methodology. The more games that you can test your predictions against, the better your optimized model might eventually become, depending on how applicable all those games are. A similar argument might be advanced in favor of optimizing your predictions against conference tournament games, which might be viewed as reasonable proxies for NCAA\u00ae tournament games.  Data Section 1 file: MSampleSubmissionStage1.csv This file illustrates the submission file format for Stage 1. It is the simplest possible submission: a 50% winning percentage is predicted for each possible matchup. A submission file lists every possible matchup between tournament teams for one or more years. During Stage 1, you are asked to make predictions for all possible matchups from the past five NCAA\u00ae tournaments (seasons 2015, 2016, 2017, 2018, 2019). In Stage 2, you will be asked to make predictions for all possible matchups from the current NCAA\u00ae tournament (season 2021). When there are 68 teams in the tournament, there are 68*67/2=2,278 predictions to make for that year, so a Stage 1 submission file will have 2,278*5=11,390 data rows. Example #1: You want to make a prediction for Duke (TeamID=1181) against Arizona (TeamID=1112) in the 2017 tournament, with Duke given a 53% chance to win and Arizona given a 47% chance to win. In this case, Arizona has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Arizona's perspective (47%): 2017_1112_1181,0.47 Example #2: You want to make a prediction for Duke (TeamID=1181) against North Carolina (TeamID=1314) in the 2018 tournament, with Duke given a 51.6% chance to win and North Carolina given a 48.4% chance to win. In this case, Duke has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Duke's perspective (51.6%): 2018_1181_1314,0.516 Also note that a single prediction row serves as a prediction for each of the two teams' winning chances. So for instance, in Example #1, the submission row of \"2017_1112_1181,0.47\" specifically gives a 47% chance for Arizona to win, and doesn't explicitly mention Duke's 53% chance to win. However, our evaluation utility will automatically infer the winning percentage in the other direction, so a 47% prediction for Arizona to win also means a 53% prediction for Duke to win. And similarly, because the submission row in Example #2 gives Duke a 51.6% chance to beat North Carolina, we will automatically figure out that this also means North Carolina has a 48.4% chance to beat Duke. This section provides game-by-game stats at a team level (free throws attempted, defensive rebounds, turnovers, etc.) for all regular season, conference tournament, and NCAA\u00ae tournament games since the 2002-03 season.  Team Box Scores are provided in \"Detailed Results\" files rather than \"Compact Results\" files. However, the two files are strongly related. In a Detailed Results file, the first eight columns (Season, DayNum, WTeamID, WScore, LTeamID, LScore, WLoc, and NumOT) are exactly the same as a Compact Results file. However, in a Detailed Results file, there are many additional columns. The column names should be self-explanatory to basketball fans (as above, \"W\" or \"L\" refers to the winning or losing team): (and then the same set of stats from the perspective of the losing team: LFGM is the number of field goals made by the losing team, and so on up to LPF). Note: by convention, \"field goals made\" (either WFGM or LFGM) refers to the total number of fields goals made by a team, a combination of both two-point field goals and three-point field goals. And \"three point field goals made\" (either WFGM3 or LFGM3) is just the three-point fields goals made, of course. So if you want to know specifically about two-point field goals, you have to subtract one from the other (e.g., WFGM - WFGM3). And the total number of points scored is most simply expressed as 2*FGM + FGM3 + FTM. Data Section 2 file: MRegularSeasonDetailedResults.csv This file provides team-level box scores for many regular seasons of historical data, starting with the 2003 season. All games listed in the MRegularSeasonCompactResults file since the 2003 season should exactly be present in the MRegularSeasonDetailedResults file. Data Section 2 file: MNCAATourneyDetailedResults.csv This file provides team-level box scores for many NCAA\u00ae tournaments, starting with the 2003 season. All games listed in the MNCAATourneyCompactResults file since the 2003 season should exactly be present in the MNCAATourneyDetailedResults file. This section provides city locations of all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season Data Section 3 file: Cities.csv This file provides a master list of cities that have been locations for games played. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two contests. Also note that if you created any supplemental data last year on cities (latitude/longitude, altitude, etc.), the CityID's match between last year and this year, so you should be able to re-use that information. Data Section 3 file: MGameCities.csv This file identifies all games, starting with the 2010 season, along with the city that the game was played in. Games from the regular season, the NCAA\u00ae tourney, and other post-season tournaments, are all listed together. There should be no games since the 2010 season where the CityID is not known. Games from the 2009 season and before are not listed in this file. This section provides weekly team rankings for dozens of top rating systems - Pomeroy, Sagarin, RPI, ESPN, etc., since the 2002-2003 season Data Section 4 file: MMasseyOrdinals.csv This file lists out rankings (e.g. #1, #2, #3, ..., #N) of teams going back to the 2002-2003 season, under a large number of different ranking system methodologies. The information was gathered by Kenneth Massey and provided on his College Basketball Ranking Composite page. Note that a rating system is more precise than a ranking system, because a rating system can provide insight about the strength gap between two adjacently-ranked teams. A ranking system will just tell you who is #1 or who is #2, but a rating system might tell you whether the gap between #1 and #2 is large or small. Nevertheless, it can be hard to compare two different rating systems that are expressed in different scales, so it can be very useful to express all the systems in terms of their ordinal ranking (1, 2, 3, ..., N) of teams. Disclaimer: you ought to be careful about your methodology when using or evaluating these ranking systems. They are presented on a weekly basis, and given a consistent date on the Massey Composite page that typically is a Sunday; that is how the ranking systems can be compared against each other on this page. However, these systems each follow their own timeline and some systems may be released on a Sunday and others on a Saturday or Monday or even Tuesday. You should remember that if a ranking is released on a Tuesday, and was calculated based on games played through Monday, it will make the system look unusually good at predicting if you use that system to forecast the very games played on Monday that already inform the rankings. To avoid this methodological trap, we have typically used a conservative RankingDayNum of Wednesday to represent the rankings that were released at approximately the end of the weekend, a few days before, even though those rankings are represented on the composite page as being on a Sunday. For some of the older years, a more precise timestamp was known for each ranking system that allowed a more precise assignment of a RankingDayNum. By convention, the final pre-tournament rankings are always expressed as RankingDayNum=133, even though sometimes the rankings for individual systems are not released until Tuesday (DayNum=134) or even Wednesday or Thursday. If you decide to use some rankings from these Massey Ordinals to inform your predictions, be forewarned that we have no control over when they are released, and not all systems may turn out to be available in time to make pre-tournament predictions by our submission deadline. In such a situation, you may wish to use the rankings from DayNum=128 or you may need to dig into the details of the actual source of the rankings, by following the respective links on the Massey Composite Page. We may also be able to provide partial releases of the final pre-tournament Massey Ordinals on the forums, so that as systems come in on Monday or Tuesday you can use them right away. This section contains additional supporting information, including coaches, conference affiliations, alternative team name spellings, bracket structure, and game results for NIT and other postseason tournaments. Data Section 5 file: MTeamCoaches.csv This file indicates the head coach for each team in each season, including a start/finish range of DayNum's to indicate a mid-season coaching change. For scenarios where a team had the same head coach the entire season, they will be listed with a DayNum range of 0 to 154 for that season. For head coaches whose term lasted many seasons, there will be many rows listed, most of which have a DayNum range of 0 to 154 for the corresponding season. Data Section 5 file: Conferences.csv This file indicates the Division I conferences that have existed over the years since 1985. Each conference is listed with an abbreviation and a longer name. There has been no attempt to link up conferences who merged with other conferences, or whose names changed over time. Thus you will see, for instance, a \"Pacific-10\" conference up through the 2011 season, and then a \"Pacific-12\" conference starting in the 2012 season, and these look like different conferences in the data, even though it was still mostly the same teams. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two contests. However, the Team Conferences data differs slightly between men's and women's, so those files do have the prefixes. Data Section 5 file: MTeamConferences.csv This file indicates the conference affiliations for each team during each season. Some conferences have changed their names from year to year, and/or changed which teams are part of the conference. This file tracks this information historically. Data Section 5 file: MConferenceTourneyGames.csv This file indicates which games were part of each year's post-season conference tournaments (all of which finished on Selection Sunday or earlier), starting from the 2001 season. Many of these conference tournament games are held on neutral sites, and many of the games are played by tournament-caliber teams just a few days before the NCAA\u00ae tournament. Thus these games could be considered as very similar to NCAA\u00ae tournament games, and (depending on your methodology) may be of use in optimizing your predictions. However, this is NOT a new listing of games; these games are already present within the MRegularSeasonCompactResults and MRegularSeasonDetailedResults files. So this file simply helps you to identify which of the \"regular season\" games since the 2001 season were actually conference tournament games, in case that is useful information. Data Section 5 file: MSecondaryTourneyTeams.csv This file identifies the teams that participated in post-season tournaments other than the NCAA\u00ae Tournament (such events would run in parallel with the NCAA\u00ae Tournament). These are teams that were not invited to the NCAA\u00ae Tournament and instead were invited to some other tournament, of which the NIT is the most prominent tournament, but there have also been the CBI, CIT, and Vegas 16 (V16) at various points in recent years. Depending on your methodology, you might find it useful to have these additional game results, above and beyond what is available from the NCAA\u00ae Tournament results. Many of these teams, especially in the NIT, were \"bubble\" teams of comparable strength to several NCAA\u00ae Tournament invitees, and so these games may be of use in model optimization for predicting NCAA\u00ae Tournament results. Also note that this information could be determined just from inspecting the MSecondaryTourneyCompactResults file, but is presented in this file as well, for your convenience. Data Section 5 file: MSecondaryTourneyCompactResults.csv This file indicates the final scores for the tournament games of \"secondary\" post-season tournaments: the NIT, CBI, CIT, and Vegas 16. The detailed results (team box scores) have not been assembled for these games. For the most part, this file is exactly like other Compact Results listings, although it also has a column for Secondary Tourney. Also note that because these games are played after DayNum=132, they are NOT listed in the MRegularSeasonCompactResults file. Data Section 5 file: MTeamSpellings.csv This file indicates alternative spellings of many team names. It is intended for use in associating external spellings against our own TeamID numbers, thereby helping to relate the external data properly with our datasets. Over the years we have identified various external spellings of different team names (as an example, for Ball State we have seen \"ball st\", and \"ball st.\", and \"ball state\", and \"ball-st\", and \"ball-state\"). Other teams have had more significant changes to their names over the years; for example, \"Texas Pan-American\" and \"Texas-Rio Grande Valley\" are actually the same school. The current list is obviously not exhaustive, and we encourage participants to identify additional mappings and upload extended versions of this file to the forums. Data Section 5 file: MNCAATourneySlots This file identifies the mechanism by which teams are paired against each other, depending upon their seeds, as the tournament proceeds through its rounds. It can be of use in identifying, for a given historical game, what round it occurred in, and what the seeds/slots were for the two teams (the meaning of \"slots\" is described below). Because of the existence of play-in games for particular seed numbers, the pairings have small differences from year to year. You may need to know these specifics if you are trying to represent/simulate the exact workings of the tournament bracket. Data Section 5 file: MNCAATourneySeedRoundSlots.csv This file helps to represent the bracket structure in any given year. No matter where the play-in seeds are located, we can always know, for a given tournament seed, exactly what bracket slot they would be playing in, on each possible game round, and what the possible DayNum values would be for that round. Thus, if we know when a historical game was played, and what the team's seed was, we can identify the slot for that game. This can be useful in representing or simulating the tournament bracket structure."
    },
    {
        "name": "March Machine Learning Mania 2021 - NCAAW",
        "url": "https://www.kaggle.com/competitions/ncaaw-march-mania-2021",
        "overview_text": "Overview text not found",
        "description_text": "Back again after a pandemic-year hiatus, Kaggle's March Machine Learning Mania challenges data scientists to predict winners and losers of the women's 2021 NCAA basketball tournament. You're provided data of historical NCAA games and are encouraged to use other sources of publicly available data to gain a winning edge.  In stage one of this two-stage competition, participants will build and test their models against previous tournaments. In the second stage, participants will predict the outcome of the 2021 tournament. You don\u2019t need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2021 results. Note: From 2018-2020, Google Cloud, of which Kaggle is a part, served as official corporate sponsors of the NCAA basketball tournaments. This official sponsorship has concluded, but we at Kaggle are happy to bring back this tradition for its 8th year! Banner image by Ben Hershey on Unsplash",
        "dataset_text": "Each season there are thousands of NCAA\u00ae basketball games played between Division I women's teams, culminating in March Madness, the 64-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness\u00ae game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past. If you are unfamiliar with the format and intricacies of the tournament, we encourage reading the wikipedia page before diving into the data. The data description and schema may seem daunting at first, but is not as complicated as it appears. As a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The WTeamSpellings file, which is listed in the bottom section below, may help you map external team references into our own Team ID structure. You will probably also need to understand exactly how dates work in our data. Remember as well that you are required to disclose your external sources of data prior to the start of the tournament. We extend our gratitude to Kenneth Massey for providing much of the historical data. Special Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition. Stage 1 - You should submit predicted probabilities for every possible matchup in the past 5 NCAA\u00ae tournaments (seasons 2015-2019). Stage 2 - You should submit predicted probabilities for every possible matchup before the 2021 tournament begins. Refer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict. Below we describe the format and fields of the contest data files. The data will likely be refreshed once in early March while Stage 1 of the competition is running. Many of the files are only complete through the end of last season. At the start of Stage 2, we will provide updates to these files to incorporate data from the current season. Right now the WSeasons and WTeamConferences files are the only two files that reference the 2021 season, since those are the only files where the final 2021 data is already finalized; the other files will be updated later on when we provide preliminary data (in early March) and final Stage 2 data (after the Selection Show) for the current season. This section provides everything you need to build a simple prediction model and submit predictions. Special note about \"Season\" numbers: the college basketball season lasts from early November until the national championship tournament that starts in the middle of March. For instance, this year the first women\u2019s Division I games were played on November 25th, 2020 and the women\u2019s national championship game will be played on April 4th, 2021. Because a basketball season spans two calendar years like this, it can be confusing to refer to the year of the season. By convention, when we identify a particular season, we will reference the year that the season ends in, not the year that it starts in. So for instance, the current season will be identified in our data as the 2021 season, not the 2020 season or the 2020-21 season or the 2020-2021 season, though you may see any of these in everyday use outside of our data. Data Section 1 file: WTeams.csv  This file identifies the different college teams present in the dataset. Each school is uniquely identified by a 4 digit id number. You will not see games present for all teams in all seasons, because the games listing is only for matchups where both teams are Division-I teams. There are 355 teams currently in Division-I, and an overall total of 369 teams in our team listing. Each year, some teams might start being Division-I programs, and others might stop being Division-I programs, and in fact this year is no exception. This year there are four teams that are new to Division I: Bellarmine (TeamID=3468), North Alabama (TeamID=3469), Tarleton State (TeamID=3470), and UC_San Diego (TeamID=3471) and so you will not see any historical data for these teams prior to the current season. In addition, some teams opted not to play during the 2021 season due to the impact of COVID-19 and will not have any games listed.  Data Section 1 file: WSeasons.csv This file identifies the different seasons included in the historical data, along with certain season-level properties. The game dates in this dataset are expressed in relative terms, as the number of days since the start of the regular season, and aligned for each season so that day number #133 is the Monday right before the tournament, when team selections are made. During any given season, day number zero is defined to be exactly 19 weeks earlier than Selection Monday, so Day #0 is a Monday in late October or early November such that Day #132 is Selection Sunday (for the men's tournament) and Day #133 is Selection Monday (for the women's tournament). This doesn't necessarily mean that the regular season will always start exactly on day #0 or day #1; in fact, during the past decade, regular season games typically start being played on a Friday that is either Day #4 or Day #11, but further back there was more variety. Data Section 1 file: WNCAATourneySeeds.csv This file identifies the seeds for all teams in each NCAA\u00ae tournament, for all seasons of historical data. Thus, there are exactly 64 rows for each year, since there are no play-in teams in the women's tournament. We will not know the seeds of the respective tournament teams, or even exactly which 64 teams it will be, until Selection Monday on March 16, 2020 (DayNum=133). Data Section 1 file: WRegularSeasonCompactResults.csv This file identifies the game-by-game results for many seasons of historical data, starting with the 1998 season. For each season, the file includes all games played from DayNum 0 through 132. It is important to realize that the \"Regular Season\" games are simply defined to be all games played on DayNum=132 or earlier (DayNum=133 is Selection Monday). Thus a game played before Selection Monday will show up here whether it was a pre-season tournament, a non-conference game, a regular conference game, a conference tournament game, or whatever. Data Section 1 file: WNCAATourneyCompactResults.csv This file identifies the game-by-game NCAA\u00ae tournament results for all seasons of historical data. The data is formatted exactly like the WRegularSeasonCompactResults data. Each season you will see 63 games listed, since there are no women's play-in games. Although the scheduling of the men's tournament rounds has been consistent for many years, there has been more variety in the scheduling of the women's rounds. There have been four different schedules over the course of the past 20+ years for the women's tournament, as follows: 2017 season through 2021 season: 2015 season and 2016 season: 2003 season through 2014 season: 1998 season through 2002 season: Data Section 1 file: WSampleSubmissionStage1.csv This file illustrates the submission file format for Stage 1. It is the simplest possible submission: a 50% winning percentage is predicted for each possible matchup. A submission file lists every possible matchup between tournament teams for one or more years. During Stage 1, you are asked to make predictions for all possible matchups from the past five NCAA\u00ae tournaments (seasons 2015, 2016, 2017, 2018, and 2019). In Stage 2, you will be asked to make predictions for all possible matchups from the current NCAA\u00ae tournament (season 2021). Since there are 64 teams in the tournament, there are 64*63/2=2,016 predictions to make for that year, so a Stage 1 submission file will have 2016*5=10,080 data rows. Example #1: You want to make a prediction for Duke (TeamID=3181) against Arizona (TeamID=3112) in the 2005 tournament, with Duke given a 53% chance to win and Arizona given a 47% chance to win. In this case, Arizona has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Arizona's perspective (47%): 2005_3112_3181,0.47 Example #2: You want to make a prediction for Duke (TeamID=3181) against North Carolina (TeamID=3314) in the 2005 tournament, with Duke given a 51.6% chance to win and North Carolina given a 48.4% chance to win. In this case, Duke has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Duke's perspective (51.6%): 2005_3181_3314,0.516 Also note that a single prediction row serves as a prediction for each of the two teams' winning chances. So for instance, in Example #1, the submission row of \"2005_3112_3181,0.47\" specifically gives a 47% chance for Arizona to win, and doesn't explicitly mention Duke's 53% chance to win. However, our evaluation utility will automatically infer the winning percentage in the other direction, so a 47% prediction for Arizona to win also means a 53% prediction for Duke to win. And similarly, because the submission row in Example #2 gives Duke a 51.6% chance to beat North Carolina, we will automatically figure out that this also means North Carolina has a 48.4% chance to beat Duke. This section provides game-by-game stats at a team level (free throws attempted, defensive rebounds, turnovers, etc.) for all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season.  Team Box Scores are provided in \"Detailed Results\" files rather than \"Compact Results\" files. However, the two files are strongly related. In a Detailed Results file, the first eight columns (Season, DayNum, WTeamID, WScore, LTeamID, LScore, WLoc, and NumOT) are exactly the same as a Compact Results file. However, in a Detailed Results file, there are many additional columns. The column names should be self-explanatory to basketball fans (as above, \"W\" or \"L\" refers to the winning or losing team): (and then the same set of stats from the perspective of the losing team: LFGM is the number of field goals made by the losing team, and so on up to LPF). Note: by convention, \"field goals made\" (either WFGM or LFGM) refers to the total number of fields goals made by a team, a combination of both two-point field goals and three-point field goals. And \"three point field goals made\" (either WFGM3 or LFGM3) is just the three-point fields goals made, of course. So if you want to know specifically about two-point field goals, you have to subtract one from the other (e.g., WFGM - WFGM3). And the total number of points scored is most simply expressed as 2*FGM + FGM3 + FTM. Data Section 2 file: WRegularSeasonDetailedResults.csv This file provides team-level box scores for many regular seasons of historical data, starting with the 2010 season. All games listed in the WRegularSeasonCompactResults file since the 2010 season should exactly be present in the WRegularSeasonDetailedResults file. Data Section 2 file: WNCAATourneyDetailedResults.csv This file provides team-level box scores for many NCAA\u00ae tournaments, starting with the 2010 season. All games listed in the WNCAATourneyCompactResults file since the 2010 season should exactly be present in the WNCAATourneyDetailedResults file. This section provides city locations of all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season Data Section 3 file: Cities.csv This file provides a master list of cities that have been locations for games played. Please notice that the Cities and Conferences files are the only two that don't start with an W; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two contests. Data Section 3 file: WGameCities.csv This file identifies all games, starting with the 2010 season, along with the city that the game was played in. Games from the regular season and the NCAA\u00ae tourney are all listed together. The CityID is present in more than 98% of games since the 2010 season. Games from the 2009 season and before are not listed in this file. This section contains additional supporting information, including alternative team name spellings and representations of bracket structure Data Section 4 file: WTeamSpellings.csv This file indicates alternative spellings of many team names. It is intended for use in associating external spellings against our own TeamID numbers, thereby helping to relate the external data properly with our datasets. Over the years we have identified various external spellings of different team names (as an example, for Ball State we have seen \"ball st\", and \"ball st.\", and \"ball state\", and \"ball-st\", and \"ball-state\"). Other teams have had more significant changes to their names over the years; for example, \"Texas Pan-American\" and \"Texas-Rio Grande Valley\" are actually the same school. The current list is obviously not exhaustive, and we encourage participants to identify additional mappings and upload extended versions of this file to the forums. Data Section 4 file: WNCAATourneySlots This file identifies the mechanism by which teams are paired against each other, depending upon their seeds, as the tournament proceeds through its rounds. It can be of use in identifying, for a given historical game, what round it occurred in, and what the seeds/slots were for the two teams (the meaning of \"slots\" is described below). You may need to know these specifics if you are trying to represent/simulate the exact workings of the tournament bracket. Unlike the analogous file on the men's side, it is not necessary to provide a Season within this file, because the women's tournament has never had play-in-games and so the 64-team women's bracket has always had the same structure each season. Data Section 4 file: Conferences.csv This file indicates the Division I conferences that have existed over the years since 1985. Each conference is listed with an abbreviation and a longer name. There has been no attempt to link up conferences who merged with other conferences, or whose names changed over time. Thus you will see, for instance, a \"Pacific-10\" conference up through the 2011 season, and then a \"Pacific-12\" conference starting in the 2012 season, and these look like different conferences in the data, even though it was still mostly the same teams. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two contests. However, the Team Conferences data differs slightly between men's and women's, so those files do have the prefixes. Data Section 4 file: WTeamConferences.csv This file indicates the conference affiliations for each team during each season. Some conferences have changed their names from year to year, and/or changed which teams are part of the conference. This file tracks this information historically."
    },
    {
        "name": "Google Landmark Retrieval 2021",
        "url": "https://www.kaggle.com/competitions/landmark-retrieval-2021",
        "overview_text": "Overview text not found",
        "description_text": "Welcome to the fourth Landmark Retrieval competition! This year, we introduce a lot more diversity in the challenge\u2019s test images in order to measure global landmark retrieval performance in a fairer manner. And following last year\u2019s success, we set this up as a code competition. Image retrieval is a central problem in computer vision, relevant to many applications. The problem is usually posed as follows: given a query image, can you find similar images in a large database? This is especially important for query images containing landmarks, which accounts for a large portion of what people like to photograph. In this competition, the developed models are expected to retrieve relevant database images to a given query image (i.e., the model should retrieve database images containing the same landmark as the query). This challenge is organized in conjunction with the Landmark Recognition Challenge 2021. Both challenges will be discussed at the Instance-Level Recognition workshop in ICCV 21. In contrast to previous editions of this challenge (2018, 2019, and 2020), this year's competition is structured in a synchronous rerun format, where participants need to submit their Kaggle notebooks for scoring.",
        "dataset_text": "In this competition, you are asked to develop models that can efficiently retrieve landmark images from a large database. The training set is available in the train/ folder, with corresponding landmark labels in train.csv. The query images are listed in the test/ folder, while the \"index\" images from which you are retrieving are listed in index/. Each image has a unique id. Since there are a large number of images, each image is placed within three subfolders according to the first three characters of the image id (i.e. image abcdef.jpg is placed in a/b/c/abcdef.jpg). This is a synchronous rerun code competition. The provided test set is a representative set of files to demonstrate the format of the private test set. When you submit your notebook, Kaggle will rerun your code on the private dataset. For this year, we introduce new test and index sets which are sampled from many countries, increasing the diversity in worldwide representation. See our paper for more details. The training data for this competition comes from a cleaned version of the Google Landmarks Dataset v2 (GLDv2), which is available here. Please refer to the paper for more details on the dataset construction and how to use it. See this code example for an example of a pretrained model. If you make use of this dataset in your research, please consider citing: and specifically for this year's challenge:"
    },
    {
        "name": "Google Landmark Recognition 2021",
        "url": "https://www.kaggle.com/competitions/landmark-recognition-2021",
        "overview_text": "Overview text not found",
        "description_text": "Welcome to the fourth Landmark Recognition competition! This year, we introduce a lot more diversity in the challenge\u2019s test images in order to measure global landmark recognition performance in a fairer manner. And following last year\u2019s success, we set this up as a code competition. Have you ever gone through your vacation photos and asked yourself: What is the name of this temple I visited in China? Who created this monument I saw in France? Landmark recognition can help! This technology can predict landmark labels directly from image pixels, to help people better understand and organize their photo collections. This competition challenges Kagglers to build models that recognize the correct landmark (if any) in a dataset of challenging test images. Many Kagglers are familiar with image classification challenges like the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which aims to recognize 1K general object categories. Landmark recognition is a little different from that: it contains a much larger number of classes (there are more than 81K classes in this challenge), and the number of training examples per class may not be very large. Landmark recognition is challenging in its own way. This year's competition is structured in a synchronous rerun format, where participants need to submit their Kaggle notebooks for scoring. This is similar to the 2020 version of the competition. In older editions (2018 and 2019), submissions had been handled by uploading prediction files to the system. This challenge is organized in conjunction with the Landmark Retrieval Challenge 2021. Both challenges will be discussed at the Instance-Level Recognition workshop in ICCV 21. Cover image credits: Muhammad Mahdi Karim. The original is available on Wikimedia here. License: GNU Free Documentation License.",
        "dataset_text": "In this competition, you are asked to take test images and recognize which landmarks (if any) are depicted in them. The training set is available in the train/ folder, with corresponding landmark labels in train.csv. The test set images are listed in the test/ folder. Each image has a unique id. Since there are a large number of images, each image is placed within three subfolders according to the first three characters of the image id (i.e. image abcdef.jpg is placed in a/b/c/abcdef.jpg). This is a synchronous rerun code competition. The provided test set is a representative set of files to demonstrate the format of the private test set. When you submit your notebook, Kaggle will rerun your code on the private dataset. Additionally, this competition also has two unique characteristics: For this year, we introduce a new test set which is sampled from many countries, increasing the diversity in worldwide representation. See our paper for more details. The training data for this competition comes from a cleaned version of the Google Landmarks Dataset v2 (GLDv2), which is available here. Please refer to the paper for more details on the dataset construction and how to use it. See this code example for an example of a pretrained model. If you make use of this dataset in your research, please consider citing: and specifically for this year's challenge:"
    },
    {
        "name": "Santa's Uncertain Bags",
        "url": "https://www.kaggle.com/competitions/santas-uncertain-bags",
        "overview_text": "Overview text not found",
        "description_text": "All was well in Santa's workshop. The gifts were made, the route was planned, the naughty and nice list complete. Santa thought this would finally be the year he didn't need Kaggle's help with his combinatorial conundrums. At last, the Claus family could take the elves and reindeer on that well deserved vacation to the South Pole. Then, with just days until the big night, Santa received an email from a panicked database admin elf. Attached was a server log with the six least jolly words a jolly old St. Nick could read: One of the North Pole elf interns had mistakenly deleted the weights for all of the inventory in the workshop! Santa didn't have a backup (remember, this is a guy who makes a list and checks it twice) and, without knowing each present's weight, he didn't know how he would safely pack his many gift bags. Gifts were already on their way to the sleigh packing facility and there wasn't time to re-weigh all the presents. It was once again necessary to summon the holiday talents of Kaggle's elite. Can you help Santa fill his multiple bags with sets of uncertain gifts? Save the season by turning Santa's uncertain probabilities into presents for good little boys and girls.",
        "dataset_text": "Santa has 1000 bags to fill to fill with 9 types of gifts. Due to regulations at the North Pole workshop, no bag can contain more than 50 pounds of gifts. If a bag is overweight, it is confiscated by regulators from the North Pole Department of Labor without warning! Even Santa has to worry about throwing out his bad back. Each present has a fixed weight, but the individual weights are unknown. The weights for each present type are not identical because the elves make them in many types and sizes. Although the weights were deleted from the database, the elves still have the blueprints for each toy. After some complex volume integrals, the elves managed to give Santa a probability distribution for the weight of each type of toy. To simulate a single gift's weight in pounds, they came up with the following numpy distribution parameters: gifts.csv contains the GiftIds which you must sort into Santa's bags. The text of the GiftId contains the type of toy. You do not need to include all GiftIds or all bags when submitting. The evaluation page provides full details on scoring."
    },
    {
        "name": "Rock, Paper, Scissors",
        "url": "https://www.kaggle.com/competitions/rock-paper-scissors",
        "overview_text": "Overview text not found",
        "description_text": "Rock, Paper, Scissors (sometimes called roshambo) has been a staple to settle playground disagreements or determine who gets to ride in the front seat on a road trip. The game is simple, with a balance of power. There are three options to choose from, each winning or losing to the other two. In a series of truly random games, each player would win, lose, and draw roughly one-third of games. But people are not truly random, which provides a fun opportunity for AI. Studies have shown that a Rock, Paper, Scissors AI can consistently beat human opponents. With previous games as input, it studies patterns to understand a player\u2019s tendencies. But what happens when we expand the simple \u201cBest-of-3\u201d game to be \u201cBest-of-1000\u201d? How well can artificial intelligence perform? In this simulation competition, you will create an AI to play against others in many rounds of this classic game. Can you find patterns to make yours win more often than it loses? It\u2019s possible to greatly outperform a random player when the matches involve non-random agents. A strong AI can consistently beat predictable AI. This problem is fundamental to the fields of machine learning, artificial intelligence, and data compression. There are even potential applications in human psychology and hierarchical temporal memory. Warm up your hands and get ready to Rock, Paper, Scissors in this challenge. Image acknowledgements:\nPhotos from The Noun Project: Rock, Paper, Scissors",
        "dataset_text": "This page appears underneath the data files. It describes what files have been provided & the format of each. It also defines the correct format for submission files. Participants should be able to answer these types of questions after reading the data description: What files do I need?\nWhat should I expect the data format to be?\nWhat am I predicting?\nWhat acronyms will I encounter?"
    },
    {
        "name": "Hungry Geese",
        "url": "https://www.kaggle.com/competitions/hungry-geese",
        "overview_text": "Overview text not found",
        "description_text": "Whether it be in an arcade, on a phone, as an app, on a computer, or maybe stumbled upon in a web search, many of us have likely developed fond memories playing a version of Snake. It\u2019s addicting to control a slithering serpent and watch it grow along the grid until you make one\u2026 wrong\u2026 move. Then you have to try again because surely you won\u2019t make the same mistake twice! With Hungry Geese, Kaggle has taken this classic in the video game industry and put a multi-player, simulation spin to it. You will create an AI agent to play against others and survive the longest. You must make sure your goose doesn\u2019t starve or run into other geese; it\u2019s a good thing that geese love peppers, donuts, and pizza\u2014which show up across the board. Extensive research exists in building Snake models using reinforcement learning, Q-learning, neural networks, and more (maybe you\u2019ll use\u2026 Python?). Take your grid-based reinforcement learning knowledge to the next level with this exciting new challenge!",
        "dataset_text": "This page appears underneath the data files. It describes what files have been provided & the format of each. It also defines the correct format for submission files. Participants should be able to answer these types of questions after reading the data description: What files do I need?\nWhat should I expect the data format to be?\nWhat am I predicting?\nWhat acronyms will I encounter?"
    },
    {
        "name": "March Machine Learning Mania 2017",
        "url": "https://www.kaggle.com/competitions/march-machine-learning-mania-2017",
        "overview_text": "Overview text not found",
        "description_text": "Another year, another chance to predict the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. In our fourth annual March Machine Learning Mania competition, Kagglers will once again join the millions of fans who attempt to predict the outcomes of this year's US men's college basketball tournament. But unlike most fans, you will pick the winners and losers using a combination of rich historical data and computing power, while the ground truth unfolds on national television.  In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible match-ups in the 2017 tournament. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2017 results.",
        "dataset_text": "If you are unfamiliar with the format and intricacies of the NCAA tournament, we encourage reading the wikipedia page before diving into the data.  The data description and schema may seem daunting at first, but is not as complicated as it appears. As a reminder, you are encouraged to incorporate your own sources of data. We have provided team-level historical data to jump-start the modeling process, but there is also player-level and game-level data that may be useful. We extend our gratitude to Kenneth Massey for providing much of the historical data. Stage 1 - You should submit predicted probabilities for every possible matchup in the past 4 NCAA tournaments (2013-2016). Stage 2 - You should submit predicted probabilities for every possible matchup before the 2017 tournament begins. Refer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict. Below we describe the format and fields of the \"essential\" data files. Optional files may be added to the data while the competition is running. You can assume that we will provide the essential files for the current season. You should not assume that we will provide optional files for the current season. Teams This file identifies the different college teams present in the dataset. Each team has a 4 digit id number. Seasons This file identifies the different seasons included in the historical data, along with certain season-level properties. RegularSeasonCompactResults This file identifies the game-by-game results for 32 seasons of historical data, from 1985 to 2015. Each year, it includes all games played from daynum 0 through 132 (which by definition is \"Selection Sunday,\" the day that tournament pairings are announced). Each row in the file represents a single game played. RegularSeasonDetailedResults This file is a more detailed set of game results, covering seasons 2003-2016. This includes team-level total statistics for each game (total field goals attempted, offensive rebounds, etc.) The column names should be self-explanatory to basketball fans (as above, \"w\" or \"l\" refers to the winning or losing team): TourneyCompactResults This file identifies the game-by-game NCAA tournament results for all seasons of historical data. The data is formatted exactly like the regular_season_compact_results.csv data. Note that these games also include the play-in games (which always occurred on day 134/135) for those years that had play-in games. TourneyDetailedResults This file contains the more detailed results for tournament games from 2003 onward. TourneySeeds This file identifies the seeds for all teams in each NCAA tournament, for all seasons of historical data. Thus, there are between 64-68 rows for each year, depending on the bracket structure. TourneySlots This file identifies the mechanism by which teams are paired against each other, depending upon their seeds. Because of the existence of play-in games for particular seed numbers, the pairings have small differences from year to year. If there were N teams in the tournament during a particular year, there were N-1 teams eliminated (leaving one champion) and therefore N-1 games played, as well as N-1 slots in the tournament bracket, and thus there will be N-1 records in this file for that season."
    },
    {
        "name": "COVID19 Global Forecasting (Week 2)",
        "url": "https://www.kaggle.com/competitions/covid19-global-forecasting-week-2",
        "overview_text": "Overview text not found",
        "description_text": "This week 2 forecasting task is now closed for submissions. Click here to visit the week 3 version, and make a submission there. This is week 2 of Kaggle's COVID19 forecasting series, following the Week 1 competition. This is the 2nd of at least 4 competitions we plan to launch in this series. The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle) to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from National Academies of Sciences, Engineering, and Medicine\u2019s (NASEM) and the World Health Organization (WHO). Kaggle is launching a companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves forecasting confirmed cases and fatalities between April 1 and April 30 by region, the primary goal isn't only to produce accurate forecasts. It\u2019s also to identify factors that appear to impact the transmission rate of COVID-19. You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook. As the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE). We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19. There is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle\u2019s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community. JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.",
        "dataset_text": "In this challenge, you will be predicting the cumulative number of confirmed COVID19 cases in various locations across the world, as well as the number of resulting fatalities, for future dates. We understand this is a serious situation, and in no way want to trivialize the human impact this crisis is causing by predicting fatalities. Our goal is to provide better methods for estimates that can assist medical and governmental institutions to prepare and adjust as pandemics unfold."
    },
    {
        "name": "COVID19 Global Forecasting (Week 3)",
        "url": "https://www.kaggle.com/competitions/covid19-global-forecasting-week-3",
        "overview_text": "Overview text not found",
        "description_text": "This week 3 forecasting task is now closed for submissions. Click here to visit the week 4 version, and make a submission there. This is week 3 of Kaggle's COVID19 forecasting series, following the Week 2 competition. This is the 3rd of at least 4 competitions we plan to launch in this series. All of the prior discussion forums have been migrated to this competition for continuity. The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle) to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from National Academies of Sciences, Engineering, and Medicine\u2019s (NASEM) and the World Health Organization (WHO). Kaggle is launching a companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves forecasting confirmed cases and fatalities between April 1 and April 30 by region, the primary goal isn't only to produce accurate forecasts. It\u2019s also to identify factors that appear to impact the transmission rate of COVID-19. You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook. As the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE). We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19. There is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle\u2019s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community. JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.",
        "dataset_text": "In this challenge, you will be predicting the cumulative number of confirmed COVID19 cases in various locations across the world, as well as the number of resulting fatalities, for future dates. We understand this is a serious situation, and in no way want to trivialize the human impact this crisis is causing by predicting fatalities. Our goal is to provide better methods for estimates that can assist medical and governmental institutions to prepare and adjust as pandemics unfold."
    },
    {
        "name": "Facebook Recruiting Competition",
        "url": "https://www.kaggle.com/competitions/FacebookRecruiting",
        "overview_text": "Overview text not found",
        "description_text": "Want an interview at Facebook?  Facebook will review the top entries in the competition and offer you an interview if they like what they see.  This is your opportunity to demonstrate your skills on a real-world social network dataset, and show them your creativity, open-mindedness and tenacity in the face of an open-ended predictive modeling problem. The challenge is to recommend missing links in a social network.  Participants will be presented with an external anonymized, directed social graph (no, not Facebook, keep guessing) from which some edges have been deleted, and asked to make ranked predictions for each user in the test set of which other users they would want to follow.   Please note: You must compete as an individual in recruiting competitions.  You may only use the data provided to make your predictions.  Facebook will review the code of the top participants before deciding whether to offer an interview.",
        "dataset_text": "The code for the benchmarks may be downloaded from github.  There are 5 files:"
    },
    {
        "name": "Digit Recognizer",
        "url": "https://www.kaggle.com/competitions/digit-recognizer",
        "overview_text": "Overview text not found",
        "description_text": "You have some experience with R or Python and machine learning basics, but you\u2019re new to computer vision. This competition is the perfect introduction to techniques like neural networks using a classic dataset including pre-extracted features. MNIST (\"Modified National Institute of Standards and Technology\") is the de facto \u201chello world\u201d dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike. In this competition, your goal is to correctly identify digits from a dataset of tens of thousands of handwritten images. We\u2019ve curated a set of tutorial-style kernels which cover everything from regression to neural networks. We encourage you to experiment with different algorithms to learn first-hand what works well and how techniques compare. More details about the dataset, including algorithms that have been tried on it and their levels of success, can be found at http://yann.lecun.com/exdb/mnist/index.html. The dataset is made available under a Creative Commons Attribution-Share Alike 3.0 license.",
        "dataset_text": "The data files train.csv and test.csv contain gray-scale images of hand-drawn digits, from zero through nine. Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive. The training data set, (train.csv), has 785 columns. The first column, called \"label\", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image. Each pixel column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero). For example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below. Visually, if we omit the \"pixel\" prefix, the pixels make up the image like this: The test data set, (test.csv), is the same as the training set, except that it does not contain the \"label\" column. Your submission file should be in the following format: For each of the 28000 images in the test set, output a single line containing the ImageId and the digit you predict. For example, if you predict that the first image is of a 3, the second image is of a 7, and the third image is of a 8, then your submission file would look like: The evaluation metric for this contest is the categorization accuracy, or the proportion of test images that are correctly classified. For example, a categorization accuracy of 0.97 indicates that you have correctly classified all but 3% of the images."
    },
    {
        "name": "CIFAR-10 - Object Recognition in Images",
        "url": "https://www.kaggle.com/competitions/cifar-10",
        "overview_text": "Overview text not found",
        "description_text": "CIFAR-10  is an established computer-vision dataset used for object recognition. It is a subset of the 80 million tiny images dataset and consists of 60,000 32x32 color images containing one of 10 object classes, with 6000 images per class. It was collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Kaggle is hosting a CIFAR-10 leaderboard for the machine learning community to use for fun and practice. You can see how your approach compares to the latest research methods on Rodrigo Benenson's classification results page.  Please cite this technical report if you use this dataset: Learning Multiple Layers of Features from Tiny Images, Alex Krizhevsky, 2009.",
        "dataset_text": "The CIFAR-10 data consists of 60,000 32x32 color images in 10 classes, with 6000 images per class. There are 50,000 training images and 10,000 test images in the official data. We have preserved the train/test split from the original dataset.  The provided files are: train.7z - a folder containing the training images in png format\ntest.7z - a folder containing the test images in png format\ntrainLabels.csv - the training labels To discourage certain forms of cheating (such as hand labeling) we have added 290,000 junk images in the test set. These images are ignored in the scoring. We have also made trivial modifications to the official 10,000 test images to prevent looking them up by file hash. These modifications should not appreciably affect the scoring. You should predict labels for all 300,000 images. The label classes in the dataset are: The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks."
    },
    {
        "name": "The Analytics Edge (15.071x)",
        "url": "https://www.kaggle.com/competitions/the-analytics-edge-mit-15-071x",
        "overview_text": "Overview text not found",
        "description_text": "(Please note: this competition is only open to students of https://www.edx.org/course/mitx/mitx-15-071x-analytics-edge-1416) What predicts happiness? In this competition, you'll be using data from Show of Hands, an informal polling platform for use on mobile devices and the web, to see what aspects and characteristics of people's lives predict happiness. Show of Hands has been downloaded over 300,000 times across Apple and Android app stores, and users have cast more than 75 million votes. In this problem, we'll use data from thousands of users and one hundred different questions to see which responses predict happiness.  This competition is brought to you by 15.071x, edX, and Show of Hands.",
        "dataset_text": "Here is a description of the files you have been provided for this competition:"
    },
    {
        "name": "Sentiment Analysis on Movie Reviews",
        "url": "https://www.kaggle.com/competitions/sentiment-analysis-on-movie-reviews",
        "overview_text": "Overview text not found",
        "description_text": "\"There's a thin line between likably old-fashioned and fuddy-duddy, and The Count of Monte Cristo ... never quite settles on either side.\" The Rotten Tomatoes movie review dataset is a corpus of movie reviews used for sentiment analysis, originally collected by Pang and Lee [1]. In their work on sentiment treebanks, Socher et al. [2] used Amazon's Mechanical Turk to create fine-grained labels for all parsed phrases in the corpus. This competition presents a chance to benchmark your sentiment-analysis ideas on the Rotten Tomatoes dataset. You are asked to label phrases on a scale of five values: negative, somewhat negative, neutral, somewhat positive, positive. Obstacles like sentence negation, sarcasm, terseness, language ambiguity, and many others make this task very challenging.  Kaggle is hosting this competition for the machine learning community to use for fun and practice. This competition was inspired by the work of Socher et al [2]. We encourage participants to explore the accompanying (and dare we say, fantastic) website that accompanies the paper: http://nlp.stanford.edu/sentiment/ There you will find have source code, a live demo, and even an online interface to help train the model. [1] Pang and L. Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In ACL, pages 115\u2013124. [2] Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank, Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Chris Manning, Andrew Ng and Chris Potts. Conference on Empirical Methods in Natural Language Processing (EMNLP 2013). Image credits: Popcorn - Maura Teague, http://www.flickr.com/photos/93496438@N06/",
        "dataset_text": "The dataset is comprised of tab-separated files with phrases from the Rotten Tomatoes dataset. The train/test split has been preserved for the purposes of benchmarking, but the sentences have been shuffled from their original order. Each Sentence has been parsed into many phrases by the Stanford parser. Each phrase has a PhraseId. Each sentence has a SentenceId. Phrases that are repeated (such as short/common words) are only included once in the data. The sentiment labels are: 0 - negative\n1 - somewhat negative\n2 - neutral\n3 - somewhat positive\n4 - positive"
    },
    {
        "name": "Billion Word Imputation",
        "url": "https://www.kaggle.com/competitions/billion-word-imputation",
        "overview_text": "Overview text not found",
        "description_text": "This competition uses the billion-word benchmark corpus provided by Chelba et al. for language modeling. Rather than ask participants to create a classic language model and evaluate sentence probabilities -- a task which is difficult to faithfully score in Kaggle's supervised ML setting -- we have introduced a variation on the language modeling task. For each sentence in the test set, we have removed exactly one word. Participants must create a model capable of inserting back the correct missing word at the correct location in the sentence. Submissions are scored using an edit distance to allow for partial credit. We extend our thanks to authors who created this corpus and shared it for the research community to use. Please cite this paper if you use this dataset in your research: Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn: One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling, CoRR, 2013. Note: the train/test split used in this competition is different than the published version used for language modeling. If you are creating full language models and scoring perplexity, you should download the official version of the corpus from the authors' website.",
        "dataset_text": "The data for this competition is a large corpus of English language sentences. You should use only the sentences in the training set to build you model. We have removed one word from each sentence in the test set. The location of the removed word was chosen uniformly randomly and is never the first or last word of the sentence (in this dataset, the last word is always a period). You must attempt to submit the sentences in the test set with the correct missing word located in the correct location.  Note: the train/test split used in this competition is different than the published version used for language modeling. If you are creating full language models and scoring perplexity, you should download the official version of the corpus from the authors' website."
    },
    {
        "name": "Forest Cover Type Prediction",
        "url": "https://www.kaggle.com/competitions/forest-cover-type-prediction",
        "overview_text": "Overview text not found",
        "description_text": "Get started on this competition with Kaggle Scripts. No data download or local environment needed! Random forests? Cover trees? Not so fast, computer nerds. We're talking about the real thing. In this competition you are asked to predict the forest cover type (the predominant kind of tree cover) from strictly cartographic variables (as opposed to remotely sensed data). The actual forest cover type for a given 30 x 30 meter cell was determined from US Forest Service (USFS) Region 2 Resource Information System data. Independent variables were then derived from data obtained from the US Geological Survey and USFS. The data is in raw form (not scaled) and contains binary columns of data for qualitative independent variables such as wilderness areas and soil type. This study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological processes rather than forest management practices. Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was provided by Jock A. Blackard and Colorado State University. We also thank the UCI machine learning repository for hosting the dataset. If you use the problem in publication, please cite: Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science",
        "dataset_text": "The study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. Each observation is a 30m x 30m patch. You are asked to predict an integer classification for the forest cover type. The seven types are: 1 - Spruce/Fir\n2 - Lodgepole Pine\n3 - Ponderosa Pine\n4 - Cottonwood/Willow\n5 - Aspen\n6 - Douglas-fir\n7 - Krummholz The training set (15120 observations) contains both features and the Cover_Type. The test set contains only the features. You must predict the Cover_Type for every row in the test set (565892 observations). Elevation - Elevation in meters\nAspect - Aspect in degrees azimuth\nSlope - Slope in degrees\nHorizontal_Distance_To_Hydrology - Horz Dist to nearest surface water features\nVertical_Distance_To_Hydrology - Vert Dist to nearest surface water features\nHorizontal_Distance_To_Roadways - Horz Dist to nearest roadway\nHillshade_9am (0 to 255 index) - Hillshade index at 9am, summer solstice\nHillshade_Noon (0 to 255 index) - Hillshade index at noon, summer solstice\nHillshade_3pm (0 to 255 index) - Hillshade index at 3pm, summer solstice\nHorizontal_Distance_To_Fire_Points - Horz Dist to nearest wildfire ignition points\nWilderness_Area (4 binary columns, 0 = absence or 1 = presence) - Wilderness area designation\nSoil_Type (40 binary columns, 0 = absence or 1 = presence) - Soil Type designation\nCover_Type (7 types, integers 1 to 7) - Forest Cover Type designation The wilderness areas are: 1 - Rawah Wilderness Area\n2 - Neota Wilderness Area\n3 - Comanche Peak Wilderness Area\n4 - Cache la Poudre Wilderness Area The soil types are: 1 Cathedral family - Rock outcrop complex, extremely stony.\n2 Vanet - Ratake families complex, very stony.\n3 Haploborolis - Rock outcrop complex, rubbly.\n4 Ratake family - Rock outcrop complex, rubbly.\n5 Vanet family - Rock outcrop complex complex, rubbly.\n6 Vanet - Wetmore families - Rock outcrop complex, stony.\n7 Gothic family.\n8 Supervisor - Limber families complex.\n9 Troutville family, very stony.\n10 Bullwark - Catamount families - Rock outcrop complex, rubbly.\n11 Bullwark - Catamount families - Rock land complex, rubbly.\n12 Legault family - Rock land complex, stony.\n13 Catamount family - Rock land - Bullwark family complex, rubbly.\n14 Pachic Argiborolis - Aquolis complex.\n15 unspecified in the USFS Soil and ELU Survey.\n16 Cryaquolis - Cryoborolis complex.\n17 Gateview family - Cryaquolis complex.\n18 Rogert family, very stony.\n19 Typic Cryaquolis - Borohemists complex.\n20 Typic Cryaquepts - Typic Cryaquolls complex.\n21 Typic Cryaquolls - Leighcan family, till substratum complex.\n22 Leighcan family, till substratum, extremely bouldery.\n23 Leighcan family, till substratum - Typic Cryaquolls complex.\n24 Leighcan family, extremely stony.\n25 Leighcan family, warm, extremely stony.\n26 Granile - Catamount families complex, very stony.\n27 Leighcan family, warm - Rock outcrop complex, extremely stony.\n28 Leighcan family - Rock outcrop complex, extremely stony.\n29 Como - Legault families complex, extremely stony.\n30 Como family - Rock land - Legault family complex, extremely stony.\n31 Leighcan - Catamount families complex, extremely stony.\n32 Catamount family - Rock outcrop - Leighcan family complex, extremely stony.\n33 Leighcan - Catamount families - Rock outcrop complex, extremely stony.\n34 Cryorthents - Rock land complex, extremely stony.\n35 Cryumbrepts - Rock outcrop - Cryaquepts complex.\n36 Bross family - Rock land - Cryumbrepts complex, extremely stony.\n37 Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.\n38 Leighcan - Moran families - Cryaquolls complex, extremely stony.\n39 Moran family - Cryorthents - Leighcan family complex, extremely stony.\n40 Moran family - Cryorthents - Rock land complex, extremely stony."
    },
    {
        "name": "Bike Sharing Demand",
        "url": "https://www.kaggle.com/competitions/bike-sharing-demand",
        "overview_text": "Overview text not found",
        "description_text": "Get started on this competition through Kaggle Scripts Bike sharing systems are a means of renting bicycles where the process of obtaining membership, rental, and bike return is automated via a network of kiosk locations throughout a city. Using these systems, people are able rent a bike from a one location and return it to a different place on an as-needed basis. Currently, there are over 500 bike-sharing programs around the world. The data generated by these systems makes them attractive for researchers because the duration of travel, departure location, arrival location, and time elapsed is explicitly recorded. Bike sharing systems therefore function as a sensor network, which can be used for studying mobility in a city. In this competition, participants are asked to combine historical usage patterns with weather data in order to forecast bike rental demand in the Capital Bikeshare program in Washington, D.C.  Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was provided by Hadi Fanaee Tork using data from Capital Bikeshare. We also thank the UCI machine learning repository for hosting the dataset. If you use the problem in publication, please cite: Fanaee-T, Hadi, and Gama, Joao, Event labeling combining ensemble detectors and background knowledge, Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg.",
        "dataset_text": "See, fork, and run a random forest benchmark model through Kaggle Scripts You are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period. datetime - hourly date + timestamp  \nseason -  1 = spring, 2 = summer, 3 = fall, 4 = winter \nholiday - whether the day is considered a holiday\nworkingday - whether the day is neither a weekend nor holiday\nweather - 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \ntemp - temperature in Celsius\natemp - \"feels like\" temperature in Celsius\nhumidity - relative humidity\nwindspeed - wind speed\ncasual - number of non-registered user rentals initiated\nregistered - number of registered user rentals initiated\ncount - number of total rentals"
    },
    {
        "name": "Random Acts of Pizza",
        "url": "https://www.kaggle.com/competitions/random-acts-of-pizza",
        "overview_text": "Overview text not found",
        "description_text": "Get started on this competition through Kaggle Scripts In machine learning, it is often said there are no free lunches. How wrong we were. This competition contains a dataset with 5671 textual requests for pizza from the Reddit community Random Acts of Pizza together with their outcome (successful/unsuccessful) and meta-data. Participants must create an algorithm capable of predicting which requests will garner a cheesy (but sincere!) act of kindness. \"I'll write a poem, sing a song, do a dance, play an instrument, whatever! I just want a pizza,\" says one hopeful poster. What about making an algorithm?  Kaggle is hosting this competition for the machine learning community to use for fun and practice. This data was collected and graciously shared by Althoff et al. (Buy them a pizza -- data collection is a thankless and tedious job!) We encourage participants to explore their accompanying paper and ask that you cite the following reference in any publications that result from your work: Tim Althoff, Cristian Danescu-Niculescu-Mizil, Dan Jurafsky. How to Ask for a Favor: A Case Study on the Success of Altruistic Requests, Proceedings of ICWSM, 2014. Pizza icons designed by Matthew Dera from the Noun Project",
        "dataset_text": "See, fork, and run a random forest benchmark model through Kaggle Scripts This dataset includes 5671 requests collected from the Reddit community Random Acts of Pizza between December 8, 2010 and September 29, 2013 (retrieved on September 30, 2013). All requests ask for the same thing: a free pizza. The outcome of each request -- whether its author received a pizza or not -- is known. Meta-data includes information such as: time of the request, activity of the requester, community-age of the requester, etc. Each JSON entry corresponds to one request (the first and only request by the requester on Random Acts of Pizza). We have removed fields from the test set which would not be available at the time of posting. \"giver_username_if_known\": Reddit username of giver if known, i.e. the person satisfying the request (\"N/A\" otherwise). \"number_of_downvotes_of_request_at_retrieval\": Number of downvotes at the time the request was collected. \"number_of_upvotes_of_request_at_retrieval\": Number of upvotes at the time the request was collected. \"post_was_edited\": Boolean indicating whether this post was edited (from Reddit). \"request_id\": Identifier of the post on Reddit, e.g. \"t3_w5491\". \"request_number_of_comments_at_retrieval\": Number of comments for the request at time of retrieval. \"request_text\": Full text of the request. \"request_text_edit_aware\": Edit aware version of \"request_text\". We use a set of rules to strip edited comments indicating the success of the request such as \"EDIT: Thanks /u/foo, the pizza was delicous\". \"request_title\": Title of the request. \"requester_account_age_in_days_at_request\": Account age of requester in days at time of request. \"requester_account_age_in_days_at_retrieval\": Account age of requester in days at time of retrieval. \"requester_days_since_first_post_on_raop_at_request\": Number of days between requesters first post on RAOP and this request (zero if requester has never posted before on RAOP). \"requester_days_since_first_post_on_raop_at_retrieval\": Number of days between requesters first post on RAOP and time of retrieval. \"requester_number_of_comments_at_request\": Total number of comments on Reddit by requester at time of request. \"requester_number_of_comments_at_retrieval\": Total number of comments on Reddit by requester at time of retrieval. \"requester_number_of_comments_in_raop_at_request\": Total number of comments in RAOP by requester at time of request. \"requester_number_of_comments_in_raop_at_retrieval\": Total number of comments in RAOP by requester at time of retrieval. \"requester_number_of_posts_at_request\": Total number of posts on Reddit by requester at time of request. \"requester_number_of_posts_at_retrieval\": Total number of posts on Reddit by requester at time of retrieval. \"requester_number_of_posts_on_raop_at_request\": Total number of posts in RAOP by requester at time of request. \"requester_number_of_posts_on_raop_at_retrieval\": Total number of posts in RAOP by requester at time of retrieval. \"requester_number_of_subreddits_at_request\": The number of subreddits in which the author had already posted in at the time of request. \"requester_received_pizza\": Boolean indicating the success of the request, i.e., whether the requester received pizza. \"requester_subreddits_at_request\": The list of subreddits in which the author had already posted in at the time of request. \"requester_upvotes_minus_downvotes_at_request\": Difference of total upvotes and total downvotes of requester at time of request. \"requester_upvotes_minus_downvotes_at_retrieval\": Difference of total upvotes and total downvotes of requester at time of retrieval. \"requester_upvotes_plus_downvotes_at_request\": Sum of total upvotes and total downvotes of requester at time of request. \"requester_upvotes_plus_downvotes_at_retrieval\": Sum of total upvotes and total downvotes of requester at time of retrieval. \"requester_user_flair\": Users on RAOP receive badges (Reddit calls them flairs) which is a small picture next to their username. In our data set the user flair is either None (neither given nor received pizza, N=4282), \"shroom\" (received pizza, but not given, N=1306), or \"PIF\" (pizza given after having received, N=83). \"requester_username\": Reddit username of requester. \"unix_timestamp_of_request\": Unix timestamp of request (supposedly in timezone of user, but in most cases it is equal to the UTC timestamp -- which is incorrect since most RAOP users are from the USA). \"unix_timestamp_of_request_utc\": Unit timestamp of request in UTC."
    },
    {
        "name": "Finding Elo",
        "url": "https://www.kaggle.com/competitions/finding-elo",
        "overview_text": "Overview text not found",
        "description_text": "Elite chess players are rated, ranked, analyzed, and compared in many ways. Classical methods of ranking chess players have focused on game histories, paying particular attention to the relative strength of the players involved. This includes the popular FIDE Elo score, which was the focus of one of Kaggle's first ever competitions - Elo vs. the Rest of the World. Recent work on chess analysis has focused on intrinsic performance ratings, where one assesses skill based on the quality of decisions rather than the outcomes of games. For an example of this kind of approach, see this draft by Kenneth Regan. Two advantages of an intrinsic approach are an increased sample size (there are many more moves than games) and the ability to approach new challenges, such as determining whether a player is cheating by performing moves above their skill level. This competition challenges Kagglers to determine players' FIDE Elo ratings at the time a game is played, based solely on the moves in one game. Do a player's moves reflect their absolute skill? Does the opponent matter? How closely does one game reflect intrinsic ability? How well can an algorithm do? Does computational horsepower increase accuracy? Let's find out! You do not need to be a chess expert -- or even know how to play chess -- to attempt this competition. You do need patience and a computer that doesn't mind some heat. The dataset includes 50,000 games between elite, ranked players. As a getting-started computational bonus, Kaggle has run these games through a chess engine to score each move. Good luck finding Elo!",
        "dataset_text": "A total of 50,000 games are provided in portable game notation (pgn) format. Each game in the training set (the first 25,000 games) has both the white and black Elo rating. Each game in the test set (the latter 25,000 games) omits the Elo ratings, which you must predict. Player ids were scrubbed from the data. The goal is to predict the Elo rating based on only a single game. Each game has the following format, with the move text in Standard Algebraic Notation (SAN): The fields with question marks are necessary to comply with the pgn standard. Many chess engines use the Universal Chess Interface (UCI) format, which uses a different representation of the move text. In UCI, the above moves would be written as: For convenience, you are provided the games in both formats. You may find pgn-extract useful if you wish to do conversions or analysis on your own. Note that some of the games may be incomplete/fragments. In addition to suggesting the best moves to make, a chess engine is capable of assessing the strength of a given board. Participants in this competition will likely want to use a chess engine to help decide whether a move is strong or weak. If you're more data scientist than chess expert, you can think of a chess engine as a kind of feature extractor for chess. To help you get started, we have run the games through the Stockfish chess engine (the world's strongest!) and provided the resulting scores in stockfish.csv. Computer programs typically represent the values of pieces and positions in centipawns (cp), where 100 cp = 1 pawn. The scores in stockfish.csv represent the current advantage, in cp, of white at each move in the game. For example, the sequence means white started with an 18 centipawn advantage and ended with 54 (this is small, and note that the associated game indeed ended in a draw). Negative values indicate black has the advantage. Here is an example of the scores in a game that black won: Each move was analyzed for one second on one core in Stockfish. If the engine was not able to assess the move in one second or there are no sensible moves left in the game, the file contains \"NA\". Letting the computer evaluate the moves for a longer time will, in theory, provide a better estimate of position strength. Chess engines have standardized on a communication protocol, which will enable you to programmatically interface with the engine. Here is a simple example using the Stockfish engine."
    },
    {
        "name": "Poker Rule Induction",
        "url": "https://www.kaggle.com/competitions/poker-rule-induction",
        "overview_text": "Overview text not found",
        "description_text": "Your friend bailed last minute on poker night? Before giving up on a much-needed evening of bad bluffs and quarter buy ins, light a cigar and get familiar with the rules of the game. Each record in this competition consists of five playing cards and an attribute representing the poker hand. You are asked to predict the best hand you can play based on the cards you've been dealt.   The order of cards is important, which means there are 480 possible Royal Flush hands instead of just four. Identify those, and the other 311,875,200 possible hands correctly, and you\u2019re in the money! \"Isn't this easy? I know two-of-a-kind when I see it\", you might rightfully wonder. And you'd be right.  The intent of this challenge is automatic rules induction, i.e. to learn the rules using machine learning, without hand coding heuristics. Pretend you are in a foreign land, have never played the game before, are given a history of thousands of games, and are asked to come up with the rules. It is potentially difficult to discover rules that can correctly classify poker hands, yet it is trivial for a human to validate the rules objectively. Remember, your algorithm will need to find rules that are general enough to be broadly useful, without being so broad that they end up being occasionally wrong. We suggest reading the paper by Cattral et al. for more background on the topic. Playground competitions are an opportunity to build and stretch your machine learning muscles. Pull up a chair to the data science poker table and ante up. Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was created by Robert Cattral and Franz Oppacher. We also thank the UCI machine learning repository for hosting the dataset. If you use the problem in publication, please cite: Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science",
        "dataset_text": "You are provided with 25,010 poker hands in train.csv and 1,000,000 in test.csv. Each hand consists of five cards with a given suit and rank, drawn from a standard deck of 52. Suits and ranks are represented as ordinal categories: Each row in the training set has the accompanying class label for the poker hand it comprises. The hands are omitted from the test set and must be predicted by participants. Hands are classified into the following ordinal categories: Note that the Straight flush and Royal flush hands are not representative of\nthe true domain because they have been over-sampled. The straight flush\nis 14.43 times more likely to occur in the training set, while the royal flush is 129.82 times more likely."
    },
    {
        "name": "CareerCon 2019 - Help Navigate Robots",
        "url": "https://www.kaggle.com/competitions/career-con-2019",
        "overview_text": "Overview text not found",
        "description_text": "CareerCon 2019 is upon us! CareerCon is a digital event all about landing your first data science job \u2014 and registration is now open! Ahead of the event, we have a fun competition to get you started. See below for a unique challenge and opportunity to share your resume with select CareerCon sponsors.  ___________________________________ The Competition Robots are smart\u2026 by design. To fully understand and properly navigate a task, however, they need input about their environment. In this competition, you\u2019ll help robots recognize the floor surface they\u2019re standing on using data collected from Inertial Measurement Units (IMU sensors). We\u2019ve collected IMU sensor data while driving a small mobile robot over different floor surfaces on the university premises. The task is to predict which one of the nine floor types (carpet, tiles, concrete) the robot is on using sensor data such as acceleration and velocity. Succeed and you'll help improve the navigation of robots without assistance across many different surfaces, so they won\u2019t fall down on the job.  The data for this competition has been collected by Heikki Huttunen and Francesco Lomio from the Department of Signal Processing and Damoon Mohamadi, Kaan Celikbilek, Pedram Ghazi and Reza Ghabcheloo from the Department of Automation and Mechanical Engineering both from Tampere University, Finland. We at Kaggle would like thank them all for kindly donating the data that has made this competition possible!",
        "dataset_text": "The orientation channels encode the current angles how the robot is oriented as a quaternion (see Wikipedia). Angular velocity describes the angle and speed of motion, and linear acceleration components describe how the speed is changing at different times. The 10 sensor channels are:"
    },
    {
        "name": "GeoLifeCLEF 2024 @ LifeCLEF & CVPR-FGVC",
        "url": "https://www.kaggle.com/competitions/geolifeclef-2024",
        "overview_text": "This challenge aims to predict plant species in a given location and time using various possible predictors: satellite images and time series, climatic time series, and other rasterized environmental data: land cover, human footprint, bioclimatic, and soil variables.",
        "description_text": "Description text not found",
        "dataset_text": "The training data comprises species observations and environmental data. Below, we explain the data in detail. \u2757New Seafile repository\u2757: repository containing all the data. To optimize download times, see the section data downloading at the bottom of this page. \u2757GLC GitHub repository\u2757: Useful codes to manipulate data with simple data loaders, examples, and sample data. More dataloaders can be added after the challenge starts. The species related training data comprises: There are two CSVs with species occurrence data on the Seafile available for training. The detailed description is provided again on SeaFile in separate ReadME files in relevant folders. Besides species data, we provide spatialized geographic and environmental data as additional input variables (see Figure 1). More precisely, For each species observation location, we provide: There are three separate folders with the relevant data on the Seafile available for training. The detailed description is provided below and again on SeaFile in separate \"Readme\" files in relevant folders.  Figure. Illustration of of the environmental data for an occurrence (glcID=4859165) collected in northern Switzerland (lon=8.5744;lat=47.7704) in 2021. A. The 1280x1280m satellite image patches were sampled in 2021 around the observation. B. Quarterly time series of six satellite bands at the point location since winter 1999-2000. C. Three example bioclimatic images (~65x65km) around the observation were extracted from the provided environmental rasters. 1280mx1280m RGB and NIR patches (four bands) centered at the observation geolocation and taken the same year. The patches are compressed in two zip files (patchs_rgb.zip, patchs_nir.zip) accessible in folder /SatelliteImages/. Each observation is associated with the time series of the satellite median point values over each season since the winter of 1999 for six satellite bands (R, G, B, NIR, SWIR1, and SWIR2). This data carries a high-resolution local signature of the past 20 years' succession of seasonal vegetation changes, potential extreme natural events (fires), or land use changes. Four climatic variables computed monthly (mean, minimum and maximum temperature, and total precipitation) from January 2000 to December 2019, yielding 960 low-resolution rasters covering Europe. For each observation, we provide additional environmental data such as GeoTIFF rasters and scalar values already extracted from the rasters. We provide CSV files, one per band raster type, i.e., Climate, Elevation, Human Footprint, LandCover, and SoilGrids. All PA-related data are already provided through Kagle; however, all the original rasters are provided on the Seafile. To download the full dataset, we recommend downloading each group separately using zipped archives."
    },
    {
        "name": "PlantTraits2024 - FGVC11",
        "url": "https://www.kaggle.com/competitions/planttraits2024",
        "overview_text": "We welcome you to PlantTraits2024 Challenge, an exciting part of the FGVC11 workshop at CVPR 2024, where you have the chance to personally contribute to advance our understanding of the global patterns of biodiversity. Our goal is to predict a broad set of 6 plant traits (e.g. leaf area, plant height) from crowd-sourced plant images and some ancillary data.",
        "description_text": "This competition aims to predict plant properties - so called plant traits - from citizen science plant photographs. Why are plant traits currently so relevant? Plant traits are plant properties that are used to describe how plants function how they interact with the environment. For instance, the trait of plant canopy height indicates how good a plant is at overshadowing its neightbors in the competition for sun light. Robust leaves (indicated by the leaf mass pear leaf area) indicate that plants optimize towards extreme conditions, such as heavy winds or droughts. Yet, environmental conditions are not static. Due to global change, the biosphere is being transformed at accelerating pace. Especially climate change is assumed to drastically impact the functioning of the ecosystems. This includes several processes, e.g. adaptions of plants and their traits to new conditions or even a altered plant species distribution with a resulting modification of the distribution of plant traits. However, we can hardly project on a global scale how plant traits and as such entire ecosystems will react to climate change because we do not have sufficient data on plant traits. A data treasure in this regard may be the growing availability of citizen science photographs. Thousands of citizens around the globe photograph plants with species identification apps (examples are iNaturalist or Pl@ntNet). The species are identified using AI algorithms, and the prediction, photograph, and geolocation are curated in open databases. There are already more than 20 million plant photographs available, covering all ecosystem types and continents.  In its original form, this data initially only provides information on the species name of a plant and not its traits. However, a pioneering study showed that artificial intelligence can predict plant traits from such photographs using Convolutional Neural Networks (Schiller et al., 2021). To achieve this, we paired sample images from the iNaturalist database with plant trait data that scientists have been curating for decades for various species. The challenge was that the images and plant trait observations were not acquired for the same plant individuals or at the same time. Nevertheless, using a weakly supervised learning approach, we trained models that demonstrated the potential of this approach for a few plant traits. However, this potential was evident only for a limited number of plant traits and a couple of thousand images. This competition aims to further unlock the potential of predicting plant traits from plant photographs. To achieve this, we gathered more training data (over 30,000 images with labels). Find here the original article:  The interested reader may also see these references for some background and the general idea:",
        "dataset_text": "To create this database, we utilized the TRY database (trait information) and the iNaturalist database (citizen science plant photographs). Based on the species names found in both databases, we linked the trait observations obtained from the TRY database (species-specific mean and standard deviation) with the plant photographs (iNaturalist). Based on the geocoordinates that comes with each plant photographs, we linked the ancillary predictors, which are derived from globally available raster data (WORLDCLIM, SOIL, VOD, MODIS). To state briefly, WORLDCLIM includes temperature and precipitation data, SOIL is the global soil grids dataset (interpolated products on various soil properties, such as sand content or pH value), MODIS is satellite data that measures optical reflectance of sun light (like a camera but with many wavelengths), while VOD represents data from a radar constellation that is sensitive to water content and biomass of plants. All these geodatasets are meant to serve as supporting information in addition to the plant photographs. "
    },
    {
        "name": "Will I Stay or Will I Go?",
        "url": "https://www.kaggle.com/competitions/customer-retention",
        "overview_text": "Overview text not found",
        "description_text": "An important part of succeeding as an insurance company is having a good understanding of which of the company\u2019s current customers will be with the company into the future.  Every customer comes with a different risk profile and it is critical to plan appropriately for that future risk.  The goal of this competition is to predict which current customers will still be with the company in 6 months, given many of the customer\u2019s characteristics. ",
        "dataset_text": "Two datasets will be provided (in the zip file rolled_up_data_Kaggle_V2.zip) during this competition: In all of the data, policies expire every 6 months.  A month before the customer's current policy ends, they will receive a letter in the mail informing them of their ability to renew their policy.  The customer\u2019s renewal period is very important because this it the only time our company will actively change a customer\u2019s price.  Of course, the customer can change their own price for an insurance policy by adding a car, changing coverage, etc.   The first dataset is provided so you can get an idea for how to use the data.  The second dataset is where most of the information is and who leverages this data will have the best chance at winning the competitions.   The files contained in rolled_up_data_Kaggle_V2.zip each include both the test and train data in one file. We're also providing the \"rolled up\" (one row per observation) data in separate zip files:"
    },
    {
        "name": "Titanic - Machine Learning from Disaster",
        "url": "https://www.kaggle.com/competitions/titanic",
        "overview_text": "Overview text not found",
        "description_text": "This is the legendary Titanic ML competition \u2013 the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works. If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here: https://discord.gg/kaggle The competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck. Read on or watch the video below to explore more details. Once you\u2019re ready to start competing, click on the \"Join Competition button to create an account and gain access to the competition data. Then check out Alexis Cook\u2019s Titanic Tutorial that walks you through step by step how to make your first submission! The sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew. While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others. In this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc). In this competition, you\u2019ll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled train.csv and the other is titled test.csv. Train.csv will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the \u201cground truth\u201d. The test.csv dataset contains similar information but does not disclose the \u201cground truth\u201d for each passenger. It\u2019s your job to predict these outcomes. Using the patterns you find in the train.csv data, predict whether the other 418 passengers on board (found in test.csv) survived. Check out the \u201cData\u201d tab to explore the datasets even further. Once you feel you\u2019ve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers. Once you\u2019re ready to make a submission and get on the leaderboard: You should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows. The file should have exactly 2 columns: Kaggle doesn\u2019t have a dedicated team to help troubleshoot your code so you\u2019ll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn! As we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data & code. In every competition, you\u2019ll find many Notebooks shared with incredible insights. It\u2019s an invaluable resource worth becoming familiar with. Check out this competition\u2019s Notebooks here.",
        "dataset_text": "The data has been split into two groups: The training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the \u201cground truth\u201d) for each passenger. Your model will be based on \u201cfeatures\u201d like passengers\u2019 gender and class. You can also use feature engineering to create new features. The test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic. We also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like. pclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them."
    },
    {
        "name": "Prescription Volume Prediction",
        "url": "https://www.kaggle.com/competitions/RxVolumePrediction",
        "overview_text": "Overview text not found",
        "description_text": "This is a private, invitation-only competition. The relevant information is provided only to contestants.",
        "dataset_text": "  This is a private, invitation-only competition."
    },
    {
        "name": "Just the Basics - Strata 2013",
        "url": "https://www.kaggle.com/competitions/just-the-basics-strata-2013",
        "overview_text": "Overview text not found",
        "description_text": "Two of Kaggle's very own are presenting an introductory tutorial at Strata 2013. Targeted for those with basic programming experience, it will cover the end-to-end analysis of predictive data problems. The tutorial is comprised of four sections, the last of which is this, a hands-on Kaggle competition in which participants can experience firsthand the joys of creating a model and the sorrows of overfitting. 9:00am Tuesday, 02/26/2013\nLocation: Ballroom AB\nCompetition Starts: Approximately 11:15 AM PT (2:15PM ET), 02/26/2013\nCompetition Ends: 12:30 PM PT (3:30 PM ET), 02/26/2013 Yes, you can participate in this for-fun competition without attending the tutorial.   You heard correctly; there's no Kaggle points or money up for grabs here. Isn't getting to lunch early a big enough motivation? With just over an hour from start to finish, this competition is going to be a sprint. Show our servers who's boss! *for small values of unlimited To prevent head starts, the data will be available at the start of the competition. Ben Hamner has worked with machine learning problems in a variety of different domains, including natural language processing, computer vision, web classification, and neuroscience. Prior to joining Kaggle, he applied machine learning to improve brain-computer interfaces as a Whitaker Fellow at the \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne in Lausanne, Switzerland. He graduated with BSE in Biomedical Engineering, Electrical Engineering, and Math from Duke University. William Cukierski has a bachelor\u2019s degree in physics from Cornell University and a Ph.D. in biomedical engineering from Rutgers University, where he studied applications of machine learning in cancer research.",
        "dataset_text": "Note: this data is encrypted! The password will be revealed when the competition starts. Unencrypted data is now available for download"
    },
    {
        "name": "Just the Basics - Strata 2013 After-party",
        "url": "https://www.kaggle.com/competitions/just-the-basics-the-after-party",
        "overview_text": "Overview text not found",
        "description_text": "Missed the one hour Just the Basics tutorial competition? Didn't get to implement that method you had in mind? Too many coffee breaks has your brain in Beautiful Mind mode? This is the after-party competition. Same data. Same problem. More time! You have until the close of Strata to have fun with the problem. Competition Starts: approximately 12:30 PM PT (3:30 PM ET), 02/26/2013\nCompetition Ends: 5:00 PM PT (8:00 PM ET), 02/28/2013",
        "dataset_text": "The data contains 100 features extracted from a corpus of emails. Some of the emails are spam and some are normal. Your task is to make a spam detector. train.csv - contains 600 emails x 100 features for use training your model(s) train_labels.csv - contains labels for the 600 training emails (1 = spam, 0 = normal) test.csv - contains 4000 emails x 100 features. Apply your trained model(s) to these. Participants should submit a file with each of their 4000 predictions on a separate line (in the same order as test.csv). No header is necessary. Predictions can be continuous numbers or 0/1 labels."
    },
    {
        "name": "Data Science London + Scikit-learn",
        "url": "https://www.kaggle.com/competitions/data-science-london-scikit-learn",
        "overview_text": "Overview text not found",
        "description_text": "Data Science London is hosting a meetup on Scikit-learn.  This competition is a practice ground for trying, sharing, and creating examples of sklearn's classification abilities (if this turns in to something useful, we can follow it up with regression, or more complex classification problems). We encourage participants to post code via the \"Tutorials\" link on the left.  Don't worry about accuracy or whether your code is perfect.  The aim here is to explore sklearn by using it. You do not need to use sklearn to enter the competition. If you're new, we hope you'll use this oppurtunity to practice a new tool.  If you're an expert, we hope you'll share the knowledge and document interesting ways to approach this problem. Scikit-learn (sklearn) is an established, open-source machine learning library, written in Python with the help of NumPy, SciPy and Cython. Scikit-learn is very user friendly, has a consistent API, and provides extensive documentation. Its implementation is high quality due to strict coding standards and high test coverage.  Behind sklearn is a very active community, which is steadily improving the library. Thursday, March 7, 2013, 6:30 PM UTC\nhttp://www.meetup.com/Data-Science-London/events/105840372/ \u201cLearning in Python with scikit-learn\" by Andreas Mueller This talk will give an overview of the library and introduce general machine learning concepts such as supervised and unsupervised learning, feature extraction, cross validation for model evaluation and hyper parameter selection. We will also touch some more advanced yet practically useful concepts such as feature hashing and ensemble learning. Andreas is a PhD student in machine learning an computer vision at Bonn University in Germany. He is one of the core developers and the maintainer of scikit-learn and the author of the blog peekaboo-vision. His interests include principles and applications of machine learning and open science. \"Parallel and large scale learning with scikit-learn\" by Olivier Grisel This talk will give a introduce practical tools and concepts to better leverage multicore machines and small clusters to perform interactive yet scalable predictive modeling with scikit-learn and IPython.parallel. In particular we will introduce: Olivier is a R&D Software Engineer working in Java by day and a Python machine learning hacker by night. He is interested in applications to Natural Language Processing, Computer Vision and predictive modelling.",
        "dataset_text": "This is a synthetic data set of 40 features, representing objects from two classes (labeled as 0 or 1). The training set has 1000 samples and the testing set has 9000."
    },
    {
        "name": "Facial Keypoints Detection",
        "url": "https://www.kaggle.com/competitions/facial-keypoints-detection",
        "overview_text": "Overview text not found",
        "description_text": "The objective of this task is to predict keypoint positions on face images. This can be used as a building block in several applications, such as: Detecing facial keypoints is a very challenging problem.  Facial features vary greatly from one individual to another, and even for a single individual, there is a large amount of variation due to 3D pose, size, position, viewing angle, and illumination conditions. Computer vision research has come a long way in addressing these difficulties, but there remain many opportunities for improvement. This getting-started competition provides a benchmark data set and an R tutorial to get you going on analysing face images. Get started with R >> The data set for this competition was graciously provided by Dr. Yoshua Bengio of the University of Montreal. The tutorial was developed by James Petterson.",
        "dataset_text": "Each predicted keypoint is specified by an (x,y) real-valued pair in the space of pixel indices. There are 15 keypoints, which represent the following elements of the face: left_eye_center, right_eye_center, left_eye_inner_corner, left_eye_outer_corner, right_eye_inner_corner, right_eye_outer_corner, left_eyebrow_inner_end, left_eyebrow_outer_end, right_eyebrow_inner_end, right_eyebrow_outer_end, nose_tip, mouth_left_corner, mouth_right_corner, mouth_center_top_lip, mouth_center_bottom_lip Left and right here refers to the point of view of the subject. In some examples, some of the target keypoint positions are misssing (encoded as missing entries in the csv, i.e., with nothing between two commas). The input image is given in the last field of the data files, and consists of a list of pixels (ordered by row), as integers in (0,255). The images are 96x96 pixels. Data files"
    },
    {
        "name": "First Steps With Julia",
        "url": "https://www.kaggle.com/competitions/street-view-getting-started-with-julia",
        "overview_text": "Overview text not found",
        "description_text": "This competition is designed to help you get started with Julia. If you are looking for a good programming language for data science, or if you are already accustomed to one language, we encourage you to also try Julia. Julia is a relatively new language for technical computing that attempts to combine the strengths of other popular programming languages.   Here we introduce two tutorials to highlight some of Julia's features. The first is focused on the basics of the language. In the second, a complete implementation of the K Nearest Neighbor algorithm is presented, highlighting features such as parallelization and speed.  Both tutorials show that it is easy to write code in Julia, due to its intuitive syntax and design. The tutorials also describe some basics of image processing and some concepts of machine learning such as cross validation. After reviewing them, we hope you will be motivated to write your own machine learning algorithms in Julia.  This tutorial focuses on the task of identifying characters from Google Street View images. It differs from traditional character recognition because the data set contains different character fonts and the background is not the same for all images.  The data was taken from the Chars74K dataset, which consists of images of characters selected from Google Street View images. We ask that you cite the following reference in any publication resulting from your work: T. E. de Campos, B. R. Babu and M. Varma, Character recognition in natural images, Proceedings of the International Conference on Computer Vision Theory and Applications (VISAPP), Lisbon, Portugal, February 2009. This tutorial was developed by Luis Tandalla during his summer 2014 internship at Kaggle.",
        "dataset_text": "Dataset description not found"
    },
    {
        "name": "Bag of Words Meets Bags of Popcorn",
        "url": "https://www.kaggle.com/competitions/word2vec-nlp-tutorial",
        "overview_text": "Overview text not found",
        "description_text": "In this tutorial competition, we dig a little \"deeper\" into sentiment analysis. Google's Word2Vec is a deep-learning inspired method that focuses on the meaning of words. Word2Vec attempts to understand meaning and semantic relationships among words. It works in a way that is similar to deep approaches, such as recurrent neural nets or deep neural nets, but is computationally more efficient. This tutorial focuses on Word2Vec for sentiment analysis. Sentiment analysis is a challenging subject in machine learning. People express their emotions in language that is often obscured by sarcasm, ambiguity, and plays on words, all of which could be very misleading for both humans and computers. There's another Kaggle competition for movie review sentiment analysis. In this tutorial we explore how Word2Vec can be applied to a similar problem. Deep learning has been in the news a lot over the past few years, even making it to the front page of the New York Times. These machine learning techniques, inspired by the architecture of the human brain and made possible by recent advances in computing power, have been making waves via breakthrough results in image recognition, speech processing, and natural language tasks. Recently, deep learning approaches won several Kaggle competitions, including a drug discovery task, and cat and dog image recognition. This tutorial will help you get started with Word2Vec for natural language processing. It has two goals:  Basic Natural Language Processing: Part 1 of this tutorial is intended for beginners and covers basic natural language processing techniques, which are needed for later parts of the tutorial. Deep Learning for Text Understanding: In Parts 2 and 3, we delve into how to train a model using Word2Vec and how to use the resulting word vectors for sentiment analysis. Since deep learning is a rapidly evolving field, large amounts of the work has not yet been published, or exists only as academic papers. Part 3 of the tutorial is more exploratory than prescriptive -- we experiment with several ways of using Word2Vec rather than giving you a recipe for using the output. To achieve these goals, we rely on an IMDB sentiment analysis data set, which has 100,000 multi-paragraph movie reviews, both positive and negative.  This dataset was collected in association with the following publication: Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). \"Learning Word Vectors for Sentiment Analysis.\" The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011). (link) Please email the author of that paper if you use the data for any research applications. The tutorial was developed by Angela Chapman during her summer 2014 internship at Kaggle.",
        "dataset_text": "The labeled data set consists of 50,000 IMDB movie reviews, specially selected for sentiment analysis. The sentiment of reviews is binary, meaning the IMDB rating < 5 results in a sentiment score of 0, and rating >=7 have a sentiment score of 1. No individual movie has more than 30 reviews. The 25,000 review labeled training set does not include any of the same movies as the 25,000 review test set. In addition, there are another 50,000 IMDB reviews provided without any rating labels. Full tutorial code lives in this github repo."
    },
    {
        "name": "House Prices - Advanced Regression Techniques",
        "url": "https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques",
        "overview_text": "Overview text not found",
        "description_text": "You have some experience with R or Python and machine learning basics. This is a perfect competition for data science students who have completed an online course in machine learning and are looking to expand their skill set before trying a featured competition.   Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence. With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home. The Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset.  Photo by Tom Thain on Unsplash.",
        "dataset_text": "Here's a brief version of what you'll find in the data description file."
    },
    {
        "name": "Housing Prices Competition for Kaggle Learn Users",
        "url": "https://www.kaggle.com/competitions/home-data-for-ml-course",
        "overview_text": "Overview text not found",
        "description_text": "You have some experience with R or Python and machine learning basics. This is a perfect competition for data science students who have completed an online course in machine learning and are looking to expand their skill set before trying a featured competition.   Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence. With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home. The Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset. ",
        "dataset_text": "Here's a brief version of what you'll find in the data description file."
    },
    {
        "name": "Connect X",
        "url": "https://www.kaggle.com/competitions/connectx",
        "overview_text": "Overview text not found",
        "description_text": "We\u2019re excited to announce a beta-version of a brand-new type of ML competition called Simulations. In Simulation Competitions, you\u2019ll compete against a set of rules, rather than against an evaluation metric. To enter, accept the rules and create a python submission file that can \u201cplay\u201d against a computer, or another user.  In this game, your objective is to get a certain number of your checkers in a row horizontally, vertically, or diagonally on the game board before your opponent. When it's your turn, you \u201cdrop\u201d one of your checkers into one of the columns at the top of the board. Then, let your opponent take their turn. This means each move may be trying to either win for you, or trying to stop your opponent from winning. The default number is four-in-a-row, but we\u2019ll have other options to come soon.  For the past 10 years, our competitions have been mostly focused on supervised machine learning. The field has grown, and we want to continue to provide the data science community cutting-edge opportunities to challenge themselves and grow their skills. So, what\u2019s next? Reinforcement learning is clearly a crucial piece in the next wave of data science learning. We hope that Simulation Competitions will provide the opportunity for Kagglers to practice and hone this burgeoning skill. Instead of submitting a CSV file, or a Kaggle Notebook, you will submit a Python .py file (more submission options are in development). You\u2019ll also notice that the leaderboard is not based on how accurate your model is but rather how well you\u2019ve performed against other users. See Evaluation for more details. This competition is a low-stakes, trial-run introduction. We\u2019re considering this a beta launch \u2013 there are complicated new mechanics in play and we\u2019re still working on refining the process. We\u2019d love your help testing the experience and want to hear your feedback. Please note that we may make changes throughout the competition that could include things like resetting the leaderboard, invalidating episodes, making changes to the interface, or changing the environment configuration (e.g. modifying the number of columns, rows, or tokens in a row required to win, etc).",
        "dataset_text": "This page appears underneath the data files. It describes what files have been provided & the format of each. It also defines the correct format for submission files. Participants should be able to answer these types of questions after reading the data description: What files do I need?\nWhat should I expect the data format to be?\nWhat am I predicting?\nWhat acronyms will I encounter?"
    },
    {
        "name": "Natural Language Processing with Disaster Tweets",
        "url": "https://www.kaggle.com/competitions/nlp-getting-started",
        "overview_text": "Overview text not found",
        "description_text": "This particular challenge is perfect for data scientists looking to get started with Natural Language Processing. The competition dataset is not too big, and even if you don\u2019t have much personal computing power, you can do all of the work in our free, no-setup, Jupyter Notebooks environment called Kaggle Notebooks. If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here: https://discord.gg/kaggle Twitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies). But, it\u2019s not always clear whether a person\u2019s words are actually announcing a disaster. Take this example: The author explicitly uses the word \u201cABLAZE\u201d but means it metaphorically. This is clear to a human right away, especially with the visual aid. But it\u2019s less clear to a machine. In this competition, you\u2019re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one\u2019s aren\u2019t. You\u2019ll have access to a dataset of 10,000 tweets that were hand classified. If this is your first time working on an NLP problem, we've created a quick tutorial to get you up and running. Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive. This dataset was created by the company figure-eight and originally shared on their \u2018Data For Everyone\u2019 website here. Tweet source: https://twitter.com/AnyOtherAnnaK/status/629195955506708480",
        "dataset_text": "You'll need train.csv, test.csv and sample_submission.csv. Each sample in the train and test set has the following information: You are predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0."
    },
    {
        "name": "Petals to the Metal - Flower Classification on TPU",
        "url": "https://www.kaggle.com/competitions/tpu-getting-started",
        "overview_text": "Overview text not found",
        "description_text": "TPUs are powerful hardware accelerators specialized in deep learning tasks. They were developed (and first used) by Google to process large image databases, such as extracting all the text from Street View. This competition is designed for you to give TPUs a try. TPU quotas are available on Kaggle at no cost to users. Watch the video below to see how to get started! You can follow along with this notebook.  It\u2019s difficult to fathom just how vast and diverse our natural world is. There are over 5,000 species of mammals, 10,000 species of birds, 30,000 species of fish \u2013 and astonishingly, over 400,000 different types of flowers. In this competition, you\u2019re challenged to build a machine learning model that identifies the type of flowers in a dataset of images (for simplicity, we\u2019re sticking to just over 100 types). Kaggle Data Scientists will be actively monitoring the competition forum - your fellow data scientists and TPU users will be there too! If you have a question or need help troubleshooting, that\u2019s the best place to find help.  Check out Kaggle\u2019s Youtube playlist for more videos introducing TPUs. Read the TPU documentation for more information and resources. Many thanks to Martin G\u00f6rner, Google Developer Advocate and author of Tensorflow without a PhD for his tireless work on the dataset, the notebooks, and the original competition that this Getting Started competition draws from.",
        "dataset_text": "In this Getting Started competition (what is a Getting Started competition?), we're classifying 104 types of flowers based on their images drawn from five different public datasets. Some classes are very narrow, containing only a particular sub-type of flower (e.g. pink primroses) while other classes contain many sub-types (e.g. wild roses). The dataset contains imperfections - images of flowers in odd places, or as a backdrop to modern machinery - but that's part of the challenge! Build a classifier than can see past all that, to the flowers at the heart of the images. This competition provides its files in TFRecord format. The TFRecord format is a container format frequently used in Tensorflow to group and shard data data files for optimal training performace.\nEach file contains the id, label (the class of the sample, for training data) and img (the actual pixels in array form) information for many images.\nPlease see our Getting Started notebook or our Learn exercise for notes on how to load and use them! Additional information is available in the TPU documentation."
    },
    {
        "name": "Contradictory, My Dear Watson",
        "url": "https://www.kaggle.com/competitions/contradictory-my-dear-watson",
        "overview_text": "Overview text not found",
        "description_text": "Our brains process the meaning of a sentence like this rather quickly. We're able to surmise: Natural language processing (NLP) has grown increasingly elaborate over the past few years. Machine learning models tackle question answering, text extraction, sentence generation, and many other complex tasks. But, can machines determine the relationships between sentences, or is that still left to humans? If NLP can be applied between sentences, this could have profound implications for fact-checking, identifying fake news, analyzing text, and much more. If you have two sentences, there are three ways they could be related: one could entail the other, one could contradict the other, or they could be unrelated. Natural Language Inferencing (NLI) is a popular NLP problem that involves determining how pairs of sentences (consisting of a premise and a hypothesis) are related. Your task is to create an NLI model that assigns labels of 0, 1, or 2 (corresponding to entailment, neutral, and contradiction) to pairs of premises and hypotheses. To make things more interesting, the train and test set include text in fifteen different languages! You can find more details on the dataset by reviewing the Data page. Today, the most common approaches to NLI problems include using embeddings and transformers like BERT. In this competition, we\u2019re providing a starter notebook to try your hand at this problem using the power of Tensor Processing Units (TPUs). TPUs are powerful hardware accelerators specialized in deep learning tasks, including Natural Language Processing. Kaggle provides all users TPU Quota at no cost, which you can use to explore this competition. Check out our TPU documentation and Kaggle\u2019s YouTube playlist for more information and resources. This is a great opportunity to flex your NLP muscles and solve an exciting problem! Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.",
        "dataset_text": "In this Getting Started Competition, we\u2019re classifying pairs of sentences (consisting of a premise and a hypothesis) into three categories - entailment, contradiction, or neutral. Let\u2019s take a look at an example of each of these cases for the following premise: Hypothesis 1: We know that this is true based on the information in the premise. So, this pair is related by entailment. Hypothesis 2: This very well might be true, but we can\u2019t conclude this based on the information in the premise. So, this relationship is neutral. Hypothesis 3: We know this isn\u2019t true, because it is the complete opposite of what the premise says. So, this pair is related by contradiction. This dataset contains premise-hypothesis pairs in fifteen different languages, including:\nArabic, Bulgarian, Chinese, German, Greek, English, Spanish, French, Hindi, Russian, Swahili, Thai, Turkish, Urdu, and Vietnamese. Special thanks to Tensorflow Datasets (TFDS) for providing this and many other useful datasets! For more information, visit: https://www.tensorflow.org/datasets Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive."
    },
    {
        "name": "I\u2019m Something of a Painter Myself",
        "url": "https://www.kaggle.com/competitions/gan-getting-started",
        "overview_text": "Overview text not found",
        "description_text": "We recognize the works of artists through their unique style, such as color choices or brush strokes. The \u201cje ne sais quoi\u201d of artists like Claude Monet can now be imitated with algorithms thanks to generative adversarial networks (GANs). In this getting started competition, you will bring that style to your photos or recreate the style from scratch! Computer vision has advanced tremendously in recent years and GANs are now capable of mimicking objects in a very convincing way. But creating museum-worthy masterpieces is thought of to be, well, more art than science. So can (data) science, in the form of GANs, trick classifiers into believing you\u2019ve created a true Monet? That\u2019s the challenge you\u2019ll take on! A GAN consists of at least two neural networks: a generator model and a discriminator model. The generator is a neural network that creates the images. For our competition, you should generate images in the style of Monet. This generator is trained using a discriminator. The two models will work against each other, with the generator trying to trick the discriminator, and the discriminator trying to accurately classify the real vs. generated images. Your task is to build a GAN that generates 7,000 to 10,000 Monet-style images. Details on the dataset can be found here and an overview of the evaluation process can be found here. To learn how to submit and answers to other FAQs, review the Frequently Asked Questions. Although the competition dataset only includes Monet images, check out this dataset for Cezanne, Ukiyo-e, and Van Gogh paintings to run your GAN on.",
        "dataset_text": "The dataset contains four directories: monet_tfrec, photo_tfrec, monet_jpg, and photo_jpg. The monet_tfrec and monet_jpg directories contain the same painting images, and the photo_tfrec and photo_jpg directories contain the same photos. We recommend using TFRecords as a Getting Started competition is a great way to become more familiar with a new data format, but JPEG images have also been provided. The monet directories contain Monet paintings. Use these images to train your model. The photo directories contain photos. Add Monet-style to these images and submit your generated jpeg images as a zip file. Other photos outside of this dataset can be transformed but keep your submission file limited to 10,000 images. Note: Monet-style art can be created from scratch using other GAN architectures like DCGAN. The submitted image files do not necessarily have to be transformed photos. Check out the CycleGAN dataset to experiment with the artistic style of other artists. Your kernel's output must be called images.zip and contain 7,000-10,000 images sized 256x256."
    },
    {
        "name": "March Machine Learning Mania 2021 - NCAAW - Spread",
        "url": "https://www.kaggle.com/competitions/ncaaw-march-mania-2021-spread",
        "overview_text": "Overview text not found",
        "description_text": "In addition to the predictive modeling competitions we typically host (NCAA Women's and Men\u2019s), we are hosting two separate, experimental challenges that ask you to not only predict the the winner, but to predict the margin of victor. The mania of March can come down to final second buzzer beaters, upsets, and even a few blowouts. Can you predict big wins as easily as you can predict close calls? This competition (and the parallel competition for the men's tournament) allows you to explore how data science and machine learning can continue to examine the depths of college basketball. You're provided data of historical NCAA games and are encouraged to use other sources of publicly available data to gain a winning edge.  In stage one of this two-stage competition, participants will build and test their models against previous tournaments. In the second stage, participants will predict the point spreads of the 2021 tournament. You don\u2019t need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2021 results. Note: From 2018-2020, Google Cloud, of which Kaggle is a part, served as official corporate sponsors of the NCAA basketball tournaments. This official sponsorship has concluded, but we at Kaggle are happy to bring back this tradition for it's 8th year! Markus Spiske on Unsplash and The Noun Project",
        "dataset_text": "Each season there are thousands of NCAA\u00ae basketball games played between Division I women's teams, culminating in March Madness, the 64-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness\u00ae game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past. If you are unfamiliar with the format and intricacies of the tournament, we encourage reading the wikipedia page before diving into the data. The data description and schema may seem daunting at first, but is not as complicated as it appears. As a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The WTeamSpellings file, which is listed in the bottom section below, may help you map external team references into our own Team ID structure. You will probably also need to understand exactly how dates work in our data. Remember as well that you are required to disclose your external sources of data prior to the start of the tournament. We extend our gratitude to Kenneth Massey for providing much of the historical data. Special Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition. Stage 1 - You should submit predicted point spread for every possible matchup in the past 5 NCAA\u00ae tournaments (seasons 2015-2019). Stage 2 - You should submit predicted point spread for every possible matchup before the 2021 tournament begins. Refer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict. Below we describe the format and fields of the contest data files. The data will likely be refreshed once in early March while Stage 1 of the competition is running. Many of the files are only complete through the end of last season. At the start of Stage 2, we will provide updates to these files to incorporate data from the current season. Right now the WSeasons and WTeamConferences files are the only two files that reference the 2021 season, since those are the only files where the final 2021 data is already finalized; the other files will be updated later on when we provide preliminary data (in early March) and final Stage 2 data (after the Selection Show) for the current season. This section provides everything you need to build a simple prediction model and submit predictions. Special note about \"Season\" numbers: the college basketball season lasts from early November until the national championship tournament that starts in the middle of March. For instance, this year the first women\u2019s Division I games were played on November 25th, 2020 and the women\u2019s national championship game will be played on April 4th, 2021. Because a basketball season spans two calendar years like this, it can be confusing to refer to the year of the season. By convention, when we identify a particular season, we will reference the year that the season ends in, not the year that it starts in. So for instance, the current season will be identified in our data as the 2021 season, not the 2020 season or the 2020-21 season or the 2020-2021 season, though you may see any of these in everyday use outside of our data. Data Section 1 file: WTeams.csv This file identifies the different college teams present in the dataset. Each school is uniquely identified by a 4 digit id number. You will not see games present for all teams in all seasons, because the games listing is only for matchups where both teams are Division-I teams. There are 355 teams currently in Division-I, and an overall total of 369 teams in our team listing. Each year, some teams might start being Division-I programs, and others might stop being Division-I programs, and in fact this year is no exception. This year there are four teams that are new to Division I: Bellarmine (TeamID=3468), North Alabama (TeamID=3469), Tarleton State (TeamID=3470), and UC_San Diego (TeamID=3471) and so you will not see any historical data for these teams prior to the current season. In addition, some teams opted not to play during the 2021 season due to the impact of COVID-19 and will not have any games listed.   Data Section 1 file: WSeasons.csv This file identifies the different seasons included in the historical data, along with certain season-level properties. The game dates in this dataset are expressed in relative terms, as the number of days since the start of the regular season, and aligned for each season so that day number #133 is the Monday right before the tournament, when team selections are made. During any given season, day number zero is defined to be exactly 19 weeks earlier than Selection Monday, so Day #0 is a Monday in late October or early November such that Day #132 is Selection Sunday (for the men's tournament) and Day #133 is Selection Monday (for the women's tournament). This doesn't necessarily mean that the regular season will always start exactly on day #0 or day #1; in fact, during the past decade, regular season games typically start being played on a Friday that is either Day #4 or Day #11, but further back there was more variety. Data Section 1 file: WNCAATourneySeeds.csv This file identifies the seeds for all teams in each NCAA\u00ae tournament, for all seasons of historical data. Thus, there are exactly 64 rows for each year, since there are no play-in teams in the women's tournament. We will not know the seeds of the respective tournament teams, or even exactly which 64 teams it will be, until Selection Monday on March 16, 2020 (DayNum=133). Data Section 1 file: WRegularSeasonCompactResults.csv This file identifies the game-by-game results for many seasons of historical data, starting with the 1998 season. For each season, the file includes all games played from DayNum 0 through 132. It is important to realize that the \"Regular Season\" games are simply defined to be all games played on DayNum=132 or earlier (DayNum=133 is Selection Monday). Thus a game played before Selection Monday will show up here whether it was a pre-season tournament, a non-conference game, a regular conference game, a conference tournament game, or whatever. Data Section 1 file: WNCAATourneyCompactResults.csv This file identifies the game-by-game NCAA\u00ae tournament results for all seasons of historical data. The data is formatted exactly like the WRegularSeasonCompactResults data. Each season you will see 63 games listed, since there are no women's play-in games. Although the scheduling of the men's tournament rounds has been consistent for many years, there has been more variety in the scheduling of the women's rounds. There have been four different schedules over the course of the past 20+ years for the women's tournament, as follows: 2017 season through 2021 season: 2015 season and 2016 season: 2003 season through 2014 season: 1998 season through 2002 season: Data Section 1 file: WSampleSubmissionStage1.csv This file illustrates the submission file format for Stage 1. It is the simplest possible submission: a point spread of 0 is predicted for each possible matchup. A submission file lists every possible matchup between tournament teams for one or more years. During Stage 1, you are asked to make predictions for all possible matchups from the past five NCAA\u00ae tournaments (seasons 2015, 2016, 2017, 2018, and 2019). In Stage 2, you will be asked to make predictions for all possible matchups from the current NCAA\u00ae tournament (season 2021). Since there are 64 teams in the tournament, there are 64*63/2=2,016 predictions to make for that year, so a Stage 1 submission file will have 2016*5=10,080 data rows. Example #1: You want to make a prediction for Duke (TeamID=3181) against Arizona (TeamID=3112) in the 2005 tournament, with Duke to beat Arizona by 3 points. In this case, Arizona has the lower numerical ID so they would be listed first, and the point spread would be expressed from Arizona's perspective (-3): 2005_3112_3181,3 Example #2: You want to make a prediction for Duke (TeamID=3181) against North Carolina (TeamID=3314) in the 2005 tournament, with Duke to win by 7 points. In this case, Duke has the lower numerical ID so they would be listed first, and the point spread would be expressed from Duke's perspective (7): 2005_3181_3314,7 This section provides game-by-game stats at a team level (free throws attempted, defensive rebounds, turnovers, etc.) for all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season.  Team Box Scores are provided in \"Detailed Results\" files rather than \"Compact Results\" files. However, the two files are strongly related. In a Detailed Results file, the first eight columns (Season, DayNum, WTeamID, WScore, LTeamID, LScore, WLoc, and NumOT) are exactly the same as a Compact Results file. However, in a Detailed Results file, there are many additional columns. The column names should be self-explanatory to basketball fans (as above, \"W\" or \"L\" refers to the winning or losing team): (and then the same set of stats from the perspective of the losing team: LFGM is the number of field goals made by the losing team, and so on up to LPF). Note: by convention, \"field goals made\" (either WFGM or LFGM) refers to the total number of fields goals made by a team, a combination of both two-point field goals and three-point field goals. And \"three point field goals made\" (either WFGM3 or LFGM3) is just the three-point fields goals made, of course. So if you want to know specifically about two-point field goals, you have to subtract one from the other (e.g., WFGM - WFGM3). And the total number of points scored is most simply expressed as 2*FGM + FGM3 + FTM. Data Section 2 file: WRegularSeasonDetailedResults.csv This file provides team-level box scores for many regular seasons of historical data, starting with the 2010 season. All games listed in the WRegularSeasonCompactResults file since the 2010 season should exactly be present in the WRegularSeasonDetailedResults file. Data Section 2 file: WNCAATourneyDetailedResults.csv This file provides team-level box scores for many NCAA\u00ae tournaments, starting with the 2010 season. All games listed in the WNCAATourneyCompactResults file since the 2010 season should exactly be present in the WNCAATourneyDetailedResults file. This section provides city locations of all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season Data Section 3 file: Cities.csv This file provides a master list of cities that have been locations for games played. Please notice that the Cities and Conferences files are the only two that don't start with an W; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two contests. Data Section 3 file: WGameCities.csv This file identifies all games, starting with the 2010 season, along with the city that the game was played in. Games from the regular season and the NCAA\u00ae tourney are all listed together. The CityID is present in more than 98% of games since the 2010 season. Games from the 2009 season and before are not listed in this file. This section contains additional supporting information, including alternative team name spellings and representations of bracket structure Data Section 4 file: WTeamSpellings.csv This file indicates alternative spellings of many team names. It is intended for use in associating external spellings against our own TeamID numbers, thereby helping to relate the external data properly with our datasets. Over the years we have identified various external spellings of different team names (as an example, for Ball State we have seen \"ball st\", and \"ball st.\", and \"ball state\", and \"ball-st\", and \"ball-state\"). Other teams have had more significant changes to their names over the years; for example, \"Texas Pan-American\" and \"Texas-Rio Grande Valley\" are actually the same school. The current list is obviously not exhaustive, and we encourage participants to identify additional mappings and upload extended versions of this file to the forums. Data Section 4 file: WNCAATourneySlots This file identifies the mechanism by which teams are paired against each other, depending upon their seeds, as the tournament proceeds through its rounds. It can be of use in identifying, for a given historical game, what round it occurred in, and what the seeds/slots were for the two teams (the meaning of \"slots\" is described below). You may need to know these specifics if you are trying to represent/simulate the exact workings of the tournament bracket. Unlike the analogous file on the men's side, it is not necessary to provide a Season within this file, because the women's tournament has never had play-in-games and so the 64-team women's bracket has always had the same structure each season. Data Section 4 file: Conferences.csv This file indicates the Division I conferences that have existed over the years since 1985. Each conference is listed with an abbreviation and a longer name. There has been no attempt to link up conferences who merged with other conferences, or whose names changed over time. Thus you will see, for instance, a \"Pacific-10\" conference up through the 2011 season, and then a \"Pacific-12\" conference starting in the 2012 season, and these look like different conferences in the data, even though it was still mostly the same teams. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two contests. However, the Team Conferences data differs slightly between men's and women's, so those files do have the prefixes. Data Section 4 file: WTeamConferences.csv This file indicates the conference affiliations for each team during each season. Some conferences have changed their names from year to year, and/or changed which teams are part of the conference. This file tracks this information historically."
    },
    {
        "name": "Store Sales - Time Series Forecasting",
        "url": "https://www.kaggle.com/competitions/store-sales-time-series-forecasting",
        "overview_text": "Overview text not found",
        "description_text": "In this \u201cgetting started\u201d competition, you\u2019ll use time-series forecasting to forecast store sales on data from Corporaci\u00f3n Favorita, a large Ecuadorian-based grocery retailer. Specifically, you'll build a model that more accurately predicts the unit sales for thousands of items sold at different Favorita stores. You'll practice your machine learning skills with an approachable training dataset of dates, store, and item information, promotions, and unit sales. Forecasts aren\u2019t just for meteorologists. Governments forecast economic growth. Scientists attempt to predict the future population. And businesses forecast product demand\u2014a common task of professional data scientists. Forecasts are especially relevant to brick-and-mortar grocery stores, which must dance delicately with how much inventory to buy. Predict a little over, and grocers are stuck with overstocked, perishable goods. Guess a little under, and popular items quickly sell out, leading to lost revenue and upset customers. More accurate forecasting, thanks to machine learning, could help ensure retailers please customers by having just enough of the right products at the right time. Current subjective forecasting methods for retail have little data to back them up and are unlikely to be automated. The problem becomes even more complex as retailers add new locations with unique needs, new products, ever-transitioning seasonal tastes, and unpredictable product marketing. If successful, you'll have flexed some new skills in a real world example. For grocery stores, more accurate forecasting can decrease food waste related to overstocking and improve customer satisfaction. The results of this ongoing competition, over time, might even ensure your local store has exactly what you need the next time you shop.",
        "dataset_text": "In this competition, you will predict sales for the thousands of product families sold at Favorita stores located in Ecuador. The training data includes dates, store and product information, whether that item was being promoted, as well as the sales numbers. Additional files include supplementary information that may be useful in building your models."
    },
    {
        "name": "Spaceship Titanic",
        "url": "https://www.kaggle.com/competitions/spaceship-titanic",
        "overview_text": "Overview text not found",
        "description_text": "Welcome to the year 2912, where your data science skills are needed to solve a cosmic mystery. We've received a transmission from four lightyears away and things aren't looking good. The Spaceship Titanic was an interstellar passenger liner launched a month ago. With almost 13,000 passengers on board, the vessel set out on its maiden voyage transporting emigrants from our solar system to three newly habitable exoplanets orbiting nearby stars. While rounding Alpha Centauri en route to its first destination\u2014the torrid 55 Cancri E\u2014the unwary Spaceship Titanic collided with a spacetime anomaly hidden within a dust cloud. Sadly, it met a similar fate as its namesake from 1000 years before. Though the ship stayed intact, almost half of the passengers were transported to an alternate dimension!  To help rescue crews and retrieve the lost passengers, you are challenged to predict which passengers were transported by the anomaly using records recovered from the spaceship\u2019s damaged computer system. Help save them and change history! If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here: https://discord.gg/kaggle Photos by Joel Filipe, Richard Gatley and ActionVance on Unsplash.",
        "dataset_text": "In this competition your task is to predict whether a passenger was transported to an alternate dimension during the Spaceship Titanic's collision with the spacetime anomaly. To help you make these predictions, you're given a set of personal records recovered from the ship's damaged computer system."
    },
    {
        "name": "LLM Classification Finetuning",
        "url": "https://www.kaggle.com/competitions/llm-classification-finetuning",
        "overview_text": "This competition challenges you to predict which responses users will prefer in a head-to-head battle between chatbots powered by large language models (LLMs). You'll be given a dataset of conversations from the Chatbot Arena, where different LLMs generate answers to user prompts. By developing a winning machine learning model, you'll help improve how chatbots interact with humans and ensure they better align with human preferences.",
        "description_text": "Large language models (LLMs) are rapidly entering our lives, but ensuring their responses resonate with users is critical for successful interaction. This competition presents a unique opportunity to tackle this challenge with real-world data and help us bridge the gap between LLM capability and human preference. We utilized a large dataset collected from Chatbot Arena, where users chat with two anonymous LLMs and choose the answer they prefer. Your task in this competition is to predict which response a user will prefer in these head-to-head battles. This challenge aligns with the concept of \"reward models\" or \"preference models\" in reinforcement learning from human feedback (RLHF). Previous research has identified limitations in directly prompting an existing LLM for preference predictions. These limitations often stem from biases such as favoring responses presented first (position bias), being overly verbose (verbosity bias), or exhibiting self-promotion (self-enhancement bias). We encourage you to explore various machine-learning techniques to build a model that can effectively predict user preferences. Your work will be instrumental in developing LLMs that can tailor responses to individual user preferences, ultimately leading to more user-friendly and widely accepted AI-powered conversation systems. This competition challenges you to predict which responses users will prefer in a head-to-head battle between chatbots powered by large language models (LLMs). Kaggle doesn\u2019t have a dedicated team to help troubleshoot your code so you\u2019ll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn! As we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with no-cost GPUs and a huge repository of community published data & code. In every competition, you\u2019ll find many Notebooks shared with incredible insights. It\u2019s an invaluable resource worth becoming familiar with. Check out this competition\u2019s Notebooks here.",
        "dataset_text": "The competition dataset consists of user interactions from the ChatBot Arena. In each user interaction a judge provides one or more prompts to two different large language models, and then indicates which of the models gave the more satisfactory response. The goal of the competition is to predict the preferences of the judges and determine the likelihood that a given prompt/response pair is selected as the winner. Please note that this is a Code Competition. When your submission is scored, this example test data will be replaced with the full test set. There are 55K rows in the training data, and you can expect roughly 25,000 rows in the test set. train.csv test.csv sample_submission.csv A submission file in the correct format. Note: the dataset for this competition contains text that may be considered profane, vulgar, or offensive."
    },
    {
        "name": "Tabular Playground Series - Mar 2021",
        "url": "https://www.kaggle.com/competitions/tabular-playground-series-mar-2021",
        "overview_text": "Overview text not found",
        "description_text": "Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation. The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard. For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. The dataset is used for this competition is synthetic but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the amount of an insurance claim. Although the features are anonymized, they have properties relating to real-world features. Good luck and have fun! Check out this Starter Notebook which walks you through how to make your very first submission! For more ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.",
        "dataset_text": "For this competition, you will be predicting a binary target based on a number of feature columns given in the data. All of the feature columns, cat0 - cat18 are categorical, and the feature columns cont0 - cont10 are continuous."
    },
    {
        "name": "March Machine Learning Mania 2021 - NCAAM - Spread",
        "url": "https://www.kaggle.com/competitions/ncaam-march-mania-2021-spread",
        "overview_text": "Overview text not found",
        "description_text": "In addition to the predictive modeling competitions we typically host (NCAA Women's and Men\u2019s), we are hosting two separate, experimental challenges that ask you to not only predict the the winner, but to predict the margin of victor. The mania of March can come down to final second buzzer beaters, upsets, and even a few blowouts. Can you predict big wins as easily as you can predict close calls? This competition (and the parallel competition for the women's tournament) allows you to explore how data science and machine learning can continue to examine the depths of college basketball. You're provided data of historical NCAA games and are encouraged to use other sources of publicly available data to gain a winning edge.  In stage one of this two-stage competition, participants will build and test their models against previous tournaments. In the second stage, participants will predict the point spreads of the 2021 tournament. You don\u2019t need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2021 results. Note: From 2018-2020, Google Cloud, of which Kaggle is a part, served as official corporate sponsors of the NCAA basketball tournaments. This official sponsorship has concluded, but we at Kaggle are happy to bring back this tradition for it's 8th year! Markus Spiske on Unsplash and The Noun Project",
        "dataset_text": "Each season there are thousands of NCAA basketball games played between Division I men's teams, culminating in March Madness\u00ae, the 68-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness\u00ae game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past. If you are unfamiliar with the format and intricacies of the NCAA\u00ae tournament, we encourage reading the wikipedia page before diving into the data.  The data description and schema may seem daunting at first, but is not as complicated as it appears. As a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The TeamSpellings file, which is listed in the bottom section below, may help you map external team references into our own Team ID structure, and you may also need to understand exactly how dates work in our data. Please also note that we have standardized the spelling of column names and some filenames, so if you are re-using code from previous instances of this contest, you may need to adjust for this. For example, we are universally referencing Team ID columns with a spelling of \"TeamID\" rather than \"team_id\". We extend our gratitude to Kenneth Massey for providing much of the historical data. Special Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition. Stage 1 - You should submit predicted point spreads for every possible matchup in the past 5 NCAA\u00ae tournaments (2015-2019). Stage 2 - You should submit predicted point spreads for every possible matchup before the 2021 tournament begins. Refer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict. Below we describe the format and fields of the contest data files. The data will likely be refreshed once in late February while Stage 1 of the competition is running. Many of the files are only complete through the end of last season. At the start of Stage 2, we will provide updates to these files to incorporate data from the current season. This section provides everything you need to build a simple prediction model and submit predictions. Special note about \"Season\" numbers: the college basketball season lasts from early November until the national championship tournament that starts in the middle of March. For instance, this year the first men\u2019s Division I games were played on November 25th, 2020 and the men\u2019s national championship game will be played on April 5th, 2021. Because a basketball season spans two calendar years like this, it can be confusing to refer to the year of the season. By convention, when we identify a particular season, we will reference the year that the season ends in, not the year that it starts in. So for instance, the current season will be identified in our data as the 2021 season, not the 2020 season or the 2020-21 season or the 2020-2021 season, though you may see any of these in everyday use outside of our data. Data Section 1 file: Teams.csv This file identifies the different college teams present in the dataset. Each school is uniquely identified by a 4 digit id number. You will not see games present for all teams in all seasons, because the games listing is only for matchups where both teams are Division-I teams. There are 357 teams currently in Division-I, and an overall total of 371 teams in our team listing (each year, some teams might start being Division-I programs, and others might stop being Division-I programs). This year there are four teams that are new to Division I: Bellarmine (TeamID=1469), North Alabama (TeamID=1469), Tarleton State (TeamID=1470), and UC_San Diego (TeamID=1471) and so you will not see any historical data for these teams prior to the current season. In addition, some teams opted not to play during the 2021 season due to the impact of COVID-19 and will not have any games listed.   Data Section 1 file: MSeasons.csv This file identifies the different seasons included in the historical data, along with certain season-level properties. Data Section 1 file: MNCAATourneySeeds.csv This file identifies the seeds for all teams in each NCAA\u00ae tournament, for all seasons of historical data. Thus, there are between 64-68 rows for each year, depending on whether there were any play-in games and how many there were. In recent years the structure has settled at 68 total teams, with four \"play-in\" games leading to the final field of 64 teams entering Round 1 on Thursday of the first week (by definition, that is DayNum=136 each season). We will not know the seeds of the respective tournament teams, or even exactly which 68 teams it will be, until Selection Sunday on March 14, 2021 (DayNum=132). Data Section 1 file: MRegularSeasonCompactResults.csv This file identifies the game-by-game results for many seasons of historical data, starting with the 1985 season (the first year the NCAA\u00ae had a 64-team tournament). For each season, the file includes all games played from DayNum 0 through 132. It is important to realize that the \"Regular Season\" games are simply defined to be all games played on DayNum=132 or earlier (DayNum=132 is Selection Sunday, and there are always a few conference tournament finals actually played early in the day on Selection Sunday itself). Thus a game played on or before Selection Sunday will show up here whether it was a pre-season tournament, a non-conference game, a regular conference game, a conference tournament game, or whatever. Data Section 1 file: MNCAATourneyCompactResults.csv This file identifies the game-by-game NCAA\u00ae tournament results for all seasons of historical data. The data is formatted exactly like the MRegularSeasonCompactResults data. All games will show up as neutral site (so WLoc is always N). Note that this tournament game data also includes the play-in games (which always occurred on day 134/135) for those years that had play-in games. Thus each season you will see between 63 and 67 games listed, depending on how many play-in games there were. Because of the consistent structure of the NCAA\u00ae tournament schedule, you can actually tell what round a game was, depending on the exact DayNum. Thus: Special note: Each year, there are also going to be other games that happened after Selection Sunday, which are not part of the NCAA\u00ae Tournament. This includes tournaments like the postseason NIT, the CBI, the CIT, and the Vegas 16. Such games are not listed in the Regular Season or the NCAA\u00ae Tourney files; they can be found in the \"Secondary Tourney\" data files within Data Section 6. Although they would not be games you would ever be predicting directly for the NCAA\u00ae tournament, and they would not be games you would have data from at the time of predicting NCAA\u00ae tournament outcomes, you may nevertheless wish to make use of these games for model optimization, depending on your methodology. The more games that you can test your predictions against, the better your optimized model might eventually become, depending on how applicable all those games are. A similar argument might be advanced in favor of optimizing your predictions against conference tournament games, which might be viewed as reasonable proxies for NCAA\u00ae tournament games.  Data Section 1 file: MSampleSubmissionStage1.csv This file illustrates the submission file format for Stage 1. It is the simplest possible submission: a point spread of 0 is predicted for each possible matchup. A submission file lists every possible matchup between tournament teams for one or more years. During Stage 1, you are asked to make predictions for all possible matchups from the past five NCAA\u00ae tournaments (seasons 2015, 2016, 2017, 2018, 2019). In Stage 2, you will be asked to make predictions for all possible matchups from the current NCAA\u00ae tournament (season 2021). When there are 68 teams in the tournament, there are 68*67/2=2,278 predictions to make for that year, so a Stage 1 submission file will have 2,278*5=11,390 data rows. Example #1: You want to make a prediction for Duke (TeamID=1181) against Arizona (TeamID=1112) in the 2017 tournament, with Duke to beat Arizona by 3 points. In this case, Arizona has the lower numerical ID so they would be listed first, and the point spread would be expressed from Arizona's perspective (-3): 2017_1112_1181,3 Example #2: You want to make a prediction for Duke (TeamID=1181) against North Carolina (TeamID=1314) in the 2018 tournament, with Duke to win by 7 points. In this case, Duke has the lower numerical ID so they would be listed first, and the point spread would be expressed from Duke's perspective (7): 2018_1181_1314,7 This section provides game-by-game stats at a team level (free throws attempted, defensive rebounds, turnovers, etc.) for all regular season, conference tournament, and NCAA\u00ae tournament games since the 2002-03 season.  Team Box Scores are provided in \"Detailed Results\" files rather than \"Compact Results\" files. However, the two files are strongly related. In a Detailed Results file, the first eight columns (Season, DayNum, WTeamID, WScore, LTeamID, LScore, WLoc, and NumOT) are exactly the same as a Compact Results file. However, in a Detailed Results file, there are many additional columns. The column names should be self-explanatory to basketball fans (as above, \"W\" or \"L\" refers to the winning or losing team): (and then the same set of stats from the perspective of the losing team: LFGM is the number of field goals made by the losing team, and so on up to LPF). Note: by convention, \"field goals made\" (either WFGM or LFGM) refers to the total number of fields goals made by a team, a combination of both two-point field goals and three-point field goals. And \"three point field goals made\" (either WFGM3 or LFGM3) is just the three-point fields goals made, of course. So if you want to know specifically about two-point field goals, you have to subtract one from the other (e.g., WFGM - WFGM3). And the total number of points scored is most simply expressed as 2*FGM + FGM3 + FTM. Data Section 2 file: MRegularSeasonDetailedResults.csv This file provides team-level box scores for many regular seasons of historical data, starting with the 2003 season. All games listed in the MRegularSeasonCompactResults file since the 2003 season should exactly be present in the MRegularSeasonDetailedResults file. Data Section 2 file: MNCAATourneyDetailedResults.csv This file provides team-level box scores for many NCAA\u00ae tournaments, starting with the 2003 season. All games listed in the MNCAATourneyCompactResults file since the 2003 season should exactly be present in the MNCAATourneyDetailedResults file. This section provides city locations of all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season Data Section 3 file: Cities.csv This file provides a master list of cities that have been locations for games played. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two contests. Also note that if you created any supplemental data last year on cities (latitude/longitude, altitude, etc.), the CityID's match between last year and this year, so you should be able to re-use that information. Data Section 3 file: MGameCities.csv This file identifies all games, starting with the 2010 season, along with the city that the game was played in. Games from the regular season, the NCAA\u00ae tourney, and other post-season tournaments, are all listed together. There should be no games since the 2010 season where the CityID is not known. Games from the 2009 season and before are not listed in this file. This section provides weekly team rankings for dozens of top rating systems - Pomeroy, Sagarin, RPI, ESPN, etc., since the 2002-2003 season Data Section 4 file: MMasseyOrdinals.csv This file lists out rankings (e.g. #1, #2, #3, ..., #N) of teams going back to the 2002-2003 season, under a large number of different ranking system methodologies. The information was gathered by Kenneth Massey and provided on his College Basketball Ranking Composite page. Note that a rating system is more precise than a ranking system, because a rating system can provide insight about the strength gap between two adjacently-ranked teams. A ranking system will just tell you who is #1 or who is #2, but a rating system might tell you whether the gap between #1 and #2 is large or small. Nevertheless, it can be hard to compare two different rating systems that are expressed in different scales, so it can be very useful to express all the systems in terms of their ordinal ranking (1, 2, 3, ..., N) of teams. Disclaimer: you ought to be careful about your methodology when using or evaluating these ranking systems. They are presented on a weekly basis, and given a consistent date on the Massey Composite page that typically is a Sunday; that is how the ranking systems can be compared against each other on this page. However, these systems each follow their own timeline and some systems may be released on a Sunday and others on a Saturday or Monday or even Tuesday. You should remember that if a ranking is released on a Tuesday, and was calculated based on games played through Monday, it will make the system look unusually good at predicting if you use that system to forecast the very games played on Monday that already inform the rankings. To avoid this methodological trap, we have typically used a conservative RankingDayNum of Wednesday to represent the rankings that were released at approximately the end of the weekend, a few days before, even though those rankings are represented on the composite page as being on a Sunday. For some of the older years, a more precise timestamp was known for each ranking system that allowed a more precise assignment of a RankingDayNum. By convention, the final pre-tournament rankings are always expressed as RankingDayNum=133, even though sometimes the rankings for individual systems are not released until Tuesday (DayNum=134) or even Wednesday or Thursday. If you decide to use some rankings from these Massey Ordinals to inform your predictions, be forewarned that we have no control over when they are released, and not all systems may turn out to be available in time to make pre-tournament predictions by our submission deadline. In such a situation, you may wish to use the rankings from DayNum=128 or you may need to dig into the details of the actual source of the rankings, by following the respective links on the Massey Composite Page. We may also be able to provide partial releases of the final pre-tournament Massey Ordinals on the forums, so that as systems come in on Monday or Tuesday you can use them right away. This section contains additional supporting information, including coaches, conference affiliations, alternative team name spellings, bracket structure, and game results for NIT and other postseason tournaments. Data Section 5 file: MTeamCoaches.csv This file indicates the head coach for each team in each season, including a start/finish range of DayNum's to indicate a mid-season coaching change. For scenarios where a team had the same head coach the entire season, they will be listed with a DayNum range of 0 to 154 for that season. For head coaches whose term lasted many seasons, there will be many rows listed, most of which have a DayNum range of 0 to 154 for the corresponding season. Data Section 5 file: Conferences.csv This file indicates the Division I conferences that have existed over the years since 1985. Each conference is listed with an abbreviation and a longer name. There has been no attempt to link up conferences who merged with other conferences, or whose names changed over time. Thus you will see, for instance, a \"Pacific-10\" conference up through the 2011 season, and then a \"Pacific-12\" conference starting in the 2012 season, and these look like different conferences in the data, even though it was still mostly the same teams. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two contests. However, the Team Conferences data differs slightly between men's and women's, so those files do have the prefixes. Data Section 5 file: MTeamConferences.csv This file indicates the conference affiliations for each team during each season. Some conferences have changed their names from year to year, and/or changed which teams are part of the conference. This file tracks this information historically. Data Section 5 file: MConferenceTourneyGames.csv This file indicates which games were part of each year's post-season conference tournaments (all of which finished on Selection Sunday or earlier), starting from the 2001 season. Many of these conference tournament games are held on neutral sites, and many of the games are played by tournament-caliber teams just a few days before the NCAA\u00ae tournament. Thus these games could be considered as very similar to NCAA\u00ae tournament games, and (depending on your methodology) may be of use in optimizing your predictions. However, this is NOT a new listing of games; these games are already present within the MRegularSeasonCompactResults and MRegularSeasonDetailedResults files. So this file simply helps you to identify which of the \"regular season\" games since the 2001 season were actually conference tournament games, in case that is useful information. Data Section 5 file: MSecondaryTourneyTeams.csv This file identifies the teams that participated in post-season tournaments other than the NCAA\u00ae Tournament (such events would run in parallel with the NCAA\u00ae Tournament). These are teams that were not invited to the NCAA\u00ae Tournament and instead were invited to some other tournament, of which the NIT is the most prominent tournament, but there have also been the CBI, CIT, and Vegas 16 (V16) at various points in recent years. Depending on your methodology, you might find it useful to have these additional game results, above and beyond what is available from the NCAA\u00ae Tournament results. Many of these teams, especially in the NIT, were \"bubble\" teams of comparable strength to several NCAA\u00ae Tournament invitees, and so these games may be of use in model optimization for predicting NCAA\u00ae Tournament results. Also note that this information could be determined just from inspecting the MSecondaryTourneyCompactResults file, but is presented in this file as well, for your convenience. Data Section 5 file: MSecondaryTourneyCompactResults.csv This file indicates the final scores for the tournament games of \"secondary\" post-season tournaments: the NIT, CBI, CIT, and Vegas 16. The detailed results (team box scores) have not been assembled for these games. For the most part, this file is exactly like other Compact Results listings, although it also has a column for Secondary Tourney. Also note that because these games are played after DayNum=132, they are NOT listed in the MRegularSeasonCompactResults file. Data Section 5 file: MTeamSpellings.csv This file indicates alternative spellings of many team names. It is intended for use in associating external spellings against our own TeamID numbers, thereby helping to relate the external data properly with our datasets. Over the years we have identified various external spellings of different team names (as an example, for Ball State we have seen \"ball st\", and \"ball st.\", and \"ball state\", and \"ball-st\", and \"ball-state\"). Other teams have had more significant changes to their names over the years; for example, \"Texas Pan-American\" and \"Texas-Rio Grande Valley\" are actually the same school. The current list is obviously not exhaustive, and we encourage participants to identify additional mappings and upload extended versions of this file to the forums. Data Section 5 file: MNCAATourneySlots This file identifies the mechanism by which teams are paired against each other, depending upon their seeds, as the tournament proceeds through its rounds. It can be of use in identifying, for a given historical game, what round it occurred in, and what the seeds/slots were for the two teams (the meaning of \"slots\" is described below). Because of the existence of play-in games for particular seed numbers, the pairings have small differences from year to year. You may need to know these specifics if you are trying to represent/simulate the exact workings of the tournament bracket. Data Section 5 file: MNCAATourneySeedRoundSlots.csv This file helps to represent the bracket structure in any given year. No matter where the play-in seeds are located, we can always know, for a given tournament seed, exactly what bracket slot they would be playing in, on each possible game round, and what the possible DayNum values would be for that round. Thus, if we know when a historical game was played, and what the team's seed was, we can identify the slot for that game. This can be useful in representing or simulating the tournament bracket structure."
    },
    {
        "name": "Tabular Playground Series - Apr 2021",
        "url": "https://www.kaggle.com/competitions/tabular-playground-series-apr-2021",
        "overview_text": "Overview text not found",
        "description_text": "Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation. The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard. For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. The dataset is used for this competition is synthetic but based on a real dataset (in this case, the actual Titanic data!) and generated using a CTGAN. The statistical properties of this dataset are very similar to the original Titanic dataset, but there's no way to \"cheat\" by using public labels for predictions. How well does your model perform on truly private test labels? Good luck and have fun! Check out the original Titanic competition which walks you through how to build various models. For more ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.",
        "dataset_text": " The dataset is used for this competition is synthetic but based on a real dataset (in this case, the actual Titanic data!) and generated using a CTGAN. The statistical properties of this dataset are very similar to the original Titanic dataset, but there's no way to \"cheat\" by using public labels for predictions. How well does your model perform on truly unseen data?  The data has been split into two groups: The training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the \u201cground truth\u201d) for each passenger. Your model will be based on \u201cfeatures\u201d like passengers\u2019 gender and class. You can also use feature engineering to create new features. The test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Synthanic. pclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them."
    },
    {
        "name": "Tabular Playground Series - May 2021",
        "url": "https://www.kaggle.com/competitions/tabular-playground-series-may-2021",
        "overview_text": "Overview text not found",
        "description_text": "Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation. The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard. For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams, to inspire broad participation we are limiting winner's of swag to once per person for this series. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the category on an eCommerce product given various attributes about the listing. Although the features are anonymized, they have properties relating to real-world features. Good luck and have fun! For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.",
        "dataset_text": "The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the category on an eCommerce product given various attributes about the listing. Although the features are anonymized, they have properties relating to real-world features."
    },
    {
        "name": "Tabular Playground Series - Jun 2021",
        "url": "https://www.kaggle.com/competitions/tabular-playground-series-jun-2021",
        "overview_text": "Overview text not found",
        "description_text": "Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation. The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard. For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the category on an eCommerce product given various attributes about the listing. Although the features are anonymized, they have properties relating to real-world features. Good luck and have fun! For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.",
        "dataset_text": "The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the category on an eCommerce product given various attributes about the listing. Although the features are anonymized, they have properties relating to real-world features. This competition dataset is similar to the Tabular Playground Series - May 2021 dataset, but with increased observations, increased features, and increased class labels."
    },
    {
        "name": "Tabular Playground Series - Jul 2021",
        "url": "https://www.kaggle.com/competitions/tabular-playground-series-jul-2021",
        "overview_text": "Overview text not found",
        "description_text": "Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation. The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard. For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. The dataset is used for this competition is based on a real dataset, but has synthetic-generated aspects to it. The original dataset deals with predicting air pollution in a city via various input sensor values (e.g., a time series). Good luck and have fun! For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.",
        "dataset_text": "In this competition you are predicting the values of air pollution measurements over time, based on basic weather information (temperature and humidity) and the input values of 5 sensors. The three target values to you to predict are: target_carbon_monoxide, target_benzene, and target_nitrogen_oxides"
    },
    {
        "name": "Tabular Playground Series - Aug 2021",
        "url": "https://www.kaggle.com/competitions/tabular-playground-series-aug-2021",
        "overview_text": "Overview text not found",
        "description_text": "Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation. The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard. For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with calculating the loss associated with a loan defaults. Although the features are anonymized, they have properties relating to real-world features. Good luck and have fun! For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.",
        "dataset_text": "For this competition, you will be predicting a target loss based on a number of feature columns given in the data. The ground truth loss is integer valued, although predictions can be continuous."
    },
    {
        "name": "Tabular Playground Series - Sep 2021",
        "url": "https://www.kaggle.com/competitions/tabular-playground-series-sep-2021",
        "overview_text": "Overview text not found",
        "description_text": "Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation. The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard. For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting whether a claim will be made on an insurance policy. Although the features are anonymized, they have properties relating to real-world features. Good luck and have fun! For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.",
        "dataset_text": "For this competition, you will predict whether a customer made a claim upon an insurance policy. The ground truth claim is binary valued, but a prediction may be any number from 0.0 to 1.0, representing the probability of a claim. The features in this dataset have been anonymized and may contain missing values."
    },
    {
        "name": "Tabular Playground Series - Oct 2021",
        "url": "https://www.kaggle.com/competitions/tabular-playground-series-oct-2021",
        "overview_text": "Overview text not found",
        "description_text": "Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation. The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard. For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the biological response of molecules given various chemical properties. Although the features are anonymized, they have properties relating to real-world features. Good luck and have fun! For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.",
        "dataset_text": "For this competition, you will be predicting a binary target based on a number of feature columns given in the data. The columns are a mix of scaled continuous features and binary features. The data is synthetically generated by a GAN that was trained on real-world molecular response data."
    },
    {
        "name": "Tabular Playground Series - Nov 2021",
        "url": "https://www.kaggle.com/competitions/tabular-playground-series-nov-2021",
        "overview_text": "Overview text not found",
        "description_text": "Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation. The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard. For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting identifying spam emails via various extracted features from the email. Although the features are anonymized, they have properties relating to real-world features. Good luck and have fun! For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.",
        "dataset_text": "For this competition, you will be predicting a binary target based on 100 feature columns given in the data. All columns are continuous. The data is synthetically generated by a GAN that was trained on a real-world dataset used to identify spam emails via various extracted features from the email."
    },
    {
        "name": "Tabular Playground Series - Dec 2021",
        "url": "https://www.kaggle.com/competitions/tabular-playground-series-dec-2021",
        "overview_text": "Overview text not found",
        "description_text": "Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation. The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard. For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. This dataset is based off of the original Forest Cover Type Prediction competition. Good luck and have fun! For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.",
        "dataset_text": "For this competition, you will be predicting a categorical target based on a number of feature columns given in the data.\nThe data is synthetically generated by a GAN that was trained on a the data from the Forest Cover Type Prediction. This dataset is (a) much larger, and (b) may or may not have the same relationship to the target as the original data. Please refer to this data page for a detailed explanation of the features."
    },
    {
        "name": "Wikipedia - Image/Caption Matching",
        "url": "https://www.kaggle.com/competitions/wikipedia-image-caption",
        "overview_text": "Overview text not found",
        "description_text": "A picture is worth a thousand words, yet sometimes a few will do. We all rely on online images for knowledge sharing, learning, and understanding. Even the largest websites are missing visual content and metadata to pair with their images. Captions and \u201calt text\u201d increase accessibility and enable better search. The majority of images on Wikipedia articles, for example, don't have any written context connected to the image. Open models could help anyone improve accessibility and learning for all. Current solutions rely on simple methods based on translations or page interlinks, which have limited coverage. Even the most advanced computer vision image captioning isn't suitable for images with complex semantics. In this competition, you\u2019ll build a model that automatically retrieves the text closest to an image. Specifically, you'll train your model to associate given images with article titles or complex captions, in multiple languages. The best models will account for the semantic granularity of Wikipedia images. If successful, you'll be contributing to the accessibility of the largest online encyclopedia. The millions of Wikipedia readers and editors will be able to more easily understand, search, and describe media at scale. As a result, you\u2019ll contribute to an open model to improve learning for all. --\nThis competition is organized by the Research team at the Wikimedia Foundation. This competition is based on the WIT dataset published by Google Research as detailed in this SIGIR paper.",
        "dataset_text": "The objective of this competition is to predict the target caption_title_and_reference_description given information about an images. The targets for this competition are in multiple languages."
    },
    {
        "name": "Tabular Playground Series - Jan 2022",
        "url": "https://www.kaggle.com/competitions/tabular-playground-series-jan-2022",
        "overview_text": "Overview text not found",
        "description_text": "We've heard your feedback from the 2021 Tabular Playground Series, and now Kaggle needs your help going forward in 2022! There are two (fictitious) independent store chains selling Kaggle merchandise that want to become the official outlet for all things Kaggle. We've decided to see if the Kaggle community could help us figure out which of the store chains would have the best sales going forward. So, we've collected some data and are asking you to build forecasting models to help us decide. Help us figure out whether KaggleMart or KaggleRama should become the official Kaggle outlet!  Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard. For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.",
        "dataset_text": "For this challenge, you will be predicting a full year worth of sales for three items at two stores located in three different countries. This dataset is completely fictional, but contains many effects you see in real-world data, e.g., weekend and holiday effect, seasonality, etc. The dataset is small enough to allow you to try numerous different modeling approaches. Good luck!"
    },
    {
        "name": "Tabular Playground Series - Feb 2022",
        "url": "https://www.kaggle.com/competitions/tabular-playground-series-feb-2022",
        "overview_text": "Overview text not found",
        "description_text": "For the February 2022 Tabular Playground Series competition, your task is to classify 10 different bacteria species using data from a genomic analysis technique that has some data compression and data loss. In this technique, 10-mer snippets of DNA are sampled and analyzed to give the histogram of base count. In other words, the DNA segment\nbecomes\n. Can you use this lossy information to accurately predict bacteria species? Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard. For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn. Good luck and have fun! The idea for this competition came from the following paper:",
        "dataset_text": "For this challenge, you will be predicting bacteria species based on repeated lossy measurements of DNA snippets. Snippets of length 10 are analyzed using Raman spectroscopy that calculates the histogram of bases in the snippet. In other words, the DNA segment\nbecomes\n. Each row of data contains a spectrum of histograms generated by repeated measurements of a sample, each row containing the output of all 286 histogram possibilities (e.g.,\nto\n), which then has a bias spectrum (of totally random ATGC) subtracted from the results. The data (both train and test) also contains simulated measurement errors (of varying rates) for many of the samples, which makes the problem more challenging."
    },
    {
        "name": "Tabular Playground Series - Mar 2022",
        "url": "https://www.kaggle.com/competitions/tabular-playground-series-mar-2022",
        "overview_text": "Overview text not found",
        "description_text": "For the March edition of the 2022 Tabular Playground Series you're challenged to forecast twelve-hours of traffic flow in a U.S. metropolis. The time series in this dataset are labelled with both location coordinates and a direction of travel -- a combination of features that will test your skill at spatio-temporal forecasting within a highly dynamic traffic network. Which model will prevail? The venerable linear regression? The deservedly-popular ensemble of decision trees? Or maybe a cutting-edge graph neural-network? We can't wait to see! Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard. For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.",
        "dataset_text": "In this competition, you'll forecast twelve-hours of traffic flow in a major U.S. metropolitan area. Time, space, and directional features give you the chance to model interactions across a network of roadways. This dataset was derived from the Chicago Traffic Tracker - Historical Congestion Estimates dataset."
    },
    {
        "name": "Tabular Playground Series - Apr 2022",
        "url": "https://www.kaggle.com/competitions/tabular-playground-series-apr-2022",
        "overview_text": "Overview text not found",
        "description_text": "Welcome to the April edition of the 2022 Tabular Playground Series! This month's challenge is a time series classification problem. You've been provided with thousands of sixty-second sequences of biological sensor data recorded from several hundred participants who could have been in either of two possible activity states. Can you determine what state a participant was in from the sensor data? Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard. For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.",
        "dataset_text": "In this competition, you'll classify 60-second sequences of sensor data, indicating whether a subject was in either of two activity states for the duration of the sequence."
    },
    {
        "name": "Tabular Playground Series - May 2022",
        "url": "https://www.kaggle.com/competitions/tabular-playground-series-may-2022",
        "overview_text": "Overview text not found",
        "description_text": "The May edition of the 2022 Tabular Playground series binary classification problem that includes a number of different feature interactions. This competition is an opportunity to explore various methods for identifying and exploiting these feature interactions.  Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard. For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn. We've also built a starter notebook for you that uses TensorFlow Decision Forests, a TensorFlow library that matches the power of XGBoost with a friendly, straightforward user interface. Good luck and have fun! Photo by Clarisse Croset on Unsplash.",
        "dataset_text": "For this challenge, you are given (simulated) manufacturing control data and are tasked to predict whether the machine is in state 0 or state 1. The data has various feature interactions that may be important in determining the machine state. Good luck!"
    },
    {
        "name": "Tabular Playground Series - Jun 2022",
        "url": "https://www.kaggle.com/competitions/tabular-playground-series-jun-2022",
        "overview_text": "Overview text not found",
        "description_text": "The June edition of the 2022 Tabular Playground series is all about data imputation. The dataset has similarities to the May 2022 Tabular Playground, except that there are no targets. Rather, there are missing data values in the dataset, and your task is to predict what these values should be.  Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard. For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn. We've also provided a notebook to get people started Good luck and have fun! Photo by Mika Baumeister on Unsplash.",
        "dataset_text": "For this challenge, you are given (simulated) manufacturing control data that contains missing values due to electronic errors. Your task is to predict the values of all missing data in this dataset. (Note, while there are continuous and categorical features, only the continuous features have missing values.) Here's a notebook that you can use to get started. Good luck!"
    },
    {
        "name": "Tabular Playground Series - Jul 2022",
        "url": "https://www.kaggle.com/competitions/tabular-playground-series-jul-2022",
        "overview_text": "Overview text not found",
        "description_text": "Welcome to Kaggle's first ever unsupervised clustering challenge! In this challenge, you are given a dataset where each row belongs to a particular cluster. Your job is to predict the cluster each row belongs to. You are not given any training data, and you are not told how many clusters are found in the ground truth labels.  Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard. For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn. Good luck and have fun! Photo above by Laura Rivera on Unsplash",
        "dataset_text": "For this challenge, you are given (simulated) manufacturing control data that can be clustered into different control states. Your task is to cluster the data into these control states. You are not given any training data, and you are not told how many possible control states there are. This is a completely unsupervised problem, one you might encounter in a real-world setting. Good luck!"
    },
    {
        "name": "Kore 2022 - Beta",
        "url": "https://www.kaggle.com/competitions/kore-2022-beta",
        "overview_text": "Overview text not found",
        "description_text": "Please note - this is a preview/beta launch of an upcoming competition. This competition is intended to help with rule balancing and establishing a fair and fun competition, soon to be launched. As such, this competition does not have cash prizes, points, or medals - but we hope to gain your feedback for when the featured competition goes live! When you want to mine kore quickly: go alone. But when you want to mine the most kore: build a fleet. In this turn-based simulation game you control a small armada of spaceships. As you mine the rare mineral \u201ckore\u201d from the depths of space, you teleport it back to your homeworld. But it turns out you aren\u2019t the only civilization with this goal. In each game four players will compete to collect the most kore from the board. Whoever has the largest kore cache by the end of 400 turns\u2014or eliminates all of their opponents from the board before that\u2014will be the winner!  Your algorithms determine the movements of your fleets to collect kore, but it's up to you to figure out how to make effective and efficient moves. You control your ships, build new ships, create shipyards, eliminate opponents, and mine the kore on the game board. May your fleet live long and prosper!",
        "dataset_text": "This page appears alongside the data files. It describes what files have been provided & the format of each. It also defines the correct format for submission files. Participants should be able to answer these types of questions after reading the data description: What files do I need?\nWhat should I expect the data format to be?\nWhat am I predicting?\nWhat acronyms will I encounter?"
    },
    {
        "name": "Tabular Playground Series - Aug 2022",
        "url": "https://www.kaggle.com/competitions/tabular-playground-series-aug-2022",
        "overview_text": "Overview text not found",
        "description_text": "The August 2022 edition of the Tabular Playground Series is an opportunity to help the fictional company Keep It Dry improve its main product Super Soaker. The product is used in factories to absorb spills and leaks. The company has just completed a large testing study for different product prototypes. Can you use this data to build a model that predicts product failures?  Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard. For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn. Good luck and have fun! Photo above by freestocks on Unsplash",
        "dataset_text": "This data represents the results of a large product testing study. For each product_code you are given a number of product attributes (fixed for the code) as well as a number of measurement values for each individual product, representing various lab testing methods. Each product is used in a simulated real-world environment experiment, and and absorbs a certain amount of fluid (loading) to see whether or not it fails. Your task is to use the data to predict individual product failures of new codes with their individual lab test results."
    },
    {
        "name": "Tabular Playground Series - Sep 2022",
        "url": "https://www.kaggle.com/competitions/tabular-playground-series-sep-2022",
        "overview_text": "Overview text not found",
        "description_text": "The competing Kaggle merchandise stores we saw in January's Tabular Playground are at it again. This time, they're selling books! The task for this month's competitions is a bit more complicated. Not only are there six countries and four books to forecast, but you're being asked to forecast sales during the tumultuous year 2021. Can you use your data science skills to predict book sales when conditions are far from the ordinary?  Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard. For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn. Good luck and have fun! Photo above by Aron Visuals on Unsplash",
        "dataset_text": "For this challenge, you will be predicting a full year worth of sales for 4 items from two competing stores located in six different countries. This dataset is completely fictional, but contains many effects you see in real-world data, e.g., weekend and holiday effect, seasonality, etc. You are given the challenging task of predicting book sales during the year 2021. Good luck!"
    },
    {
        "name": "Tabular Playground Series - Oct 2022",
        "url": "https://www.kaggle.com/competitions/tabular-playground-series-oct-2022",
        "overview_text": "Overview text not found",
        "description_text": "This may be one of the most challenging Tabular Playground competitions to date! It just so happens that one of Kaggle's software engineers is an avid Rocket League player and he's assembled a dataset of Rocket League gameplay for this month's TPS. This month's challenge is to predict the probability of each team scoring within the next 10 seconds of the game given a snapshot from a Rocket League match. Sounds awesome, right?  Well, it's not that simple. The training data is fairly large; trying to read and model it in a single go might pose some challenges. The purpose of this month's competition is for you to explore ways you can take a big dataset and make it manageable within the time and resources you have. For most people, typical brute force approaches aren't going to work well. In addition to that challenge, while your predictions must be made pointwise, the training data is made up of timeseries\u2014maybe you can use that temporal information to improve your model? This competition also has plenty of opportunity for data visualizations. Let's see some pretty graphs! So, share your ideas about tackling this beast of a dataset and have a great time!  This competition includes Rocket League data and images from the Rocket League Community Tournament Assets.",
        "dataset_text": "The dataset consists of sequences of snapshots of the state of a Rocket League match, including position and velocity of all players and the ball, as well as extra information. The goal of the competition is to predict -- from a given snapshot in the game -- for each team, the probability that they will score within the next 10 seconds of game time. The data was taken from professional Rocket League matches. Each event consists of a chronological series of frames recorded at 10 frames per second. All events begin with a kickoff, and most end in one team scoring a goal, but some are truncated and end with no goal scored due to circumstances which can cause gameplay strategies to shift, for example 1) nearing end of regulation (where the game continues until the ball touches the ground) or 2) becoming non-competitive, eg one team winning by 3+ goals with little time remaining. Files: Columns: Example Python code to read CSV using pandas dtypes:"
    },
    {
        "name": "Tabular Playground Series - Nov 2022",
        "url": "https://www.kaggle.com/competitions/tabular-playground-series-nov-2022",
        "overview_text": "Overview text not found",
        "description_text": "You may have heard that blending predictions from model predictions can give better results than using the output of a single model. There are many different strategies that can be employed for this, and they are great to learn if you're looking for an effectively free boost in model scores. The November Tabular Playground is the chance to practice this skill!  Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard. For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn. Good luck and have fun! Photo by RhondaK Native Florida Folk Artist on Unsplash",
        "dataset_text": "In this competition, you are given a folder of submissions that contain predictions for a binary classification task. The ground truth for these rows are provided in the file train_labels.csv. Each file name in the submissions folder corresponds to the logloss score of the the first half of the prediction rows against the ground truth labels in that file. This is effectively the \"training\" set. Your task is to use blend (ensemble) the various submission files to produce better model predictions. You can tell if the blend produces better results because you have the ground truth labels for the first half of the rows. In other words, you can blend two files together, check the score on the first half of the rows, and the score improves, you can submit the results from the rest of the rows to the leaderboard. A simple example is shown in this notebook."
    },
    {
        "name": "Scrabble Player Rating",
        "url": "https://www.kaggle.com/competitions/scrabble-player-rating",
        "overview_text": "Overview text not found",
        "description_text": "Are you a Kaggle Scrabble Grandmaster? In the second edition of this Competition featuring data from Woogles.io, participants are challenged to predict the ratings of players based on Scrabble gameplay. The evaluation algorithm is RMSE. Find inspiration from the first Competition based on Woogles.io data where participants were challenged to predict the point value of the 20th turn from Scrabble games. You get TWO SUBMISSIONS per day. They must be submitted before the end of the competition deadline. You can hand select up to two submissions from the \"My Submissions\" tab, otherwise the submission with the best public score (on 30% of the test data) will be taken and scored on the private leaderboard (on 70% of the test data). Thank you to woogles.io for providing their platform for playing Scrabble online. Woogles is an entirely volunteer-run 501(c)(3) non-profit. If you enjoy the site, please feel free to contribute by clicking \"Want to help?\" at https://woogles.io/.",
        "dataset_text": "The dataset includes information about ~73,000 Scrabble games played by three bots on Woogles.io: BetterBot (beginner), STEEBot (intermediate), and HastyBot (advanced). The games are between the bots and their opponents who are regular registered users. You are using metadata about the games as well as turns in each game (i.e., players' racks and where and what they played, AKA gameplay) to predict the rating of the human opponents in the test set (test.csv). You will train your model on gameplay data from one set of human opponents to make predictions about a different set of human opponents in the test set. There is metadata for each game (games.csv), gameplay data about turns played by each player in each game (turns.csv), and final scores and ratings from BEFORE a given game was played for each player in each game (train.csv and test.csv). Here is an example of a game played on woogles.io: https://woogles.io/game/icNJtmxy. Use the \"Examine\" button to replay the game turn-by-turn. turns.csv contains full data for every turn for each game. There's score and rating data about 1031 players in train.csv and 443 players in test.csv. Except for the three bots, no players are in both train.csv and test.csv. Ratings for the players are from BEFORE the game was played. Your task is to predict what the rating of the human player was in test.csv BEFORE the given game was played."
    },
    {
        "name": "Lux AI 2022 - Beta",
        "url": "https://www.kaggle.com/competitions/lux-ai-2022-beta",
        "overview_text": "Overview text not found",
        "description_text": "Please note - this is a preview/beta launch of an upcoming competition. This competition is intended to help with rule balancing and establishing a fair and fun competition, soon to be launched. As such, this competition does not have cash prizes, points, or medals - but we hope to gain your feedback to help make the upcoming competition as fun as possible! As the sun set on the world an array of lights dotted the once dark horizon. With the help of a brigade of toads, Lux had made it past the terrors in the night to see the dawn of a new age. Seeking new challenges, plans were made to send a forward force with one mission: terraform Mars! Welcome to the Lux AI Challenge Season 2! The Lux AI Challenge is a competition where competitors design agents to tackle a multi-variable optimization, resource gathering, and allocation problem in a 1v1 scenario against other competitors. In addition to optimization, successful agents must be capable of analyzing their opponents and developing appropriate policies to get the upper hand. All code can be found at our Github, make sure to give it a star while you are there! Make sure to join our community discord to chat, strategize, and learn with other competitors! We will be posting announcements on the Kaggle Forums and on the discord.",
        "dataset_text": "This is the python starter kit. Download this to get started competing in python!"
    },
    {
        "name": "Regression with a Tabular California Housing Dataset",
        "url": "https://www.kaggle.com/competitions/playground-series-s3e1",
        "overview_text": "Overview text not found",
        "description_text": "You can now create your own synthetic versions of this dataset by forking and running this notebook. Thank you to everyone who participated in and contributed to last year's Tabular Playground Series. And many thanks to all those who took the time to provide constructive feedback! We're thrilled that there continues to be interest in these types of challenges, and we're continuing the series this year but with a few changes. First, the series is getting upgraded branding. We've dropped \"Tabular\" from the name because, while we anticipate this series will still have plenty of tabular competitions, we'll also be having some other formats as well. You'll also notice freshly-upgraded (better looking and more fun!) banner and thumbnail images. Second, rather than naming the challenges by month and year, we're moving to a Season-Edition format. This year is Season 3, and each challenge will be a new Edition. We're doing this to have more flexibility. Competitions going forward won't necessarily align with each month like they did in previous years (although some might!), we'll have competitions with different time durations, and we may have multiple competitions running at the same time on occasion. Regardless of these changes, the goals of the Playground Series remain the same\u2014to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. We hope we continue to meet this objective! To start the year with some fun, January will be the month of Tabular Tuesday. We're launching four week-long tabular competitions, with each starting Tuesday 00:00 UTC. These will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!",
        "dataset_text": "NOTE: You can now create your own synthetic versions of this dataset by forking and running this notebook. The dataset for this competition (both train and test) was generated from a deep learning model trained on the California Housing Dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance."
    },
    {
        "name": "Binary Classification with a Tabular Stroke Prediction Dataset",
        "url": "https://www.kaggle.com/competitions/playground-series-s3e2",
        "overview_text": "Overview text not found",
        "description_text": "Thank you to everyone who participated in and contributed to last year's Tabular Playground Series. And many thanks to all those who took the time to provide constructive feedback! We're thrilled that there continues to be interest in these types of challenges, and we're continuing the series this year but with a few changes. First, the series is getting upgraded branding. We've dropped \"Tabular\" from the name because, while we anticipate this series will still have plenty of tabular competitions, we'll also be having some other formats as well. You'll also notice freshly-upgraded (better looking and more fun!) banner and thumbnail images. Second, rather than naming the challenges by month and year, we're moving to a Season-Edition format. This year is Season 3, and each challenge will be a new Edition. We're doing this to have more flexibility. Competitions going forward won't necessarily align with each month like they did in previous years (although some might!), we'll have competitions with different time durations, and we may have multiple competitions running at the same time on occasion. Regardless of these changes, the goals of the Playground Series remain the same\u2014to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. We hope we continue to meet this objective! To start the year with some fun, January will be the month of Tabular Tuesday. We're launching four week-long tabular competitions, with each starting Tuesday 00:00 UTC. These will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!",
        "dataset_text": "The dataset for this competition (both train and test) was generated from a deep learning model trained on the Stroke Prediction Dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance."
    },
    {
        "name": "Binary Classification with a Tabular Employee Attrition Dataset",
        "url": "https://www.kaggle.com/competitions/playground-series-s3e3",
        "overview_text": "Overview text not found",
        "description_text": "Thank you to everyone who participated in and contributed to last year's Tabular Playground Series. And many thanks to all those who took the time to provide constructive feedback! We're thrilled that there continues to be interest in these types of challenges, and we're continuing the series this year but with a few changes. First, the series is getting upgraded branding. We've dropped \"Tabular\" from the name because, while we anticipate this series will still have plenty of tabular competitions, we'll also be having some other formats as well. You'll also notice freshly-upgraded (better looking and more fun!) banner and thumbnail images. Second, rather than naming the challenges by month and year, we're moving to a Season-Edition format. This year is Season 3, and each challenge will be a new Edition. We're doing this to have more flexibility. Competitions going forward won't necessarily align with each month like they did in previous years (although some might!), we'll have competitions with different time durations, and we may have multiple competitions running at the same time on occasion. Regardless of these changes, the goals of the Playground Series remain the same\u2014to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. We hope we continue to meet this objective! To start the year with some fun, January will be the month of Tabular Tuesday. We're launching four week-long tabular competitions, with each starting Tuesday 00:00 UTC. These will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!",
        "dataset_text": "The dataset for this competition (both train and test) was generated from a deep learning model trained on a Employee Attrition. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance."
    },
    {
        "name": "Binary Classification with a Tabular Credit Card Fraud Dataset",
        "url": "https://www.kaggle.com/competitions/playground-series-s3e4",
        "overview_text": "Overview text not found",
        "description_text": "Thank you to everyone who participated in and contributed to last year's Tabular Playground Series. And many thanks to all those who took the time to provide constructive feedback! We're thrilled that there continues to be interest in these types of challenges, and we're continuing the series this year but with a few changes. First, the series is getting upgraded branding. We've dropped \"Tabular\" from the name because, while we anticipate this series will still have plenty of tabular competitions, we'll also be having some other formats as well. You'll also notice freshly-upgraded (better looking and more fun!) banner and thumbnail images. Second, rather than naming the challenges by month and year, we're moving to a Season-Edition format. This year is Season 3, and each challenge will be a new Edition. We're doing this to have more flexibility. Competitions going forward won't necessarily align with each month like they did in previous years (although some might!), we'll have competitions with different time durations, and we may have multiple competitions running at the same time on occasion. Regardless of these changes, the goals of the Playground Series remain the same\u2014to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. We hope we continue to meet this objective! To start the year with some fun, January will be the month of Tabular Tuesday. We're launching four week-long tabular competitions, with each starting Tuesday 00:00 UTC. These will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!",
        "dataset_text": "The dataset for this competition (both train and test) was generated from a deep learning model trained on the Credit Card Fraud Detection. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. Note, this base dataset for this competition was much larger than previous Tabular Tuesdays datasets, and thus may contain more artifacts than the last three competitions."
    },
    {
        "name": "Ordinal Regression with a Tabular Wine Quality Dataset",
        "url": "https://www.kaggle.com/competitions/playground-series-s3e5",
        "overview_text": "Welcome to the 2023 Kaggle Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far!",
        "description_text": "Description text not found",
        "dataset_text": "The dataset for this competition (both train and test) was generated from a deep learning model trained on the Wine Quality dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance."
    },
    {
        "name": "Regression with a Tabular Paris Housing Price Dataset",
        "url": "https://www.kaggle.com/competitions/playground-series-s3e6",
        "overview_text": "Overview text not found",
        "description_text": "Thank you to everyone who participated in and contributed to last year's Tabular Playground Series. And many thanks to all those who took the time to provide constructive feedback! We're thrilled that there continues to be interest in these types of challenges, and we're continuing the series this year but with a few changes. First, the series is getting upgraded branding. We've dropped \"Tabular\" from the name because, while we anticipate this series will still have plenty of tabular competitions, we'll also be having some other formats as well. You'll also notice freshly-upgraded (better looking and more fun!) banner and thumbnail images. Second, rather than naming the challenges by month and year, we're moving to a Season-Edition format. This year is Season 3, and each challenge will be a new Edition. We're doing this to have more flexibility. Competitions going forward won't necessarily align with each month like they did in previous years (although some might!), we'll have competitions with different time durations, and we may have multiple competitions running at the same time on occasion. Regardless of these changes, the goals of the Playground Series remain the same\u2014to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. We hope we continue to meet this objective! With the great start and participation in January, we will continue launching the Tabular Tuesday in February every Tuesday 00:00 UTC, with each competition running for 2 weeks instead. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!",
        "dataset_text": "The dataset for this competition (both train and test) was generated from a deep learning model trained on the Paris Housing Price Prediction. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance."
    },
    {
        "name": "Binary Classification with a Tabular Reservation Cancellation Dataset",
        "url": "https://www.kaggle.com/competitions/playground-series-s3e7",
        "overview_text": "Overview text not found",
        "description_text": "Thank you to everyone who participated in and contributed to last year's Tabular Playground Series. And many thanks to all those who took the time to provide constructive feedback! We're thrilled that there continues to be interest in these types of challenges, and we're continuing the series this year but with a few changes. First, the series is getting upgraded branding. We've dropped \"Tabular\" from the name because, while we anticipate this series will still have plenty of tabular competitions, we'll also be having some other formats as well. You'll also notice freshly-upgraded (better looking and more fun!) banner and thumbnail images. Second, rather than naming the challenges by month and year, we're moving to a Season-Edition format. This year is Season 3, and each challenge will be a new Edition. We're doing this to have more flexibility. Competitions going forward won't necessarily align with each month like they did in previous years (although some might!), we'll have competitions with different time durations, and we may have multiple competitions running at the same time on occasion. Regardless of these changes, the goals of the Playground Series remain the same\u2014to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. We hope we continue to meet this objective! With the great start and participation in January, we will continue launching the Tabular Tuesday in February every Tuesday 00:00 UTC, with each competition running for 2 weeks instead. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!",
        "dataset_text": "The dataset for this competition (both train and test) was generated from a deep learning model trained on the Reservation Cancellation Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance."
    },
    {
        "name": "Regression with a Tabular Gemstone Price Dataset",
        "url": "https://www.kaggle.com/competitions/playground-series-s3e8",
        "overview_text": "Overview text not found",
        "description_text": "Thank you to everyone who participated in and contributed to last year's Tabular Playground Series. And many thanks to all those who took the time to provide constructive feedback! We're thrilled that there continues to be interest in these types of challenges, and we're continuing the series this year but with a few changes. First, the series is getting upgraded branding. We've dropped \"Tabular\" from the name because, while we anticipate this series will still have plenty of tabular competitions, we'll also be having some other formats as well. You'll also notice freshly-upgraded (better looking and more fun!) banner and thumbnail images. Second, rather than naming the challenges by month and year, we're moving to a Season-Edition format. This year is Season 3, and each challenge will be a new Edition. We're doing this to have more flexibility. Competitions going forward won't necessarily align with each month like they did in previous years (although some might!), we'll have competitions with different time durations, and we may have multiple competitions running at the same time on occasion. Regardless of these changes, the goals of the Playground Series remain the same\u2014to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. We hope we continue to meet this objective! With the great start and participation in January, we will continue launching the Tabular Tuesday in February every Tuesday 00:00 UTC, with each competition running for 2 weeks instead. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!",
        "dataset_text": "The dataset for this competition (both train and test) was generated from a deep learning model trained on the Gemstone Price Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance."
    },
    {
        "name": "Regression with a Tabular Concrete Strength Dataset",
        "url": "https://www.kaggle.com/competitions/playground-series-s3e9",
        "overview_text": "Overview text not found",
        "description_text": "Thank you to everyone who participated in and contributed to last year's Tabular Playground Series. And many thanks to all those who took the time to provide constructive feedback! We're thrilled that there continues to be interest in these types of challenges, and we're continuing the series this year but with a few changes. First, the series is getting upgraded branding. We've dropped \"Tabular\" from the name because, while we anticipate this series will still have plenty of tabular competitions, we'll also be having some other formats as well. You'll also notice freshly-upgraded (better looking and more fun!) banner and thumbnail images. Second, rather than naming the challenges by month and year, we're moving to a Season-Edition format. This year is Season 3, and each challenge will be a new Edition. We're doing this to have more flexibility. Competitions going forward won't necessarily align with each month like they did in previous years (although some might!), we'll have competitions with different time durations, and we may have multiple competitions running at the same time on occasion. Regardless of these changes, the goals of the Playground Series remain the same\u2014to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. We hope we continue to meet this objective! With the great start and participation in January, we will continue launching the Tabular Tuesday in February every Tuesday 00:00 UTC, with each competition running for 2 weeks instead. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!",
        "dataset_text": "The dataset for this competition (both train and test) was generated from a deep learning model trained on the Concrete Strength Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance."
    },
    {
        "name": "Binary Classification with a Tabular Pulsar Dataset",
        "url": "https://www.kaggle.com/competitions/playground-series-s3e10",
        "overview_text": "Overview text not found",
        "description_text": "Thank you to everyone who participated in and contributed to Season 3 Playground Series so far! With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in March every Tuesday 00:00 UTC, with each competition running for 2 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.. Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!",
        "dataset_text": "The dataset for this competition (both train and test) was generated from a deep learning model trained on the Pulsar Classification. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance."
    },
    {
        "name": "Regression with a Tabular Media Campaign Cost Dataset",
        "url": "https://www.kaggle.com/competitions/playground-series-s3e11",
        "overview_text": "Overview text not found",
        "description_text": "Thank you to everyone who participated in and contributed to Season 3 Playground Series so far! With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in March every Tuesday 00:00 UTC, with each competition running for 2 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.. Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!",
        "dataset_text": "The dataset for this competition (both train and test) was generated from a deep learning model trained on the Media Campaign Cost Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance."
    },
    {
        "name": "Binary Classification with a Tabular Kidney Stone Prediction Dataset",
        "url": "https://www.kaggle.com/competitions/playground-series-s3e12",
        "overview_text": "Overview text not found",
        "description_text": "You can now create your own synthetic versions of this dataset by forking and running this notebook. Thank you to everyone who participated in and contributed to Season 3 Playground Series so far! With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in April every Tuesday 00:00 UTC, with each competition running for 2 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.. Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!",
        "dataset_text": "NOTE: You can now create your own synthetic versions of this dataset by forking and running this notebook. The dataset for this competition (both train and test) was generated from a deep learning model trained on the Kidney Stone Prediction based on Urine Analysis dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance."
    },
    {
        "name": "Classification with a Tabular Vector Borne Disease Dataset",
        "url": "https://www.kaggle.com/competitions/playground-series-s3e13",
        "overview_text": "Overview text not found",
        "description_text": "Thank you to everyone who participated in and contributed to Season 3 Playground Series so far! With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in April every Tuesday 00:00 UTC, with each competition running for 2 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.. Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!",
        "dataset_text": "The dataset for this competition (both train and test) was generated from a deep learning model trained on the Vector Borne Disease Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. Note that in the original dataset some prognoses contain spaces, but in the competition dataset spaces have been replaced with underscores to work with the MPA@K metric."
    },
    {
        "name": "Regression with a Wild Blueberry Yield Dataset",
        "url": "https://www.kaggle.com/competitions/playground-series-s3e14",
        "overview_text": "Welcome to the 2023 Kaggle Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far!",
        "description_text": "Description text not found",
        "dataset_text": "NOTE: You can now create your own synthetic versions of this dataset by forking and running this notebook. The dataset for this competition (both train and test) was generated from a deep learning model trained on the Wild blueberry Yield Prediction Dataset. (Since this is Playground 3.14, it seems like we need a Blueberry Pie joke here?) Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance."
    },
    {
        "name": "Feature Imputation with a Heat Flux Dataset",
        "url": "https://www.kaggle.com/competitions/playground-series-s3e15",
        "overview_text": "Overview text not found",
        "description_text": "Thank you to everyone who participated in and contributed to Season 3 Playground Series so far! With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in May every Tuesday 00:00 UTC, with each competition running for 2 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.. Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!",
        "dataset_text": "The dataset for this competition (both train and test) was generated from a deep learning model trained on the Predicting Critical Heat Flux dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance."
    },
    {
        "name": "Regression with a Crab Age Dataset",
        "url": "https://www.kaggle.com/competitions/playground-series-s3e16",
        "overview_text": "Welcome to the 2023 Kaggle Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far!",
        "description_text": "Description text not found",
        "dataset_text": "The dataset for this competition (both train and test) was generated from a deep learning model trained on the Crab Age Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. Note: You can use this notebook to generate additional synthetic data for this competition if you would like."
    },
    {
        "name": "Binary Classification of Machine Failures",
        "url": "https://www.kaggle.com/competitions/playground-series-s3e17",
        "overview_text": "Overview text not found",
        "description_text": "Thank you to everyone who participated in and contributed to Season 3 Playground Series so far! With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in June every Tuesday 00:00 UTC, with each competition running for 2 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!",
        "dataset_text": "The dataset for this competition (both train and test) was generated from a deep learning model trained on the Machine Failure Predictions. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance."
    },
    {
        "name": "Explore Multi-Label Classification with an Enzyme Substrate Dataset",
        "url": "https://www.kaggle.com/competitions/playground-series-s3e18",
        "overview_text": "Overview text not found",
        "description_text": "Thank you to everyone who participated in and contributed to Season 3 Playground Series so far! With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in June every Tuesday 00:00 UTC, with each competition running for 2 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!",
        "dataset_text": "The dataset for this competition (both train and test) was generated from a deep learning model trained on a portion of the Multi-label Classification of enzyme substrates. This dataset only uses a subset of features from the original (the features that had the most signal). Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. Note: For this challenge, you are given 6 features in the training data, but only asked to predict the first two features (EC1 and EC2)."
    },
    {
        "name": "Forecasting Mini-Course Sales",
        "url": "https://www.kaggle.com/competitions/playground-series-s3e19",
        "overview_text": "Overview text not found",
        "description_text": "Thank you to everyone who participated in and contributed to Season 3 Playground Series so far! With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in July every Tuesday 00:00 UTC, with each competition running for 3 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!",
        "dataset_text": "For this challenge, you will be predicting a full year worth of sales for various fictitious learning modules from different fictitious Kaggle-branded stores in different (real!) countries. This dataset is completely synthetic, but contains many effects you see in real-world data, e.g., weekend and holiday effect, seasonality, etc. You are given the task of predicting sales during for year 2022. Good luck!"
    },
    {
        "name": "Predict CO2 Emissions in Rwanda",
        "url": "https://www.kaggle.com/competitions/playground-series-s3e20",
        "overview_text": "Overview text not found",
        "description_text": "Thank you to everyone who participated in and contributed to Season 3 Playground Series so far! With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in July every Tuesday 00:00 UTC, with each competition running for 3 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. This Episode is a similar to the Kaggle/Zindi Hackathon that was held at the Kaggle@ICLR 2023: ML Solutions in Africa workshop in Rwanda, and builds on an ongoing partnership between Kaggle and Zindi to build community-driven impact across Africa. Zindi is a professional network for data scientists to learn, grow their careers, and get jobs. If you haven't done so recently, stop by Zindi and see what they're up to!  The ability to accurately monitor carbon emissions is a critical step in the fight against climate change. Precise carbon readings allow researchers and governments to understand the sources and patterns of carbon mass output. While Europe and North America have extensive systems in place to monitor carbon emissions on the ground, there are few available in Africa. The objective of this challenge is to create a machine learning models using open-source CO2 emissions data from Sentinel-5P satellite observations to predict future carbon emissions. These solutions may help enable governments, and other actors to estimate carbon emission levels across Africa, even in places where on-the-ground monitoring is not possible. We acknowledge Carbon Monitor for the use of the GRACED dataset, and special thanks Darius Moruri from Zindi for his work in preparing the dataset and starter notebooks.",
        "dataset_text": "The objective of this challenge is to create machine learning models that use open-source emissions data (from Sentinel-5P satellite observations) to predict carbon emissions. Approximately 497 unique locations were selected from multiple areas in Rwanda, with a distribution around farm lands, cities and power plants. The data for this competition is split by time; the years 2019 - 2021 are included in the training data, and your task is to predict the CO2 emissions data for 2022 through November. Seven main features were extracted weekly from Sentinel-5P from January 2019 to November 2022. Each feature (Sulphur Dioxide, Carbon Monoxide, etc) contain sub features such as column_number_density which is the vertical column density at ground level, calculated using the DOAS technique. You can read more about each feature in the below links, including how they are measured and variable definitions. You are given the values of these features in the test set and your goal to predict CO2 emissions using time information as well as these features. Important: Please only use the data provided for this challenge as part of your modeling effort. Do not use any external data, including any data from Sentinel-5P not provided on this page."
    },
    {
        "name": "Improve a Fixed Model the Data-Centric Way!",
        "url": "https://www.kaggle.com/competitions/playground-series-s3e21",
        "overview_text": "Overview text not found",
        "description_text": "Thank you to everyone who participated in and contributed to Season 3 Playground Series so far! With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in August every Tuesday 00:00 UTC, with each competition running for 3 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!",
        "dataset_text": "This is a very different type of challenge! For this challenge, your task is to improve a dataset that is being used to train a random forest model; in other words, your submission will be training data, not predictions. A random forest model will be trained on your submission, used to make predictions, and then those predictions will be used to generate your score. The dataset for this competition is a synthetic dataset based off of the Dissolved oxygen prediction in river water dataset. You are free to use the original in any way that you find useful. Please see important information on the Evaluation tab about the model that will be trained on your submitted data. Good luck!"
    },
    {
        "name": "Predict Health Outcomes of Horses",
        "url": "https://www.kaggle.com/competitions/playground-series-s3e22",
        "overview_text": "Thank you to everyone who participated in and contributed to Season 3 Playground Series so far!",
        "description_text": "Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!",
        "dataset_text": "The dataset for this competition (both train and test) was generated from a deep learning model trained on a portion of the Horse Survival Dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. Good luck!"
    },
    {
        "name": "Binary Classification with a Software Defects Dataset",
        "url": "https://www.kaggle.com/competitions/playground-series-s3e23",
        "overview_text": "Thank you to everyone who participated in and contributed to Season 3 Playground Series so far!",
        "description_text": "Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!",
        "dataset_text": "The dataset for this competition (both train and test) was generated from a deep learning model trained on the Software Defect Dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance."
    },
    {
        "name": "Binary Prediction of Smoker Status using Bio-Signals",
        "url": "https://www.kaggle.com/competitions/playground-series-s3e24",
        "overview_text": "Welcome to the 2023 Kaggle Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far!",
        "description_text": "Description text not found",
        "dataset_text": "The dataset for this competition (both train and test) was generated from a deep learning model trained on the Smoker Status Prediction using Bio-Signals dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance."
    },
    {
        "name": "Regression with a Mohs Hardness Dataset",
        "url": "https://www.kaggle.com/competitions/playground-series-s3e25",
        "overview_text": "Welcome to the 2023 Kaggle Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far!",
        "description_text": "Description text not found",
        "dataset_text": "The dataset for this competition (both train and test) was generated from a deep learning model trained on the Prediction of Mohs Hardness with Machine Learning dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance."
    },
    {
        "name": "Multi-Class Prediction of Cirrhosis Outcomes",
        "url": "https://www.kaggle.com/competitions/playground-series-s3e26",
        "overview_text": "Welcome to the 2023 Kaggle Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far. This is our last episode for the Season 3 and we wish you all a Happy New Year! Stay tuned for the new season next year!",
        "description_text": "Description text not found",
        "dataset_text": "The dataset for this competition (both train and test) was generated from a deep learning model trained on the Cirrhosis Patient Survival Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance."
    },
    {
        "name": "Binary Classification with a Bank Churn Dataset",
        "url": "https://www.kaggle.com/competitions/playground-series-s4e1",
        "overview_text": "Welcome to the 2024 Kaggle Playground Series! Happy New Year! This is the 1st episode of Season 4. We plan to continue in the spirit of previous playgrounds, providing interesting and approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month.",
        "description_text": "Description text not found",
        "dataset_text": "The dataset for this competition (both train and test) was generated from a deep learning model trained on the Bank Customer Churn Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance."
    },
    {
        "name": "Multi-Class Prediction of Obesity Risk",
        "url": "https://www.kaggle.com/competitions/playground-series-s4e2",
        "overview_text": "Welcome to the 2024 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting and approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month.",
        "description_text": "Description text not found",
        "dataset_text": "The dataset for this competition (both train and test) was generated from a deep learning model trained on the Obesity or CVD risk dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. Note: This dataset is particularly well suited for visualizations, clustering, and general EDA. Show off your skills!"
    },
    {
        "name": "Steel Plate Defect Prediction",
        "url": "https://www.kaggle.com/competitions/playground-series-s4e3",
        "overview_text": "Welcome to the 2024 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting and approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month.",
        "description_text": "Description text not found",
        "dataset_text": "The dataset for this competition (both train and test) was generated from a deep learning model trained on the Steel Plates Faults dataset from UCI. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance."
    },
    {
        "name": "Regression with an Abalone Dataset",
        "url": "https://www.kaggle.com/competitions/playground-series-s4e4",
        "overview_text": "Welcome to the 2024 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting and approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month.",
        "description_text": "Description text not found",
        "dataset_text": "The dataset for this competition (both train and test) was generated from a deep learning model trained on the Abalone dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance."
    },
    {
        "name": "Regression with a Flood Prediction Dataset",
        "url": "https://www.kaggle.com/competitions/playground-series-s4e5",
        "overview_text": "Welcome to the 2024 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting and approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month.",
        "description_text": "Description text not found",
        "dataset_text": "The dataset for this competition (both train and test) was generated from a deep learning model trained on the Flood Prediction Factors dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. Note: This dataset is particularly well suited for visualizations, clustering, and general EDA. Show off your skills!"
    },
    {
        "name": "Classification with an Academic Success Dataset",
        "url": "https://www.kaggle.com/competitions/playground-series-s4e6",
        "overview_text": "Welcome to the 2024 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting and approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month.",
        "description_text": "Description text not found",
        "dataset_text": "The dataset for this competition (both train and test) was generated from a deep learning model trained on the Predict Students' Dropout and Academic Success dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. Please refer to the original dataset for feature feature explanations."
    },
    {
        "name": "Binary Classification of Insurance Cross Selling",
        "url": "https://www.kaggle.com/competitions/playground-series-s4e7",
        "overview_text": "Welcome to the 2024 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting and approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month.",
        "description_text": "Description text not found",
        "dataset_text": "The dataset for this competition (both train and test) was generated from a deep learning model trained on the Health Insurance Cross Sell Prediction Data dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. This notebook gives more details about the dataset used for this competition."
    },
    {
        "name": "Binary Prediction of Poisonous Mushrooms",
        "url": "https://www.kaggle.com/competitions/playground-series-s4e8",
        "overview_text": "Welcome to the 2024 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting and approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month.",
        "description_text": "Description text not found",
        "dataset_text": "The dataset for this competition (both train and test) was generated from a deep learning model trained on the UCI Mushroom dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. Note: Unlike many previous Tabular Playground datasets, data artifacts have not been cleaned up. There are categorical values in the dataset that are not found in the original. It is up to the competitors how to handle this."
    },
    {
        "name": "15.071x - The Analytics Edge (Spring 2015)",
        "url": "https://www.kaggle.com/competitions/15-071x-the-analytics-edge-spring-20152",
        "overview_text": "Overview text not found",
        "description_text": "IMPORTANT NOTE: This competition is only open to students of 15.071x - The Analytics Edge (https://www.edx.org/course/analytics-edge-mitx-15-071x) What makes online news articles popular? Newspapers and online news aggregators like Google News need to understand which news articles will be the most popular, so that they can prioritize the order in which stories appear. In this competition, you will predict the popularity of a set of New York Times blog articles from the time period September 2014-December 2014.  The following screenshot shows an example of the New York Times technology blog \"Bits\" homepage:  Many blog articles are published each day, and the New York Times has to decide which articles should be featured. In this competition, we challenge you to develop an analytics model that will help the New York Times understand the features of a blog post that make it popular.  To download the data and learn how this competition works, please be sure to read the \"Data\" page, as well as the \"Evaluation\" page, which can both be found in the panel on the left. This competition is brought to you by MITx and edX.",
        "dataset_text": "The data provided for this competition is split into two files: We have also provided a sample submission file, SampleSubmission.csv. This file gives an example of the format of submission files (see the Evaluation page for more information). The data for this competition comes from the New York Times website. The dependent variable in this problem is the variable Popular, which labels if an article had 25 or more comments in its online comment section (equal to 1 if it did, and 0 if it did not). The dependent variable is provided in the training data set, but not the testing dataset. This is an important difference from what you are used to - you will not be able to see how well your model does on the test set until you make a submission on Kaggle. The independent variables consist of 8 pieces of article data available at the time of publication, and a unique identifier:"
    },
    {
        "name": "15.071x - The Analytics Edge (Spring 2015)",
        "url": "https://www.kaggle.com/competitions/15-071x-the-analytics-edge-competition-spring-2015",
        "overview_text": "Overview text not found",
        "description_text": "IMPORTANT NOTE: This competition is only open to students of the MITx free, online course 15.071x - The Analytics Edge. What makes online news articles popular? Newspapers and online news aggregators like Google News need to understand which news articles will be the most popular, so that they can prioritize the order in which stories appear. In this competition, you will predict the popularity of a set of New York Times blog articles from the time period September 2014-December 2014. The following screenshot shows an example of the New York Times technology blog \"Bits\" homepage:  Many blog articles are published each day, and the New York Times has to decide which articles should be featured. In this competition, we challenge you to develop an analytics model that will help the New York Times understand the features of a blog post that make it popular. To download the data and learn how this competition works, please be sure to read the \"Data\" page, as well as the \"Evaluation\" page, which can both be found in the panel on the left. This competition is brought to you by MITx and edX.",
        "dataset_text": "The data provided for this competition is split into two files: We have also provided a sample submission file, SampleSubmission.csv. This file gives an example of the format of submission files (see the Evaluation page for more information). The data for this competition comes from the New York Times website. The dependent variable in this problem is the variable Popular, which labels if an article had 25 or more comments in its online comment section (equal to 1 if it did, and 0 if it did not). The dependent variable is provided in the training data set, but not the testing dataset. This is an important difference from what you are used to - you will not be able to see how well your model does on the test set until you make a submission on Kaggle. The independent variables consist of 8 pieces of article data available at the time of publication, and a unique identifier:"
    },
    {
        "name": "Invasive Species Monitoring",
        "url": "https://www.kaggle.com/competitions/invasive-species-monitoring",
        "overview_text": "Overview text not found",
        "description_text": "Tangles of kudzu overwhelm trees in Georgia while cane toads threaten habitats in over a dozen countries worldwide. These are just two invasive species of many which can have damaging effects on the environment, the economy, and even human health. Despite widespread impact, efforts to track the location and spread of invasive species are so costly that they\u2019re difficult to undertake at scale. Currently, ecosystem and plant distribution monitoring depends on expert knowledge. Trained scientists visit designated areas and take note of the species inhabiting them. Using such a highly qualified workforce is expensive, time inefficient, and insufficient since humans cannot cover large areas when sampling.  Because scientists cannot sample a large quantity of areas, some machine learning algorithms are used in order to predict the presence or absence of invasive species in areas that have not been sampled. The accuracy of this approach is far from optimal, but still contributes to approaches to solving ecological problems. In this playground competition, Kagglers are challenged to develop algorithms to more accurately identify whether images of forests and foliage contain invasive hydrangea or not. Techniques from computer vision alongside other current technologies like aerial imaging can make invasive species monitoring cheaper, faster, and more reliable. Data providers: Christian Requena Mesa, Thore Engel, Amrita Menon, Emma Bradley.",
        "dataset_text": "The data set contains pictures taken in a Brazilian national forest. In some of the pictures there is Hydrangea, a beautiful invasive species original of Asia. Based on the training pictures and the labels provided, the participant should predict the presence of the invasive species in the testing set of pictures."
    },
    {
        "name": "iNaturalist Challenge at FGVC 2017",
        "url": "https://www.kaggle.com/competitions/inaturalist-challenge-at-fgvc-2017",
        "overview_text": "Overview text not found",
        "description_text": "With so much diversity, accurately classifying animals and plants is a tough challenge. Check out the photos below. Alpaca or Llama? Donkey or mule? Roses or kale?  It\u2019s estimated that our planet contains several million species of plants and animals\u2013many that look really similar to each other. Because of this, a lot of species in the natural world are too hard to classify without an expert. As part of the FGVC4 workshop at CVPR 2017 we are conducting the iNat Challenge 2017 large scale species classification competition, sponsored by Google. It is estimated that the natural world contains several million species of plants and animals. Without expert knowledge, many of these species are extremely difficult to accurately classify due to their visual similarity. The goal of this competition is to push the state of the art in automatic image classification for real world data that features fine-grained categories, big class imbalances, and large numbers of classes. The iNat Challenge 2017 dataset contains 5,089 species, with a combined training and validation set of 675,000 images that have been collected and verified by multiple users from inaturalist.org. The dataset features many visually similar species, captured in a wide variety of situations, from all over the world.  Example images, along with their unique GBIF ID numbers (where available), can be viewed here. Teams with top submissions, at the discretion of the workshop organizers, will be invited to present their work at the FGVC4 workshop.  Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.",
        "dataset_text": "These data files can also be accessed via links here. All images have been saved in the JPEG format and have been resized to have a maximum dimension of 800 pixels.  We closely follow the annotation format of the COCO dataset. The annotations are stored in the JSON format and are organized as follows:"
    },
    {
        "name": "iMaterialist Challenge at FGVC 2017",
        "url": "https://www.kaggle.com/competitions/imaterialist-challenge-FGVC2017",
        "overview_text": "Overview text not found",
        "description_text": " As shoppers move online, it\u2019d be a dream come true to have product attributes in photos detected automatically. But, automatic product recognition is tough because for the same product, a picture can be taken in different lighting, angles, backgrounds, and levels of occlusion. Meanwhile different fine grained attribute labels may look very similar, for example, royal blue vs turquoise in color. Many of today\u2019s general-purpose recognition machines simply can\u2019t perceive such subtle differences between photos. Tackling issues like this is why the Conference on Computer Vision and Pattern Recognition (CVPR) has put together a workshop specifically for data scientists focused on fine-grained visual categorization called the FGVC4 workshop. As part of this workshop, CVPR is partnering with Google to challenge the data science community to help push the state of the art in automatic image classification. In this competition, FGVC workshop organizers and Google challenge you to develop algorithms that will help with the an important step towards automatic product detection\u2013accurately assigning attribute labels for product images. Individuals/Teams with top submissions will be invited to present their work live at the FGVC4 workshop. Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.",
        "dataset_text": "All the data described below are txt files in JSON format. Training Data The training dataset includes images from four apparel classes (outerwear, dresses, pants, shoes), with a varied number of attribute label for each image. It includes a total of 50,461 images and 381 attribute labels. The training data are splitted into train and validation sets. There are 8432 images in the validation set. Both sets have the same format as shown below: Note that for each image, we only provide URLs instead of the image content. Users need to download the images by themselves. We provide multiple URLs in case some of them are not available anymore. Multiple URLs for the same image id correspond to the same image content. Note that we only provide image urls, which may become unavailable over time. Therefore we suggest that the participants start downloading the images as early as possible. Dictionary for tasks and labels We also provide the dictionary of tasks and labels for participants to better understand the problem. The formats are: The task_name is in the format of \"class_attribute\", for example \"dress_color\" task would require participants to predict the color of the dress in the test image. Testing data and submissions The testing data only has images as shown below: Note that the test image format has one more field called \"task_id\". It lists the tasks to be evaluated for a given test image. Please note that due to the evaluation mechanism, participants are required to submit predictions for all tasks for a given image. The tasks listed in the test file are just provided to help the participants gain a better understanding of the evaluation results. We also provide a sample submission csv file as an example. The evaluation section has a more detailed description of the submission format."
    },
    {
        "name": "ImageNet Object Localization Challenge",
        "url": "https://www.kaggle.com/competitions/imagenet-object-localization-challenge",
        "overview_text": "Overview text not found",
        "description_text": "While It's pretty easy for people to identify subtle differences in photos, computers still have a ways to go. Visually similar items are tough for computers to count, like this overlapping bunch of bananas:  Or, consider this photo of a family of foxes camouflaged in the wild - where do the foxes end and where does the grass begin?  To solve this problem and enhance the state of the art in object detection and classification, the annual ImageNet Large Scale Visual Recognition Challenge (ILSVRC) began in 2010. Kaggle is excited and honored to be the new home of the official ImageNet Object Localization competition. Participants are challenged with identifying all objects within an image so those images can then be classified and annotated. Already, because of this competition, there\u2019s been a 4.2\u00d7 reduction in image classification error (from 28.2% to 6.7%) and a 1.7\u00d7 reduction in localization error (from 42.5% to 25.3%) between 2010 and 2014 alone. Can you improve the accuracy even further? The validation and test data will consist of 150,000 photographs, collected from Flickr and other search engines, hand labeled with the presence or absence of 1000 object categories. The 1000 object categories contain both internal nodes and leaf nodes of ImageNet, but do not overlap with each other. A random subset of 50,000 of the images with labels will be released as the training set along with a list of the 1000 categories. The remaining images will be used as the test set. The validation and test data for this competition are not contained in the ImageNet training data.",
        "dataset_text": "Dataset description not found"
    },
    {
        "name": "Dog Breed Identification",
        "url": "https://www.kaggle.com/competitions/dog-breed-identification",
        "overview_text": "Overview text not found",
        "description_text": "Who's a good dog? Who likes ear scratches? Well, it seems those fancy deep neural networks don't have all the answers. However, maybe they can answer that ubiquitous question we all ask when meeting a four-legged stranger: what kind of good pup is that? In this playground competition, you are provided a strictly canine subset of ImageNet in order to practice fine-grained image categorization. How well you can tell your Norfolk Terriers from your Norwich Terriers? With 120 breeds of dogs and a limited number training images per class, you might find the problem more, err, ruff than you anticipated.  We extend our gratitude to the creators of the Stanford Dogs Dataset for making this competition possible: Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Fei-Fei Li.",
        "dataset_text": "You are provided with a training set and a test set of images of dogs. Each image has a filename that is its unique id. The dataset comprises 120 breeds of dogs. The goal of the competition is to create a classifier capable of determining a dog's breed from a photo. The list of breeds is as follows:"
    },
    {
        "name": "iNaturalist Challenge at FGVC5",
        "url": "https://www.kaggle.com/competitions/inaturalist-2018",
        "overview_text": "Overview text not found",
        "description_text": " As part of the FGVC5 workshop at CVPR 2018 we are conducting the iNat Challenge 2018 large scale species classification competition. It is estimated that the natural world contains several million species of plants and animals. Without expert knowledge, many of these species are extremely difficult to accurately classify due to their visual similarity. The goal of this competition is to push the state of the art in automatic image classification for real world data that features a large number of fine-grained categories with high class imbalance. The iNat Challenge 2018 dataset contains over 8,000 species, with a combined training and validation set of 450,000 images that have been collected and verified by multiple users from iNaturalist. The dataset features many visually similar species, captured in a wide variety of situations, from all over the world. Teams with top submissions, at the discretion of the workshop organizers, will be invited to present their work at the FGVC5 workshop. The iNat Challenge 2018 is sponsored by Microsoft. ",
        "dataset_text": "Download the images and annotations here. All images have been saved in the JPEG format and have been resized to have a maximum dimension of 800 pixels. We follow the annotation format of the COCO dataset and add additional fields. The annotations are stored in the JSON format and are organized as follows:"
    },
    {
        "name": "Freesound General-Purpose Audio Tagging Challenge",
        "url": "https://www.kaggle.com/competitions/freesound-audio-tagging",
        "overview_text": "Overview text not found",
        "description_text": "Some sounds are distinct and instantly recognizable, like a baby\u2019s laugh or the strum of a guitar. Other sounds aren\u2019t clear and are difficult to pinpoint. If you close your eyes, can you tell which of the sounds below is a chainsaw versus a blender? Moreover, we often experience a mix of sounds that create an ambience \u2013 like the clamoring of construction, a hum of traffic from outside the door, blended with loud laughter from the room, and the ticking of the clock on your wall. The sound clip below is of a busy food court in the UK. Partly because of the vastness of sounds we experience, no reliable automatic general-purpose audio tagging systems exist. Currently, a lot of manual effort is required for tasks like annotating sound collections and providing captions for non-speech events in audiovisual content. To tackle this problem, Freesound (an initiative by MTG-UPF that maintains a collaborative database with over 370,000 Creative Commons Licensed sounds) and Google Research\u2019s Machine Perception Team (creators of AudioSet, a large-scale dataset of manually annotated audio events with over 500 classes) have teamed up to develop the dataset for this competition. You\u2019re challenged to build a general-purpose automatic audio tagging system using a dataset of audio files covering a wide range of real-world environments. Sounds in the dataset include things like musical instruments, human sounds, domestic sounds, and animals from Freesound\u2019s library, annotated using a vocabulary of more than 40 labels from Google\u2019s AudioSet ontology. To succeed in this competition your systems will need to be able to recognize an increased number of sound events of very diverse nature, and to leverage subsets of training data featuring annotations of varying reliability (see Data section for more information).",
        "dataset_text": "UPDATE: If you are interested in the FSDKaggle2018 dataset used for this competition, we recommend to download it from Zenodo. The Zenodo version contains all the files of dataset, without the addition of a set of padding clips in the test set that are not used for evaluation. In this way, you won't download these unnecessary files and the download will be faster. For this competition, the objective is to predict the audio classification label for the audio files found in the audio_test folder. If you participated to this task, or you use the FSDKaggle2018 dataset or baseline code, please cite our DCASE 2018 paper: Here are some other relevant characteristics of FSDKaggle2018: The data labeling process started from a manual mapping between Freesound tags and AudioSet Ontology categories (or labels), which was carried out by researchers at the Music Technology Group (Universitat Pompeu Fabra, Barcelona). Using this mapping, a number of Freesound audio samples were automatically annotated with labels from the AudioSet Ontology. These annotations can be understood as weak labels since they express the presence of a sound category in an audio sample.\nThen, a data validation process was carried out in which a number of participants did listen to the annotated sounds and manually assessed the presence/absence of an automatically assigned sound category, according to the AudioSet category description.\nAudio samples in FSDKaggle2018 are only annotated with a single ground truth label (see train.csv). A total of 3,710 annotations included in the train set of FSDKaggle2018 are annotations that have been manually validated as present and predominant (some with inter-annotator agreement but not all of them). This means that in most cases there is no additional acoustic material other than the labeled category. In few cases there may be some additional sound events, but these additional events won't belong to any of the 41 categories of FSDKaggle2018.\nThe rest of the annotations have not been manually validated and therefore some of them could be inaccurate. Nonetheless, we have estimated that at least 65-70% of the non-verified annotations per category in the train set are indeed correct. It can happen that some of these non-verified audio samples present several sound sources even though only one label is provided as ground truth. This additional sources are typically out of the set of the 41 categories, but in a few cases they could be within.\nMore details about the data labeling process can be found in [3]. [1] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal, and Marvin Ritter. \"Audio set: An ontology and human-labeled dartaset for audio events.\" Proceedings of the Acoustics, Speech and Signal Pro- cessing International Conference, 2017.\n[2] Frederic Font, Gerard Roma, and Xavier Serra. \"Freesound technical demo.\" Proceedings of the 21st ACM international conference on Multimedia, 2013. https://freesound.org\n[3] Eduardo Fonseca, Jordi Pons, Xavier Favory, Frederic Font, Dmitry Bogdanov, Andres Ferraro, Sergio Oramas, Alastair Porter, and Xavier Serra. \"Freesound Datasets: A Platform for the Creation of Open Audio Datasets.\" Proceedings of the International Conference on Music Information Retrieval, 2017. PDF here"
    },
    {
        "name": "iNaturalist 2019 at FGVC6",
        "url": "https://www.kaggle.com/competitions/inaturalist-2019-fgvc6",
        "overview_text": "Overview text not found",
        "description_text": " As part of the FGVC6 workshop at CVPR 2019 we are conducting the iNat Challenge 2019 large scale species classification competition, sponsored by Microsoft. It is estimated that the natural world contains several million species of plants and animals. Without expert knowledge, many of these species are extremely difficult to accurately classify due to their visual similarity. The goal of this competition is to push the state of the art in automatic image classification for real world data that features a large number of fine-grained categories. Previous versions of the challenge have focused on classifying large numbers of species. This year features a smaller number of highly similar categories captured in a wide variety of situations, from all over the world. In total, the iNat Challenge 2019 dataset contains 1,010 species, with a combined training and validation set of 268,243 images that have been collected and verified by multiple users from iNaturalist. Teams with top submissions, at the discretion of the workshop organizers, will be invited to present their work at the FGVC6 workshop. Participants who make a submission that beats the sample submission can fill out this form to receive $150 in Google Cloud credits.  Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.",
        "dataset_text": "All images have been saved in the JPEG format and have been resized to have a maximum dimension of 800 pixels. We follow the annotation format of the [COCO dataset][2] and add additional fields. The annotations are stored in the [JSON format][3] and are organized as follows:"
    },
    {
        "name": "Tabular Playground Series - Feb 2021",
        "url": "https://www.kaggle.com/competitions/tabular-playground-series-feb-2021",
        "overview_text": "Overview text not found",
        "description_text": "Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation. The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard. For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the amount of an insurance claim. Although the features are anonymized, they have properties relating to real-world features. Good luck and have fun! Check out this Starter Notebook which walks you through how to make your very first submission! For more ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.",
        "dataset_text": "For this competition, you will be predicting a continuous target based on a number of feature columns given in the data. All of the feature columns, cat0 - cat9 are categorical, and the feature columns cont0 - cont13 are continuous."
    },
    {
        "name": "Plant Pathology 2021 - FGVC8",
        "url": "https://www.kaggle.com/competitions/plant-pathology-2021-fgvc8",
        "overview_text": "Overview text not found",
        "description_text": "Apples are one of the most important temperate fruit crops in the world. Foliar (leaf) diseases pose a major threat to the overall productivity and quality of apple orchards. The current process for disease diagnosis in apple orchards is based on manual scouting by humans, which is time-consuming and expensive. Although computer vision-based models have shown promise for plant disease identification, there are some limitations that need to be addressed. Large variations in visual symptoms of a single disease across different apple cultivars, or new varieties that originated under cultivation, are major challenges for computer vision-based disease identification. These variations arise from differences in natural and image capturing environments, for example, leaf color and leaf morphology, the age of infected tissues, non-uniform image background, and different light illumination during imaging etc. Plant Pathology 2020-FGVC7 challenge competition had a pilot dataset of 3,651 RGB images of foliar disease of apples. For Plant Pathology 2021-FGVC8, we have significantly increased the number of foliar disease images and added additional disease categories. This year\u2019s dataset contains approximately 23,000 high-quality RGB images of apple foliar diseases, including a large expert-annotated disease dataset. This dataset reflects real field scenarios by representing non-homogeneous backgrounds of leaf images taken at different maturity stages and at different times of day under different focal camera settings. The main objective of the competition is to develop machine learning-based models to accurately classify a given leaf image from the test dataset to a particular disease category, and to identify an individual disease from multiple disease symptoms on a single leaf image.   Details and background information on the dataset and Kaggle competition \u2018Plant Pathology 2020 Challenge\u2019 were published as a peer-reviewed research article. If you use the dataset for your project, please cite the following Thapa, Ranjita; Zhang, Kai; Snavely, Noah; Belongie, Serge; Khan, Awais. The Plant Pathology Challenge 2020 data set to classify foliar disease of apples. Applications in Plant Sciences, 8 (9), 2020. We acknowledge sponsorship from Cornell Initiative for Digital Agriculture (CIDA). ",
        "dataset_text": "Can you help detect farmers detect apple diseases? This competition builds on last year's by challenging you to handle additional diseases and to provide more detailed information about leaves that have multiple infections. train.csv - the training set metadata. sample_submission.csv - A sample submission file in the correct format. train_images - The training set images. test_images - The test set images. This competition has a hidden test set: only three images are provided here as samples while the remaining 5,000 images will be available to your notebook once it is submitted."
    },
    {
        "name": "Hotel-ID to Combat Human Trafficking 2021 - FGVC8",
        "url": "https://www.kaggle.com/competitions/hotel-id-2021-fgvc8",
        "overview_text": "Overview text not found",
        "description_text": "Victims of human trafficking are often photographed in hotel rooms as in the below examples. Identifying these hotels is vital to these trafficking investigations but poses particular challenges due to low quality of images and uncommon camera angles.  Even without victims in the images, hotel identification in general is a challenging fine-grained visual recognition task with a huge number of classes and potentially high intraclass and low interclass variation. In order to support research into this challenging task and create image search tools for human trafficking investigators, we created the TraffickCam mobile application, which allows every day travelers to submit photos of their hotel room. Read more about TraffickCam on TechCrunch. Example images from one hotel in the TraffickCam dataset are shown below:  In this contest, competitors are tasked with identifying the hotel seen in test images from the TraffickCam dataset, which are based on a large gallery of training images with known hotel IDs. Our team currently supports an image search system used at the National Center for Missing and Exploited Children in human trafficking investigations. Novel and interesting approaches have the potential to be incorporated in this search system. ",
        "dataset_text": "Identifying the location of a hotel room is a challenging problem of great interest for combating human trafficking. This competition provides a rich dataset of photos of hotel room interiors, without any people present, for this purpose. Many of the hotels are independent or part of very small chains, where shared decor isn't a concern. However, the shared standards for their interior decoration for the larger chains means that many hotels can look quite similar at first glance. Identifying the chain can narrow the range of possibilities, but only down to a set that is much harder to tell apart and is still scattered across a wide geographic area.\nThe real value lies in getting the number of candidates to a small enough number that a human investigator could follow up on all of them. train.csv - The training set metadata. sample_submission.csv - A sample submission file in the correct format. train_images - The training set contains 97000+ images from around 7700 hotels from across the globe. All of the images for each hotel chain are in a dedicated subfolder for that chain. test_images - The test set images. This competition has a hidden test set: only three images are provided here as samples while the remaining 13,000 images will be available to your notebook once it is submitted."
    },
    {
        "name": "GeoLifeCLEF 2022 - LifeCLEF 2022 x FGVC9",
        "url": "https://www.kaggle.com/competitions/geolifeclef-2022-lifeclef-2022-fgvc9",
        "overview_text": "Overview text not found",
        "description_text": "The aim of this competition is to predict the localization of plant and animal species.\nTo do so, 1.6M geo-localized observations from France and the US of 17K species are provided (9K plant species and 8K animal species).\nThese observations are paired with aerial images and environmental features around them (as illustrated above).\nThe goal is, for each GPS position in the test set (for which we provide the associated aerial images and environmental features), to return a set of candidate species that should contain the true observed species. Automatic prediction of the list of species most likely to be observed at a given location is useful for many scenarios related to biodiversity management and conservation.\nFirst, this would allow to improve species identification tools - automatic, semi-automatic, or based on traditional field guides - by reducing the list of candidate species observable at a given site.\nMore generally, it could facilitate biodiversity inventories through the development of location-based recommendation services (e.g. on mobile phones), encourage the involvement of citizen scientist observers, and accelerate the annotation and validation of species observations to produce large, high-quality data sets.\nFinally, this could be used for educational purposes through biodiversity discovery applications with features such as contextualized educational pathways. This competition is held jointly as part of:  Being part of scientific research, the participants are encouraged to participate to both event.\nIn particular, only participants who submitted a working note paper to LifeCLEF (see below) will be part of the officially published ranking used for scientific communication. This competition is part of the Fine-Grained Visual Categorization FGVC9 workshop at the Computer Vision and Pattern Recognition Conference CVPR 2022.\nA panel will review the top submissions for the competition based on the description of the methods provided.\nFrom this, a subset may be invited to present their results at the workshop.\nAttending the workshop is not required to participate in the competition; however, only teams that are attending the workshop will be considered to present their work. CVPR 2022 will take place in New Orleans, USA, 19-24 June 2022.\nPLEASE NOTE: CVPR frequently sells out early, we cannot guarantee CVPR registration after the competition's end.\nIf you are interested in attending, please plan ahead. You can see a list of all of the FGVC9 competitions here. LifeCLEF lab is part of the Conference and Labs of the Evaluation Forum (CLEF).\nCLEF consists of independent peer-reviewed workshops on a broad range of challenges in the fields of multilingual and multimodal information access evaluation, and a set of benchmarking activities carried in various labs designed to test different aspects of mono and cross-language Information retrieval systems.\nCLEF 2022 will be hosted by the Universit\u00e0 di Bologna, Italy, 5-8 September 2022.\nMore details can be found on the CLEF 2022 website. To participate to the LifeCLEF lab, participants must register using this form (and checking \"Task 3 - GeoLifeCLEF\" of \"LifeCLEF\" section).\nThis registration is free of charge and will close on 22 April 2022, however free of charge late registration will still be possible at the end of the competition.\nThis will allow those participants to submit, at the end of the competition, a working note paper to LifeCLEF which will be peer-reviewed and published in CEUR-WS proceedings.\nThis paper should provide sufficient information to reproduce the final submitted runs. Submitting a working note with the full description of the methods used in each run is mandatory.\nAny run that could not be reproduced thanks to its description in the working notes might be removed from the official publication of the results.\nWorking notes are published within CEUR-WS proceedings, resulting in an assignment of an individual DOI (URN) and an indexing by many bibliography systems including DBLP.\nAccording to the CEUR-WS policies, a light review of the working notes will be conducted by LifeCLEF organizing committee to ensure quality.\nAs an illustration, LifeCLEF 2021 working notes (task overviews and participant working notes) can be found within CLEF 2021 CEUR-WS proceedings.",
        "dataset_text": "This year's challenge, GeoLifeCLEF 2022, uses a cleaned-up version of the data used for last year's challenge, GeoLifeCLEF 2021.\nA changelog is available at the end of this page. The challenge relies on a collection of 1.6 million of geo-localized observations of plants and animals in the US and France, coming from the iNaturalist and Pl@ntNet citizen science platforms.\nThere are 17K species in the dataset, 9K plant species and 8K animal species.\nEach observation consists of a species name with the GPS coordinates where it was observed.\nIn addition, observations are paired with a set of covariates characterizing the landscape and environment around them.\nCovariates include high-resolution remote sensing imagery, land cover data, and altitude, as well as traditional low-resolution climate and soil variables. A walkthrough notebook describing the data structure and how to load it and visualize it is available in the Code tab.\nA complete description of how the original GeoLifeCLEF dataset was built can be found in the associated paper.\nData loading and visualization utilities are provided on the GitHub of the competition.      Here is an example of the patches corresponding to observation 10171444 on the Pic Saint-Loup mountain, France: In more detail, each observation is paired with the following covariates:  Changelog from GeoLifeCLEF 2021 dataset:"
    },
    {
        "name": "Herbarium 2022 - FGVC9",
        "url": "https://www.kaggle.com/competitions/herbarium-2022-fgvc9",
        "overview_text": "Overview text not found",
        "description_text": "(opens in a new tab)\"> The Herbarium 2022: Flora of North America is a part of a project of the New York Botanical Garden funded by the National Science Foundation to build tools to identify novel plant species around the world. The dataset strives to represent all known vascular plant taxa in North America, using images gathered from 60 different botanical institutions around the world. In botany, a \u2018flora\u2019 is a complete account of the plants found in a geographic region. The dichotomous keys and detailed descriptions of diagnostic morphological features contained within a flora are used by botanists to determine which names to apply to plant specimens. This year's competition dataset aims to encapsulate the flora of North America so that we can test the capability of artificial intelligence to replicate this traditional tool \u2014a crucial first step to harnessing AI\u2019s potential botanical applications. The Herbarium 2022: Flora of North America dataset comprises 1.05 M images of 15,501 vascular plants, which constitute more than 90% of the taxa documented in North America. Our dataset is constrained to include only vascular land plants (lycophytes, ferns, gymnosperms, and flowering plants). Our dataset has a long-tail distribution. The number of images per taxon is as few as seven and as many as 100 images. Although more images are available, we capped the maximum number in an attempt to ensure sufficient but manageable training data size for competition participants. This is an FGVC competition hosted as part of the FGVC9 workshop at CVPR 2022 and sponsored by NYBG. Details of this competition are mirrored on the github page. Please post in the forum or open an issue if you have any questions or problems with the dataset. The images are provided by the New York Botanical Garden and 59 other institutions around the world.",
        "dataset_text": "The training and test sets contain images of herbarium specimens from 15,501 species of vascular plants. Each image contains exactly one specimen. The text labels on the specimen images have been blurred to remove category information in the image. The data has been approximately split 80%/20% for training/test. Each category has at least 1 instance in both the training and test datasets. Note that the test set distribution is slightly different from the training set distribution. The training set has a number of examples representing species capped at a maximum of 80. Each image has different image dimensions, with a maximum of 1000 pixels in the larger dimension. These have been resized from the original image resolution. All images are in JPEG format. In addition to the images, we also include a hierarchical taxonomic structure of category_id. The categories in the training_metadata.json contain three levels of hierarchical structure, family - genus - species, from the highest rank to the lowest rank. One can think about this as a directed graph, where families are the root nodes and the species are the leaf nodes. Please note that species is only unique under its parent node genus, which means that we can find multiple categories with the same species name under different genus names. This is due to the taxonomic nature that plants are named after, and the genus-species pair is always unique in our data. What makes this year's data unique is that we include a set of pairwise phylogenetic distances among genera, so that one could test if the difference in morphological features of plant taxa well correspond to their taxonomic distances. You can find the distance data with key name distances from train_metadata.json. This dataset uses the COCO dataset format with additional annotation fields. In addition to the species category labels, we also provide supercategory information. The training set metadata (train_metadata.json) and test set metadata (test_metadata.json) are JSON files in the format below. Naturally, the test set metadata file omits the annotations, categories, and other elements. The training set images are organized in subfolders h22-train/images/<subfolder1>/<subfolder2>/<image_id>.jpg, where <subfolder1> and <subfolder2> comes from the first three and the last two digits of the image_id. Image_id is a result of combination between <category_id> and unique numbers that differentiates images within plant taxa. Please be mindful that category_ids are unique, but not complete. Taxa are originally numbered from 1 to 15505, but the competition data has 15501 taxa because we lost four taxa during data cleaning process. The test set images are organized in subfolders test/images/<subfolder>/<image id>.jpg, where <subfolder> corresponds to the integer division of the image_id by 1000. For example, a test image with and image_id of 8005, can be found at h22-test/images/008/test-008005.jpg"
    },
    {
        "name": "iWildCam 2022 - FGVC9",
        "url": "https://www.kaggle.com/competitions/iwildcam2022-fgvc9",
        "overview_text": "Overview text not found",
        "description_text": "Camera Traps enable the automatic collection of large quantities of image data. Ecologists all over the world use camera traps to monitor biodiversity and population density of animal species. In order to estimate the abundance (how many there are) and population density of species in camera trap data, ecologists need to know not just which species were seen, but also how many of each species were seen. However, because images are taken in motion-triggered bursts to increase the likelihood of capturing the animal(s) of interest, object detection alone is not sufficient as it could lead to over- or under-counting. For example, if you get 3 images taken at one frame per second, and in the first you see 3 gazelles, in the second you see 5 gazelles, and in the last you see 4 gazelles, how many total gazelles have you seen? This is more challenging than strictly detecting and categorizing species, as it requires reasoning and tracking of individuals across sparse temporal samples.  This year our iWildCam competition will focus entirely on counting animals. We have prepared a challenge where the training data and test data are from different cameras spread across the globe. The set of species seen in each camera overlap, but are not identical. The challenge is to count individual animals across sequences in the test cameras. To explore multimodal solutions, we allow competitors to train on the following data: Check the Data section for a more comprehensive description of all these resources and for accessing the train set, test set and metadata. These are mirrored on the competition's GitHub page as well, where we also provide the multispectral data, a taxonomy file mapping our classes into the iNaturalist taxonomy, a subset of the iNaturalist data mapped into our class set, a camera trap detection model (the MegaDetector) along with the corresponding detections, and a class-agnostic instance segmentation model (DeepMAC) along with the segmentation masks for the MegaDetector's bounding boxes. This competition is part of the FGVC9 workshop at CVPR 2022 and is sponsored by Wildlife Insights. Data is primarily provided by the Wildlife Conservation Society (WCS) and iNaturalist, and is hosted on Azure by Microsoft AI for Earth. Count annotations were generously provided by Centaur Labs. Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.",
        "dataset_text": "The iWildCam 2022 WCS training set contains 201,399 images from 323 locations, and the WCS test set contains 60,029 images from 91 locations. These 414 locations are spread across the globe. A location ID (location) is given for each image, and in some special cases where two cameras were set up by ecologists at the same location, we have provided a sub_location identifier. Camera traps operate with a motion trigger and, after motion is detected, the camera will take a sequence of photos (from 1 to 10 images depending on the camera). We provide a seq_id for each sequence, and your task is to count the number of individuals across each test sequence. This year we are also providing count annotations on 1780 of the 36,292 train sequences (check the metadata/train_sequence_counts.csv file). We hope you will find them useful in building better models. We do not provide any count annotations for the test set. We provide GPS locations for the majority of the camera traps, obfuscated within 1km for security and privacy reasons. Some of the obfuscated GPS locations (all from one country) were not released at the request of WCS, but knowing that the locations not listed in the metadata/gps_locations.json file are all from the same country should help competitors narrow down the set of possible species for those locations based on what is seen in the training data. You may also choose to use supplemental training data from the iNaturalist 2017, iNaturalist 2018, iNaturalist 2019, and iNaturalist 2021 competition datasets. As a courtesy, we have curated all the images from iNaturalist 2017-2018 datasets containing classes that might be in the test set, and mapped them into the iWildCam categories. We provide Landsat-8 multispectral imagery for each camera location as supplementary data. In particular, each site is associated with a series of patches collected between 2013 and 2019. The patches are extracted from a \"Tier 1\" Landsat product, which consists only of data that meets certain geometric and radiometric quality standards. Consequently, the number of patches per site varies from 39 to 406 (median: 147). Each patch is 200x200x9 pixels, covering an area of 6km^2 at a resolution of 30 meters / pixel across 9 spectral bands. Note that all patches for a given site are registered, but are not centered exactly at the camera location to protect the integrity of the site. The metadata we provide follows the COCO CameraTraps annotation format, with additional fields. Each training image has at least one associated annotation, containing a category_id that maps the annotation to its corresponding category label. We allow the use of the Microsoft AI for Earth MegaDetector (described in this paper), a general and robust camera trap detection model which competitors are free to use as they see fit. MegaDetector v3 detects animal and person classes, while the MegaDetector v4 adds a vehicle class. Any version of the MegaDetector is allowed to be used in this competition. The models can be downloaded from here. Sample code for running the MegaDetector over a folder of images can be found here. We have run MegaDetector v4 over the WCS dataset, and we are providing the top bounding boxes and associated confidences along with the metadata. We are also providing a general weakly-supervised segmentation model which competitors are free to use as they see fit. We have run the segmentation model over the WCS dataset using the bounding boxes from the MegaDetector v4, and provide the segmentation for each box. The segmentations come from DeepMAC, which provides class-agnostic instance segmentation masks and achieves state-of-the-art performance on partially supervised instance segmentation tasks. Below, we show a sample visualization of instance masks on WCS.  We provide an instance mask for each detected object by MegaDetector (detected objects are stored in metadata/iwildcam2022_mdv4_detections.json). For each image in the train or test directory with name <ID>.jpg, if there are any objects detected in the image, its corresponding instance masks will be stored in instance_masks/<ID>.png. The instance mask details are stored in a single channel PNG image. The pixels in the PNG image are 1-indexed and indicate which detection they belong to (0 is reserved for background). The indices follow the same order as the detections in MegaDetector's output (addressed by ['images']['detections']). When there are overlapping instances, we only preserve the ID of the instance with the higher detection confidence ('conf' field). Data can be downloaded from Kaggle's Data section and from the competition's GitHub page."
    },
    {
        "name": "Sorghum -100 Cultivar Identification - FGVC 9",
        "url": "https://www.kaggle.com/competitions/sorghum-id-fgvc-9",
        "overview_text": "Overview text not found",
        "description_text": "The Sorghum-100 dataset is a curated subset of the RGB imagery captured during the TERRA-REF experiments, labeled by cultivar. This data could be used to develop and assess a variety of plant phenotyping models which seek to answer questions relating to the presence or absence of desirable traits (e.g., \"does this plant exhibit signs of water stress?''). In this contest, we focus on the question: \"What cultivar is shown in this image?''  Predicting the cultivar in an image is an especially good challenge problem for familiarizing the machine learning community with the TERRA-REF data. At first blush, the task of predicting the cultivar from an image of a plant may not seem to be the most biologically compelling question to answer -- in the context of plant breeding, the cultivar, or parental lines are typically known. A high accuracy machine learning predictor of the species captured by the sensor data, however, can be used to determine where errors in the planting process may have occurred. For example, seed may be mislabeled prior to planting, or planters may get jammed, depositing seeds non-uniformly in a field. Both types of errors are surprisingly common and can cause major problems when processing data from large-scale field experiments with hundreds of cultivars and complex field planting layouts. The Sorghum-100 dataset consists of 48,106 images and 100 different sorghum cultivars grown in June of 2017 (the images come from the middle of the growing season when the plants were quite large but not yet lodging -- or falling over). Each image is taken using an RGB spectral camera taken from a vertical view of the sorghum plants in the TERRA-REF field in Arizona.",
        "dataset_text": " The Sorghum-100 dataset consists of 48,106 images and 100 different sorghum cultivars grown in June of 2017 (the images come from the middle of the growing season when the plants were quite large but not yet lodging -- or falling over). In the above image, we show a sample of images from four different cultivars. Each row includes six images from different dates in June. This figure highlights the high inter-class visual similarity between the different classes, as well as the high variability in the imaging conditions from one day to the next, or even over the course of a day. The dataset is divided into a training dataset and a testing dataset. Each cultivar was grown in two separate plots in the TERRA-REF field to account for extremely local field or soil conditions that might impact the growth of plants in one particular plot. We leverage this natural split in the data when dividing our dataset between train and test -- images for a given cultivar in the training dataset come from one plot, while the test images from that same cultivar come from the other plot. This means that a model cannot achieve high performance by memorizing features that aren't meaningful phenotypes (e.g., by memorizing patterns observed in the dirt)."
    },
    {
        "name": "Hotel-ID to Combat Human Trafficking 2022 - FGVC9",
        "url": "https://www.kaggle.com/competitions/hotel-id-to-combat-human-trafficking-2022-fgvc9",
        "overview_text": "Overview text not found",
        "description_text": "Victims of human trafficking are often photographed in hotel rooms as in the below examples. Identifying these hotels is vital to these trafficking investigations but poses particular challenges due to low quality of images and uncommon camera angles.  Even without victims in the images, hotel identification in general is a challenging fine-grained visual recognition task with a huge number of classes and potentially high intraclass and low interclass variation. In order to support research into this challenging task and create image search tools for human trafficking investigators, we created the TraffickCam mobile application, which allows every day travelers to submit photos of their hotel room. Read more about TraffickCam on TechCrunch. Example images from one hotel in the TraffickCam dataset are shown below:  In this contest, competitors are tasked with identifying the hotel seen in test images from the TraffickCam dataset, which are based on a large gallery of training images with known hotel IDs. Our team currently supports an image search system used at the National Center for Missing and Exploited Children in human trafficking investigations. Novel and interesting approaches have the potential to be incorporated in this search system. ",
        "dataset_text": "Identifying the location of a hotel room is a challenging problem of great interest for combating human trafficking. This competition provides a rich dataset of photos of hotel room interiors for this purpose. Unlike last year's competition the test set images in this competition are all partially masked to emulate the added challenge of having a person in the blocking part of the view of the room. Models that can overcome this extra layer of difficulty should be even more useful in practice. sample_submission.csv - A sample submission file in the correct format. train_images - The training set contains many images from hotels from across the globe. All of the images for each hotel are in a dedicated subfolder for that chain. These images do not have any occlusions by default. train_masks - Occlusions like the ones that will be present in the images in the test set. test_images - The test set images. This competition has a hidden test set: only one image is provided here as a sample while the remaining 5,000 images will be available to your notebook once it is submitted."
    },
    {
        "name": "FathomNet 2023",
        "url": "https://www.kaggle.com/competitions/fathomnet-out-of-sample-detection",
        "overview_text": "Overview text not found",
        "description_text": "Ocean-going camera systems have given scientists access to amazing data products that allow them to monitor populations and discover new animals. Many groups have had success training and deploying machine learning models to help sort incoming visual data. But these models typically do not generalize well to new situations -- different cameras or illumination, new organisms appearing, changes in the appearance of the seafloor -- an especially vexing problem in a dynamic ocean. Improving the robustness of these tools will allow marine ecologists to better leverage existing data and provide engineers with the ability to deploy instruments in ever more remote parts of the ocean. For this competition, we have selected data from the broader FathomNet annotated image set that represents a challenging use-case: the training set is collected in the upper ocean (< 800 m) while the target data is collected from deeper waters. This is a common scenario in ocean research: deeper waters are more difficult to access and typically more annotated data is available closer to the surface. The species distributions are often overlapping, but not identical, and diverge as the vertical distance between the samples increases. The challenge is both to identify animals in a target image and assess if the image is from a different distribution relative to the training data. Such out-of-sample detection could help scientists discover new animals and improve ecosystem management practices.  The images for both the training and target data were collected by a single camera system deployed by the Monterey Bay Aquarium Research Institute (MBARI) on several of its Remotely Operated Vehicles off the coast of Central California. We encourage participants to leverage outside data sources as they see fit. Teams should plan on specifying additional data sources and/or pretrained models used when uploading results. Pretrained models can be used for initialization of training (e.g. ImageNet or COCO classification or detection models provided by many deep learning packages). This competition is presented as part of the 10th Fine-Grained Visual Categorization (FGVC10) workshp at CVPR 2023 in Vancouver, Canada. Please feel free to open an issue on the competition git repo if you have a question, comment, or problem. We will also respond to threads opened on the competition discussion board here. The images for this competition has been generously provided by MBARI. Annotations were produced by the experts in the MBARI Video Lab.",
        "dataset_text": "The training and test images for the competition were all collected in the Monterey Bay Area between the surface and 1300 meters depth. The images contain bounding box annotations of 290 categories of bottom dwelling animals. The training and evaluation data are split across an 800 meter depth threshold: all training data is collected from 0-800 meters, evaluation data comes from the whole 0-1300 meter range. Since an organisms' habitat range is partially a function of depth, the species distributions in the two regions are overlapping but not identical. Test images are drawn from the same region but may come from above or below the depth horizon. The competition goal is to label the animals present in a given image (i.e. multi-label classification) and determine whether the image is out-of-sample. The training dataset are provide in two different formats: multi-label classification and object detection. The different formats live in the corresponding named directories. The datasets in these directories are identical aside from how they are organized. Each line of the csv files indicates an image by its id and a list of corresponding categories present in the frame. The ids correspond those in the object detection files. The categories are the set of all unique annotations found in the associated image. The datasets are formatted to adhere to the COCO Object Detection standard. Every training image contains at least one annotation corresponding to a category_id ranging from 1 to 290 and a supercategory from 1 to 20. The fine-grained annotations are taxonomic thought not always at the same level of the taxonomic tree. Each category also belongs to one of 20 semantic supercategories as indicated in category_key.csv: These supercategories might be useful for certain training procedures. The supercategories are represented in both the training and validation set. But please note that submissions must be made identifying the 290 fine grained categories. We are not able to provide images as a single downloadable archive due to FathomNet's Terms of Use. Images should be downloaded using the indicated coco_url field in each of the annotation files. Participants can either write their own script or use the provided download_images.py python script. By downloading and using this dataset you are agreeing to FathomNet's data use policy. In particular: For more details please see the full FathomNet Use Policy. Images are made available for download by the unique URLs in the COCO formatted object detection annotation files. The download script can be run from the command line. To install the requirements, create a virtual environment running python 3.9: To download the images, run the script from the command line: If no --output directory is specified, the script by default downloads images to the directory the command is executed from."
    },
    {
        "name": "Lux AI Season 2 - NeurIPS Stage 2",
        "url": "https://www.kaggle.com/competitions/lux-ai-season-2-neurips-stage-2",
        "overview_text": "Overview text not found",
        "description_text": "As the sun set on the world an array of lights dotted the once dark horizon. With the help of a brigade of toads, Lux had made it past the terrors in the night to see the dawn of a new age. Seeking new challenges, plans were made to send a forward force with one mission: terraform Mars! Welcome to the Lux AI Challenge Season 2 - NeurIPS Edition (Stage 2)!  The Lux AI Challenge is a competition where competitors design agents to tackle a multi-variable optimization, resource gathering, and allocation problem in a 1v1 scenario against other competitors. In addition to optimization, successful agents must be capable of analyzing their opponents and developing appropriate policies to get the upper hand. This competition is an official NeurIPS 2023 competition and its results will be presented at the NeurIPS 2023 conference and additionally be submitted as a benchmark paper to NeurIPS 2024. In Season 2 NeurIPS Edition, we are providing a platform to research large scale decision-making, benchmarking reinforcement learning, imitation learning etc. at scale. There are 2 modes of scale through which our competition is actively supporting. The first is the environment itself, it is a 64x64 map with hundreds to thousands of controllable units per team with complex interactions and rules, meaning any successful agent must be capable of processing and making optimal decisions for a large number of units. The map is procedurally generated meaning a strong agent must be able to adapt their strategy according to the map, in addition to the opponent. Moreover, we open source a GPU parallelized version of the environment powered by Jax, allowing you to easily parallel run thousands of environments on a single GPU for an over 100x speedup compared to running on a CPU. Massive thanks to our team and collaborators at Parametrix.ai for their contributions! Stage 2 of the competition is a scaled up version of the Season 2 environment with now larger 64x64 sized maps, which previously ran as a competition on Kaggle where 600+ teams competed, resulting in extremely diverse rule-based strategies at the top along with RL and IL solutions in the top 10. All code can be found at our Github, make sure to give it a star while you are there! To get started go to the getting started section on our GitHub, which will show you how to install the environment and run baselines. Make sure to join our community discord to chat, strategize, and learn with other competitors! We will be posting announcements on the Kaggle Forums and on the discord. Finally, if you use this competition/environment in you work, please cite it as so",
        "dataset_text": "This is the folder for the Python kit. Please make sure to read the instructions as they are important regarding how you will write a bot and submit it to the competition."
    },
    {
        "name": "Denoising Dirty Documents",
        "url": "https://www.kaggle.com/competitions/denoising-dirty-documents",
        "overview_text": "Overview text not found",
        "description_text": "Description text not found",
        "dataset_text": "You are provided two sets of images, train and test. These images contain various styles of text, to which synthetic noise has been added to simulate real-world, messy artifacts. The training set includes the test without the noise (train_cleaned). You must create an algorithm to clean the images in the test set. "
    },
    {
        "name": "San Francisco Crime Classification",
        "url": "https://www.kaggle.com/competitions/sf-crime",
        "overview_text": "Overview text not found",
        "description_text": "From 1934 to 1963, San Francisco was infamous for housing some of the world's most notorious criminals on the inescapable island of Alcatraz. Today, the city is known more for its tech scene than its criminal past. But, with rising wealth inequality, housing shortages, and a proliferation of expensive digital toys riding BART to work, there is no scarcity of crime in the city by the bay. From Sunset to SOMA, and Marina to Excelsior, this competition's dataset provides nearly 12 years of crime reports from across all of San Francisco's neighborhoods. Given time and location, you must predict the category of crime that occurred. We're also encouraging you to explore the dataset visually. What can we learn about the city through visualizations like this Top Crimes Map? The top most up-voted scripts from this competition will receive official Kaggle swag as prizes.   Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset is brought to you by SF OpenData, the central clearinghouse for data published by the City and County of San Francisco.",
        "dataset_text": "This dataset contains incidents derived from SFPD Crime Incident Reporting system. The data ranges from 1/1/2003 to 5/13/2015. The training set and test set rotate every week, meaning week 1,3,5,7... belong to test set, week 2,4,6,8 belong to training set.  "
    },
    {
        "name": "BigQuery-Geotab Intersection Congestion",
        "url": "https://www.kaggle.com/competitions/bigquery-geotab-intersection-congestion",
        "overview_text": "Overview text not found",
        "description_text": "We\u2019ve all been there: Stuck at a traffic light, only to be given mere seconds to pass through an intersection, behind a parade of other commuters. Imagine if you could help city planners and governments anticipate traffic hot spots ahead of time and reduce the stop-and-go stress of millions of commuters like you. Geotab provides a wide variety of aggregate datasets gathered from commercial vehicle telematics devices. Harnessing the insights from this data has the power to improve safety, optimize operations, and identify opportunities for infrastructure challenges. The dataset for this competition includes aggregate stopped vehicle information and intersection wait times. Your task is to predict congestion, based on an aggregate measure of stopping distance and waiting times, at intersections in 4 major US cities: Atlanta, Boston, Chicago & Philadelphia. This competition is being hosted in partnership with BigQuery, a data warehouse for manipulating, joining, and querying large scale tabular datasets. BigQuery also offers BigQuery ML, an easy way for users to create and run machine learning models to generate predictions through a SQL query interface. Kaggle recently released a BigQuery integration within our kernels notebook environment, and this starter kernel gives you a great starting point for how to use BQ & BQML. You\u2019re encouraged to use your data savvy, resourcefulness & intuition to find and join in additional external datasets that will increase your models\u2019 predictive power. Alright, stop waiting and get started!  A big thanks to Geotab for providing the dataset for this competition! Geotab is advancing security, connecting commercial vehicles to the internet and providing web-based analytics to help customers better manage their fleets. Geotab\u2019s open platform and Marketplace, offering hundreds of third-party solution options, allows both small and large businesses to automate operations by integrating vehicle data with their other data assets. As an IoT hub, the in-vehicle device provides additional functionality through IOX Add-Ons. Processing billions of data points a day, Geotab leverages data analytics and machine learning to help customers improve productivity, optimize fleets through the reduction of fuel consumption, enhance driver safety, and achieve strong compliance to regulatory changes. Geotab\u2019s products are represented and sold worldwide through Authorized Geotab Resellers. To learn more, please visit www.geotab.com and follow us @GEOTAB and on LinkedIn.",
        "dataset_text": "The data consists of aggregated trip logging metrics from commercial vehicles, such as semi-trucks. The data have been grouped by intersection, month, hour of day, direction driven through the intersection, and whether the day was on a weekend or not. For each grouping in the test set, you need to make predictions for three different quantiles of two different metrics covering how long it took the group of vehicles to drive through the intersection. Specifically, the 20th, 50th, and 80th percentiles for the total time stopped at an intersection and the distance between the intersection and the first place a vehicle stopped while waiting. You can think of your goal as summarizing the distribution of wait times and stop distances at each intersection. Each of those six predictions goes on a new row in the submission file. Read the submission TargetId fields, such as 1_1, as the first number being the RowId and the second being the metric id. You can unpack the submission metric id codes with submission_metric_map.json. The training set includes an optional additional output metric (TimeFromFirstStop) in case you find that useful for building your models. It was only excluded from the test set to limit the number of predictions that must be made. The instructions in BigQuery-Dataset-Access.md are now deprecated. The dataset is only available through Kaggle, via download below or API."
    },
    {
        "name": "Categorical Feature Encoding Challenge",
        "url": "https://www.kaggle.com/competitions/cat-in-the-dat",
        "overview_text": "Overview text not found",
        "description_text": "Is there a cat in your dat? A common task in machine learning pipelines is encoding categorical variables for a given algorithm in a format that allows as much useful signal as possible to be captured. Because this is such a common task and important skill to master, we've put together a dataset that contains only categorical features, and includes: This Playground competition will give you the opportunity to try different encoding schemes for different algorithms to compare how they perform. We encourage you to share what you find with the community. If you're not sure how to get started, you can check out the Categorical Variables section of Kaggle's Intermediate Machine Learning course.",
        "dataset_text": "In this competition, you will be predicting the probability [0, 1] of a binary target column. The data contains binary features (bin_*), nominal features (nom_*), ordinal features (ord_*) as well as (potentially cyclical) day (of the week) and month features. The string ordinal features ord_{3-5} are lexically ordered according to string.ascii_letters. Since the purpose of this competition is to explore various encoding strategies, the data has been simplified in that (1) there are no missing values, and (2) the test set does not contain any unseen feature values (See this). (Of course, in real-world settings both of these factors are often important to consider!)"
    },
    {
        "name": "Ciphertext Challenge III",
        "url": "https://www.kaggle.com/competitions/ciphertext-challenge-iii",
        "overview_text": "Overview text not found",
        "description_text": "We've done the 2010's, the 1990s\u2026 now it's time for the 80s. The 1580s!! In this new decryption competition's dataset, we've gone from perfectly respectable sources of electronic horror to a time before computers\u2014heck, before calculus was called \"calculus\"! Shakespeare's plays are encrypted, and we time travelers must un-encrypt them so people can do innovative stage productions with intricate makeup, costumes, and possibly\u2014possibly!\u2014Leonardo DiCaprio. Think about it, folks: Leo.* As in previous ciphertext challenges, simple classic ciphers have been used to encrypt this dataset, along with a slightly less simple surprise that expands our definition of \"classic\" into the modern age. The mission is the same: to correctly match each piece of ciphertext with its corresponding piece of plaintext. Daunting! Meta-puzzles and difficulty await! Swag prizes go to the first three teams to crack all four ciphers OR to the top three teams on the leaderboard (in case the ciphers are not all cracked). Additionally, swag prizes will be awarded to the best competition-related kernels, in both visualization and cryptanalysis, based on upvotes. Last, the coveted \"Phil Prize\"\u2014for the team that correctly deduces the form AND key of the final cipher\u2014is up for grabs again. Go ahead. Get cracking! * - Leo! Many thanks to Kaggler LiamLarson for their excellent Shakespeare dataset.",
        "dataset_text": "The exact cipher(s) used to encrypt this data will be revealed at the end of the competition - if they're not discovered first! Each document has been encrypted with up to 4 layered ciphers (ciphers 1, 2, 3, and 4). We've used a difficulty value here to denote which ciphers were used for a particular piece of text. Every document in the dataset has been padded to the next hundred characters (95->100, 213->300) with random (in-alphabet) characters, then encrypted based on its difficulty level. A difficulty of 1 means that only cipher #1 was used. A difficulty of 2 means cipher #1 was applied, followed by cipher #2, and so on. The difficulty level denotes exactly which ciphers were applied, and in what order. To avoid issues with reproducing the plaintext (some cleaning was required to remove spurious characters), the plaintext is provided in training.csv. You'll need training.csv and test.csv. sample_submission.csv contains the same Ids that test.csv does. Each row in training.csv provides the text of a line from a Shakespeare play and the index values you need to predict / match, while test.csv provides the corresponding ciphertext and difficulty levels. The target is a unique identifier (index) for each piece of ciphertext. Each piece of ciphertext must be matched with its corresponding piece of plaintext. To assign ciphertext_id ID_0827e580b to the line of Shakespeare with an index of 10, our prediction would look like this: ID_0827e580b,10"
    },
    {
        "name": "Kannada MNIST",
        "url": "https://www.kaggle.com/competitions/Kannada-MNIST",
        "overview_text": "Overview text not found",
        "description_text": "The goal of this competition is to provide a simple extension to the classic MNIST competition we're all familiar with. Instead of using Arabic numerals, it uses a recently-released dataset of Kannada digits. Kannada is a language spoken predominantly by people of Karnataka in southwestern India. The language has roughly 45 million native speakers and is written using the Kannada script. Wikipedia  This competition uses the same format as the MNIST competition in terms of how the data is structured, but it's different in that it is a synchronous re-run Kernels competition. You write your code in a Kaggle Notebook, and when you submit the results, your code is scored on both the public test set, as well as a private (unseen) test set. All details of the dataset curation has been captured in the paper titled: Prabhu, Vinay Uday. \"Kannada-MNIST: A new handwritten digits dataset for the Kannada language.\" arXiv preprint arXiv:1908.01242 (2019) The github repo of the author can be found here. On the originally-posted dataset, the author suggests some interesting questions you may be interested in exploring. Please note, although this dataset has been released in full, the purpose of this competition is for practice, not to find the labels to submit a perfect score. In addition to the main dataset, the author also disseminated an additional real world handwritten dataset (with 10k images), termed as the 'Dig-MNIST dataset' that can serve as an out-of-domain test dataset. It was created with the help of volunteers that were non-native users of the language, authored on a smaller sheet and scanned with different scanner settings compared to the main dataset. This 'dig-MNIST' dataset serves as a more difficult test-set (An accuracy of 76.1% was reported in the paper cited above) and achieving ~98+% accuracy on this test dataset would be rather commendable. Kaggle thanks Vinay Prabhu for providing this interesting dataset for a Playground competition. Image reference: https://www.researchgate.net/figure/speech-for-Kannada-numbers_fig2_313113588",
        "dataset_text": "The data files train.csv and test.csv contain gray-scale images of hand-drawn digits, from zero through nine, in the Kannada script. Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive. The training data set, train.csv, has 785 columns. The first column, called label, is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image. Each pixel column in the training set has a name like pixel{x}, where x is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. Then pixel{x} is located on row i and column j of a 28 x 28 matrix, (indexing by zero). For example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below. Visually, if we omit the \"pixel\" prefix, the pixels make up the image like this: The test data set, test.csv, is the same as the training set, except that it does not contain the label column. The evaluation metric for this contest is the categorization accuracy, or the proportion of test images that are correctly classified. For example, a categorization accuracy of 0.97 indicates that you have correctly classified all but 3% of the images."
    },
    {
        "name": "Categorical Feature Encoding Challenge II",
        "url": "https://www.kaggle.com/competitions/cat-in-the-dat-ii",
        "overview_text": "Overview text not found",
        "description_text": " Can you find more cat in your dat? We loved the participation and engagement with the first Cat in the Dat competition. Because this is such a common task and important skill to master, we've put together a dataset that contains only categorical features, and includes: This follow-up competition offers an even more challenging dataset so that you can continue to build your skills with the common machine learning task of encoding categorical variables. This challenge adds the additional complexity of feature interactions, as well as missing data. This Playground competition will give you the opportunity to try different encoding schemes for different algorithms to compare how they perform. We encourage you to share what you find with the community. If you're not sure how to get started, you can check out the Categorical Variables section of Kaggle's Intermediate Machine Learning course.  Have Fun! ",
        "dataset_text": "In this competition, you will be predicting the probability [0, 1] of a binary target column. The data contains binary features (bin_*), nominal features (nom_*), ordinal features (ord_*) as well as (potentially cyclical) day (of the week) and month features. The string ordinal features ord_{3-5} are lexically ordered according to string.ascii_letters. Since the purpose of this competition is to explore various encoding strategies. Unlike the first Categorical Feature Encoding Challenge, the data for this challenge has missing values and feature interactions."
    },
    {
        "name": "Santa 2019 - Revenge of the Accountants",
        "url": "https://www.kaggle.com/competitions/santa-2019-revenge-of-the-accountants",
        "overview_text": "Overview text not found",
        "description_text": "Santa was thrilled with the Kaggle community for minimizing his workshop costs! He had heard rumors that Kagglers were adept at cracking holiday challenges, but, wow, even Santa was surprised at this one. Unfortunately, the North Pole accountants were less pleased. It turns out, the accountants didn't like being one-upped by machine learning experts on the internet. To complicate matters, they've decided to allow an additional 1,000 families attend the workshop. And they've also \"fine tuned\" their accounting formula to try and trip up those fancy solvers some people have at their disposal. Of course, we know that nothing trips up the Kaggle community! (Well, except for maybe over-fitting. But fortunately, that doesn't apply here!) So this is a bonus Santa competition for those who want an additional challenge and the opportunity to continue to improve their optimization skills. Since Santa used up all his budget on accounting fees, this is strictly a Playground competition, with the chance to win some coveted Kaggle Swag. Have fun, and Happy Holidays from the Kaggle Team! Banner/Listing Photo by Helloquence on Unsplash",
        "dataset_text": "Your task is to schedule the families to Santa's Workshop in a way that minimizes the penalty cost to Santa (as described on the Evaluation page). Each family has listed their top 10 preferences for the dates they'd like to attend Santa's workshop tour. Dates are integer values representing the days before Christmas, e.g., the value 1 represents Dec 24, the value 2 represents Dec 23, etc. Each family also has a number of people attending, n_people. Every family must be scheduled for one and only one assigned_day."
    },
    {
        "name": "Flower Classification with TPUs",
        "url": "https://www.kaggle.com/competitions/flower-classification-with-tpus",
        "overview_text": "Overview text not found",
        "description_text": "Tensor Processing Unit (TPU) quotas are now available on Kaggle, at no cost to you! TPUs are powerful hardware accelerators specialized in deep learning tasks. They were developed (and first used) by Google to process large image databases, such as extracting all the text from Street View. This competition is designed for you to give TPUs a try. The latest Tensorflow release (TF 2.1) was focused on TPUs and they\u2019re now supported both through the Keras high-level API and at a lower level, in models using a custom training loop. We can\u2019t wait to see how your solutions are accelerated by TPUs! It\u2019s difficult to fathom just how vast and diverse our natural world is. There are over 5,000 species of mammals, 10,000 species of birds, 30,000 species of fish \u2013 and astonishingly, over 400,000 different types of flowers. In this competition, you\u2019re challenged to build a machine learning model that identifies the type of flowers in a dataset of images (for simplicity, we\u2019re sticking to just over 100 types). Martin G\u00f6rner, Google Developer Advocate and author of Tensorflow without a PhD will be actively engaged in the competition forum. If you have a question or need help troubleshooting, that\u2019s the best place to find help.",
        "dataset_text": "In this competition we're classifying 104 types of flowers based on their images drawn from five different public datasets. Some classes are very narrow, containing only a particular sub-type of flower (e.g. pink primroses) while other classes contain many sub-types (e.g. wild roses). This competition is different in that images are provided in TFRecord format. The TFRecord format is a container format frequently used in Tensorflow to group and shard data data files for optimal training performace.\nEach file contains the id, label (the class of the sample, for training data) and img (the actual pixels in array form) information for many images.\nPlease see our getting started notebook for notes on how to load and use them! Additional information is available in the TPU documentation."
    },
    {
        "name": "Hash Code Archive - Photo Slideshow Optimization",
        "url": "https://www.kaggle.com/competitions/hashcode-photo-slideshow",
        "overview_text": "Overview text not found",
        "description_text": "Note: Put your heads together to solve programming challenges. Google's coding competition, Hash Code, has just finished for 2020. Use this online qualifier from 2019 to keep your skills sharp for future competitions! As the saying goes, \"a picture is worth a thousand words.\" We agree \u2013 photos are an important part of contemporary digital and cultural life. How we experience photos largely depends on the story they\u2019re arranged to tell. The same shots could be a monotonous series of snaps or form a narrative masterpiece. Approximately 2.5 billion people around the world carry a camera \u2013 in the form of a smartphone \u2013 in their pocket every day. We tend to make good use of it, too, taking more photos than ever (back in 2017, Google Photos announced it was backing up more than 1.2 billion photos and videos per day)! The rise of digital photography creates an interesting challenge: what should we do with all of these photos? In this competition, you will compose a slideshow out of a photo collection. Given a list of photos and the tags associated with each photo, you are challenged to arrange the photos into a slideshow that is as interesting as possible (the evaluation section explains what we mean by \u201cinteresting\u201d) Will your slideshow tell a good story or be a major snoozefest?",
        "dataset_text": "A photo is described by a set of tags. For example, a photo with a cat on a beach, during a sunny afternoon could be tagged with the following tags: [cat, beach, sun]. Each photo's orientation is either horizontal or vertical. A slideshow is an ordered list of slides. Each slide contains either: If the slide contains a single horizontal photo, the tags of the slide are the same as the tags of the single photo it contains. For example, a slide containing a single horizontal photo with tags [cat, beach, sun], has tags [cat, beach, sun]. If the slide contains two vertical photos, the tags of the slide are all the tags present in any or both of the two photos it contains. For example, a slide containing two vertical photos with tags [selfie, smile] for the first photo, and tags [garden, selfie] for the second photo, has tags [selfie, smile, garden]. Each photo can be used either once or not at all. The slideshow must have at least one slide. The input data set is provided in a plain text file containing exclusively ASCII characters with lines terminated with a single '\\n' character (UNIX-style line endings). The rest line of the data set contains a single integer N (1 \u2264N\u2264 10^5) \u2014 the number of photos in the collection. This is followed by N lines, where line i contains a description of the photo with ID i (0\u2264 i\u2264 N). The description of photo i contains the following data, separated by a single space:"
    },
    {
        "name": "INGV - Volcanic Eruption Prediction",
        "url": "https://www.kaggle.com/competitions/predict-volcanic-eruptions-ingv-oe",
        "overview_text": "Overview text not found",
        "description_text": "What if scientists could anticipate volcanic eruptions as they predict the weather? While determining rain or shine days in advance is more difficult, weather reports become more accurate on shorter time scales. A similar approach with volcanoes could make a big impact. Just one unforeseen eruption can result in tens of thousands of lives lost. If scientists could reliably predict when a volcano will next erupt, evacuations could be more timely and the damage mitigated. Currently, scientists often identify \u201ctime to eruption\u201d by surveying volcanic tremors from seismic signals. In some volcanoes, this intensifies as volcanoes awaken and prepare to erupt. Unfortunately, patterns of seismicity are difficult to interpret. In very active volcanoes, current approaches predict eruptions some minutes in advance, but they usually fail at longer-term predictions. Enter Italy's Istituto Nazionale di Geofisica e Vulcanologia (INGV), with its focus on geophysics and volcanology. The INGV's main objective is to contribute to the understanding of the Earth's system while mitigating the associated risks. Tasked with the 24-hour monitoring of seismicity and active volcano activity across the country, the INGV seeks to find the earliest detectable precursors that provide information about the timing of future volcanic eruptions. In this competition, using your data science skills, you\u2019ll predict when a volcano's next eruption will occur. You'll analyze a large geophysical dataset collected by sensors deployed on active volcanoes. If successful, your algorithms will identify signatures in seismic waveforms that characterize the development of an eruption. With enough notice, areas around a volcano can be safely evacuated prior to their destruction. Seismic activity is a good indicator of an impending eruption, but earlier precursors must be identified to improve longer-term predictability. The impact of your participation could be felt worldwide with tens of thousands of lives saved by more predictable volcanic ruptures and earlier evacuations.",
        "dataset_text": "Detecting volcanic eruptions before they happen is an important problem that has historically proven to be a very difficult. This competition provides you with readings from several seismic sensors around a volcano and challenges you to estimate how long it will be until the next eruption. The data represent a classic signal processing setup that has resisted traditional methods. Identifying the exact sensors may be possible but would not be in the spirit of the competition nor further the scientific objectives. Please respect the importance of the problem and the time invested by the researchers at INGV in making this problem available by not seeking more metadata or information that would be unavailable in a real prediction context. train.csv Metadata for the train files. [train|test]/*.csv: the data files. Each file contains ten minutes of logs from ten different sensors arrayed around a volcano. The readings have been normalized within each segment, in part to ensure that the readings fall within the range of int16 values. If you are using the Pandas library you may find that you still need to load the data as float32 due to the presence of some nulls."
    },
    {
        "name": "Open Images Object Detection RVC 2020 edition",
        "url": "https://www.kaggle.com/competitions/open-images-object-detection-rvc-2020",
        "overview_text": "Overview text not found",
        "description_text": "Computer vision has advanced considerably but is still challenged in matching the precision of human perception. Open Images is a collaborative release of ~9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, visual relationships, and localized narratives. This uniquely large and diverse dataset is designed to spur state of the art advances in analyzing and understanding images. This year the Open Images Object Detection competition is a part of the larger Robust Vision Challenge 2020. This challenge encourages the participants to develop robust computer vision algorithms able to perform well across multiple datasets. Please refer to the RVC 2020 page and the Open Images Challenge page for more details. Participants are also welcome to submit to this playground competition beyond the context of RVC. In this track, you are asked to predict a tight bounding box around object instances. The training set contains 12.2M bounding-boxes across 500 categories on 1.7M images. The boxes have been largely manually drawn by professional annotators to ensure accuracy and consistency. The images are very diverse and often contain complex scenes with several objects (7 per image on average).  Example annotations. Left: Mark Paul Gosselaar plays the guitar by Rhys A. Right: the house by anita kluska. Both images used under CC BY 2.0 license. The training data, format, and submission modalities are identical to the 2019 Open Images Challenge.",
        "dataset_text": "The train and validation sets of images and their ground truth (bounding boxes and labels) should be downloaded from Open Images Challenge page. Please note that the test images used in this competition is independent from those released as part of the Open Images Dataset. The images can be downloaded from: You should expect 99,999 images in total in the test set. For each image in the test set, you must predict a list of boxes describing objects in the image. Each box is described as"
    },
    {
        "name": "Open Images Instance Segmentation RVC 2020 edition",
        "url": "https://www.kaggle.com/competitions/open-images-instance-segmentation-rvc-2020",
        "overview_text": "Overview text not found",
        "description_text": "Computer vision has advanced considerably but is still challenged in matching the precision of human perception. Open Images is a collaborative release of ~9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, visual relationships, and localized narratives. This uniquely large and diverse dataset is designed to spur state of the art advances in analyzing and understanding images. This year the Open Images Instance Segmentation competition is a part of the larger Robust Vision Challenge 2020. This challenge encourages the participants to develop robust computer vision algorithms able to perform well across multiple datasets. Please refer to the RVC 2020 page and the Open Images Challenge page for more details. Participants are also welcome to submit to this playground competition beyond the context of RVC. In this track of the Challenge, you are asked to provide segmentation masks of objects. This track\u2019s training set represents 2.1M segmentation masks for object instances in 300 categories; with a validation set containing an additional 23k masks. The train set masks were produced by our state-of-the-art interactive segmentation process, where professional human annotators iteratively correct the output of a segmentation neural network. The validation and test set masks have been annotated manually with a strong focus on quality.  Example train set annotations. Left: Wuxi science park, 1995 by Gary Stevens. Right: Cat Cafe Shinjuku calico by Ari Helminen. Both images used under CC BY 2.0 license.  The training data, format, and submission modalities are identical to the 2019 Open Images Challenge.",
        "dataset_text": "The train and validation sets of images and their ground truth (instance masks) should be downloaded from the Open Images Challenge page. The test images used in this competition are independent from those released as part of the Open Images Dataset.\nThe test images are the same as in the Object Detection track, so you might not need to re-download them.\nThe challenge test set images can be downloaded from: You should expect 99,999 images in total in the challenge test set."
    },
    {
        "name": "Hash Code Archive - Drone Delivery",
        "url": "https://www.kaggle.com/competitions/hashcode-drone-delivery",
        "overview_text": "Overview text not found",
        "description_text": "This is a synthetic code challenge to sharpen your programming skills. This problem was first released during the 2016 qualification round of Google's annual coding competition, Hash Code. We\u2019ve re-released it as a Playground Code Competition to help you sharpen your skills. Along with the Photo Slideshow Optimization competition, open for late submissions, you can use it as practice in advance of Hash Code 2021. The Internet has profoundly changed the way we buy things, but the online shopping of today is likely not the end of that change; the expectations for purchase delivery has gone from a week, to two days, to one day, to same day. What about in just a few hours? With drones, this may be possible, and they\u2019ll bring a whole new fleet of problems to solve with data science. Drones are\u00ad autonomous, electric vehicles often used to deliver online purchases. Current experiments use flying drones, so they\u2019re never stuck in traffic. As drone technology improves every year, there remains a major issue: how would we manage and coordinate all those drones? In this competition, you are given a hypothetical fleet of drones, a list of customer orders, and availability of the individual products in warehouses. Can you schedule the drone operations so that the orders are completed as soon as possible? When flying delivery drones become the norm, scheduling is one of the many problems to be solved. Get a head start\u2014and improve your data science skills at the same time. Photo by Ian Usher on Unsplash",
        "dataset_text": "The Internet has profoundly changed the way we buy things, but the online shopping of today is likely not\nthe end of that change\u037e after each purchase we still need to wait multiple days for physical goods to be\ncarried to our doorstep.\nThis is where drones come in \u00ad autonomous, electric vehicles delivering online purchases. Flying, so never\nstuck in traffic. As drone technology improves every year, there remains a major issue: how do we manage\nand coordinate all those drones? Given a hypothetical fleet of drones, a list of customer orders and availability of the individual products in warehouses, your task is to schedule the drone operations so that the orders are completed as soon as possible. You will need to handle the complications of multiple drones, customer orders, product types and weights, warehouses, and delivery destinations. This is a more involved optimization problem so for the full details please see the pdf in the dataset. Note that the pdf discusses the possibility of multiple input files but this competition only uses one."
    },
    {
        "name": "Conway's Reverse Game of Life 2020",
        "url": "https://www.kaggle.com/competitions/conways-reverse-game-of-life-2020",
        "overview_text": "Overview text not found",
        "description_text": "This is a relaunch of a previous competition, Conway's Reverse Game of Life, with the following changes: Obligatory Disclaimer: A lot has changed since the original competition was launched 6 years ago. With the change from \"exact starting point\" to \"any correct starting point\", it is possible to get a perfect score. We just don't know how difficult that will be. Use it as a fun learning experience, and don't spoil it for others by posting perfect solutions! ~~~~~~~~~ The Game of Life is a cellular automaton created by mathematician John Conway in 1970. The game consists of a board of cells that are either on or off. One creates an initial configuration of these on/off states and observes how it evolves. There are four simple rules to determine the next state of the game board, given the current state: These simple rules result in many interesting behaviors and have been the focus of a large body of mathematics. As Wikipedia states  The emergence of order from simple rules begs an interesting question\u2014what happens if we set time backwards? This competition is an experiment to see if machine learning (or optimization, or any method) can predict the game of life in reverse. Is the chaotic start of Life predictable from its orderly ends? We have created many games, evolved them, and provided only the end boards. You are asked to predict the starting board that resulted in each end board.",
        "dataset_text": "We have provided 50,000 training games and 50,000 test games, whose starting board you must predict. Each board is 25x25, for a total of 625 cells per board. Values are listed in a row-wise order. You are free to create more training games if you desire. The provided variables are: Your test-set predictions should be the starting board at delta steps before the stopping board. The games were created by the following procedure:"
    },
    {
        "name": "Halite by Two Sigma - Playground Edition",
        "url": "https://www.kaggle.com/competitions/halite-iv-playground-edition",
        "overview_text": "Overview text not found",
        "description_text": "Note: This simulation is a playground competition extending the fourth season of Halite for participation. We have modified the rules to serve as a two-player game instead of four-player game. No points or medals will be awarded for this competition. Ahoy there! There's halite to be had and ships to be deployed! Are you ready to navigate the skies and secure your territory? Halite by Two Sigma (\"Halite\") is a resource management game where you build and control a small armada of ships. Your algorithms determine their movements to collect halite, a luminous energy source. The most halite at the end of the match wins, but it's up to you to figure out how to make effective and efficient moves. You control your fleet, build new ships, create shipyards, and mine the regenerating halite on the game board. Created by Two Sigma in 2016, more than 15,000 people around the world have participated in a Halite challenge. Players apply advanced algorithms in a dynamic, open source game setting. The strategic depth and immersive, interactive nature of Halite games make each challenge a unique learning environment. Halite IV builds on the core game design of Halite III with a number of key changes that shift the focus of the game towards tighter competition on a smaller board. New game features include regenerating halite, shipyard creation, no more ship movement costs, and stealing halite from other players! So dust off your halite meters and fasten your seatbelts. The fourth season of Halite is about to begin!",
        "dataset_text": "This page appears underneath the data files. It describes what files have been provided & the format of each. It also defines the correct format for submission files. Participants should be able to answer these types of questions after reading the data description: What files do I need?\nWhat should I expect the data format to be?\nWhat am I predicting?\nWhat acronyms will I encounter?"
    },
    {
        "name": "Tabular Playground Series - Jan 2021",
        "url": "https://www.kaggle.com/competitions/tabular-playground-series-jan-2021",
        "overview_text": "Overview text not found",
        "description_text": "Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions, and thus more beginner-friendly. In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching a month-long tabular Playground competition on the 1st of every month, and continue the experiment as long as there's sufficient interest and participation. The goal of these competitions is to provide a fun, but less challenging, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard. For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. Good luck and have fun! Check out this Starter Notebook which walks you through how to make your very first submission! For more ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.",
        "dataset_text": "For this competition, you will be predicting a continuous target based on a number of feature columns given in the data. All of the feature columns, cont1 - cont14 are continuous."
    },
    {
        "name": "Hash Code 2021 - Traffic Signaling",
        "url": "https://www.kaggle.com/competitions/hashcode-2021-oqr-extension",
        "overview_text": "Overview text not found",
        "description_text": "This is an extended version of the Hash Code Online Qualifications 2021 problem. After Hash Code's official Online Qualifications, which lasts only 4 hours, you can improve your submissions here on Kaggle, optimize them in more detail, and discuss your approaches with other teams. We created a new data set just for this Kaggle competition that was not used in Hash Code before. Please find the full problem statement as a PDF file under the data tab. Note that this is not Hash Code's official Online Qualifications and that there are no prizes for this competition. You can't qualify for Hash Code's Finals on Kaggle. Problem statement The world's first traffic light dates back to 1868. It was installed in London to control traffic for\u2026 horse-drawn vehicles! Today, traffic lights can be found at street intersections in almost every city in the world, making it safer for vehicles to go through them. Traffic lights have at least two states- and use one color (usually red) to signal \"stop\"- and another (usually green) to signal that cars can proceed through. The very first traffic lights were manually controlled. Nowadays they are automatic, meaning that they have to be carefully designed and timed in order to optimize the overall travel time for all the participants in traffic. Given the description of a city plan and planned paths for all cars in that city, you will be optimizing the schedule of traffic lights to minimize the total amount of time spent in traffic, and help as many cars as possible reach their destination before a given deadline. Photo by Eliobed Suarez on Unsplash",
        "dataset_text": "There are two files provided for this competition: Note that the data set provided here is different than the data sets used in Hash Code's Online Qualifications. This is a new data set just for this competition. There is only one data set for this Kaggle competition. The input data set is provided in a plain text file. The file contains only ASCII characters with lines ending with a single '\\n' character (also called \u201cUNIX-\u00adstyle\u201d line endings). When multiple numbers are given in one line, they are separated by a single space between each two numbers. The first line contains five numbers: The next S lines contain descriptions of streets. Each line contains The next V lines describe the paths of each car. Each line contains:"
    },
    {
        "name": "Excellence in Research Award (Phase II)",
        "url": "https://www.kaggle.com/competitions/phase-ii-widsdatathon2022",
        "overview_text": "Overview text not found",
        "description_text": "We invite you to build a team, hone your data science skills, and join us for Phase II of the 5th Annual WiDS Datathon focused on social impact! This year\u2019s WiDS Datathon, organized by the WiDS Worldwide team, Stanford University, Harvard University IACS, and the WiDS Datathon Committee, will address the multi-faceted impacts of climate change. The WiDS Datathon Committee is partnering with experts from many disciplines at Climate Change AI (CCAI), Lawrence Berkeley National Laboratory (Berkeley Lab), US Environmental Protection Agency (EPA), and MIT Critical Data. Phase I of this year's datathon focused on an important way to mitigate the effects of climate change - improving building energy efficiency through forecasting usage. In the WiDS Datathon Excellence in Research Award (Phase II), we will broaden our focus to examine the impacts of climate change across multiple domains. Climate change is a globally relevant, urgent, and multi-faceted issue heavily impacting many industries and aspects of public life. Participants in Phase II will have the opportunity to examine the climate change from different perspectives. Participants will choose to explore one dataset among several, spanning sectors including healthcare, energy and environmental protection. Participants will also have opportunities to take deeper dives into their dataset and tackle a range of impactful real-world tasks. Teams will submit a research report at the end of Phase II. New this year, participants in Phase II can receive mentorship from experts in the domain related to their choice of dataset and task. Domain expert mentorship in Phase II will allow participants to both strengthen their foundational data science skills as well as develop skills needed to conduct research in data science. Teams with outstanding paper submissions from Phase II will be invited to submit their work for publication. The Excellence in Research Award (Phase II) is open from March 8 - June 30, 2022. Submissions are due on Submittable. After the June 30, 2022 research paper deadline, submissions will be reviewed for their potential for real-world impact, rigor in scientific methodology, and clarity of communication, by subject matter experts from the WiDS Datathon Committee, the National Science Foundation Big Data Innovation Hubs, and Datathon partners. Phase II participants will be able to choose one of three research tracks to explore: To be eligible for the award, all entrants must have participated in the first phase of the WiDS Datathon 2022 on Kaggle. We encourage Phase II individuals and author teams of up to 8 people to work together. You may collaborate with individuals in a \u201cnew\u201d team for the second phase of the Datathon, as long as (1) each person participated in the first phase of the datathon on Kaggle and (2) your new team still includes 50% or more individuals identifying as women. Each team member must complete Phase II registration. Note that you do NOT need to merge teams within the Kaggle platform (it's actually disabled), but all team members must be listed as collaborators on your submitted research paper, and all team members must accept the competition rules before the submission deadline of June 30th. The WiDS Datathon Excellence in Research Award 2022 is a collaboration led by the WiDS Worldwide team at Stanford University, the Institute for Applied Computational Sciences at Harvard University and the WiDS Datathon Committee. WiDS Datathon 2022 cash prizes are provided by Kaggle. The Excellence in Research Award is supported by the National Science Foundation under Grants 1916573, 1916481, and 1915774, as part of a network of Big Data Innovation Hubs. Special thanks to our datathon partners Climate Change AI, US Environmental Protection Agency (EPA), and MIT Critical Data. ",
        "dataset_text": "Participants must choose one of following three tracks to explore: Each track comes with a starter dataset as well as a list of suggested research questions. Some tracks require participants to download additional data in order to answer research questions. For example, the MIT Critical Data track requires participants to download COVID outcomes in order to build predictive models. The necessary links to required additional datasets will be provided as part of the starter files (data_links.txt). Participants are welcome to explore their own research questions as well as augment the starter dataset with additional data from any public source. For example, participants are encouraged to explore the relationship between factors involved in climate change and the outcomes in each track. To do this, participants are welcomed to enrich the dataset from their track with climate change related datasets (e.g. the US Climate Extremes Index). Throughout the competition, participants will have opportunities to interact with domain experts in their chosen track and receive feedback on their work. Sign up to attend the Phase II mentorship office hours."
    },
    {
        "name": "LLM Prompting with MakerSuite",
        "url": "https://www.kaggle.com/competitions/llm-prompting-with-makersuite",
        "overview_text": "Large Language Models (LLMs) are capturing the imagination of millions of people around the world. The goal of this competition is to tap into the expertise of the Kaggle community to develop practical, creative, and innovative prompts that demonstrate real-world use-cases for LLMs. We have several categories in this competition, including prompts for education, developers, utilities, and more. Winning prompts will be included in MakerSuite, a tool from Google that provides an easy to use interface to interact with LLMs including Google\u2019s PaLM API.",
        "description_text": "In this analytics competition, participants will design prompts in MakerSuite in one of the following categories: Your submissions will be evaluated by Google, and kept private until winners and prizes are announced. This competition will not have a leaderboard. This competition is a collaboration between MakerSuite, Google Developer Relations, and Kaggle.",
        "dataset_text": "This competition does not require use of a dataset. The submission_instructions.md and prompt_categories.md files are included below for reference."
    },
    {
        "name": "Introducing Kaggle Scripts",
        "url": "https://www.kaggle.com/competitions/introducing-kaggle-scripts",
        "overview_text": "Overview text not found",
        "description_text": "Kaggle is first and foremost a community. If you're here to learn, it is from the community. If you're here to compete, it's against the community. If you're here to share, it's with our community. Continuing the theme, we've just launched a product called Scripts. The concept is simple: we'll host, version, & run your code for you. Why? The community does a tremendous amount of data science tinkering each day on Kaggle. The fruits of this labor are too often lost on old drives, never checked into version control, never shared, never critiqued, never reproduced, never visualized, never assimilated into a broader knowledge. Scripts is not a better IDE or better database or new language. It is the beginning of communal input, collaboration, and useful workflow features for the data science that's done on Kaggle. You don't need a full tutorial to learn how to use scripts (it has one button and runs code). In lieu of yet another iris dataset tutorial, we invite you to kick the tires and try it out. Punish our servers by setting ntree = 500 in that random forest call. You deserve it. To add to the experience, we're giving out a truckload of extremely prestigious prizes. The details on Scripts: You can view site-wide scripts at https://www.kaggle.com/scripts. We're also in the habit of naming naming our favorite scripts of the week on the blog.",
        "dataset_text": "Looking to try scripts with real data? Load the R datasets package or view the same menu with Julia. "
    },
    {
        "name": "What's Cooking?",
        "url": "https://www.kaggle.com/competitions/whats-cooking",
        "overview_text": "Overview text not found",
        "description_text": "Picture yourself strolling through your local, open-air market... What do you see? What do you smell? What will you make for dinner tonight? If you're in Northern California, you'll be walking past the inevitable bushels of leafy greens, spiked with dark purple kale and the bright pinks and yellows of chard. Across the world in South Korea, mounds of bright red kimchi greet you, while the smell of the sea draws your attention to squids squirming nearby. India\u2019s market is perhaps the most colorful, awash in the rich hues and aromas of dozens of spices: turmeric, star anise, poppy seeds, and garam masala as far as the eye can see. Some of our strongest geographic and cultural associations are tied to a region's local foods. This playground competitions asks you to predict the category of a dish's cuisine given a list of its ingredients.  We want to thank Yummly for providing this unique dataset. Kaggle is hosting this playground competition for fun and practice. ",
        "dataset_text": "In the dataset, we include the recipe id, the type of cuisine, and the list of ingredients of each recipe (of variable length). The data is stored in JSON format.  An example of a recipe node in train.json: In the test file test.json, the format of a recipe is the same as train.json, only the cuisine type is removed, as it is the target variable you are going to predict."
    },
    {
        "name": "Shelter Animal Outcomes",
        "url": "https://www.kaggle.com/competitions/shelter-animal-outcomes",
        "overview_text": "Overview text not found",
        "description_text": "Every year, approximately 7.6 million companion animals end up in US shelters. Many animals are given up as unwanted by their owners, while others are picked up after getting lost or taken out of cruelty situations. Many of these animals find forever families to take them home, but just as many are not so lucky. 2.7 million dogs and cats are euthanized in the US every year.  Using a dataset of intake information including breed, color, sex, and age from the Austin Animal Center, we're asking Kagglers to predict the outcome for each animal. We also believe this dataset can help us understand trends in animal outcomes. These insights could help shelters focus their energy on specific animals who need a little extra help finding a new home. We encourage you to publish your insights on Scripts so they are publicly accessible. Kaggle is hosting this competition for the machine learning community to use for data science practice and social good. The dataset is brought to you by Austin Animal Center. Shelter animal statistics were taken from the ASPCA. Glamour shots of Kaggle's shelter pets are pictured above. From left to right: Shelby, Bailey, Hazel, Daisy, and Yeti.",
        "dataset_text": "The data comes from Austin Animal Center from October 1st, 2013 to March, 2016. Outcomes represent the status of animals as they leave the Animal Center. All animals receive a unique Animal ID during intake.  In this competition, you are going to predict the outcome of the animal as they leave the Animal Center. These outcomes include: Adoption, Died, Euthanasia, Return to owner, and Transfer.  The train and test data are randomly split.  File descriptions"
    },
    {
        "name": "Painter by Numbers",
        "url": "https://www.kaggle.com/competitions/painter-by-numbers",
        "overview_text": "Overview text not found",
        "description_text": "With an original Picasso carrying a 106 million dollar price tag, identifying an authentic work of art from a forgery is a high-stakes industry. While algorithms have gotten good at telling us if a still life is of a basket of apples or a sunflower bouquet, they aren't yet able to tell us with certainty if both paintings are by van Gogh.   In this playground competition, we're challenging Kagglers to examine pairs of paintings and determine if they are by the same artist. This is an excellent opportunity to improve your computer vision skills and engage with a unique dataset of art. From the movement of brushstrokes to the use of light and dark, successful algorithms will likely incorporate many aspects of a painter's unique style.  Many of the images in this dataset were obtained from wikiart.org. Additional paintings were provided by artists whose contributions will be acknowledged at the close of the competition. This playground competition and its datasets were prepared by Small Yellow Duck (Kiri Nichol). This includes the design of the pairwise-evaluation scheme.",
        "dataset_text": "Most of the images in this competition are from WikiArt.org. Please assume that all images are protected by copyright and utilize the images only for the purposes of data mining, which constitutes a form of fair use. Some artists have work in both the training and test set, while others have work in just the training set or just the test set. This competition uses a pairwise comparison scheme: your algorithm needs to examine two images and predict whether the two images are by the same artist or not. As there are 23817 unique images in the test data set, comparing each image to all of the others would be onerous and produce a huge submission file. We've reduced the number of comparisons to be evaluated by creating groups of images: each image in a group is compared to all the other images in that group. There are 13 groups, each with approximately 1400 - 2000 images. train_info.csv submission_info.csv sampleSubmission.csv Edited 11/4/2016: After the competition ended, the author (small little duck) has made the dataset public, adding the solution file and the data info file into this dataset.  all_data_info.csv"
    },
    {
        "name": "Kobe Bryant Shot Selection",
        "url": "https://www.kaggle.com/competitions/kobe-bryant-shot-selection",
        "overview_text": "Overview text not found",
        "description_text": "Kobe Bryant marked his retirement from the NBA by scoring 60 points in his final game as a Los Angeles Laker on Wednesday, April 12, 2016. Drafted into the NBA at the age of 17, Kobe earned the sport\u2019s highest accolades throughout his long career. Using 20 years of data on Kobe's swishes and misses, can you predict which shots will find the bottom of the net? This competition is well suited for practicing classification basics, feature engineering, and time series analysis. Practice got Kobe an eight-figure contract and 5 championship rings. What will it get you? Kaggle is hosting this competition for the data science community to use for fun and education. For more data on Kobe and other NBA greats, visit stats.nba.com.",
        "dataset_text": "This data contains the location and circumstances of every field goal attempted by Kobe Bryant took during his 20-year career. Your task is to predict whether the basket went in (shot_made_flag). We have removed 5000 of the shot_made_flags (represented as missing values in the csv file). These are the test set shots for which you must submit a prediction. You are provided a sample submission file with the correct shot_ids needed for a valid prediction. To avoid leakage, your method should only train on events that occurred prior to the shot for which you are predicting! Since this is a playground competition with public answers, it's up to you to abide by this rule. The field names are self explanatory and contain the following attributes:"
    },
    {
        "name": "Integer Sequence Learning",
        "url": "https://www.kaggle.com/competitions/integer-sequence-learning",
        "overview_text": "Overview text not found",
        "description_text": "7. You read that correctly. That's the start to a real integer sequence, the powers of primes. Want something easier? How about the next number in 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55? If you answered 89, you may enjoy this challenge. Your computer may find it considerably less enjoyable. The On-Line Encyclopedia of Integer Sequences is a 50+ year effort by mathematicians the world over to catalog sequences of integers. If it has a pattern, it's probably in the OEIS, and probably described with amazing detail. This competition challenges you create a machine learning algorithm capable of guessing the next number in an integer sequence. While this sounds like pattern recognition in its most basic form, a quick look at the data will convince you this is anything but basic! Kaggle is hosting this competition for the data science community to use for fun and education. We thank the OEIS and its contributors for cataloging this data.",
        "dataset_text": "This dataset contains the majority of the integer sequences from the OEIS. It is split into a training set, where you are given the full sequence, and a test set, where we have removed the last number from the sequence. The task is to predict this removed integer. Note that some sequences may have identical beginnings (or even be identical altogether). We have not removed these from the dataset."
    },
    {
        "name": "Leaf Classification",
        "url": "https://www.kaggle.com/competitions/leaf-classification",
        "overview_text": "Overview text not found",
        "description_text": " There are estimated to be nearly half a million species of plant in the world. Classification of species has been historically problematic and often results in duplicate identifications.  The objective of this playground competition is to use binary leaf images and extracted features, including shape, margin & texture, to accurately identify 99 species of plants. Leaves, due to their volume, prevalence, and unique characteristics, are an effective means of differentiating plant species. They also provide a fun introduction to applying techniques that involve image-based features. As a first step, try building a classifier that uses the provided pre-extracted features. Next, try creating a set of your own features. Finally, examine the errors you're making and see what you can do to improve. Kaggle is hosting this competition for the data science community to use for fun and education. This dataset originates from leaf images collected by  \nJames Cope, Thibaut Beghin, Paolo Remagnino, & Sarah Barman of the Royal Botanic Gardens, Kew, UK. Charles Mallah, James Cope, James Orwell. Plant Leaf Classification Using Probabilistic Integration of Shape, Texture and Margin Features. Signal Processing, Pattern Recognition and Applications, in press. 2013. We thank the UCI machine learning repository for hosting the dataset.",
        "dataset_text": "The dataset consists approximately 1,584 images of leaf specimens (16 samples each of 99 species) which have been converted to binary black leaves against white backgrounds. Three sets of features are also provided per image: a shape contiguous descriptor, an interior texture histogram, and a \ufb01ne-scale margin histogram. For each feature, a 64-attribute vector is given per leaf sample. Note that of the original 100 species, we have eliminated one on account of incomplete associated data in the original dataset."
    },
    {
        "name": "Dogs vs. Cats Redux: Kernels Edition",
        "url": "https://www.kaggle.com/competitions/dogs-vs-cats-redux-kernels-edition",
        "overview_text": "Overview text not found",
        "description_text": "In 2013, we hosted one of our favorite for-fun competitions:  Dogs vs. Cats. Much has since changed in the machine learning landscape, particularly in deep learning and image analysis. Back then, a tensor flow was the diffusion of the creamer in a bored mathematician's cup of coffee. Now, even the cucumber farmers are neural netting their way to a bounty. Much has changed at Kaggle as well. Our online coding environment Kernels didn't exist in 2013, and so it was that we approached sharing by scratching primitive glpyhs on cave walls with sticks and sharp objects. No more. Now, Kernels have taken over as the way to share code on Kaggle. IPython is out and Jupyter Notebook is in. We even have TensorFlow. What more could a data scientist ask for? But seriously, what more? Pull requests welcome.  We are excited to bring back the infamous Dogs vs. Cats classification problem as a playground competition with kernels enabled. Although modern techniques may make light of this once-difficult problem, it is through practice of new techniques on old datasets that we will make light of machine learning's future challenges.",
        "dataset_text": "The train folder contains 25,000 images of dogs and cats. Each image in this folder has the label as part of the filename. The test folder contains 12,500 images, named according to a numeric id. For each image in the test set, you should predict a probability that the image is a dog (1 = dog, 0 = cat)."
    },
    {
        "name": "Transfer Learning on Stack Exchange Tags",
        "url": "https://www.kaggle.com/competitions/transfer-learning-on-stack-exchange-tags",
        "overview_text": "Overview text not found",
        "description_text": "What does physics have in common with biology, cooking, cryptography, diy, robotics, and travel? If you answered \"all pursuits are governed by the immutable laws of physics\" we'll begrudgingly give you partial credit. If you answered \"all were chosen randomly by a scheming Kaggle employee for a twisted transfer learning competition\", congratulations, we accept your answer and mark the question as solved. In this competition, we provide the titles, text, and tags of Stack Exchange questions from six different sites. We then ask for tag predictions on unseen physics questions. Solving this problem via a standard machine approach might involve training an algorithm on a corpus of related text. Here, you are challenged to train on material from outside the field. Can an algorithm learn appropriate physics tags from \"extreme-tourism Antarctica\"? Let's find out. Kaggle is hosting this competition for the data science community to use for fun and education. This dataset originates from the Stack Exchange data dump.",
        "dataset_text": "In this dataset, you are provided with question titles, content, and tags for Stack Exchange sites on a variety of topics (biology, cooking, cryptography, diy, robotics, and travel). The content of each question is given as HTML. The tags are words or phrases that describe the topic of the question. The test set is comprised of questions from the physics.stackexchange.com. For each question in the test set, you should use the title and question content in order to generate potential tags."
    },
    {
        "name": "Ghouls, Goblins, and Ghosts... Boo!",
        "url": "https://www.kaggle.com/competitions/ghouls-goblins-and-ghosts-boo",
        "overview_text": "Overview text not found",
        "description_text": "Get out your dowsing rods, electromagnetic sensors, \u2026 and gradient boosting machines. Kaggle is haunted and we need your help. After a month of making scientific observations and taking careful measurements, we\u2019ve determined that 900 ghouls, ghosts, and goblins are infesting our halls and frightening our data scientists. When trying garlic, asking politely, and using reverse psychology didn't work, it became clear that machine learning is the only answer to banishing our unwanted guests.  So now the hour has come to put the data we\u2019ve collected in your hands. We\u2019ve managed to identify 371 of the ghastly creatures, but need your help to vanquish the rest. And only an accurate classification algorithm can thwart them. Use bone length measurements, severity of rot, extent of soullessness, and other characteristics to distinguish (and extinguish) the intruders. Are you ghost-busters up for the challenge?",
        "dataset_text": "Dataset description not found"
    },
    {
        "name": "Humpback Whale Identification Challenge",
        "url": "https://www.kaggle.com/competitions/whale-categorization-playground",
        "overview_text": "Overview text not found",
        "description_text": " After centuries of intense whaling, recovering whale populations still have a hard time adapting to warming oceans and struggle to compete every day with the industrial fishing industry for food. To aid whale conservation efforts, scientists use photo surveillance systems to monitor ocean activity. They use the shape of whales\u2019 tails and unique markings found in footage to identify what species of whale they\u2019re analyzing and meticulously log whale pod dynamics and movements. For the past 40 years, most of this work has been done manually by individual scientists, leaving a huge trove of data untapped and underutilized. In this competition, you\u2019re challenged to build an algorithm to identifying whale species in images. You\u2019ll analyze Happy Whale\u2019s database of over 25,000 images, gathered from research institutions and public contributors. By contributing, you\u2019ll help to open rich fields of understanding for marine mammal population dynamics around the globe. We'd like to thank Happy Whale for providing this data and problem. Happy Whale is a platform that uses image process algorithms to let anyone to submit their whale photo and have it automatically identified.",
        "dataset_text": "This training data contains thousands of images of humpback whale flukes. Individual whales have been identified by researchers and given an Id. The challenge is to predict the whale Id of images in the test set. What makes this such a challenge is that there are only a few examples for each of 3,000+ whale Ids."
    },
    {
        "name": "Plant Seedlings Classification",
        "url": "https://www.kaggle.com/competitions/plant-seedlings-classification",
        "overview_text": "Overview text not found",
        "description_text": "Can you differentiate a weed from a crop seedling? The ability to do so effectively can mean better crop yields and better stewardship of the environment. The Aarhus University Signal Processing group, in collaboration with University of Southern Denmark, has recently released a dataset containing images of approximately 960 unique plants belonging to 12 species at several growth stages.  We're hosting this dataset as a Kaggle competition in order to give it wider exposure, to give the community an opportunity to experiment with different image recognition techniques, as well to provide a place to cross-pollenate ideas. We extend our appreciation to the Aarhus University Department of Engineering Signal Processing Group for hosting the original data. A Public Image Database for Benchmark of Plant Seedling Classification Algorithms",
        "dataset_text": "You are provided with a training set and a test set of images of plant seedlings at various stages of grown. Each image has a filename that is its unique id. The dataset comprises 12 plant species. The goal of the competition is to create a classifier capable of determining a plant's species from a photo. The list of species is as follows:"
    },
    {
        "name": "DonorsChoose.org Application Screening",
        "url": "https://www.kaggle.com/competitions/donorschoose-application-screening",
        "overview_text": "Overview text not found",
        "description_text": "Founded in 2000 by a high school teacher in the Bronx, DonorsChoose.org empowers public school teachers from across the country to request much-needed materials and experiences for their students. At any given time, there are thousands of classroom requests that can be brought to life with a gift of any amount. DonorsChoose.org receives hundreds of thousands of project proposals each year for classroom projects in need of funding. Right now, a large number of volunteers is needed to manually screen each submission before it's approved to be posted on the DonorsChoose.org website. Next year, DonorsChoose.org expects to receive close to 500,000 project proposals. As a result, there are three main problems they need to solve: The goal of the competition is to predict whether or not a DonorsChoose.org project proposal submitted by a teacher will be approved, using the text of project descriptions as well as additional metadata about the project, teacher, and school. DonorsChoose.org can then use this information to identify projects most likely to need further review before approval. With an algorithm to pre-screen applications, DonorsChoose.org can auto-approve some applications quickly so that volunteers can spend their time on more nuanced and detailed project vetting processes, including doing more to help teachers develop projects that qualify for specific funding opportunities. Your machine learning algorithm can help more teachers get funded more quickly, and with less cost to DonorsChoose.org, allowing them to channel even more funding directly to classrooms across the country. Get familiar with the competition data and the machine learning objective quickly using Kernels. Google's engineering education team has put together a starter tutorial implementing benchmark linear classification model. Machine Learning Crash Course was created by Google's engineering education team in partnership with numerous Machine Learning subject matter experts across Google.",
        "dataset_text": "Update 2021-03-02: per request from the hosts, this dataset has been removed. The competition dataset contains information from teachers' project applications to DonorsChoose.org including teacher attributes, school attributes, and the project proposals including application essays. Your objective is to predict whether or not a DonorsChoose.org project proposal submitted by a teacher will be approved. test.csv and train.csv: * Note: Prior to May 17, 2016, the prompts for the essays were as follows: Starting on May 17, 2016, the number of essays was reduced from 4 to 2, and the prompts for the first 2 essays were changed to the following: For all projects with project_submitted_datetime of 2016-05-17 and later, the values of project_essay_3 and project_essay_4 will be NaN. resources.csv: Proposals also include resources requested. Each project may include multiple requested resources. Each row in resources.csv corresponds to a resource, so multiple rows may tie to the same project by id."
    },
    {
        "name": "Costa Rican Household Poverty Level Prediction",
        "url": "https://www.kaggle.com/competitions/costa-rican-household-poverty-prediction",
        "overview_text": "Overview text not found",
        "description_text": "The Inter-American Development Bank is asking the Kaggle community for help with income qualification for some of the world's poorest families. Are you up for the challenge? Here's the backstory: Many social programs have a hard time making sure the right people are given enough aid. It\u2019s especially tricky when a program focuses on the poorest segment of the population. The world\u2019s poorest typically can\u2019t provide the necessary income and expense records to prove that they qualify. In Latin America, one popular method uses an algorithm to verify income qualification. It\u2019s called the Proxy Means Test (or PMT). With PMT, agencies use a model that considers a family\u2019s observable household attributes like the material of their walls and ceiling, or the assets found in the home to classify them and predict their level of need. While this is an improvement, accuracy remains a problem as the region\u2019s population grows and poverty declines. To improve on PMT, the IDB (the largest source of development financing for Latin America and the Caribbean) has turned to the Kaggle community. They believe that new methods beyond traditional econometrics, based on a dataset of Costa Rican household characteristics, might help improve PMT\u2019s performance. Beyond Costa Rica, many countries face this same problem of inaccurately assessing social need. If Kagglers can generate an improvement, the new algorithm could be implemented in other countries around the world. This is a Kernels-Only Competition, so you must submit your code through Kernels, rather than uploading .csv predictions. You can create private Kernels and even share/edit your work with teammates by adding them as collaborators.",
        "dataset_text": "Variable name, Variable description"
    },
    {
        "name": "Forest Cover Type (Kernels Only)",
        "url": "https://www.kaggle.com/competitions/forest-cover-type-kernels-only",
        "overview_text": "Overview text not found",
        "description_text": "Random forests? Cover trees? Not so fast, computer nerds. We're talking about the real thing. In this competition you are asked to predict the forest cover type (the predominant kind of tree cover) from cartographic variables. The actual forest cover type for a given 30 x 30 meter cell was determined from US Forest Service (USFS) Region 2 Resource Information System data. Independent variables were then derived from data obtained from the US Geological Survey and USFS. The data is in raw form and contains binary columns of data for qualitative independent variables such as wilderness areas and soil type. This study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological processes rather than forest management practices. This competition originally ran in 2015. We are relaunching it as a kernels-only version here. Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was provided by Jock A. Blackard and Colorado State University. We also thank the UCI machine learning repository for hosting the dataset. If you use the problem in publication, please cite: Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science",
        "dataset_text": "The study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. Each observation is a 30m x 30m patch. You are asked to predict an integer classification for the forest cover type. The seven types are: 1 - Spruce/Fir\n2 - Lodgepole Pine\n3 - Ponderosa Pine\n4 - Cottonwood/Willow\n5 - Aspen\n6 - Douglas-fir\n7 - Krummholz The training set (15120 observations) contains both features and the Cover_Type. The test set contains only the features. You must predict the Cover_Type for every row in the test set (565892 observations). Elevation - Elevation in meters\nAspect - Aspect in degrees azimuth\nSlope - Slope in degrees\nHorizontal_Distance_To_Hydrology - Horz Dist to nearest surface water features\nVertical_Distance_To_Hydrology - Vert Dist to nearest surface water features\nHorizontal_Distance_To_Roadways - Horz Dist to nearest roadway\nHillshade_9am (0 to 255 index) - Hillshade index at 9am, summer solstice\nHillshade_Noon (0 to 255 index) - Hillshade index at noon, summer solstice\nHillshade_3pm (0 to 255 index) - Hillshade index at 3pm, summer solstice\nHorizontal_Distance_To_Fire_Points - Horz Dist to nearest wildfire ignition points\nWilderness_Area (4 binary columns, 0 = absence or 1 = presence) - Wilderness area designation\nSoil_Type (40 binary columns, 0 = absence or 1 = presence) - Soil Type designation\nCover_Type (7 types, integers 1 to 7) - Forest Cover Type designation The wilderness areas are: 1 - Rawah Wilderness Area\n2 - Neota Wilderness Area\n3 - Comanche Peak Wilderness Area\n4 - Cache la Poudre Wilderness Area The soil types are: 1 Cathedral family - Rock outcrop complex, extremely stony.\n2 Vanet - Ratake families complex, very stony.\n3 Haploborolis - Rock outcrop complex, rubbly.\n4 Ratake family - Rock outcrop complex, rubbly.\n5 Vanet family - Rock outcrop complex complex, rubbly.\n6 Vanet - Wetmore families - Rock outcrop complex, stony.\n7 Gothic family.\n8 Supervisor - Limber families complex.\n9 Troutville family, very stony.\n10 Bullwark - Catamount families - Rock outcrop complex, rubbly.\n11 Bullwark - Catamount families - Rock land complex, rubbly.\n12 Legault family - Rock land complex, stony.\n13 Catamount family - Rock land - Bullwark family complex, rubbly.\n14 Pachic Argiborolis - Aquolis complex.\n15 unspecified in the USFS Soil and ELU Survey.\n16 Cryaquolis - Cryoborolis complex.\n17 Gateview family - Cryaquolis complex.\n18 Rogert family, very stony.\n19 Typic Cryaquolis - Borohemists complex.\n20 Typic Cryaquepts - Typic Cryaquolls complex.\n21 Typic Cryaquolls - Leighcan family, till substratum complex.\n22 Leighcan family, till substratum, extremely bouldery.\n23 Leighcan family, till substratum - Typic Cryaquolls complex.\n24 Leighcan family, extremely stony.\n25 Leighcan family, warm, extremely stony.\n26 Granile - Catamount families complex, very stony.\n27 Leighcan family, warm - Rock outcrop complex, extremely stony.\n28 Leighcan family - Rock outcrop complex, extremely stony.\n29 Como - Legault families complex, extremely stony.\n30 Como family - Rock land - Legault family complex, extremely stony.\n31 Leighcan - Catamount families complex, extremely stony.\n32 Catamount family - Rock outcrop - Leighcan family complex, extremely stony.\n33 Leighcan - Catamount families - Rock outcrop complex, extremely stony.\n34 Cryorthents - Rock land complex, extremely stony.\n35 Cryumbrepts - Rock outcrop - Cryaquepts complex.\n36 Bross family - Rock land - Cryumbrepts complex, extremely stony.\n37 Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.\n38 Leighcan - Moran families - Cryaquolls complex, extremely stony.\n39 Moran family - Cryorthents - Leighcan family complex, extremely stony.\n40 Moran family - Cryorthents - Rock land complex, extremely stony."
    },
    {
        "name": "Store Item Demand Forecasting Challenge",
        "url": "https://www.kaggle.com/competitions/demand-forecasting-kernels-only",
        "overview_text": "Overview text not found",
        "description_text": "This competition is provided as a way to explore different time series techniques on a relatively simple and clean dataset. You are given 5 years of store-item sales data, and asked to predict 3 months of sales for 50 different items at 10 different stores. What's the best way to deal with seasonality? Should stores be modeled separately, or can you pool them together? Does deep learning work better than ARIMA? Can either beat xgboost? This is a great competition to explore different models and improve your skills in forecasting.",
        "dataset_text": "The objective of this competition is to predict 3 months of item-level sales data at different store locations."
    },
    {
        "name": "What's Cooking? (Kernels Only)",
        "url": "https://www.kaggle.com/competitions/whats-cooking-kernels-only",
        "overview_text": "Overview text not found",
        "description_text": "Picture yourself strolling through your local, open-air market... What do you see? What do you smell? What will you make for dinner tonight? If you're in Northern California, you'll be walking past the inevitable bushels of leafy greens, spiked with dark purple kale and the bright pinks and yellows of chard. Across the world in South Korea, mounds of bright red kimchi greet you, while the smell of the sea draws your attention to squids squirming nearby. India\u2019s market is perhaps the most colorful, awash in the rich hues and aromas of dozens of spices: turmeric, star anise, poppy seeds, and garam masala as far as the eye can see. Some of our strongest geographic and cultural associations are tied to a region's local foods. This playground competitions asks you to predict the category of a dish's cuisine given a list of its ingredients.  We want to thank Yummly for providing this unique dataset. Kaggle is hosting this playground competition for fun and practice. ",
        "dataset_text": "In the dataset, we include the recipe id, the type of cuisine, and the list of ingredients of each recipe (of variable length). The data is stored in JSON format.  An example of a recipe node in train.json: In the test file test.json, the format of a recipe is the same as train.json, only the cuisine type is removed, as it is the target variable you are going to predict. In the dataset, we include the recipe id, the type of cuisine, and the list of ingredients of each recipe (of variable length). The data is stored in JSON format.  An example of a recipe node in train.json: In the test file test.json, the format of a recipe is the same as train.json, only the cuisine type is removed, as it is the target variable you are going to predict."
    },
    {
        "name": "Flavours of Physics: Finding \u03c4 \u2192 \u03bc\u03bc\u03bc (Kernels Only)",
        "url": "https://www.kaggle.com/competitions/flavours-of-physics-kernels-only",
        "overview_text": "Overview text not found",
        "description_text": "The aim of this playground challenge is to find a phenomenon that is not already known to exist \u2013 charged lepton flavour violation \u2013 thereby helping to establish \"new physics\".  The laws of nature ensure that some physical quantities, such as energy or momentum, are conserved. From Noether\u2019s theorem, we know that each conservation law is associated with a fundamental symmetry. For example, conservation of energy is due to the time-invariance (the outcome of an experiment would be the same today or tomorrow) of physical systems. The fact that physical systems behave the same, regardless of where they are located or how they are oriented, gives rise to the conservation of linear and angular momentum. Symmetries are also crucial to the structure of the Standard Model of particle physics, our present theory of interactions at microscopic scales. Some are built into the model, while others appear accidentally from it. In the Standard Model, lepton flavour, the number of electrons and electron-neutrinos, muons and muon-neutrinos, and tau and tau-neutrinos, is one such conserved quantity.  Interestingly, in many proposed extensions to the Standard Model, this symmetry doesn\u2019t exist, implying decays that do not conserve lepton flavour are possible. One decay searched for at the LHC is \u03c4- \u2192 \u03bc+\u03bc-\u03bc- (or \u03c4 \u2192 3\u03bc). Observation of this decay would be a clear indication of the violation of lepton flavour and a sign of long-sought new physics. You will be working with real data from the LHCb experiment at the LHC, mixed with simulated datasets of the decay. The metric used in this challenge includes checks that physicists do in their analysis to make sure the results are unbiased. These checks have been built into the competition design to help ensure that the results will be useful for physicists in future studies.  To get started, review the Data Page, and be sure to download the Starter Kit. The Starter Kit will help you to get used to the unique submission procedure for this competition. You've got lots of questions. Researchers at CERN & LCHb have the answers. open_in_newhttps://player.vimeo.com/video/134110342 - What is the goal of this competition? (1:56)\n- Why is finding \u03c4 \u2192 \u03bc\u03bc\u03bc exciting? (2:18)\n- What are flavours? (4:10)\n- Why use machine learning to find \u03c4 \u2192 \u03bc\u03bc\u03bc? (4:57)\n- How did you decide on the size of the dataset? (5:31)\n- Why is weighted AUC the evaluation metric? (6:09)\n- Why use Ds \u2192 \u03c6\u03c0 data for the Agreement Test? (7:53)\n- Why do we need a Correlation Check? (8:44)\n- How will the competition results impact what you do? (11:38)\n- How will the competition results be used at CERN? (12:17) Flavour of Physics, Research Documentation Roel Aaij et al., Search for the lepton flavour violating decay \u03c4 \u2192 \u00b5\u00b5\u00b5, 2015, JHEP, 1502:121, 2015 New approaches for boosting to uniformity This competition is brought to you by:                             Co-sponsored by:  Additional support from:                         ",
        "dataset_text": "In this competition, you are given a list of collision events and their properties. You will then predict whether a \u03c4 \u2192 3\u03bc decay happened in this collision. This \u03c4 \u2192 3\u03bc is currently assumed by scientists not to happen, and the goal of this competition is to discover \u03c4 \u2192 3\u03bc happening more frequently than scientists currently can understand. It is challenging to design a machine learning problem for something you have never observed before. Scientists at CERN developed the following designs to achieve the goal. This is a labelled dataset (the label \u2018signal\u2019 being \u20181\u2019 for signal events, \u20180\u2019 for background events) to train the classifier. Signal events have been simulated, while background events are real data. This real data is collected by the LHCb detectors observing collisions of accelerated particles with a specific mass range in which \u03c4 \u2192 3\u03bc can\u2019t happen. We call these events \u201cbackground\u201d and label them 0. The test dataset has all the columns that training.csv has, except mass, production, min_ANNmuon, and signal.  The test dataset consists of a few parts: You need to submit predictions for ALL the test entries. You will need to treat them all the same and predict as if they are all the same channel's collision events.  A submission is only scored after passing both the agreement test and the correlation test.  This dataset contains simulated and real events from the Control channel Ds \u2192 \u03c6\u03c0 to evaluate your simulated-real data of submission agreement locally. It contains the same columns as test.csv and weight column. For more details see agreement test. This dataset contains only real background events recorded at LHCb to evaluate your submission correlation with mass locally. It contains the same columns as test.csv and mass column to check correlation with. For more details see correlation test."
    },
    {
        "name": "Movie Review Sentiment Analysis (Kernels Only)",
        "url": "https://www.kaggle.com/competitions/movie-review-sentiment-analysis-kernels-only",
        "overview_text": "Overview text not found",
        "description_text": "\"There's a thin line between likably old-fashioned and fuddy-duddy, and The Count of Monte Cristo ... never quite settles on either side.\" The Rotten Tomatoes movie review dataset is a corpus of movie reviews used for sentiment analysis, originally collected by Pang and Lee [1]. In their work on sentiment treebanks, Socher et al. [2] used Amazon's Mechanical Turk to create fine-grained labels for all parsed phrases in the corpus. This competition presents a chance to benchmark your sentiment-analysis ideas on the Rotten Tomatoes dataset. You are asked to label phrases on a scale of five values: negative, somewhat negative, neutral, somewhat positive, positive. Obstacles like sentence negation, sarcasm, terseness, language ambiguity, and many others make this task very challenging.  Kaggle is hosting this competition for the machine learning community to use for fun and practice. This competition was inspired by the work of Socher et al [2]. We encourage participants to explore the accompanying (and dare we say, fantastic) website that accompanies the paper: http://nlp.stanford.edu/sentiment/ There you will find have source code, a live demo, and even an online interface to help train the model. [1] Pang and L. Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In ACL, pages 115\u2013124. [2] Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank, Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Chris Manning, Andrew Ng and Chris Potts. Conference on Empirical Methods in Natural Language Processing (EMNLP 2013). Image credits: Popcorn - Maura Teague, http://www.flickr.com/photos/93496438@N06/",
        "dataset_text": "The dataset is comprised of tab-separated files with phrases from the Rotten Tomatoes dataset. The train/test split has been preserved for the purposes of benchmarking, but the sentences have been shuffled from their original order. Each Sentence has been parsed into many phrases by the Stanford parser. Each phrase has a PhraseId. Each sentence has a SentenceId. Phrases that are repeated (such as short/common words) are only included once in the data. The sentiment labels are: 0 - negative\n1 - somewhat negative\n2 - neutral\n3 - somewhat positive\n4 - positive"
    },
    {
        "name": "New York City Taxi Fare Prediction",
        "url": "https://www.kaggle.com/competitions/new-york-city-taxi-fare-prediction",
        "overview_text": "Overview text not found",
        "description_text": "In this playground competition, hosted in partnership with Google Cloud and Coursera, you are tasked with predicting the fare amount (inclusive of tolls) for a taxi ride in New York City given the pickup and dropoff locations. While you can get a basic estimate based on just the distance between the two points, this will result in an RMSE of $5-$8, depending on the model used (see the starter code for an example of this approach in Kernels). Your challenge is to do better than this using Machine Learning techniques! To learn how to handle large datasets with ease and solve this problem using TensorFlow, consider taking the Machine Learning with TensorFlow on Google Cloud Platform specialization on Coursera -- the taxi fare problem is one of several real-world problems that are used as case studies in the series of courses. To make this easier, head to Coursera.org/NEXTextended to claim this specialization for free for the first month!",
        "dataset_text": "Dataset description not found"
    }
]